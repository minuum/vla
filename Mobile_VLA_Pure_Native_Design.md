# ğŸš€ Pure Mobile VLA System (Calvin ì—†ëŠ” ìˆœìˆ˜ Mobile ë„¤ì´í‹°ë¸Œ ì‹œìŠ¤í…œ)

## ğŸ¯ ê¸°ë³¸ ì² í•™: mobile_vla_data_collector.py 100% í™œìš©

Calvin í˜•ì‹ì€ ì™„ì „íˆ ë²„ë¦¬ê³ , mobile_vla_data_collector.pyê°€ ìƒì„±í•˜ëŠ” **ìˆœìˆ˜ Mobile ë°ì´í„° í˜•ì‹**ì„ ì§ì ‘ í™œìš©í•˜ëŠ” VLM í•™ìŠµ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## ğŸ“Š ì‹¤ì œ Mobile ë°ì´í„° êµ¬ì¡° (í™•ì¸ëœ í˜•ì‹)

### ğŸ” HDF5 íŒŒì¼ êµ¬ì¡° ë¶„ì„ ê²°ê³¼
```python
# ì‹¤ì œ mobile_vla_data_collector.py ì¶œë ¥ (70ê°œ íŒŒì¼ í™•ì¸)
mobile_data_structure = {
    "images": {
        "shape": "(18, 720, 1280, 3)",  # 18í”„ë ˆì„, 720p í•´ìƒë„
        "dtype": "uint8",
        "description": "RGB ì¹´ë©”ë¼ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤"
    },
    "actions": {
        "shape": "(18, 3)",              # 3D ì•¡ì…˜ (4Dê°€ ì•„ë‹˜!)
        "dtype": "float32", 
        "content": "[linear_x, linear_y, angular_z]",
        "sample": "[[0.0, 0.0, 0.0], [1.15, 0.0, 0.0], [1.15, 0.0, 0.0]]"
    },
    "action_event_types": {
        "shape": "(18,)",
        "dtype": "object (bytes)",
        "content": "['episode_start', 'start_action', 'start_action', ...]"
    },
    "metadata": {
        "episode_name": "episode_20250808_123136_1box_vert_left",
        "action_chunk_size": 8,
        "num_frames": 18,
        "total_duration": 18.87,
        "scenario": "1box_vert_left"  # ì—í”¼ì†Œë“œëª…ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥
    }
}
```

### ğŸ”¥ í•µì‹¬ ë°œê²¬ì‚¬í•­
1. **ì•¡ì…˜ì´ 3Dì„!** (4Dê°€ ì•„ë‹ˆë¼ linear_x, linear_y, angular_zë§Œ ìˆìŒ)
2. **18í”„ë ˆì„ì´ í‘œì¤€** (í”„ë ˆì„ 18ê°œ ë°ì´í„°ì˜ ì¤‘ìš”ì„± í™•ì¸)
3. **720p ê³ í•´ìƒë„** (1280x720, ê¸°ì¡´ 224x224ë³´ë‹¤ í›¨ì”¬ ë†’ìŒ)
4. **ì´ë²¤íŠ¸ ê¸°ë°˜ íƒ€ì„ìŠ¤íƒ¬í”„** (episode_start, start_action, stop_action)

---

## ğŸ§  Pure Mobile VLM ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. ğŸ“¸ Native Mobile Image Encoder
```python
# /home/soda/vla/Mobile_VLA/models/encoders/mobile_image_encoder.py
class MobileImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # mobile_vla_data_collector.pyì˜ ì‹¤ì œ í•´ìƒë„ ì²˜ë¦¬
        self.input_size = (720, 1280, 3)  # âœ… ì‹¤ì œ ë°ì´í„° í•´ìƒë„
        
        # ê³ í•´ìƒë„ ì²˜ë¦¬ë¥¼ ìœ„í•œ íš¨ìœ¨ì  CNN
        self.backbone = torchvision.models.efficientnet_v2_s(pretrained=True)
        self.backbone.features[0][0] = nn.Conv2d(3, 24, kernel_size=3, stride=2, padding=1)
        
        # ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ (18í”„ë ˆì„ ì‹œí€€ìŠ¤)
        self.temporal_encoder = nn.LSTM(
            input_size=1000,  # EfficientNet output
            hidden_size=512,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )
        
    def forward(self, image_sequence):
        # image_sequence: [B, 18, 720, 1280, 3]
        B, T, H, W, C = image_sequence.shape
        
        # ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # ê° í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ
        frame_features = self.backbone(images_flat)  # [B*T, 1000]
        frame_features = frame_features.view(B, T, -1)  # [B, T, 1000]
        
        # ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        temporal_features, _ = self.temporal_encoder(frame_features)
        
        return temporal_features  # [B, T, 1024]
```

### 2. ğŸ—£ï¸ Korean Instruction Encoder  
```python
# /home/soda/vla/Mobile_VLA/models/encoders/korean_text_encoder.py
class KoreanInstructionEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # í•œêµ­ì–´ íŠ¹í™” í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.tokenizer = AutoTokenizer.from_pretrained("klue/roberta-base")
        self.text_encoder = AutoModel.from_pretrained("klue/roberta-base")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í•œêµ­ì–´ ëª…ë ¹ì–´ í…œí”Œë¦¿
        self.scenario_instructions = {
            "1box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "1box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "2box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”"
        }
        
    def forward(self, scenario_names):
        # ì‹œë‚˜ë¦¬ì˜¤ëª…ì—ì„œ í•œêµ­ì–´ ëª…ë ¹ì–´ ìƒì„±
        instructions = [self.scenario_instructions[scenario] for scenario in scenario_names]
        
        # í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenizer(
            instructions, 
            padding=True, 
            truncation=True, 
            return_tensors="pt"
        )
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features = self.text_encoder(**tokenized)
        
        return text_features.last_hidden_state  # [B, seq_len, 768]
```

### 3. ğŸ¯ Mobile Action Predictor (3D ì•¡ì…˜)
```python
# /home/soda/vla/Mobile_VLA/models/policy_heads/mobile_action_predictor.py
class MobileActionPredictor(nn.Module):
    def __init__(self, visual_dim=1024, text_dim=768):
        super().__init__()
        
        # mobile_vla_data_collector.pyì˜ ì‹¤ì œ 3D ì•¡ì…˜ì— ë§ì¶¤
        self.action_dim = 3  # [linear_x, linear_y, angular_z]
        
        # ë©€í‹°ëª¨ë‹¬ ìœµí•©
        self.fusion = nn.MultiheadAttention(
            embed_dim=visual_dim + text_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # 3D ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ (mobile_vla_data_collector.py WASD_TO_CONTINUOUS ê¸°ì¤€)
        self.action_head = nn.Sequential(
            nn.Linear(visual_dim + text_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 3),  # [linear_x, linear_y, angular_z]
        )
        
        # ì´ë²¤íŠ¸ íƒ€ì… ì˜ˆì¸¡ (start_action, stop_action ì˜ˆì¸¡)
        self.event_head = nn.Sequential(
            nn.Linear(visual_dim + text_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 3)  # [episode_start, start_action, stop_action]
        )
        
        # mobile_vla_data_collector.pyì˜ ì•¡ì…˜ ë²”ìœ„
        self.action_bounds = {
            "linear_x": 2.0,    # Â±2.0 (ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©)
            "linear_y": 2.0,    # Â±2.0 (ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©)  
            "angular_z": 2.0    # Â±2.0 (ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©)
        }
        
    def forward(self, visual_features, text_features):
        # visual_features: [B, T, 1024]
        # text_features: [B, seq_len, 768]
        
        B, T = visual_features.shape[:2]
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• í‰ê· í™”
        text_pooled = text_features.mean(dim=1)  # [B, 768]
        text_expanded = text_pooled.unsqueeze(1).repeat(1, T, 1)  # [B, T, 768]
        
        # ì‹œê°-í…ìŠ¤íŠ¸ ìœµí•©
        fused_features = torch.cat([visual_features, text_expanded], dim=-1)  # [B, T, 1792]
        
        # Attention ìœµí•©
        fused_attended, _ = self.fusion(fused_features, fused_features, fused_features)
        
        # ì•¡ì…˜ ì˜ˆì¸¡ (mobile_vla_data_collector.py í˜•ì‹)
        raw_actions = self.action_head(fused_attended)  # [B, T, 3]
        
        # ì‹¤ì œ ì•¡ì…˜ ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
        actions = torch.tanh(raw_actions) * 2.0  # [-2.0, 2.0] ë²”ìœ„
        
        # ì´ë²¤íŠ¸ íƒ€ì… ì˜ˆì¸¡
        event_logits = self.event_head(fused_attended)  # [B, T, 3]
        
        return {
            "actions": actions,                    # [B, T, 3] - mobile í˜•ì‹
            "event_logits": event_logits,         # [B, T, 3] - ì´ë²¤íŠ¸ ì˜ˆì¸¡
            "action_events": torch.argmax(event_logits, dim=-1)  # [B, T] - ì˜ˆì¸¡ëœ ì´ë²¤íŠ¸
        }
```

---

## ğŸ“¦ Pure Mobile Dataset Loader

### ğŸ”¥ Calvin ì—†ëŠ” ìˆœìˆ˜ Mobile Dataset
```python
# /home/soda/vla/Mobile_VLA/data/mobile_native_dataset.py
class MobileVLADataset(Dataset):
    def __init__(self, data_dir="/home/soda/vla/ROS_action/mobile_vla_dataset/"):
        self.data_dir = Path(data_dir)
        self.h5_files = list(self.data_dir.glob("*.h5"))
        
        print(f"ğŸ“ {len(self.h5_files)}ê°œì˜ Mobile VLA ì—í”¼ì†Œë“œ ë¡œë“œë¨")
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ ë° í†µê³„
        self.scenarios = []
        self.scenario_stats = defaultdict(int)
        
        for h5_file in self.h5_files:
            scenario = self.extract_scenario_from_filename(h5_file.name)
            self.scenarios.append(scenario)
            self.scenario_stats[scenario] += 1
            
        print(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬: {dict(self.scenario_stats)}")
        
    def extract_scenario_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ (mobile_vla_data_collector.py ë°©ì‹)"""
        for scenario in ["1box_vert_left", "1box_vert_right", "1box_hori_left", "1box_hori_right",
                        "2box_vert_left", "2box_vert_right", "2box_hori_left", "2box_hori_right"]:
            if scenario in filename:
                return scenario
        return "unknown"
    
    def __len__(self):
        return len(self.h5_files)
    
    def __getitem__(self, idx):
        h5_file = self.h5_files[idx]
        scenario = self.scenarios[idx]
        
        with h5py.File(h5_file, 'r') as f:
            # mobile_vla_data_collector.py ë°ì´í„° ì§ì ‘ ë¡œë“œ
            images = f['images'][:]                    # [18, 720, 1280, 3]
            actions = f['actions'][:]                  # [18, 3]
            action_events = f['action_event_types'][:]  # [18]
            
            # ë©”íƒ€ë°ì´í„°
            episode_name = f.attrs['episode_name']
            num_frames = f.attrs['num_frames']
            duration = f.attrs['total_duration']
            
        # ì´ë²¤íŠ¸ íƒ€ì…ì„ ì •ìˆ˜ë¡œ ë³€í™˜
        event_mapping = {
            b'episode_start': 0,
            b'start_action': 1, 
            b'stop_action': 2
        }
        event_indices = np.array([event_mapping.get(event, 1) for event in action_events])
        
        return {
            "images": torch.FloatTensor(images) / 255.0,     # [18, 720, 1280, 3] ì •ê·œí™”
            "actions": torch.FloatTensor(actions),           # [18, 3]  
            "action_events": torch.LongTensor(event_indices), # [18]
            "scenario": scenario,                            # str
            "episode_name": episode_name,                    # str
            "num_frames": num_frames,                        # int
            "duration": duration                             # float
        }
```

---

## ğŸ‹ï¸ Pure Mobile Trainer

### ğŸ“ˆ Calvin ì—†ëŠ” ìˆœìˆ˜ Mobile í•™ìŠµ
```python
# /home/soda/vla/Mobile_VLA/training/mobile_native_trainer.py
class MobileVLATrainer(pl.LightningModule):
    def __init__(self, configs):
        super().__init__()
        
        # Pure Mobile VLM êµ¬ì„±ìš”ì†Œ
        self.image_encoder = MobileImageEncoder()
        self.text_encoder = KoreanInstructionEncoder() 
        self.action_predictor = MobileActionPredictor()
        
        # mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜
        self.scenario_weights = {
            "1box_vert_left": 1.0,
            "1box_vert_right": 1.0,
            "1box_hori_left": 1.2,   # ë” ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
            "1box_hori_right": 1.1,
            "2box_vert_left": 1.5,   # ê°€ì¥ ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
            "2box_vert_right": 1.4,
            "2box_hori_left": 1.8,
            "2box_hori_right": 1.6
        }
        
        # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
        self.action_loss_weight = 1.0
        self.event_loss_weight = 0.5
        
    def forward(self, batch):
        images = batch["images"]      # [B, 18, 720, 1280, 3]
        scenarios = batch["scenario"] # [B] list of scenario names
        
        # ì¸ì½”ë”©
        visual_features = self.image_encoder(images)        # [B, 18, 1024]
        text_features = self.text_encoder(scenarios)        # [B, seq_len, 768]
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        predictions = self.action_predictor(visual_features, text_features)
        
        return predictions
    
    def training_step(self, batch, batch_idx):
        # í¬ì›Œë“œ íŒ¨ìŠ¤
        predictions = self.forward(batch)
        
        # íƒ€ê²Ÿ ë°ì´í„°
        target_actions = batch["actions"]          # [B, 18, 3]
        target_events = batch["action_events"]     # [B, 18]
        scenarios = batch["scenario"]
        
        # ì†ì‹¤ ê³„ì‚°
        action_loss = F.mse_loss(predictions["actions"], target_actions)
        event_loss = F.cross_entropy(
            predictions["event_logits"].view(-1, 3), 
            target_events.view(-1)
        )
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        scenario_weights = torch.tensor([
            self.scenario_weights.get(scenario, 1.0) for scenario in scenarios
        ]).to(self.device)
        
        weighted_action_loss = (action_loss * scenario_weights.mean())
        weighted_event_loss = (event_loss * scenario_weights.mean())
        
        total_loss = (
            self.action_loss_weight * weighted_action_loss + 
            self.event_loss_weight * weighted_event_loss
        )
        
        # ë¡œê¹…
        self.log_dict({
            "train_total_loss": total_loss,
            "train_action_loss": weighted_action_loss,
            "train_event_loss": weighted_event_loss,
            "train_action_accuracy": self.compute_action_accuracy(predictions["actions"], target_actions),
            "train_event_accuracy": self.compute_event_accuracy(predictions["action_events"], target_events)
        }, prog_bar=True)
        
        return total_loss
    
    def compute_action_accuracy(self, pred_actions, target_actions):
        """ì•¡ì…˜ ì •í™•ë„ ê³„ì‚° (mobile_vla_data_collector.py ê¸°ì¤€)"""
        # ê° ì¶•ë³„ ì˜¤ì°¨ê°€ 0.1 ì´ë‚´ë©´ ì •í™•í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
        action_diff = torch.abs(pred_actions - target_actions)
        accurate_actions = (action_diff < 0.1).all(dim=-1)  # [B, T]
        return accurate_actions.float().mean()
    
    def compute_event_accuracy(self, pred_events, target_events):
        """ì´ë²¤íŠ¸ íƒ€ì… ì˜ˆì¸¡ ì •í™•ë„"""
        return (pred_events == target_events).float().mean()
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
        return {"optimizer": optimizer, "lr_scheduler": scheduler}
```

---

## ğŸš€ ì‹¤ì‹œê°„ Mobile VLA Inference

### ğŸ¯ mobile_vla_data_collector.pyì™€ ì™„ì „ í˜¸í™˜
```python
# /home/soda/vla/Mobile_VLA/inference/mobile_real_time_inference.py
class MobileVLAInference:
    def __init__(self, model_checkpoint_path):
        # í•™ìŠµëœ Pure Mobile VLA ëª¨ë¸ ë¡œë“œ
        self.model = MobileVLATrainer.load_from_checkpoint(model_checkpoint_path)
        self.model.eval()
        
        # mobile_vla_data_collector.pyì™€ ë™ì¼í•œ ì•¡ì…˜ í˜•ì‹
        self.action_converter = MobileActionConverter()
        
    def predict_next_action(self, current_image, scenario):
        """
        ì‹¤ì‹œê°„ ì•¡ì…˜ ì˜ˆì¸¡ (mobile_vla_data_collector.py í˜¸í™˜)
        
        Args:
            current_image: numpy array [720, 1280, 3]
            scenario: str (e.g., "1box_hori_left")
            
        Returns:
            mobile_action: dict compatible with mobile_vla_data_collector.py
        """
        with torch.no_grad():
            # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ 18í”„ë ˆì„ ì‹œí€€ìŠ¤ë¡œ í™•ì¥ (ìµœì‹  ì´ë¯¸ì§€ ë°˜ë³µ)
            image_sequence = np.tile(current_image[None, ...], (18, 1, 1, 1))  # [18, 720, 1280, 3]
            image_tensor = torch.FloatTensor(image_sequence).unsqueeze(0) / 255.0  # [1, 18, 720, 1280, 3]
            
            # ë°°ì¹˜ ìƒì„±
            batch = {
                "images": image_tensor,
                "scenario": [scenario]
            }
            
            # ì˜ˆì¸¡
            predictions = self.model.forward(batch)
            
            # ë§ˆì§€ë§‰ í”„ë ˆì„ì˜ ì•¡ì…˜ ì‚¬ìš©
            predicted_action = predictions["actions"][0, -1].cpu().numpy()  # [3]
            predicted_event = predictions["action_events"][0, -1].cpu().item()
            
            # mobile_vla_data_collector.py í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            mobile_action = {
                "linear_x": float(predicted_action[0]),
                "linear_y": float(predicted_action[1]), 
                "angular_z": float(predicted_action[2]),
                "event_type": ["episode_start", "start_action", "stop_action"][predicted_event]
            }
            
        return mobile_action
    
    def integrate_with_data_collector(self, data_collector):
        """mobile_vla_data_collector.pyì™€ í†µí•©"""
        # ê¸°ì¡´ í‚¤ë³´ë“œ ì œì–´ ëŒ€ì‹  VLA ì˜ˆì¸¡ ì‚¬ìš©
        def vla_action_callback():
            if data_collector.collecting:
                # í˜„ì¬ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
                current_image = data_collector.get_latest_image_via_service()
                if current_image is not None:
                    # í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ
                    scenario = data_collector.extract_scenario_from_episode_name(
                        data_collector.episode_name
                    )
                    
                    if scenario:
                        # VLA ì•¡ì…˜ ì˜ˆì¸¡
                        predicted_action = self.predict_next_action(current_image, scenario)
                        
                        # mobile_vla_data_collector.py ì•¡ì…˜ ì‹¤í–‰
                        data_collector.publish_cmd_vel(predicted_action)
                        data_collector.collect_data_point_with_action(
                            "vla_predicted_action", predicted_action, current_image
                        )
        
        return vla_action_callback
```

---

## ğŸ¯ í•µì‹¬ ì¥ì : Pure Mobile ì‹œìŠ¤í…œ

### âœ… Calvin ì œê±°ì˜ ì´ì 
1. **ë°ì´í„° ë³€í™˜ ë¶ˆí•„ìš”**: HDF5 â†’ ì§ì ‘ í•™ìŠµ
2. **ë„¤ì´í‹°ë¸Œ í•´ìƒë„**: 720p ê³ í™”ì§ˆ ê·¸ëŒ€ë¡œ í™œìš©  
3. **ì‹¤ì œ ì•¡ì…˜ ê³µê°„**: 3D ëª¨ë°”ì¼ ì•¡ì…˜ ì§ì ‘ í•™ìŠµ
4. **ì´ë²¤íŠ¸ ê¸°ë°˜ í•™ìŠµ**: start/stop íƒ€ì´ë° í•™ìŠµ ê°€ëŠ¥
5. **ì‹œë‚˜ë¦¬ì˜¤ ë„¤ì´í‹°ë¸Œ**: 8ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ ì§ì ‘ ì¸ì‹

### ğŸš€ êµ¬í˜„ ìš°ì„ ìˆœìœ„
1. **Week 1**: MobileVLADataset + ê¸°ë³¸ ë°ì´í„° ë¡œë”©
2. **Week 2**: Pure Mobile VLM ëª¨ë¸ êµ¬í˜„  
3. **Week 3**: MobileVLATrainer + í•™ìŠµ íŒŒì´í”„ë¼ì¸
4. **Week 4**: ì‹¤ì‹œê°„ ì¶”ë¡  + mobile_vla_data_collector í†µí•©

ì´ì œ Calvin í˜•ì‹ì— ì˜ì¡´í•˜ì§€ ì•Šê³  **mobile_vla_data_collector.pyì˜ ìˆœìˆ˜í•œ ë°ì´í„° í˜•ì‹**ì„ 100% í™œìš©í•˜ëŠ” VLA ì‹œìŠ¤í…œì´ ì™„ì„±ë©ë‹ˆë‹¤! ğŸ‰
