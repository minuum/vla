version: '3.8'

services:
  # Mobile VLA Training Service
  train_mobile_vla:
    build:
      context: .
      dockerfile: Dockerfile
    image: robovlms-mobile-vla:latest
    container_name: mobile_vla_train
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - PYTHONUNBUFFERED=1
      - HF_HOME=/root/.cache/huggingface
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      # Code
      - ./:/workspace/RoboVLMs
      # Data
      - ../ROS_action/mobile_vla_dataset:/data/mobile_vla_dataset:ro
      # Outputs
      - ./runs:/workspace/RoboVLMs/runs
      # Cache
      - ~/.cache/huggingface:/root/.cache/huggingface
      # X11
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
    working_dir: /workspace/RoboVLMs
    command: >
      bash -c "
      source /opt/ros/humble/setup.bash &&
      python3 train_mobile_vla.py 
        --config configs/mobile_vla/train_mobile_vla_full_ft.json
      "
    networks:
      - vla_network
    ipc: host
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Mobile VLA Inference Service
  inference_mobile_vla:
    build:
      context: .
      dockerfile: Dockerfile
    image: robovlms-mobile-vla:latest
    container_name: mobile_vla_inference
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
      - DISPLAY=${DISPLAY}
      - QT_X11_NO_MITSHM=1
      - PYTHONUNBUFFERED=1
      - HF_HOME=/root/.cache/huggingface
      - ROS_DOMAIN_ID=42
      - RMW_IMPLEMENTATION=rmw_cyclonedds_cpp
    volumes:
      # Code
      - ./:/workspace/RoboVLMs
      # Checkpoints
      - ./runs:/workspace/RoboVLMs/runs:ro
      # Cache
      - ~/.cache/huggingface:/root/.cache/huggingface
      # X11
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      # ROS2 DDS
      - /dev/shm:/dev/shm
    working_dir: /workspace/RoboVLMs
    command: >
      bash -c "
      source /opt/ros/humble/setup.bash &&
      python3 eval/mobile_vla/inference_wrapper.py
        --checkpoint runs/mobile_vla/checkpoints/mobile_vla-best.ckpt
        --config configs/mobile_vla/train_mobile_vla_full_ft.json
        --device cuda
        --ros2
      "
    networks:
      - vla_network
    network_mode: host
    ipc: host
    privileged: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Mobile VLA Test/Debug Service
  test_mobile_vla:
    build:
      context: .
      dockerfile: Dockerfile
    image: robovlms-mobile-vla:latest
    container_name: mobile_vla_test
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics
      - PYTHONUNBUFFERED=1
      - HF_HOME=/root/.cache/huggingface
    volumes:
      - ./:/workspace/RoboVLMs
      - ../ROS_action/mobile_vla_dataset:/data/mobile_vla_dataset:ro
      - ./runs:/workspace/RoboVLMs/runs
      - ~/.cache/huggingface:/root/.cache/huggingface
    working_dir: /workspace/RoboVLMs
    command: >
      bash -c "
      python3 train_mobile_vla.py 
        --config configs/mobile_vla/train_mobile_vla_full_ft.json
        --test
      "
    networks:
      - vla_network
    ipc: host
    shm_size: '4gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  vla_network:
    driver: bridge

