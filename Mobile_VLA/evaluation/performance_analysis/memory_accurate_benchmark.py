#!/usr/bin/env python3
"""
ì •í™•í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ FPS ì¸¡ì • ë²¤ì¹˜ë§ˆí¬
- GPU/CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
- ì •í™•í•œ FPS ê³„ì‚° ë°©ë²• ê²€ì¦
- ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
import psutil
import gc
from typing import Dict, Any, List
import onnxruntime as ort

class Kosmos2CLIPHybridModel(nn.Module):
    """Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212)"""
    
    def __init__(self):
        super().__init__()
        
        # ëª¨ë¸ êµ¬ì¡° ì •ì˜
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        self.text_encoder = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        
        self.fusion_layer = nn.Sequential(
            nn.Linear(256 + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 3)  # linear_x, linear_y, angular_z
        )
    
    def forward(self, images, text_embeddings):
        """ì „ë°© ì „íŒŒ"""
        image_features = self.image_encoder(images)
        text_features = self.text_encoder(text_embeddings)
        combined_features = torch.cat([image_features, text_features], dim=1)
        actions = self.fusion_layer(combined_features)
        return actions

class MemoryAccurateBenchmark:
    """ì •í™•í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ FPS ì¸¡ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = []
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        self.test_images = torch.randn(1, 3, 224, 224, device=self.device)
        self.test_text = torch.randn(1, 512, device=self.device)
        
        print(f"ğŸ”§ Device: {self.device}")
        print(f"ğŸ“Š Test data shape: images {self.test_images.shape}, text {self.test_text.shape}")
        print(f"ğŸ¯ Target: Kosmos2 + CLIP Hybrid (MAE 0.212)")
    
    def get_memory_usage(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_used_mb = cpu_memory.used / (1024 * 1024)
        cpu_total_mb = cpu_memory.total / (1024 * 1024)
        cpu_percent = cpu_memory.percent
        
        # GPU ë©”ëª¨ë¦¬ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_used_mb = 0
        gpu_total_mb = 0
        gpu_percent = 0
        
        if torch.cuda.is_available():
            gpu_used_mb = torch.cuda.memory_allocated() / (1024 * 1024)
            gpu_total_mb = torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)
            gpu_percent = (gpu_used_mb / gpu_total_mb) * 100
        
        return {
            'cpu_used_mb': cpu_used_mb,
            'cpu_total_mb': cpu_total_mb,
            'cpu_percent': cpu_percent,
            'gpu_used_mb': gpu_used_mb,
            'gpu_total_mb': gpu_total_mb,
            'gpu_percent': gpu_percent
        }
    
    def benchmark_pytorch_with_memory(self, num_runs: int = 100):
        """PyTorch ë²¤ì¹˜ë§ˆí¬ (ë©”ëª¨ë¦¬ ì¸¡ì • í¬í•¨)"""
        print(f"\nğŸ“ˆ Benchmarking PyTorch with Memory Measurement ({num_runs} runs)")
        print("-" * 60)
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ì¸¡ì •
        initial_memory = self.get_memory_usage()
        print(f"Initial Memory:")
        print(f"   CPU: {initial_memory['cpu_used_mb']:.1f}MB / {initial_memory['cpu_total_mb']:.1f}MB ({initial_memory['cpu_percent']:.1f}%)")
        print(f"   GPU: {initial_memory['gpu_used_mb']:.1f}MB / {initial_memory['gpu_total_mb']:.1f}MB ({initial_memory['gpu_percent']:.1f}%)")
        
        # PyTorch ìµœì í™” ì„¤ì •
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        model = Kosmos2CLIPHybridModel().to(self.device)
        model.eval()
        
        # TorchScript ìµœì í™” ì‹œë„
        try:
            model = torch.jit.script(model)
            print(f"   âœ… TorchScript optimization applied")
        except Exception as e:
            print(f"   âš ï¸ TorchScript optimization failed: {e}")
        
        # ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
        model_memory = self.get_memory_usage()
        print(f"After Model Load:")
        print(f"   CPU: {model_memory['cpu_used_mb']:.1f}MB / {model_memory['cpu_total_mb']:.1f}MB ({model_memory['cpu_percent']:.1f}%)")
        print(f"   GPU: {model_memory['gpu_used_mb']:.1f}MB / {model_memory['gpu_total_mb']:.1f}MB ({model_memory['gpu_percent']:.1f}%)")
        
        # ì›Œë°ì—…
        print("ğŸ”¥ Warming up PyTorch model...")
        with torch.no_grad():
            for i in range(50):
                _ = model(self.test_images, self.test_text)
                if (i + 1) % 10 == 0:
                    print(f"   Warmup: {i + 1}/50")
        
        # ë²¤ì¹˜ë§ˆí¬
        print(f"âš¡ Running PyTorch benchmark...")
        times = []
        memory_samples = []
        
        for i in range(num_runs):
            start_time = time.perf_counter()
            
            with torch.no_grad():
                outputs = model(self.test_images, self.test_text)
            
            inference_time = time.perf_counter() - start_time
            times.append(inference_time)
            
            # ë©”ëª¨ë¦¬ ìƒ˜í”Œë§ (10ê°œë§ˆë‹¤)
            if i % 10 == 0:
                memory_samples.append(self.get_memory_usage())
            
            if (i + 1) % 20 == 0:
                print(f"   Progress: {i + 1}/{num_runs}")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì¸¡ì •
        final_memory = self.get_memory_usage()
        
        # ê²°ê³¼ ë¶„ì„
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        
        # FPS ê³„ì‚° ë°©ë²• ê²€ì¦
        fps_method1 = 1.0 / avg_time  # ê¸°ì¡´ ë°©ë²•
        fps_method2 = num_runs / sum(times)  # ì „ì²´ ì‹œê°„ ê¸°ë°˜
        fps_method3 = 1000.0 / avg_time  # ms ê¸°ë°˜ (ì˜ëª»ëœ ë°©ë²•)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        avg_gpu_memory = np.mean([sample['gpu_used_mb'] for sample in memory_samples])
        avg_cpu_memory = np.mean([sample['cpu_used_mb'] for sample in memory_samples])
        
        result = {
            "framework": "PyTorch (Optimized)",
            "optimization": "TorchScript + cuDNN",
            "average_time_ms": avg_time * 1000,
            "std_time_ms": std_time * 1000,
            "min_time_ms": min_time * 1000,
            "max_time_ms": max_time * 1000,
            "fps_method1": fps_method1,  # 1/avg_time
            "fps_method2": fps_method2,  # num_runs/total_time
            "fps_method3": fps_method3,  # 1000/avg_time (ì˜ëª»ëœ ë°©ë²•)
            "num_runs": num_runs,
            "memory": {
                "initial_gpu_mb": initial_memory['gpu_used_mb'],
                "model_gpu_mb": model_memory['gpu_used_mb'],
                "final_gpu_mb": final_memory['gpu_used_mb'],
                "avg_gpu_mb": avg_gpu_memory,
                "initial_cpu_mb": initial_memory['cpu_used_mb'],
                "model_cpu_mb": model_memory['cpu_used_mb'],
                "final_cpu_mb": final_memory['cpu_used_mb'],
                "avg_cpu_mb": avg_cpu_memory
            }
        }
        
        print(f"ğŸ“Š PyTorch Results:")
        print(f"   Average: {avg_time*1000:.3f} ms")
        print(f"   Std Dev: {std_time*1000:.3f} ms")
        print(f"   Min: {min_time*1000:.3f} ms")
        print(f"   Max: {max_time*1000:.3f} ms")
        print(f"   FPS (1/avg_time): {fps_method1:.1f}")
        print(f"   FPS (num_runs/total): {fps_method2:.1f}")
        print(f"   FPS (1000/avg_time): {fps_method3:.1f} âš ï¸ (ì˜ëª»ëœ ë°©ë²•)")
        print(f"   GPU Memory: {avg_gpu_memory:.1f}MB")
        print(f"   CPU Memory: {avg_cpu_memory:.1f}MB")
        
        return result
    
    def benchmark_onnx_with_memory(self, onnx_path: str, num_runs: int = 100):
        """ONNX Runtime ë²¤ì¹˜ë§ˆí¬ (ë©”ëª¨ë¦¬ ì¸¡ì • í¬í•¨)"""
        print(f"\nğŸ“ˆ Benchmarking ONNX Runtime with Memory Measurement ({num_runs} runs)")
        print("-" * 60)
        
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ì¸¡ì •
        initial_memory = self.get_memory_usage()
        print(f"Initial Memory:")
        print(f"   CPU: {initial_memory['cpu_used_mb']:.1f}MB / {initial_memory['cpu_total_mb']:.1f}MB ({initial_memory['cpu_percent']:.1f}%)")
        print(f"   GPU: {initial_memory['gpu_used_mb']:.1f}MB / {initial_memory['gpu_total_mb']:.1f}MB ({initial_memory['gpu_percent']:.1f}%)")
        
        # ONNX Runtime ì„¤ì • (CPU ì‚¬ìš© - CUDA ë¬¸ì œë¡œ ì¸í•´)
        providers = ['CPUExecutionProvider']
        
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        session_options.intra_op_num_threads = 4
        session_options.inter_op_num_threads = 4
        
        try:
            # ONNX Runtime ì„¸ì…˜ ìƒì„±
            session = ort.InferenceSession(onnx_path, session_options, providers=providers)
            
            # ì…ë ¥ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            input_names = [input.name for input in session.get_inputs()]
            output_names = [output.name for output in session.get_outputs()]
            
            # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
            inputs = {
                input_names[0]: self.test_images.cpu().numpy(),
                input_names[1]: self.test_text.cpu().numpy()
            }
            
            # ì„¸ì…˜ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
            session_memory = self.get_memory_usage()
            print(f"After Session Load:")
            print(f"   CPU: {session_memory['cpu_used_mb']:.1f}MB / {session_memory['cpu_total_mb']:.1f}MB ({session_memory['cpu_percent']:.1f}%)")
            print(f"   GPU: {session_memory['gpu_used_mb']:.1f}MB / {session_memory['gpu_total_mb']:.1f}MB ({session_memory['gpu_percent']:.1f}%)")
            
            # ì›Œë°ì—…
            print("ğŸ”¥ Warming up ONNX Runtime model...")
            for i in range(50):
                _ = session.run(output_names, inputs)
                if (i + 1) % 10 == 0:
                    print(f"   Warmup: {i + 1}/50")
            
            # ë²¤ì¹˜ë§ˆí¬
            print(f"âš¡ Running ONNX Runtime benchmark...")
            times = []
            memory_samples = []
            
            for i in range(num_runs):
                start_time = time.perf_counter()
                
                outputs = session.run(output_names, inputs)
                
                inference_time = time.perf_counter() - start_time
                times.append(inference_time)
                
                # ë©”ëª¨ë¦¬ ìƒ˜í”Œë§ (10ê°œë§ˆë‹¤)
                if i % 10 == 0:
                    memory_samples.append(self.get_memory_usage())
                
                if (i + 1) % 20 == 0:
                    print(f"   Progress: {i + 1}/{num_runs}")
            
            # ìµœì¢… ë©”ëª¨ë¦¬ ì¸¡ì •
            final_memory = self.get_memory_usage()
            
            # ê²°ê³¼ ë¶„ì„
            avg_time = np.mean(times)
            std_time = np.std(times)
            min_time = np.min(times)
            max_time = np.max(times)
            
            # FPS ê³„ì‚° ë°©ë²• ê²€ì¦
            fps_method1 = 1.0 / avg_time
            fps_method2 = num_runs / sum(times)
            fps_method3 = 1000.0 / avg_time
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            avg_gpu_memory = np.mean([sample['gpu_used_mb'] for sample in memory_samples])
            avg_cpu_memory = np.mean([sample['cpu_used_mb'] for sample in memory_samples])
            
            result = {
                "framework": "ONNX Runtime (Optimized)",
                "optimization": "Graph Optimization + CPU",
                "average_time_ms": avg_time * 1000,
                "std_time_ms": std_time * 1000,
                "min_time_ms": min_time * 1000,
                "max_time_ms": max_time * 1000,
                "fps_method1": fps_method1,
                "fps_method2": fps_method2,
                "fps_method3": fps_method3,
                "num_runs": num_runs,
                "model_size_mb": os.path.getsize(onnx_path) / (1024 * 1024),
                "memory": {
                    "initial_gpu_mb": initial_memory['gpu_used_mb'],
                    "session_gpu_mb": session_memory['gpu_used_mb'],
                    "final_gpu_mb": final_memory['gpu_used_mb'],
                    "avg_gpu_mb": avg_gpu_memory,
                    "initial_cpu_mb": initial_memory['cpu_used_mb'],
                    "session_cpu_mb": session_memory['cpu_used_mb'],
                    "final_cpu_mb": final_memory['cpu_used_mb'],
                    "avg_cpu_mb": avg_cpu_memory
                }
            }
            
            print(f"ğŸ“Š ONNX Runtime Results:")
            print(f"   Average: {avg_time*1000:.3f} ms")
            print(f"   Std Dev: {std_time*1000:.3f} ms")
            print(f"   Min: {min_time*1000:.3f} ms")
            print(f"   Max: {max_time*1000:.3f} ms")
            print(f"   FPS (1/avg_time): {fps_method1:.1f}")
            print(f"   FPS (num_runs/total): {fps_method2:.1f}")
            print(f"   FPS (1000/avg_time): {fps_method3:.1f} âš ï¸ (ì˜ëª»ëœ ë°©ë²•)")
            print(f"   GPU Memory: {avg_gpu_memory:.1f}MB")
            print(f"   CPU Memory: {avg_cpu_memory:.1f}MB")
            print(f"   Model Size: {result['model_size_mb']:.1f}MB")
            
            return result
            
        except Exception as e:
            print(f"âŒ ONNX benchmark failed: {e}")
            return None
    
    def create_detailed_report(self, results: List[Dict]):
        """ìƒì„¸í•œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n" + "="*80)
        print("ğŸ“Š DETAILED PERFORMANCE ANALYSIS")
        print("="*80)
        
        if len(results) < 2:
            print("âŒ Need at least 2 results for comparison")
            return
        
        # ê²°ê³¼ ì •ë ¬ (FPS ê¸°ì¤€)
        sorted_results = sorted(results, key=lambda x: x['fps_method1'], reverse=True)
        
        print(f"\nğŸ† Performance Ranking (by FPS):")
        print("-" * 80)
        
        for i, result in enumerate(sorted_results, 1):
            framework = result['framework']
            optimization = result.get('optimization', 'N/A')
            avg_time = result['average_time_ms']
            fps = result['fps_method1']
            fps_method2 = result['fps_method2']
            fps_method3 = result['fps_method3']
            
            print(f"{i}. {framework}")
            print(f"   Optimization: {optimization}")
            print(f"   â±ï¸  Time: {avg_time:.3f} ms")
            print(f"   ğŸš€ FPS (1/avg_time): {fps:.1f}")
            print(f"   ğŸš€ FPS (num_runs/total): {fps_method2:.1f}")
            print(f"   âš ï¸  FPS (1000/avg_time): {fps_method3:.1f} (ì˜ëª»ëœ ë°©ë²•)")
            
            if 'memory' in result:
                mem = result['memory']
                print(f"   ğŸ’¾ GPU Memory: {mem['avg_gpu_mb']:.1f}MB")
                print(f"   ğŸ’¾ CPU Memory: {mem['avg_cpu_mb']:.1f}MB")
            
            if 'model_size_mb' in result:
                print(f"   ğŸ“ Model Size: {result['model_size_mb']:.1f}MB")
            print()
        
        # FPS ê³„ì‚° ë°©ë²• ë¹„êµ
        print(f"ğŸ” FPS Calculation Method Analysis:")
        print("-" * 80)
        
        pytorch_result = next((r for r in results if 'PyTorch' in r['framework']), None)
        onnx_result = next((r for r in results if 'ONNX' in r['framework']), None)
        
        if pytorch_result and onnx_result:
            print(f"PyTorch FPS Methods:")
            print(f"   1/avg_time: {pytorch_result['fps_method1']:.1f}")
            print(f"   num_runs/total: {pytorch_result['fps_method2']:.1f}")
            print(f"   1000/avg_time: {pytorch_result['fps_method3']:.1f} âš ï¸")
            print()
            print(f"ONNX Runtime FPS Methods:")
            print(f"   1/avg_time: {onnx_result['fps_method1']:.1f}")
            print(f"   num_runs/total: {onnx_result['fps_method2']:.1f}")
            print(f"   1000/avg_time: {onnx_result['fps_method3']:.1f} âš ï¸")
            print()
            print(f"ğŸ”§ Correct FPS calculation: 1/avg_time (seconds)")
            print(f"âš ï¸  Wrong FPS calculation: 1000/avg_time (treats ms as seconds)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        print(f"\nğŸ’¾ Memory Usage Comparison:")
        print("-" * 80)
        
        for result in results:
            if 'memory' in result:
                mem = result['memory']
                framework = result['framework']
                print(f"{framework}:")
                print(f"   GPU: {mem['avg_gpu_mb']:.1f}MB")
                print(f"   CPU: {mem['avg_cpu_mb']:.1f}MB")
                print()
        
        # ê²°ê³¼ ì €ì¥
        report_path = "Robo+/Mobile_VLA/memory_accurate_results.json"
        with open(report_path, "w") as f:
            json.dump({
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "device": str(self.device),
                "results": results,
                "ranking": [r['framework'] for r in sorted_results]
            }, f, indent=2)
        
        print(f"\nâœ… Detailed report saved: {report_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting Memory-Accurate Benchmark")
    print("ğŸ¯ Measuring Memory Usage and Validating FPS Calculation")
    
    benchmark = MemoryAccurateBenchmark()
    
    try:
        results = []
        
        # 1. PyTorch ë²¤ì¹˜ë§ˆí¬
        print(f"\n1. PyTorch Benchmark with Memory Measurement")
        pytorch_result = benchmark.benchmark_pytorch_with_memory()
        results.append(pytorch_result)
        
        # 2. ONNX Runtime ë²¤ì¹˜ë§ˆí¬
        print(f"\n2. ONNX Runtime Benchmark with Memory Measurement")
        onnx_path = "Robo+/Mobile_VLA/optimized_onnx/model.onnx"
        
        if not os.path.exists(onnx_path):
            print(f"âŒ ONNX model not found: {onnx_path}")
            print(f"   Creating ONNX model first...")
            # ONNX ëª¨ë¸ ìƒì„± ë¡œì§ ì¶”ê°€ í•„ìš”
        
        onnx_result = benchmark.benchmark_onnx_with_memory(onnx_path)
        if onnx_result:
            results.append(onnx_result)
        
        # 3. ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
        benchmark.create_detailed_report(results)
        
        print(f"\nâœ… Memory-accurate benchmark completed!")
        print(f"ğŸ“Š Tested {len(results)} frameworks with memory measurement")
        print(f"ğŸ”§ Device: {benchmark.device}")
        
    except Exception as e:
        print(f"âŒ Memory-accurate benchmark failed: {e}")
        raise

if __name__ == "__main__":
    main()
