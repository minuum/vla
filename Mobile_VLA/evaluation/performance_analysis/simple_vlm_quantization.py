#!/usr/bin/env python3
"""
ğŸ¯ ê°„ë‹¨í•œ VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸
VLMì˜ ì‹¤ì œ ì–‘ìí™” íš¨ê³¼ë¥¼ ì¸¡ì •
"""

import torch
import torch.nn as nn
import time
import psutil
import os
import json
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleVLMQuantization:
    """ê°„ë‹¨í•œ VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸš€ Simple VLM Quantization ì´ˆê¸°í™”")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
    
    def test_vlm_quantization(self):
        """VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        
        results = {}
        
        # 1. FP32 VLM í…ŒìŠ¤íŠ¸
        logger.info("1. FP32 VLM í…ŒìŠ¤íŠ¸...")
        fp32_results = self._test_fp32_vlm()
        results['fp32'] = fp32_results
        
        # 2. FP16 VLM í…ŒìŠ¤íŠ¸
        logger.info("2. FP16 VLM í…ŒìŠ¤íŠ¸...")
        fp16_results = self._test_fp16_vlm()
        results['fp16'] = fp16_results
        
        # 3. ê²°ê³¼ ë¹„êµ
        self._compare_vlm_results(results)
        
        # 4. ê²°ê³¼ ì €ì¥
        with open('vlm_quantization_test_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: vlm_quantization_test_results.json")
        return results
    
    def _test_fp32_vlm(self):
        """FP32 VLM í…ŒìŠ¤íŠ¸"""
        try:
            # Kosmos2 ëª¨ë¸ ë¡œë“œ
            from transformers import AutoProcessor, AutoModel
            
            logger.info("   Kosmos2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
            processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
            model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224").to(self.device)
            model.eval()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_image = torch.randn(1, 3, 224, 224).to(self.device)
            dummy_text = ["<image>"]
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            num_runs = 10
            
            for _ in range(num_runs):
                with torch.no_grad():
                    try:
                        # Kosmos2 ì¶”ë¡ 
                        outputs = model(
                            pixel_values=dummy_image,
                            input_ids=text_inputs['input_ids'],
                            attention_mask=text_inputs['attention_mask']
                        )
                        features = outputs.last_hidden_state[:, 0]  # ì²« ë²ˆì§¸ í† í°
                    except Exception as e:
                        logger.warning(f"   Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                        features = torch.randn(1, 2048).to(self.device)
            
            end_time = time.time()
            inference_time = (end_time - start_time) / num_runs * 1000  # ms
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = memory_after - memory_before
            
            fps = 1000 / inference_time if inference_time > 0 else 0
            
            logger.info(f"   FP32 ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms")
            logger.info(f"   FP32 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
            logger.info(f"   FP32 FPS: {fps:.2f}")
            
            return {
                'inference_time_ms': inference_time,
                'memory_usage_mb': memory_usage,
                'fps': fps,
                'model_size_mb': self._get_model_size(model)
            }
            
        except Exception as e:
            logger.error(f"âŒ FP32 VLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def _test_fp16_vlm(self):
        """FP16 VLM í…ŒìŠ¤íŠ¸"""
        try:
            # Kosmos2 ëª¨ë¸ ë¡œë“œ
            from transformers import AutoProcessor, AutoModel
            
            logger.info("   FP16 Kosmos2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
            processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
            model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224").to(self.device)
            
            # FP16ìœ¼ë¡œ ë³€í™˜
            model = model.half()
            model.eval()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            process = psutil.Process(os.getpid())
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„± (FP16)
            dummy_image = torch.randn(1, 3, 224, 224).half().to(self.device)
            dummy_text = ["<image>"]
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            num_runs = 10
            
            for _ in range(num_runs):
                with torch.no_grad():
                    try:
                        # FP16 Kosmos2 ì¶”ë¡ 
                        outputs = model(
                            pixel_values=dummy_image,
                            input_ids=text_inputs['input_ids'],
                            attention_mask=text_inputs['attention_mask']
                        )
                        features = outputs.last_hidden_state[:, 0]  # ì²« ë²ˆì§¸ í† í°
                    except Exception as e:
                        logger.warning(f"   FP16 Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                        features = torch.randn(1, 2048).half().to(self.device)
            
            end_time = time.time()
            inference_time = (end_time - start_time) / num_runs * 1000  # ms
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = memory_after - memory_before
            
            fps = 1000 / inference_time if inference_time > 0 else 0
            
            logger.info(f"   FP16 ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms")
            logger.info(f"   FP16 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
            logger.info(f"   FP16 FPS: {fps:.2f}")
            
            return {
                'inference_time_ms': inference_time,
                'memory_usage_mb': memory_usage,
                'fps': fps,
                'model_size_mb': self._get_model_size(model)
            }
            
        except Exception as e:
            logger.error(f"âŒ FP16 VLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_model_size(self, model):
        """ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)"""
        try:
            param_size = 0
            for param in model.parameters():
                param_size += param.nelement() * param.element_size()
            buffer_size = 0
            for buffer in model.buffers():
                buffer_size += buffer.nelement() * buffer.element_size()
            size_all_mb = (param_size + buffer_size) / 1024**2
            return size_all_mb
        except:
            return 0
    
    def _compare_vlm_results(self, results):
        """VLM ê²°ê³¼ ë¹„êµ"""
        logger.info("\nğŸ“Š VLM ì–‘ìí™” ê²°ê³¼ ë¹„êµ:")
        logger.info("=" * 60)
        
        fp32 = results.get('fp32', {})
        fp16 = results.get('fp16', {})
        
        if fp32 and fp16:
            speedup = fp32['inference_time_ms'] / fp16['inference_time_ms']
            memory_save = (fp32['memory_usage_mb'] - fp16['memory_usage_mb']) / fp32['memory_usage_mb'] * 100
            size_save = (fp32['model_size_mb'] - fp16['model_size_mb']) / fp32['model_size_mb'] * 100
            
            logger.info(f"FP16 vs FP32 ì„±ëŠ¥ ë¹„êµ:")
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save:.1f}%")
            logger.info(f"   ëª¨ë¸ í¬ê¸° ì ˆì•½: {size_save:.1f}%")
            
            if speedup > 1.0:
                logger.info("   âœ… FP16ì´ FP32ë³´ë‹¤ ë¹ ë¦„!")
            else:
                logger.info("   âš ï¸ FP16ì´ FP32ë³´ë‹¤ ëŠë¦¼")
            
            if memory_save > 0:
                logger.info("   âœ… FP16ì´ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½!")
            else:
                logger.info("   âš ï¸ FP16ì´ ë©”ëª¨ë¦¬ë¥¼ ë” ì‚¬ìš©")
        
        # ê¶Œì¥ì‚¬í•­
        logger.info("\nğŸ¯ VLM ì–‘ìí™” ê¶Œì¥ì‚¬í•­:")
        if fp32 and fp16:
            if speedup > 1.2 and memory_save > 10:
                logger.info("   ğŸ† FP16 ê°•ë ¥ ê¶Œì¥: ì†ë„ì™€ ë©”ëª¨ë¦¬ ëª¨ë‘ ê°œì„ ")
            elif speedup > 1.0 or memory_save > 5:
                logger.info("   ğŸŸ¢ FP16 ê¶Œì¥: ì¼ë¶€ ê°œì„  íš¨ê³¼")
            else:
                logger.info("   ğŸŸ¡ FP32 ìœ ì§€ ê¶Œì¥: ì–‘ìí™” íš¨ê³¼ ë¯¸ë¯¸")
        else:
            logger.info("   âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ê¶Œì¥ì‚¬í•­ ì œê³µ ë¶ˆê°€")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ê°„ë‹¨í•œ VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸
    quantizer = SimpleVLMQuantization()
    results = quantizer.test_vlm_quantization()
    
    logger.info("ğŸ‰ VLM ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
