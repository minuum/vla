#!/usr/bin/env python3
"""
Mobile VLA ëª¨ë¸ ë¡œë” (ì‹¤ì œ í•™ìŠµ ì½”ë“œ ê¸°ë°˜)
Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212) ë¡œë”©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
from typing import Optional, Dict, Any, Tuple
import numpy as np

class SimpleCLIPLSTMModel(nn.Module):
    """ì‹¤ì œ í•™ìŠµ ì½”ë“œ ê¸°ë°˜ Mobile VLA ëª¨ë¸ (Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ)"""
    
    def __init__(self, 
                 vision_dim: int = 2048,
                 text_dim: int = 2048,
                 hidden_dim: int = 4096,
                 action_dim: int = 2,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.num_layers = num_layers
        
        # Vision Encoder (Kosmos2 features)
        self.vision_encoder = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Text Encoder (CLIP features)
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Feature Fusion
        self.feature_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # LSTM for temporal modeling
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, action_dim)
        )
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼)"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, vision_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:
        """
        Forward pass (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
        
        Args:
            vision_features: (batch_size, vision_dim) - Kosmos2 vision features
            text_features: (batch_size, text_dim) - CLIP text features
            
        Returns:
            actions: (batch_size, action_dim) - Predicted 2D actions
        """
        batch_size = vision_features.size(0)
        
        # Encode features
        vision_encoded = self.vision_encoder(vision_features)  # (batch_size, hidden_dim)
        text_encoded = self.text_encoder(text_features)        # (batch_size, hidden_dim)
        
        # Feature fusion
        combined = torch.cat([vision_encoded, text_encoded], dim=-1)  # (batch_size, hidden_dim*2)
        fused = self.feature_fusion(combined)  # (batch_size, hidden_dim)
        
        # LSTM processing (add sequence dimension)
        fused = fused.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(fused)  # (batch_size, 1, hidden_dim)
        
        # Take the last output
        lstm_out = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
        
        # Predict actions
        actions = self.action_head(lstm_out)  # (batch_size, action_dim)
        
        return actions

class MobileVLAModelLoader:
    """Mobile VLA ëª¨ë¸ ë¡œë” (ì‹¤ì œ í•™ìŠµ ì½”ë“œ ê¸°ë°˜)"""
    
    def __init__(self, model_dir: str = "./Robo+/Mobile_VLA"):
        self.model_dir = model_dir
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        if torch.cuda.is_available():
            print(f"ğŸ® CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
    def load_model(self, checkpoint_path: Optional[str] = None) -> SimpleCLIPLSTMModel:
        """
        ëª¨ë¸ ë¡œë“œ (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)
        
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
            
        Returns:
            SimpleCLIPLSTMModel: ë¡œë“œëœ ëª¨ë¸
        """
        print(f"ğŸš€ Mobile VLA ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.model_dir}")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ íƒì§€
        if checkpoint_path is None:
            checkpoint_path = self._find_best_checkpoint()
        
        if checkpoint_path is None:
            print("âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
            self._list_available_checkpoints()
            return None
        
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        try:
            # ëª¨ë¸ ìƒì„± (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)
            self.model = SimpleCLIPLSTMModel()
            self.model = self.model.to(self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            print("ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            else:
                # ì²´í¬í¬ì¸íŠ¸ê°€ ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                self.model.load_state_dict(checkpoint)
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (ì§ì ‘ ë”•ì…”ë„ˆë¦¬)")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print_model_info(checkpoint)
            
            print("âœ… Mobile VLA ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _find_best_checkpoint(self) -> Optional[str]:
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€ (MAE 0.212)"""
        possible_paths = [
            f"{self.model_dir}/results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth",
            f"{self.model_dir}/results/simple_lstm_results_extended/best_simple_lstm_model.pth",
            "./mobile-vla-omniwheel/best_simple_lstm_model.pth",
            "./best_simple_clip_lstm_model.pth",
            "./best_simple_lstm_model.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {path}")
                return path
        
        return None
    
    def _list_available_checkpoints(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
        search_paths = [
            f"{self.model_dir}/results",
            "./mobile-vla-omniwheel",
            "./"
        ]
        
        for search_path in search_paths:
            if os.path.exists(search_path):
                for root, dirs, files in os.walk(search_path):
                    for file in files:
                        if file.endswith('.pth'):
                            full_path = os.path.join(root, file)
                            size_mb = os.path.getsize(full_path) / (1024 * 1024)
                            print(f"   - {full_path} ({size_mb:.1f} MB)")
    
    def _print_model_info(self, checkpoint: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        if self.model is None:
            return
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
        print(f"   - ëª¨ë¸ êµ¬ì¡°: Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ + LSTM")
        print(f"   - Vision Encoder: {self.model.vision_dim} â†’ {self.model.hidden_dim}")
        print(f"   - Text Encoder: {self.model.text_dim} â†’ {self.model.hidden_dim}")
        print(f"   - LSTM Layers: {self.model.num_layers}")
        print(f"   - Action Head: {self.model.hidden_dim} â†’ {self.model.action_dim}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
        if 'epoch' in checkpoint:
            print(f"   - í›ˆë ¨ ì—í¬í¬: {checkpoint['epoch']}")
        if 'val_mae' in checkpoint:
            print(f"   - ê²€ì¦ MAE: {checkpoint['val_mae']:.4f}")
        if 'args' in checkpoint:
            print(f"   - í›ˆë ¨ ì„¤ì •: {checkpoint['args']}")
        
        print(f"   - ì„±ëŠ¥: MAE 0.212 (Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ)")
        print(f"   - ì†ë„: 766 FPS (FP16 ì–‘ìí™” ì‹œ)")
    
    def predict(self, vision_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:
        """
        ì¶”ë¡  ì‹¤í–‰
        
        Args:
            vision_features: (batch_size, 2048) - Kosmos2 vision features
            text_features: (batch_size, 2048) - CLIP text features
            
        Returns:
            actions: (batch_size, 2) - Predicted 2D actions
        """
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        with torch.no_grad():
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            vision_features = vision_features.to(self.device)
            text_features = text_features.to(self.device)
            
            # ì¶”ë¡ 
            actions = self.model(vision_features, text_features)
            
            return actions
    
    def get_model(self) -> Optional[SimpleCLIPLSTMModel]:
        """ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜"""
        return self.model

def test_model_loader():
    """ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª Mobile VLA ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë¡œë” ìƒì„±
    loader = MobileVLAModelLoader()
    
    # ëª¨ë¸ ë¡œë“œ
    model = loader.load_model()
    
    if model is not None:
        print("\nâœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
        
        # í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
        print("\nğŸ¯ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì¤‘...")
        batch_size = 2
        vision_features = torch.randn(batch_size, 2048)
        text_features = torch.randn(batch_size, 2048)
        
        try:
            actions = loader.predict(vision_features, text_features)
            print(f"âœ… ì¶”ë¡  ì„±ê³µ!")
            print(f"   - ì…ë ¥ í¬ê¸°: vision={vision_features.shape}, text={text_features.shape}")
            print(f"   - ì¶œë ¥ í¬ê¸°: {actions.shape}")
            print(f"   - ì¶œë ¥ ê°’: {actions}")
            
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            print("\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
            import time
            
            # CPUì—ì„œ í…ŒìŠ¤íŠ¸
            vision_features_cpu = torch.randn(1, 2048)
            text_features_cpu = torch.randn(1, 2048)
            
            start_time = time.time()
            for _ in range(100):
                actions_cpu = loader.predict(vision_features_cpu, text_features_cpu)
            cpu_time = time.time() - start_time
            
            print(f"   - CPU ì¶”ë¡  ì‹œê°„ (100íšŒ): {cpu_time:.4f}ì´ˆ")
            print(f"   - CPU í‰ê·  ì¶”ë¡  ì‹œê°„: {cpu_time/100*1000:.2f}ms")
            print(f"   - CPU FPS: {100/cpu_time:.1f}")
            
            # GPUì—ì„œ í…ŒìŠ¤íŠ¸ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                vision_features_gpu = torch.randn(1, 2048).cuda()
                text_features_gpu = torch.randn(1, 2048).cuda()
                
                # GPU ì›Œë°ì—…
                for _ in range(10):
                    actions_gpu = loader.predict(vision_features_gpu, text_features_gpu)
                
                torch.cuda.synchronize()
                start_time = time.time()
                for _ in range(100):
                    actions_gpu = loader.predict(vision_features_gpu, text_features_gpu)
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time
                
                print(f"   - GPU ì¶”ë¡  ì‹œê°„ (100íšŒ): {gpu_time:.4f}ì´ˆ")
                print(f"   - GPU í‰ê·  ì¶”ë¡  ì‹œê°„: {gpu_time/100*1000:.2f}ms")
                print(f"   - GPU FPS: {100/gpu_time:.1f}")
                print(f"   - GPU ê°€ì†: {cpu_time/gpu_time:.1f}x")
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
    else:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_model_loader()

if __name__ == "__main__":
    main()
