# Mobile VLA LoRA Fine-tuning ì‹¤í–‰ ìš”ì•½

## ğŸ“‹ ìƒì„±ëœ íŒŒì¼

### 1. Config íŒŒì¼
```
Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json
```
- RoboVLMs upstream config ê¸°ë°˜
- LoRA ì„¤ì •: r=32, alpha=16, dropout=0.1
- 2D ì•¡ì…˜ ê³µê°„: action_dim=2 (linear_x, linear_y)
- Jetson ìµœì í™”: batch_size=2, fp16, memory_limit=14GB

### 2. ë°ì´í„°ì…‹ êµ¬í˜„
```
Mobile_VLA/src/data/mobile_vla_h5_dataset.py
```
- HDF5 íŒŒì¼ ë¡œë“œ (20251106 ì—í”¼ì†Œë“œ)
- Window size=8, Action chunk=10
- ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (224x224, ImageNet ì •ê·œí™”)
- Train/Val ë¶„í•  (80/20)

### 3. LoRA Fine-tuning ìŠ¤í¬ë¦½íŠ¸
```
Mobile_VLA/src/training/finetune_lora_20251106.py
```
- Kosmos-2 VLM + LoRA ì ìš©
- AdamW Optimizer + Cosine Annealing LR
- Gradient Clipping (max_norm=1.0)
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (best + ì£¼ê¸°ì )

### 4. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```
Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```
- CUDA í™•ì¸
- ë°ì´í„°ì…‹ í™•ì¸
- LoRA Fine-tuning ì‹¤í–‰
- í•™ìŠµ ì‹œê°„ ì¸¡ì •

### 5. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```
Mobile_VLA/scripts/test_dataset_20251106.py
```
- ë°ì´í„°ì…‹ ë¡œë“œ í…ŒìŠ¤íŠ¸
- ë°°ì¹˜ ìƒì„± í…ŒìŠ¤íŠ¸

### 6. ê°€ì´ë“œ ë¬¸ì„œ
```
Mobile_VLA/README_LORA_FINETUNING.md
```
- ì „ì²´ ì‹¤í–‰ ê°€ì´ë“œ
- ë¬¸ì œ í•´ê²° ë°©ë²•
- ì°¸ê³  ìë£Œ

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### 1ë‹¨ê³„: ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
```bash
cd /home/billy/25-1kp/vla
python3 Mobile_VLA/scripts/test_dataset_20251106.py
```

**ì˜ˆìƒ ê²°ê³¼:**
- âœ… 11ê°œ Training ì—í”¼ì†Œë“œ
- âœ… 2ê°œ Validation ì—í”¼ì†Œë“œ
- âœ… ë°°ì¹˜ ë¡œë“œ ì„±ê³µ

### 2ë‹¨ê³„: LoRA ì‹œê°„ ì¸¡ì • (1 ì—í¬í¬)
```bash
# Config ìˆ˜ì •: max_epochsë¥¼ 1ë¡œ ë³€ê²½
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json

# ì‹¤í–‰
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh

# ê²°ê³¼ í™•ì¸
cat Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json | grep avg_epoch_time
```

### 3ë‹¨ê³„: ì „ì²´ í•™ìŠµ (50 ì—í¬í¬)
```bash
# Config ìˆ˜ì •: max_epochsë¥¼ 50ìœ¼ë¡œ ë³€ê²½
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json

# ì‹¤í–‰
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### í•™ìŠµ ì‹œê°„ (Jetson AGX Orin 16GB)
- **1 ì—í¬í¬**: ~5-10ë¶„ (ì˜ˆìƒ)
- **50 ì—í¬í¬**: ~4-8ì‹œê°„ (ì˜ˆìƒ)

### ëª¨ë¸ í¬ê¸°
- **Full Model**: ~2GB (Kosmos-2)
- **LoRA Adapter**: ~50MB (í•™ìŠµ íŒŒë¼ë¯¸í„°ë§Œ)

### í•™ìŠµ íŒŒë¼ë¯¸í„°
- **Total Parameters**: ~1.3B (Kosmos-2)
- **Trainable Parameters**: ~10M (LoRA, <1%)

---

## ğŸ¯ í•µì‹¬ ì°¨ì´ì : RoboVLMs vs Mobile VLA

| í•­ëª© | RoboVLMs | Mobile VLA |
|------|----------|------------|
| **Fine-tuning** | Full FT | LoRA |
| **Action Space** | 7D (6-DOF + gripper) | 2D (linear_x, linear_y) |
| **Dataset** | CALVIN 24K episodes | 13 episodes (20251106) |
| **Epochs** | 5 | 50 |
| **Learning Rate** | 2e-5 | 1e-4 |
| **Batch Size** | 4 | 2 |
| **Hidden Size** | 1024 | 512 |
| **Trainable %** | 100% | <1% |

---

## ğŸ“ ì¶œë ¥ êµ¬ì¡°

```
Mobile_VLA/runs/mobile_vla_lora/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”‚   â”œâ”€â”€ checkpoint_epoch_10.pth
â”‚   â”œâ”€â”€ checkpoint_epoch_20.pth
â”‚   â”œâ”€â”€ checkpoint_epoch_30.pth
â”‚   â”œâ”€â”€ checkpoint_epoch_40.pth
â”‚   â””â”€â”€ checkpoint_epoch_50.pth
â””â”€â”€ logs/
    â”œâ”€â”€ training_results.json       # í•™ìŠµ ê²°ê³¼ ìš”ì•½
    â”œâ”€â”€ events.out.tfevents.*       # TensorBoard
    â””â”€â”€ metrics.csv
```

---

## ğŸ” í•™ìŠµ ëª¨ë‹ˆí„°ë§

### TensorBoard
```bash
tensorboard --logdir=Mobile_VLA/runs/mobile_vla_lora/logs
# http://localhost:6006
```

### ì‹¤ì‹œê°„ ë¡œê·¸
```bash
tail -f Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json
```

---

## âœ… ê²€ì¦ í•­ëª©

### ë°ì´í„°ì…‹
- [x] 20251106 ì—í”¼ì†Œë“œ 13ê°œ í™•ì¸
- [x] HDF5 êµ¬ì¡° í™•ì¸ (images, actions, action_event_types)
- [x] Train/Val ë¶„í•  (11/2)

### ëª¨ë¸
- [x] Kosmos-2 ë¡œë“œ
- [x] LoRA ì ìš© (r=32, alpha=16)
- [x] 2D ì•¡ì…˜ í—¤ë“œ (action_dim=2)
- [x] Gripper ì œê±°

### í•™ìŠµ
- [x] AdamW Optimizer
- [x] Cosine Annealing LR
- [x] Gradient Clipping
- [x] ì²´í¬í¬ì¸íŠ¸ ì €ì¥

### Config
- [x] RoboVLMs upstream ì°¸ì¡°
- [x] Mobile VLA íƒœìŠ¤í¬ ì ì‘
- [x] Jetson ìµœì í™”

---

## ğŸ“š ì°¸ê³  ì½”ë“œ

### RoboVLMs Upstream
```
RoboVLMs_upstream/configs/calvin_finetune/
â””â”€â”€ finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json
```

### Mobile VLA êµ¬í˜„
```
Mobile_VLA/src/
â”œâ”€â”€ model/mobile_vla_model.py           # LoRA ì ìš©ëœ Kosmos-2
â”œâ”€â”€ data/mobile_vla_h5_dataset.py       # HDF5 ë°ì´í„°ì…‹
â””â”€â”€ training/finetune_lora_20251106.py  # LoRA Fine-tuning
```

---

## ğŸ‰ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„

1. **ì¶”ë¡  í…ŒìŠ¤íŠ¸**: í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ 
2. **ì„±ëŠ¥ í‰ê°€**: MAE, MSE ê³„ì‚°
3. **ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸**: Jetsonì—ì„œ ë¡œë´‡ ì œì–´
4. **100 Dataset ìˆ˜ì§‘**: ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘
5. **ë…¼ë¬¸ ì‘ì„±**: RoboVLMs + Robot Manipulator

---

**ì‘ì„±ì¼**: 2025-11-06  
**ìƒíƒœ**: êµ¬í˜„ ì™„ë£Œ, ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ  
**ë‹¤ìŒ**: LoRA ì‹œê°„ ì¸¡ì • (1 ì—í¬í¬)

