# π‰ Mobile VLA LoRA Fine-tuning μµμΆ… μ”μ•½ (20251106)

## β… μ™„λ£λ μ‘μ—…

### 1. LoRA Fine-tuning κµ¬ν„ μ™„λ£
- β… RoboVLMs upstream config κΈ°λ° μ„¤μ •
- β… Kosmos-2 VLM + LoRA μ μ©
- β… 2D μ•΅μ… κ³µκ°„ (linear_x, linear_y)
- β… HDF5 λ°μ΄ν„°μ…‹ λ΅λ”
- β… ν•™μµ μ¤ν¬λ¦½νΈ
- β… μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
- β… ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- β… λ¬Έμ„ μ‘μ„±

### 2. ν™κ²½ κ²€μ¦ μ™„λ£
- β… λ¨λ“  νμΌ μƒμ„± ν™•μΈ
- β… 13κ° μ—ν”Όμ†λ“ ν™•μΈ
- β… Python ν¨ν‚¤μ§€ μ„¤μΉ ν™•μΈ
- β… CUDA μ‚¬μ© κ°€λ¥ (NVIDIA RTX A5000)

---

## π“ κµ¬ν„ λ‚΄μ©

### νμΌ κµ¬μ΅°
```
Mobile_VLA/
β”β”€β”€ configs/
β”‚   β””β”€β”€ finetune_mobile_vla_lora_20251106.json    # LoRA μ„¤μ •
β”β”€β”€ src/
β”‚   β”β”€β”€ data/
β”‚   β”‚   β””β”€β”€ mobile_vla_h5_dataset.py              # HDF5 λ°μ΄ν„°μ…‹
β”‚   β””β”€β”€ training/
β”‚       β””β”€β”€ finetune_lora_20251106.py             # LoRA Fine-tuning
β”β”€β”€ scripts/
β”‚   β”β”€β”€ run_lora_finetune_20251106.sh             # μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
β”‚   β””β”€β”€ test_dataset_20251106.py                  # ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ README_LORA_FINETUNING.md                     # μ „μ²΄ κ°€μ΄λ“
β”β”€β”€ LORA_FINETUNING_SUMMARY.md                    # κµ¬ν„ μ”μ•½
β”β”€β”€ IMPLEMENTATION_STATUS_20251106.md             # κµ¬ν„ μƒνƒ
β””β”€β”€ FINAL_SUMMARY_20251106.md                     # μµμΆ… μ”μ•½ (μ΄ λ¬Έμ„)
```

### λ°μ΄ν„°μ…‹
- **μ—ν”Όμ†λ“**: 13κ° (20251106)
- **Train/Val**: 11/2 (80/20 λ¶„ν• )
- **Window size**: 8 ν”„λ μ„
- **Action chunk**: 10 ν”„λ μ„
- **μμƒ μƒν”**: ~100-200κ°

### LoRA μ„¤μ •
- **LoRA Rank (r)**: 32
- **LoRA Alpha**: 16
- **LoRA Dropout**: 0.1
- **ν•™μµ νλΌλ―Έν„°**: <1% (Full Model: ~1.3B, LoRA: ~10M)

---

## π€ μ‹¤ν–‰ μμ„

### 1λ‹¨κ³„: λ°μ΄ν„°μ…‹ ν…μ¤νΈ β…
```bash
cd /home/billy/25-1kp/vla
python3 Mobile_VLA/scripts/test_dataset_20251106.py
```

### 2λ‹¨κ³„: LoRA μ‹κ°„ μΈ΅μ • (1 μ—ν¬ν¬) β³
```bash
# Config μμ •: max_epochs=1
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json

# μ‹¤ν–‰
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

**μμƒ μ‹κ°„**: ~5-10λ¶„

### 3λ‹¨κ³„: μ „μ²΄ ν•™μµ (50 μ—ν¬ν¬) β³
```bash
# Config μμ •: max_epochs=50
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json

# μ‹¤ν–‰
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

**μμƒ μ‹κ°„**: ~4-8μ‹κ°„

---

## π“ ν•µμ‹¬ μ°¨μ΄μ : RoboVLMs vs Mobile VLA

| ν•­λ© | RoboVLMs | Mobile VLA | μ΄μ  |
|------|----------|------------|------|
| **Fine-tuning** | Full FT | LoRA | λ©”λ¨λ¦¬ ν¨μ¨ |
| **Action Space** | 7D | 2D | λ¨λ°”μΌ λ΅λ΄‡ |
| **Dataset** | 24K episodes | 13 episodes | μ΄κΈ° λ°μ΄ν„° |
| **Epochs** | 5 | 50 | μ μ€ λ°μ΄ν„° λ³΄μ™„ |
| **Learning Rate** | 2e-5 | 1e-4 | LoRA ν•™μµλ¥  |
| **Batch Size** | 4 | 2 | λ©”λ¨λ¦¬ μ μ•½ |
| **Hidden Size** | 1024 | 512 | κ²½λ‰ν™” |
| **Trainable %** | 100% | <1% | LoRA |

---

## π― μμƒ κ²°κ³Ό

### ν•™μµ μ‹κ°„
- **1 μ—ν¬ν¬**: ~5-10λ¶„
- **50 μ—ν¬ν¬**: ~4-8μ‹κ°„
- **μ΄ ν•™μµ μ‹κ°„**: μ•½ λ°λ‚μ 

### λ¨λΈ ν¬κΈ°
- **Full Model**: ~2GB (Kosmos-2)
- **LoRA Adapter**: ~50MB (ν•™μµ νλΌλ―Έν„°λ§)
- **μ €μ¥ κ³µκ°„**: μ²΄ν¬ν¬μΈνΈ 10κ° Γ— 50MB = ~500MB

### μ¶λ ¥ νμΌ
```
Mobile_VLA/runs/mobile_vla_lora/
β”β”€β”€ checkpoints/
β”‚   β”β”€β”€ best_model.pth              # μµκ³  μ„±λ¥
β”‚   β””β”€β”€ checkpoint_epoch_*.pth      # μ£ΌκΈ°μ  μ €μ¥
β””β”€β”€ logs/
    β”β”€β”€ training_results.json       # ν•™μµ κ²°κ³Ό
    β””β”€β”€ events.out.tfevents.*       # TensorBoard
```

---

## π“ μ°Έκ³  μλ£

### ν”„λ΅μ νΈ λ¬Έμ„
1. **λΉ λ¥Έ μ‹μ‘**: `/QUICK_START_LORA_20251106.md`
2. **μ „μ²΄ κ°€μ΄λ“**: `Mobile_VLA/README_LORA_FINETUNING.md`
3. **κµ¬ν„ μ”μ•½**: `Mobile_VLA/LORA_FINETUNING_SUMMARY.md`
4. **κµ¬ν„ μƒνƒ**: `Mobile_VLA/IMPLEMENTATION_STATUS_20251106.md`

### RoboVLMs μ°Έμ΅°
- **Upstream Config**: `RoboVLMs_upstream/configs/calvin_finetune/`
- **GitHub**: https://github.com/Robot-VLAs/RoboVLMs

### μ™Έλ¶€ μλ£
- **PEFT (LoRA)**: https://github.com/huggingface/peft
- **Kosmos-2**: https://huggingface.co/microsoft/kosmos-2-patch14-224

---

## π― λ‹¤μ λ‹¨κ³„

### μ¦‰μ‹ μ‹¤ν–‰ κ°€λ¥
1. β… ν™κ²½ κ²€μ¦ μ™„λ£
2. β³ λ°μ΄ν„°μ…‹ ν…μ¤νΈ μ‹¤ν–‰
3. β³ LoRA μ‹κ°„ μΈ΅μ • (1 μ—ν¬ν¬)
4. β³ μ „μ²΄ ν•™μµ (50 μ—ν¬ν¬)

### ν•™μµ μ™„λ£ ν›„
1. β³ ν•™μµ κ²°κ³Ό λ¶„μ„
2. β³ μ¶”λ΅  ν…μ¤νΈ
3. β³ μ„±λ¥ ν‰κ°€ (MAE, MSE)
4. β³ μ‹¤μ‹κ°„ λ΅λ΄‡ μ μ–΄ ν…μ¤νΈ

### μ¥κΈ° κ³„ν
1. β³ 100 Dataset μμ§‘ (November 6, 2025)
2. β³ RoboVLMs + Robot Manipulator λ…Όλ¬Έ 2-3κ° (λ°©ν•™ μ¤‘)
3. β³ μ¶”κ°€ λ°μ΄ν„° μμ§‘ (1000κ°)

---

## π’΅ ν•µμ‹¬ ν¬μΈνΈ

### 1. LoRA μ„ νƒ μ΄μ 
- **λ©”λ¨λ¦¬ ν¨μ¨**: Jetson 16GB μ μ•½
- **ν•™μµ μ‹κ°„ λ‹¨μ¶•**: νλΌλ―Έν„° <1%λ§ ν•™μµ
- **μ μ€ λ°μ΄ν„°**: 13κ° μ—ν”Όμ†λ“λ΅ Full FTλ” κ³Όμ ν•© μ„ν—
- **λ°°ν¬ ν¨μ¨**: LoRA μ–΄λ‘ν„°λ§ μ €μ¥ (~50MB)

### 2. 2D μ•΅μ… κ³µκ°„
- **RoboVLMs**: 7D (6-DOF arm + 1-DOF gripper)
- **Mobile VLA**: 2D (linear_x, linear_y)
- **μ΄μ **: λ¨λ°”μΌ λ΅λ΄‡ λ‚΄λΉ„κ²μ΄μ… νƒμ¤ν¬

### 3. ν•™μµ μ „λµ
- **λ†’μ€ μ—ν¬ν¬ μ (50)**: μ μ€ λ°μ΄ν„° λ³΄μ™„
- **μ •κ·ν™” κ°•ν™”**: weight_decay=0.01, dropout=0.1
- **ν•™μµλ¥  μ΅°μ •**: 1e-4 (LoRA κ¶μ¥ ν•™μµλ¥ )

---

## β… μµμΆ… μ²΄ν¬λ¦¬μ¤νΈ

### κµ¬ν„
- [x] Config νμΌ μ‘μ„±
- [x] λ°μ΄ν„°μ…‹ κµ¬ν„
- [x] LoRA Fine-tuning μ¤ν¬λ¦½νΈ
- [x] μ‹¤ν–‰ μ¤ν¬λ¦½νΈ
- [x] ν…μ¤νΈ μ¤ν¬λ¦½νΈ
- [x] λ¬Έμ„ μ‘μ„±

### ν™κ²½
- [x] νμΌ μ΅΄μ¬ ν™•μΈ
- [x] λ°μ΄ν„°μ…‹ ν™•μΈ (13κ°)
- [x] Python ν¨ν‚¤μ§€ ν™•μΈ
- [x] CUDA ν™•μΈ

### μ‹¤ν–‰
- [ ] λ°μ΄ν„°μ…‹ ν…μ¤νΈ
- [ ] LoRA μ‹κ°„ μΈ΅μ •
- [ ] μ „μ²΄ ν•™μµ
- [ ] κ²°κ³Ό λ¶„μ„

---

## π‰ κ²°λ΅ 

**20251106 μ—ν”Όμ†λ“λ¥Ό Kosmos VLMμ— LoRAλ΅ νμΈνλ‹ν•λ” λ¨λ“  μ½”λ“μ™€ λ¬Έμ„κ°€ μ™„μ„±λμ—μµλ‹λ‹¤!**

### μ¤€λΉ„ μ™„λ£ ν•­λ©
β… RoboVLMs upstream config κΈ°λ° μ„¤μ •  
β… LoRA Fine-tuning κµ¬ν„  
β… 2D μ•΅μ… κ³µκ°„ μ μ‘  
β… HDF5 λ°μ΄ν„°μ…‹ λ΅λ”  
β… ν•™μµ/μ‹¤ν–‰/ν…μ¤νΈ μ¤ν¬λ¦½νΈ  
β… μ „μ²΄ λ¬Έμ„ν™”  
β… ν™κ²½ κ²€μ¦

### λ‹¤μ μ‹¤ν–‰ λ…λ Ή
```bash
# 1. λ°μ΄ν„°μ…‹ ν…μ¤νΈ
python3 Mobile_VLA/scripts/test_dataset_20251106.py

# 2. LoRA μ‹κ°„ μΈ΅μ • (1 μ—ν¬ν¬)
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

---

**μ‘μ„±μΌ**: 2025-11-06  
**μ‘μ„±μ**: Mobile VLA Team  
**μƒνƒ**: π‰ κµ¬ν„ μ™„λ£, μ‹¤ν–‰ μ¤€λΉ„ μ™„λ£  
**λ‹¤μ**: λ°μ΄ν„°μ…‹ ν…μ¤νΈ β†’ LoRA μ‹κ°„ μΈ΅μ • β†’ μ „μ²΄ ν•™μµ

