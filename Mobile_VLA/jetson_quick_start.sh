#!/bin/bash

# ğŸš€ K-í”„ë¡œì íŠ¸ Jetson í™˜ê²½ ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
# ëª©ì : NVIDIA Jetsonì—ì„œ RoboVLMs ë¡œë´‡ì¹´ ë„¤ë¹„ê²Œì´ì…˜ ì‹¤í—˜ì„ ì¦‰ì‹œ ì‹œì‘

set -e  # ì—ëŸ¬ ë°œìƒì‹œ ì¤‘ë‹¨

echo "ğŸ¯ K-í”„ë¡œì íŠ¸ RoboVLMs Jetson ë¹ ë¥¸ ì‹œì‘"
echo "=================================================="

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# 1. ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
echo -e "${BLUE}ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸${NC}"
echo "CUDA ë²„ì „: $(nvcc --version | grep release | awk '{print $6}' | cut -c2-)"
echo "GPU ë©”ëª¨ë¦¬: $(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits) MB"
echo "ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: $(free -m | awk 'NR==2{printf "%.0f MB (%.1f%%)", $7, $7*100/$2}')"
echo ""

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo -e "${BLUE}ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •${NC}"
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export TORCH_DTYPE=bfloat16
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
echo "âœ… CUDA í™˜ê²½ ì„¤ì • ì™„ë£Œ"

# 3. ROS2 í™˜ê²½ í™œì„±í™”
echo -e "${BLUE}ğŸ¤– ROS2 í™˜ê²½ í™œì„±í™”${NC}"
if [ -f "/opt/ros/humble/setup.bash" ]; then
    source /opt/ros/humble/setup.bash
    echo "âœ… ROS2 Humble í™˜ê²½ í™œì„±í™”"
else
    echo -e "${RED}âŒ ROS2 Humbleì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤${NC}"
    exit 1
fi

# 4. Python í™˜ê²½ í™•ì¸
echo -e "${BLUE}ğŸ Python í™˜ê²½ í™•ì¸${NC}"
if command -v conda &> /dev/null; then
    echo "Conda ì‚¬ìš© ê°€ëŠ¥"
    # robovlms í™˜ê²½ì´ ìˆìœ¼ë©´ í™œì„±í™”
    if conda env list | grep -q robovlms; then
        echo "robovlms í™˜ê²½ í™œì„±í™” ì¤‘..."
        conda activate robovlms
    fi
elif [ -d "venv" ]; then
    echo "Python ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
    source venv/bin/activate
fi

python --version
echo "âœ… Python í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ"

# 5. ì‹¤í–‰ ê¶Œí•œ í™•ì¸
echo -e "${BLUE}ğŸ“‹ ì‹¤í–‰ ê¶Œí•œ í™•ì¸${NC}"
chmod +x *.sh 2>/dev/null || echo "âš ï¸ ì¼ë¶€ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
echo "âœ… ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ì„¤ì • ì™„ë£Œ"

# 6. GPU ë° ëª¨ë¸ í…ŒìŠ¤íŠ¸
echo -e "${BLUE}ğŸ§ª GPU ë° ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸${NC}"
python3 -c "
import torch
print(f'CUDA ì‚¬ìš©ê°€ëŠ¥: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU ì¥ì¹˜: {torch.cuda.get_device_name(0)}')
    print(f'GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')
else:
    print('âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
    exit(1)
"

echo ""
echo -e "${YELLOW}âš ï¸  PaliGemma-3B ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)${NC}"
python3 -c "
try:
    from transformers import PaliGemmaForConditionalGeneration
    import torch
    
    print('PaliGemma ëª¨ë¸ ë¡œë”© ì¤‘...')
    model = PaliGemmaForConditionalGeneration.from_pretrained(
        'google/paligemma-3b-mix-224',
        torch_dtype=torch.bfloat16,
        device_map='auto',
        low_cpu_mem_usage=True
    )
    
    memory_used = torch.cuda.memory_allocated() / 1e9
    print(f'âœ… PaliGemma-3B ë¡œë”© ì„±ê³µ')
    print(f'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}GB')
    
    if memory_used > 14:
        print('âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. bfloat16 ëª¨ë“œ í™•ì¸ í•„ìš”')
    
    del model
    torch.cuda.empty_cache()
    print('âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬')
    
except Exception as e:
    print(f'âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}')
    print('Hugging Face ìºì‹œë¥¼ ì •ë¦¬í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”')
    print('rm -rf ~/.cache/huggingface/')
    exit(1)
"

# 7. ëˆ„ë½ëœ íŒŒì¼ í™•ì¸
echo -e "${BLUE}ğŸ“ í•„ìˆ˜ íŒŒì¼ í™•ì¸${NC}"
missing_files=0

# í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸
if [ ! -f "launch_event_triggered_vla.sh" ]; then
    echo -e "${RED}âŒ launch_event_triggered_vla.sh ëˆ„ë½${NC}"
    missing_files=$((missing_files + 1))
fi

if [ ! -f "send_text_command.sh" ]; then
    echo -e "${RED}âŒ send_text_command.sh ëˆ„ë½${NC}"  
    missing_files=$((missing_files + 1))
fi

if [ ! -f "docker-compose.yml" ]; then
    echo -e "${RED}âŒ docker-compose.yml ëˆ„ë½${NC}"
    missing_files=$((missing_files + 1))
fi

# 8. ì™„ë£Œ ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
echo ""
if [ $missing_files -eq 0 ]; then
    echo -e "${GREEN}ğŸ‰ Jetson í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!${NC}"
else
    echo -e "${YELLOW}âš ï¸  Jetson í™˜ê²½ ë¶€ë¶„ ì¤€ë¹„ ì™„ë£Œ ($missing_filesê°œ íŒŒì¼ ëˆ„ë½)${NC}"
fi
echo "=================================================="
echo ""
echo -e "${BLUE}ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:${NC}"

if [ $missing_files -eq 0 ]; then
    echo "1. Event-Triggered VLA ì‹œìŠ¤í…œ ì‹œì‘:"
    echo "   ${YELLOW}./launch_event_triggered_vla.sh${NC}"
    echo ""
    echo "2. ê¸°ë³¸ ëª…ë ¹ì–´ í…ŒìŠ¤íŠ¸:"
    echo "   ${YELLOW}./send_text_command.sh \"ì•ìœ¼ë¡œ ê°€\"${NC}"
    echo "   ${YELLOW}./send_text_command.sh \"ë©ˆì¶°\"${NC}"
    echo ""
    echo "3. ëŒ€í™”í˜• ëª…ë ¹ ëª¨ë“œ:"
    echo "   ${YELLOW}./send_text_command.sh -i${NC}"
else
    echo "1. ëˆ„ë½ëœ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ë“¤ì„ ë¨¼ì € ë³µêµ¬í•˜ì„¸ìš”:"
    echo "   - launch_event_triggered_vla.sh"
    echo "   - send_text_command.sh"  
    echo "   - docker-compose.yml"
    echo ""
    echo "2. ë³µêµ¬ í›„ ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”"
fi

echo ""
echo "4. ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§:"
echo "   ${YELLOW}docker ps${NC}"
echo "   ${YELLOW}ros2 topic list${NC}"
echo ""
echo -e "${BLUE}ğŸ“– ìì„¸í•œ ê°€ì´ë“œ:${NC}"
echo "- ì „ì²´ ë§¥ë½: ../Robo+/K-í”„ë¡œì íŠ¸/RoboVLMs_ì‹¤í—˜ì„¤ê³„_ëŒ€í™”ìš”ì•½_20250725.md"
echo "- ë‹¤ìŒ ë‹¨ê³„: ../Robo+/K-í”„ë¡œì íŠ¸/ë‹¤ìŒë‹¨ê³„_ì•¡ì…˜ì•„ì´í…œ.md"
echo ""

if [ $missing_files -eq 0 ]; then  
    echo -e "${GREEN}ğŸš€ K-í”„ë¡œì íŠ¸ ë¡œë´‡ì¹´ ë„¤ë¹„ê²Œì´ì…˜ ì‹¤í—˜ì„ ì‹œì‘í•˜ì„¸ìš”!${NC}"
else
    echo -e "${YELLOW}ğŸ”§ ëˆ„ë½ëœ íŒŒì¼ë“¤ì„ ë³µêµ¬í•œ í›„ ì‹¤í—˜ì„ ì‹œì‘í•˜ì„¸ìš”!${NC}"
fi