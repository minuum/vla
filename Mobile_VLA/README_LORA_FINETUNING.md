# Mobile VLA LoRA Fine-tuning Guide (20251106 Episodes)

## π“‹ κ°μ”

20251106 λ‚ μ§μ— μμ§‘ν• μ—ν”Όμ†λ“λ¥Ό Kosmos VLMμ— LoRAλ΅ νμΈνλ‹ν•λ” κ°€μ΄λ“μ…λ‹λ‹¤.

μ°Έμ΅°: [RoboVLMs GitHub](https://github.com/Robot-VLAs/RoboVLMs)

---

## π― λ©ν‘

- **λ°μ΄ν„°**: 20251106 μ—ν”Όμ†λ“ (13κ° HDF5 νμΌ)
- **λ¨λΈ**: Kosmos-2 (microsoft/kosmos-2-patch14-224)
- **λ°©λ²•**: LoRA Fine-tuning (r=32, alpha=16)
- **νƒμ¤ν¬**: 2D Mobile Robot Navigation (linear_x, linear_y)

---

## π“ λ°μ΄ν„°μ…‹ κµ¬μ΅°

### HDF5 νμΌ κµ¬μ΅°
```python
episode_20251106_*.h5
β”β”€β”€ images: (T, 720, 1280, 3) uint8      # RGB μ΄λ―Έμ§€
β”β”€β”€ actions: (T, 3) float32              # [linear_x, linear_y, angular_z]
β””β”€β”€ action_event_types: (T,)             # μ•΅μ… μ΄λ²¤νΈ νƒ€μ…
```

### μμ§‘λ μ—ν”Όμ†λ“
```bash
ROS_action/mobile_vla_dataset/
β”β”€β”€ episode_20251106_145248_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_145456_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_145609_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_145705_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_145841_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_145934_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_150243_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_150407_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_151110_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_151305_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_151417_1box_hori_left_core_medium.h5
β”β”€β”€ episode_20251106_151744_1box_hori_left_core_medium.h5
β””β”€β”€ episode_20251106_151851_1box_hori_left_core_medium.h5
```

**μ΄ 13κ° μ—ν”Όμ†λ“**

---

## β™οΈ LoRA μ„¤μ •

### RoboVLMs vs Mobile VLA

| ν•­λ© | RoboVLMs (Full FT) | Mobile VLA (LoRA) |
|------|-------------------|-------------------|
| **Fine-tuning λ°©λ²•** | Full Fine-tuning | LoRA |
| **freeze_backbone** | false | true |
| **lora_enable** | false | true |
| **lora_r** | 64 | 32 |
| **lora_alpha** | 16 | 16 |
| **lora_dropout** | 0.05 | 0.1 |
| **train_vision** | true | false |
| **train_text_embedding** | true | false |
| **learning_rate** | 2e-5 | 1e-4 |
| **weight_decay** | 0 | 0.01 |
| **batch_size** | 4 | 2 |
| **max_epochs** | 5 | 50 |
| **action_dim** | 7 (6-DOF + gripper) | 2 (linear_x, linear_y) |
| **hidden_size** | 1024 | 512 |
| **window_size** | 8 | 8 |
| **action_chunk_size** | 10 | 10 |

### LoRA μ μ© μ΄μ 

1. **λ©”λ¨λ¦¬ ν¨μ¨**: Jetson 16GB λ©”λ¨λ¦¬ μ μ•½
2. **ν•™μµ μ‹κ°„ λ‹¨μ¶•**: νλΌλ―Έν„° 1% λ―Έλ§λ§ ν•™μµ
3. **μ μ€ λ°μ΄ν„°**: 13κ° μ—ν”Όμ†λ“λ΅ Full FTλ” κ³Όμ ν•© μ„ν—
4. **λ°°ν¬ ν¨μ¨**: LoRA μ–΄λ‘ν„°λ§ μ €μ¥ (μ MB)

---

## π€ μ‹¤ν–‰ λ°©λ²•

### 1. λ°μ΄ν„°μ…‹ ν…μ¤νΈ

```bash
cd /home/billy/25-1kp/vla
python3 Mobile_VLA/scripts/test_dataset_20251106.py
```

**μμƒ μ¶λ ¥:**
```
π§ 20251106 μ—ν”Όμ†λ“ λ°μ΄ν„°μ…‹ ν…μ¤νΈ μ‹μ‘
π“ Validation λ°μ΄ν„°μ…‹: 2κ° μ—ν”Όμ†λ“
π“ Training λ°μ΄ν„°μ…‹: 11κ° μ—ν”Όμ†λ“
β… μ΄ XXXκ° μƒν” μƒμ„±
β… λ°°μΉ λ΅λ“ μ„±κ³µ:
  - images shape: torch.Size([2, 8, 3, 224, 224])
  - actions shape: torch.Size([2, 10, 2])
  - language: go to the red box
β… λ°μ΄ν„°μ…‹ ν…μ¤νΈ μ„±κ³µ!
```

### 2. LoRA Fine-tuning μ‹¤ν–‰

```bash
cd /home/billy/25-1kp/vla
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

**μ‹¤ν–‰ κ³Όμ •:**
1. CUDA ν™•μΈ
2. λ°μ΄ν„°μ…‹ ν™•μΈ
3. λ¨λΈ λ΅λ“ (Kosmos-2)
4. LoRA μ μ©
5. ν•™μµ μ‹μ‘ (50 μ—ν¬ν¬)
6. μ²΄ν¬ν¬μΈνΈ μ €μ¥

### 3. ν•™μµ λ¨λ‹ν„°λ§

```bash
# TensorBoard μ‹¤ν–‰
tensorboard --logdir=Mobile_VLA/runs/mobile_vla_lora/logs

# ν•™μµ κ²°κ³Ό ν™•μΈ
cat Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json
```

---

## π“ μμƒ ν•™μµ μ‹κ°„

### Jetson AGX Orin (16GB)

- **μ—ν¬ν¬λ‹Ή μ‹κ°„**: ~5-10λ¶„ (μμƒ)
- **μ΄ ν•™μµ μ‹κ°„**: 50 μ—ν¬ν¬ Γ— 5λ¶„ = ~4μ‹κ°„ (μμƒ)
- **μ²΄ν¬ν¬μΈνΈ ν¬κΈ°**: ~50MB (LoRA μ–΄λ‘ν„°λ§)

### ν•™μµ μ‹κ°„ μΈ΅μ • λ°©λ²•

```bash
# 1 μ—ν¬ν¬λ§ μ‹¤ν–‰ν•μ—¬ μ‹κ°„ μΈ΅μ •
python3 Mobile_VLA/src/training/finetune_lora_20251106.py \
    --config Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json \
    --device cuda

# training_results.jsonμ—μ„ avg_epoch_time ν™•μΈ
```

---

## π“ μ¶λ ¥ νμΌ

### μ²΄ν¬ν¬μΈνΈ
```
Mobile_VLA/runs/mobile_vla_lora/checkpoints/
β”β”€β”€ best_model.pth              # μµκ³  μ„±λ¥ λ¨λΈ
β”β”€β”€ checkpoint_epoch_10.pth     # 10 μ—ν¬ν¬ μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ checkpoint_epoch_20.pth     # 20 μ—ν¬ν¬ μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ checkpoint_epoch_30.pth     # 30 μ—ν¬ν¬ μ²΄ν¬ν¬μΈνΈ
β”β”€β”€ checkpoint_epoch_40.pth     # 40 μ—ν¬ν¬ μ²΄ν¬ν¬μΈνΈ
β””β”€β”€ checkpoint_epoch_50.pth     # 50 μ—ν¬ν¬ μ²΄ν¬ν¬μΈνΈ
```

### λ΅κ·Έ
```
Mobile_VLA/runs/mobile_vla_lora/logs/
β”β”€β”€ training_results.json       # ν•™μµ κ²°κ³Ό μ”μ•½
β”β”€β”€ events.out.tfevents.*       # TensorBoard λ΅κ·Έ
β””β”€β”€ metrics.csv                 # CSV λ΅κ·Έ
```

---

## π” ν•™μµ κ²°κ³Ό λ¶„μ„

### training_results.json κµ¬μ΅°
```json
{
  "config": {...},
  "train_losses": [0.5, 0.4, 0.3, ...],
  "val_losses": [0.6, 0.5, 0.4, ...],
  "learning_rates": [1e-4, 9e-5, ...],
  "epoch_times": [300, 310, 295, ...],
  "avg_epoch_time": 302.5,
  "total_epochs": 50,
  "best_val_loss": 0.25,
  "timestamp": "2025-11-06T15:30:00"
}
```

### μ„±λ¥ μ§€ν‘

- **Train Loss**: ν•™μµ μ†μ‹¤ (λ‚®μ„μλ΅ μΆ‹μ)
- **Val Loss**: κ²€μ¦ μ†μ‹¤ (λ‚®μ„μλ΅ μΆ‹μ)
- **Learning Rate**: ν•™μµλ¥  λ³€ν™” (Cosine Annealing)
- **Epoch Time**: μ—ν¬ν¬λ‹Ή μ†μ” μ‹κ°„

---

## π› λ¬Έμ  ν•΄κ²°

### 1. CUDA Out of Memory

**μ¦μƒ:**
```
RuntimeError: CUDA out of memory
```

**ν•΄κ²°:**
```json
// finetune_mobile_vla_lora_20251106.json μμ •
{
  "batch_size": 1,              // 2 β†’ 1
  "accumulate_grad_batches": 8  // 4 β†’ 8
}
```

### 2. λ°μ΄ν„°μ…‹ λ΅λ“ μ‹¤ν¨

**μ¦μƒ:**
```
ValueError: No episodes found matching pattern
```

**ν•΄κ²°:**
```bash
# μ—ν”Όμ†λ“ νμΌ ν™•μΈ
ls -lh /home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset/episode_20251106_*.h5

# κ²½λ΅ ν™•μΈ
pwd
```

### 3. λ¨λΈ λ΅λ“ μ‹¤ν¨

**μ¦μƒ:**
```
OSError: Can't load tokenizer for 'microsoft/kosmos-2-patch14-224'
```

**ν•΄κ²°:**
```bash
# Hugging Face ν† ν° μ„¤μ •
export HUGGING_FACE_HUB_TOKEN="your_token_here"

# λλ” λ΅κ·ΈμΈ
huggingface-cli login
```

---

## π“ μ°Έκ³  μλ£

### RoboVLMs μ›λ³Έ Config
- `RoboVLMs_upstream/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json`

### Mobile VLA κµ¬ν„
- `Mobile_VLA/src/model/mobile_vla_model.py` - λ¨λΈ κµ¬μ΅°
- `Mobile_VLA/src/data/mobile_vla_h5_dataset.py` - λ°μ΄ν„°μ…‹
- `Mobile_VLA/src/training/finetune_lora_20251106.py` - ν•™μµ μ¤ν¬λ¦½νΈ

### GitHub
- [RoboVLMs](https://github.com/Robot-VLAs/RoboVLMs)
- [PEFT (LoRA)](https://github.com/huggingface/peft)

---

## β… μ²΄ν¬λ¦¬μ¤νΈ

- [ ] λ°μ΄ν„°μ…‹ ν…μ¤νΈ μ™„λ£
- [ ] CUDA μ‚¬μ© κ°€λ¥ ν™•μΈ
- [ ] LoRA Fine-tuning μ‹¤ν–‰
- [ ] ν•™μµ μ‹κ°„ μΈ΅μ • (1 μ—ν¬ν¬)
- [ ] μ „μ²΄ ν•™μµ μ™„λ£ (50 μ—ν¬ν¬)
- [ ] μµκ³  λ¨λΈ μ²΄ν¬ν¬μΈνΈ ν™•μΈ
- [ ] ν•™μµ κ²°κ³Ό λ¶„μ„
- [ ] μ¶”λ΅  ν…μ¤νΈ

---

## π― λ‹¤μ λ‹¨κ³„

1. **LoRA μ‹κ°„ μΈ΅μ •**: 1 μ—ν¬ν¬ μ‹¤ν–‰ν•μ—¬ μ‹κ°„ μΈ΅μ •
2. **μ „μ²΄ ν•™μµ**: 50 μ—ν¬ν¬ ν•™μµ μ™„λ£
3. **μ¶”λ΅  ν…μ¤νΈ**: ν•™μµλ λ¨λΈλ΅ μ¶”λ΅  ν…μ¤νΈ
4. **μ„±λ¥ ν‰κ°€**: MAE, MSE λ“± λ©”νΈλ¦­ κ³„μ‚°
5. **λ°°ν¬**: Jetsonμ—μ„ μ‹¤μ‹κ°„ μ¶”λ΅  ν…μ¤νΈ

---

**μ‘μ„±μΌ**: 2025-11-06  
**μ‘μ„±μ**: Mobile VLA Team  
**μ°Έμ΅°**: RoboVLMs GitHub

