#!/usr/bin/env python3
"""
Mobile VLA LoRA Fine-tuning Script for 20251106 Episodes
ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import logging
from typing import Dict, Optional
from pathlib import Path
import json
import time
from datetime import datetime
from tqdm import tqdm
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

# Mobile VLA ëª¨ë¸ì€ ê¸°ì¡´ êµ¬í˜„ ì‚¬ìš©
from data.mobile_vla_h5_dataset import create_mobile_vla_h5_dataloader
from model.mobile_vla_model import MobileVLAModel, MobileVLALoss

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LoRAFineTuner:
    """
    Mobile VLA LoRA Fine-tuning í´ë˜ìŠ¤
    ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
    RoboVLMsì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ LoRA Fine-tuningì— ë§ê²Œ ìˆ˜ì •
    """
    
    def __init__(
        self,
        config_path: str,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        """
        LoRA Fine-tuner ì´ˆê¸°í™”
        
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
        """
        self.device = device
        
        # ì„¤ì • ë¡œë“œ
        with open(config_path, 'r') as f:
            self.config = json.load(f)
        
        logger.info(f"ğŸš€ LoRA Fine-tuning ì´ˆê¸°í™” (Device: {self.device})")
        logger.info(f"ğŸ“„ Config: {config_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(self.config['output_root'])
        self.log_dir = Path(self.config['log_root'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self._setup_dataloaders()
        
        # ëª¨ë¸ ìƒì„±
        self._setup_model()
        
        # Optimizer & Scheduler ì„¤ì •
        self._setup_optimizer()
        
        # í•™ìŠµ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.learning_rates = []
        self.epoch_times = []
        
        logger.info("âœ… LoRA Fine-tuner ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_dataloaders(self):
        """ë°ì´í„° ë¡œë” ì„¤ì •"""
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë” ì„¤ì • ì¤‘...")
        
        train_config = self.config['train_dataset']
        
        self.train_loader, self.val_loader = create_mobile_vla_h5_dataloader(
            data_dir=train_config['data_dir'],
            episode_pattern=train_config['episode_pattern'],
            batch_size=self.config['batch_size'],
            num_workers=self.config['num_workers'],
            window_size=self.config['window_size'],
            action_chunk_size=self.config['fwd_pred_next_n'],
            train_split=train_config['train_split']
        )
        
        logger.info(f"âœ… ë°ì´í„° ë¡œë” ì„¤ì • ì™„ë£Œ")
        logger.info(f"  - Train batches: {len(self.train_loader)}")
        logger.info(f"  - Val batches: {len(self.val_loader)}")
    
    def _setup_model(self):
        """ëª¨ë¸ ì„¤ì •"""
        logger.info("ğŸ¤– ëª¨ë¸ ì„¤ì • ì¤‘...")
        
        act_head_config = self.config['act_head']
        train_setup = self.config['train_setup']
        
        # Mobile VLA ëª¨ë¸ ìƒì„±
        # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/model/backbone/base_backbone.py:34-50
        self.model = MobileVLAModel(
            vlm_model_name=self.config['model_url'],
            action_dim=act_head_config['action_dim'],
            hidden_size=act_head_config['hidden_size'],
            lstm_layers=2,
            lora_r=train_setup['lora_r'],
            lora_alpha=train_setup['lora_alpha'],
            lora_dropout=train_setup['lora_dropout'],
            window_size=self.config['window_size']
        ).to(self.device)
        
        # Loss í•¨ìˆ˜ ìƒì„±
        # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/model/policy_head/base_policy.py:118-160
        self.loss_fn = MobileVLALoss(
            movement_weight=1.0,
            gripper_weight=0.0  # Mobile VLAëŠ” gripper ì—†ìŒ
        )
        
        # ëª¨ë¸ í¬ê¸° ì •ë³´
        size_info = self.model.get_model_size()
        logger.info("âœ… ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        logger.info(f"  - Total params: {size_info['total_parameters']:,}")
        logger.info(f"  - Trainable params: {size_info['trainable_parameters']:,}")
        logger.info(f"  - LoRA ë¹„ìœ¨: {size_info['trainable_parameters'] / size_info['total_parameters'] * 100:.2f}%")
    
    def _setup_optimizer(self):
        """Optimizer & Scheduler ì„¤ì •"""
        logger.info("âš™ï¸ Optimizer ì„¤ì • ì¤‘...")
        
        # AdamW Optimizer
        # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # Cosine Annealing LR Scheduler
        # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        total_steps = len(self.train_loader) * self.config['trainer']['max_epochs']
        warmup_steps = int(total_steps * self.config['warmup_epochs'])
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps - warmup_steps,
            eta_min=self.config['learning_rate'] * self.config['min_lr_scale']
        )
        
        logger.info("âœ… Optimizer ì„¤ì • ì™„ë£Œ")
        logger.info(f"  - Learning rate: {self.config['learning_rate']}")
        logger.info(f"  - Weight decay: {self.config['weight_decay']}")
        logger.info(f"  - Warmup steps: {warmup_steps}")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        í•œ ì—í¬í¬ í•™ìŠµ
        ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        RoboVLMsì˜ í•™ìŠµ ë£¨í”„ êµ¬ì¡° ì°¸ê³ 
        """
        self.model.train()
        total_loss = 0.0
        movement_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {epoch+1}/{self.config['trainer']['max_epochs']} [Train]",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            images = batch["images"].to(self.device)  # (B, T, 3, 224, 224)
            actions = batch["actions"].to(self.device)  # (B, T, 2)
            language = batch["language"]
            
            # íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„ (2D ì•¡ì…˜ë§Œ)
            # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/data/calvin_dataset.py:884-887
            targets = {
                "movement_targets": actions.mean(dim=1)  # (B, 2) - linear_x, linear_y
            }
            
            # Forward pass
            self.optimizer.zero_grad()
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/model/backbone/base_backbone.py:470-540
            batch_outputs = []
            for i in range(images.shape[0]):
                single_image = images[i:i+1]
                single_text = language[i] if isinstance(language, list) else language
                
                outputs = self.model(single_image, single_text)
                batch_outputs.append(outputs)
            
            # ë°°ì¹˜ ì¶œë ¥ ê²°í•© (2D ì•¡ì…˜ë§Œ)
            combined_outputs = {
                "action_logits": torch.cat([out["action_logits"] for out in batch_outputs])
            }
            
            # Loss ê³„ì‚°
            # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/model/policy_head/base_policy.py:118-160
            losses = self.loss_fn(combined_outputs, targets)
            
            # Backward pass
            losses["total_loss"].backward()
            
            # Gradient clipping
            # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.config['trainer']['gradient_clip_val']
            )
            
            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()
            
            # Loss ê¸°ë¡
            total_loss += losses["total_loss"].item()
            movement_loss += losses["movement_loss"].item()
            num_batches += 1
            
            # Progress bar ì—…ë°ì´íŠ¸
            current_lr = self.optimizer.param_groups[0]['lr']
            progress_bar.set_postfix({
                "Loss": f"{losses['total_loss'].item():.4f}",
                "LR": f"{current_lr:.6f}"
            })
        
        # í‰ê·  Loss ê³„ì‚°
        avg_total_loss = total_loss / num_batches
        avg_movement_loss = movement_loss / num_batches
        
        return {
            "total_loss": avg_total_loss,
            "movement_loss": avg_movement_loss
        }
    
    def validate_epoch(self, epoch: int) -> Dict[str, float]:
        """
        í•œ ì—í¬í¬ ê²€ì¦
        ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        RoboVLMsì˜ ê²€ì¦ ë£¨í”„ êµ¬ì¡° ì°¸ê³ 
        """
        self.model.eval()
        total_loss = 0.0
        movement_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            progress_bar = tqdm(
                self.val_loader,
                desc=f"Epoch {epoch+1}/{self.config['trainer']['max_epochs']} [Val]",
                leave=False
            )
            
            for batch_idx, batch in enumerate(progress_bar):
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                images = batch["images"].to(self.device)
                actions = batch["actions"].to(self.device)
                language = batch["language"]
                
                # íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„ (2D ì•¡ì…˜ë§Œ)
                targets = {
                    "movement_targets": actions.mean(dim=1)
                }
                
                # Forward pass
                batch_outputs = []
                for i in range(images.shape[0]):
                    single_image = images[i:i+1]
                    single_text = language[i] if isinstance(language, list) else language
                    
                    outputs = self.model(single_image, single_text)
                    batch_outputs.append(outputs)
                
                # ë°°ì¹˜ ì¶œë ¥ ê²°í•© (2D ì•¡ì…˜ë§Œ)
                combined_outputs = {
                    "action_logits": torch.cat([out["action_logits"] for out in batch_outputs])
                }
                
                # Loss ê³„ì‚°
                losses = self.loss_fn(combined_outputs, targets)
                
                # Loss ê¸°ë¡
                total_loss += losses["total_loss"].item()
                movement_loss += losses["movement_loss"].item()
                num_batches += 1
                
                # Progress bar ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({
                    "Loss": f"{losses['total_loss'].item():.4f}"
                })
        
        # í‰ê·  Loss ê³„ì‚°
        avg_total_loss = total_loss / num_batches
        avg_movement_loss = movement_loss / num_batches
        
        return {
            "total_loss": avg_total_loss,
            "movement_loss": avg_movement_loss
        }
    
    def train(self):
        """
        ì „ì²´ í•™ìŠµ ì‹¤í–‰
        ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        RoboVLMsì˜ í•™ìŠµ ì‹¤í–‰ ë°©ì‹ ì°¸ê³ 
        """
        max_epochs = self.config['trainer']['max_epochs']
        check_val_every_n_epoch = self.config['trainer']['check_val_every_n_epoch']
        
        logger.info(f"ğŸš€ LoRA Fine-tuning ì‹œì‘ ({max_epochs} ì—í¬í¬)")
        logger.info(f"ğŸ“Š ì—í”¼ì†Œë“œ: {self.config['train_dataset']['episode_pattern']}")
        
        best_val_loss = float('inf')
        start_time = time.time()
        
        for epoch in range(max_epochs):
            epoch_start = time.time()
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            
            # ê²€ì¦ (ì£¼ê¸°ì )
            if (epoch + 1) % check_val_every_n_epoch == 0:
                val_metrics = self.validate_epoch(epoch)
            else:
                val_metrics = {"total_loss": 0.0, "movement_loss": 0.0}
            
            # Learning rate
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ì—í¬í¬ ì‹œê°„
            epoch_time = time.time() - epoch_start
            self.epoch_times.append(epoch_time)
            
            # ê¸°ë¡ ì €ì¥
            self.train_losses.append(train_metrics["total_loss"])
            self.val_losses.append(val_metrics["total_loss"])
            self.learning_rates.append(current_lr)
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"Epoch {epoch+1}/{max_epochs} ({epoch_time:.1f}s)")
            logger.info(f"  Train Loss: {train_metrics['total_loss']:.4f}")
            if (epoch + 1) % check_val_every_n_epoch == 0:
                logger.info(f"  Val Loss: {val_metrics['total_loss']:.4f}")
            logger.info(f"  Learning Rate: {current_lr:.6f}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            # ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
            if (epoch + 1) % check_val_every_n_epoch == 0 and val_metrics["total_loss"] < best_val_loss:
                best_val_loss = val_metrics["total_loss"]
                self.save_checkpoint(epoch, val_metrics, "best_model.pth")
                logger.info(f"  âœ… ìµœê³  ëª¨ë¸ ì €ì¥ (Val Loss: {best_val_loss:.4f})")
            
            # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch, val_metrics, f"checkpoint_epoch_{epoch+1}.pth")
        
        total_time = time.time() - start_time
        avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
        
        logger.info("ğŸ‰ LoRA Fine-tuning ì™„ë£Œ!")
        logger.info(f"  - ì´ ì‹œê°„: {total_time:.1f}s ({total_time/60:.1f}min)")
        logger.info(f"  - í‰ê·  ì—í¬í¬ ì‹œê°„: {avg_epoch_time:.1f}s")
        logger.info(f"  - ìµœê³  Val Loss: {best_val_loss:.4f}")
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        self.save_training_results()
    
    def save_checkpoint(self, epoch: int, metrics: Dict, filename: str):
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ì°¸ì¡°: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/train
        RoboVLMsì˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë°©ì‹ ì°¸ê³ 
        """
        filepath = self.output_dir / filename
        
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "metrics": metrics,
            "config": self.config,
            "train_losses": self.train_losses,
            "val_losses": self.val_losses,
            "learning_rates": self.learning_rates
        }
        
        torch.save(checkpoint, filepath)
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
    
    def save_training_results(self):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥"""
        results = {
            "config": self.config,
            "train_losses": self.train_losses,
            "val_losses": self.val_losses,
            "learning_rates": self.learning_rates,
            "epoch_times": self.epoch_times,
            "avg_epoch_time": sum(self.epoch_times) / len(self.epoch_times),
            "total_epochs": len(self.train_losses),
            "best_val_loss": min([loss for loss in self.val_losses if loss > 0]),
            "timestamp": datetime.now().isoformat()
        }
        
        results_file = self.log_dir / "training_results.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"ğŸ“Š í•™ìŠµ ê²°ê³¼ ì €ì¥: {results_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Mobile VLA LoRA Fine-tuning")
    parser.add_argument(
        "--config",
        type=str,
        default="/home/billy/25-1kp/vla/Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json",
        help="Config file path"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device (cuda/cpu)"
    )
    
    args = parser.parse_args()
    
    # LoRA Fine-tuning ì‹¤í–‰
    finetuner = LoRAFineTuner(
        config_path=args.config,
        device=args.device
    )
    
    finetuner.train()

if __name__ == "__main__":
    main()

