#!/usr/bin/env python3
"""
ìµœê³  ì„±ëŠ¥ ëª¨ë¸ TensorRT ì–‘ìí™”
- Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212)
- Poetry í™˜ê²½ì—ì„œ TensorRT ë³€í™˜
- ì„±ëŠ¥ ë¹„êµ ë° ë²¤ì¹˜ë§ˆí¬
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from typing import Dict, Any, Optional
from PIL import Image

# TensorRT imports
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    print("Warning: TensorRT not available. Install with: pip install tensorrt pycuda")
    TENSORRT_AVAILABLE = False

class Kosmos2CLIPHybridModel(nn.Module):
    """Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212)"""
    
    def __init__(self, model_path: str = "results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth"):
        super().__init__()
        self.model_path = model_path
        
        # ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ì‹¤ì œ ëª¨ë¸ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        self.text_encoder = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        
        self.fusion_layer = nn.Sequential(
            nn.Linear(256 + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 3)  # linear_x, linear_y, angular_z
        )
        
        self.load_model()
    
    def load_model(self):
        """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            if os.path.exists(self.model_path):
                checkpoint = torch.load(self.model_path, map_location='cpu')
                self.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… Model loaded from {self.model_path}")
                print(f"ğŸ“Š Model performance: MAE {checkpoint.get('best_mae', 'N/A')}")
            else:
                print(f"âš ï¸ Model not found: {self.model_path}")
                print("Using randomly initialized model for testing")
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            print("Using randomly initialized model for testing")
    
    def forward(self, images, text_embeddings):
        """ì „ë°© ì „íŒŒ"""
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        image_features = self.image_encoder(images)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features = self.text_encoder(text_embeddings)
        
        # íŠ¹ì§• ìœµí•©
        combined_features = torch.cat([image_features, text_features], dim=1)
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.fusion_layer(combined_features)
        
        return actions

class BestModelTensorRTConverter:
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ TensorRT ë³€í™˜ê¸°"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.output_dir = "Robo+/Mobile_VLA/tensorrt_best_model"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = Kosmos2CLIPHybridModel()
        self.model.to(self.device)
        self.model.eval()
        
        print(f"ğŸ¯ Best Model TensorRT Converter initialized")
        print(f"ğŸ“Š Target performance: MAE 0.212")
        print(f"ğŸ”§ Device: {self.device}")
    
    def prepare_sample_inputs(self, batch_size: int = 1):
        """ìƒ˜í”Œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„"""
        # ì´ë¯¸ì§€ ì…ë ¥ (224x224 RGB)
        images = torch.randn(batch_size, 3, 224, 224, device=self.device)
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© (512ì°¨ì›)
        text_embeddings = torch.randn(batch_size, 512, device=self.device)
        
        return images, text_embeddings
    
    def convert_to_onnx(self):
        """ONNX ëª¨ë¸ë¡œ ë³€í™˜"""
        print("ğŸ”¨ Converting best model to ONNX")
        
        # ìƒ˜í”Œ ì…ë ¥ ì¤€ë¹„
        sample_images, sample_text = self.prepare_sample_inputs()
        
        # ONNX ëª¨ë¸ ì €ì¥
        onnx_path = os.path.join(self.output_dir, "best_model_kosmos2_clip.onnx")
        
        try:
            torch.onnx.export(
                self.model,
                (sample_images, sample_text),
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['images', 'text_embeddings'],
                output_names=['actions'],
                dynamic_axes={
                    'images': {0: 'batch_size'},
                    'text_embeddings': {0: 'batch_size'},
                    'actions': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNX model saved: {onnx_path}")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            size_mb = os.path.getsize(onnx_path) / (1024 * 1024)
            print(f"ğŸ“Š ONNX size: {size_mb:.1f} MB")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ONNX conversion failed: {e}")
            return None
    
    def convert_to_tensorrt(self, onnx_path: str, precision: str = "fp16"):
        """TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜"""
        if not TENSORRT_AVAILABLE:
            print("âŒ TensorRT not available")
            return None
        
        print(f"ğŸ”¨ Converting to TensorRT {precision.upper()}")
        
        # TensorRT ì„¤ì •
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        config = builder.create_builder_config()
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        
        # ONNX íŒŒì„œë¡œ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        parser = trt.OnnxParser(network, logger)
        
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                for error in range(parser.num_errors):
                    print(f"âŒ ONNX parse error: {parser.get_error(error)}")
                return None
        
        # ì •ë°€ë„ ì„¤ì •
        if precision == "fp16" and builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            print("âœ… FP16 precision enabled")
        elif precision == "int8" and builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)
            config.set_flag(trt.BuilderFlag.STRICT_TYPES)
            print("âœ… INT8 quantization enabled")
        
        # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í¬ê¸° ì„¤ì •
        config.max_workspace_size = 1 << 30  # 1GB
        
        # ì—”ì§„ ë¹Œë“œ
        engine = builder.build_engine(network, config)
        
        if engine is None:
            print("âŒ Failed to build TensorRT engine")
            return None
        
        # ì—”ì§„ ì €ì¥
        engine_path = os.path.join(self.output_dir, f"best_model_{precision}.engine")
        with open(engine_path, "wb") as f:
            f.write(engine.serialize())
        
        print(f"âœ… TensorRT engine saved: {engine_path}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        size_mb = os.path.getsize(engine_path) / (1024 * 1024)
        print(f"ğŸ“Š Engine size: {size_mb:.1f} MB")
        
        return engine_path
    
    def benchmark_pytorch_model(self, num_runs: int = 100):
        """PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        print(f"ğŸ“ˆ Benchmarking PyTorch model ({num_runs} runs)")
        
        # ìƒ˜í”Œ ì…ë ¥ ì¤€ë¹„
        test_images, test_text = self.prepare_sample_inputs()
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(10):
                _ = self.model(test_images, test_text)
        
        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for i in range(num_runs):
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model(test_images, test_text)
            
            inference_time = time.time() - start_time
            times.append(inference_time)
            
            if (i + 1) % 20 == 0:
                print(f"Progress: {i + 1}/{num_runs}")
        
        # ê²°ê³¼ ë¶„ì„
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1.0 / avg_time
        
        results = {
            "framework": "PyTorch",
            "average_time_ms": avg_time * 1000,
            "std_time_ms": std_time * 1000,
            "fps": fps,
            "num_runs": num_runs
        }
        
        print(f"ğŸ“Š PyTorch Results:")
        print(f"  Average: {avg_time*1000:.2f} ms")
        print(f"  Std Dev: {std_time*1000:.2f} ms")
        print(f"  FPS: {fps:.1f}")
        
        return results
    
    def benchmark_tensorrt_engine(self, engine_path: str, num_runs: int = 100):
        """TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬"""
        if not TENSORRT_AVAILABLE:
            print("âŒ TensorRT not available for benchmarking")
            return None
        
        print(f"ğŸ“ˆ Benchmarking TensorRT engine ({num_runs} runs)")
        
        try:
            # ì—”ì§„ ë¡œë“œ
            with open(engine_path, "rb") as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            engine = runtime.deserialize_cuda_engine(engine_data)
            context = engine.create_execution_context()
            
            # ë©”ëª¨ë¦¬ í• ë‹¹
            input_images = cuda.mem_alloc(1 * 3 * 224 * 224 * 4)  # float32
            input_text = cuda.mem_alloc(1 * 512 * 4)  # float32
            output = cuda.mem_alloc(1 * 3 * 4)  # float32
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_images = np.random.randn(1, 3, 224, 224).astype(np.float32)
            test_text = np.random.randn(1, 512).astype(np.float32)
            
            # ì›Œë°ì—…
            for _ in range(10):
                cuda.memcpy_htod(input_images, test_images)
                cuda.memcpy_htod(input_text, test_text)
                context.execute_v2(bindings=[int(input_images), int(input_text), int(output)])
            
            # ë²¤ì¹˜ë§ˆí¬
            times = []
            for i in range(num_runs):
                start_time = time.time()
                
                cuda.memcpy_htod(input_images, test_images)
                cuda.memcpy_htod(input_text, test_text)
                context.execute_v2(bindings=[int(input_images), int(input_text), int(output)])
                
                inference_time = time.time() - start_time
                times.append(inference_time)
                
                if (i + 1) % 20 == 0:
                    print(f"Progress: {i + 1}/{num_runs}")
            
            # ê²°ê³¼ ë¶„ì„
            avg_time = np.mean(times)
            std_time = np.std(times)
            fps = 1.0 / avg_time
            
            precision = "FP16" if "fp16" in engine_path else "INT8" if "int8" in engine_path else "FP32"
            
            results = {
                "framework": f"TensorRT {precision}",
                "average_time_ms": avg_time * 1000,
                "std_time_ms": std_time * 1000,
                "fps": fps,
                "num_runs": num_runs
            }
            
            print(f"ğŸ“Š TensorRT {precision} Results:")
            print(f"  Average: {avg_time*1000:.2f} ms")
            print(f"  Std Dev: {std_time*1000:.2f} ms")
            print(f"  FPS: {fps:.1f}")
            
            return results
            
        except Exception as e:
            print(f"âŒ TensorRT benchmark failed: {e}")
            return None
    
    def create_comparison_report(self, benchmark_results: list):
        """ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“Š Creating performance comparison report")
        
        report = {
            "model_info": {
                "name": "Kosmos2 + CLIP Hybrid",
                "performance": "MAE 0.212",
                "architecture": "Hybrid Vision-Language Model"
            },
            "benchmark_results": benchmark_results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # ì„±ëŠ¥ ë¹„êµ
        if len(benchmark_results) > 1:
            pytorch_result = next((r for r in benchmark_results if r["framework"] == "PyTorch"), None)
            
            if pytorch_result:
                pytorch_time = pytorch_result["average_time_ms"]
                
                for result in benchmark_results:
                    if result["framework"] != "PyTorch":
                        speedup = pytorch_time / result["average_time_ms"]
                        result["speedup_vs_pytorch"] = speedup
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(self.output_dir, "performance_comparison.json")
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… Performance report saved: {report_path}")
        
        # ì½˜ì†” ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ† PERFORMANCE COMPARISON REPORT")
        print("="*60)
        
        for result in benchmark_results:
            framework = result["framework"]
            avg_time = result["average_time_ms"]
            fps = result["fps"]
            speedup = result.get("speedup_vs_pytorch", 1.0)
            
            print(f"\nğŸ“Š {framework}:")
            print(f"  â±ï¸  Average Time: {avg_time:.2f} ms")
            print(f"  ğŸš€ FPS: {fps:.1f}")
            if speedup > 1.0:
                print(f"  âš¡ Speedup: {speedup:.2f}x")
        
        return report_path
    
    def create_ros_inference_node(self, engine_path: str):
        """ROS ì¶”ë¡  ë…¸ë“œ ìƒì„±"""
        print("ğŸ”§ Creating ROS inference node")
        
        node_code = f'''#!/usr/bin/env python3
"""
Best Model TensorRT ROS Inference Node
- Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212)
- TensorRT ê°€ì† ì¶”ë¡ 
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CompressedImage
from geometry_msgs.msg import Twist
from std_msgs.msg import String
import cv2
import numpy as np
from PIL import Image as PILImage
import json
import time
import os

# TensorRT import
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    print("Warning: TensorRT not available. Using mock inference.")
    TENSORRT_AVAILABLE = False

class BestModelTensorRTNode(Node):
    def __init__(self):
        super().__init__('best_model_tensorrt_node')
        
        # ëª¨ë¸ ì„¤ì •
        self.engine_path = self.declare_parameter('engine_path', '').value
        self.use_tensorrt = self.declare_parameter('use_tensorrt', True).value
        
        # TensorRT ì—”ì§„ ë¡œë“œ
        if self.use_tensorrt and TENSORRT_AVAILABLE:
            self.load_tensorrt_engine()
        else:
            self.get_logger().info("Using mock inference")
        
        # ROS ì„¤ì •
        self.setup_ros()
        
        # ìƒíƒœ ë³€ìˆ˜
        self.inference_count = 0
        self.last_inference_time = 0.0
        
    def load_tensorrt_engine(self):
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        if not self.engine_path or not os.path.exists(self.engine_path):
            self.get_logger().warn("No valid TensorRT engine path provided")
            return
        
        try:
            with open(self.engine_path, "rb") as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            self.engine = runtime.deserialize_cuda_engine(engine_data)
            self.context = self.engine.create_execution_context()
            
            # ë©”ëª¨ë¦¬ í• ë‹¹
            self.input_images = cuda.mem_alloc(1 * 3 * 224 * 224 * 4)  # float32
            self.input_text = cuda.mem_alloc(1 * 512 * 4)  # float32
            self.output = cuda.mem_alloc(1 * 3 * 4)  # float32
            
            self.get_logger().info(f"âœ… Best Model TensorRT engine loaded: {self.engine_path}")
            self.get_logger().info("ğŸ¯ Model: Kosmos2 + CLIP Hybrid (MAE 0.212)")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Failed to load TensorRT engine: {{e}}")
            self.use_tensorrt = False
    
    def setup_ros(self):
        """ROS ì„¤ì •"""
        # ì´ë¯¸ì§€ ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„
        self.image_sub = self.create_subscription(
            CompressedImage,
            '/camera/image/compressed',
            self.image_callback,
            10
        )
        
        # ì•¡ì…˜ í¼ë¸”ë¦¬ì…”
        self.action_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # ê²°ê³¼ í¼ë¸”ë¦¬ì…”
        self.result_pub = self.create_publisher(
            String,
            '/best_model/inference_result',
            10
        )
        
    def image_callback(self, msg):
        """ì´ë¯¸ì§€ ì½œë°±"""
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            np_arr = np.frombuffer(msg.data, np.uint8)
            image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # PIL Imageë¡œ ë³€í™˜ ë° ë¦¬ì‚¬ì´ì¦ˆ
            pil_image = PILImage.fromarray(image_rgb)
            pil_image = pil_image.resize((224, 224))
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image, dtype=np.float32) / 255.0
            image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
            image_array = np.expand_dims(image_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•„ìš”)
            text_embedding = np.random.randn(1, 512).astype(np.float32)
            
            # ì¶”ë¡  ì‹¤í–‰
            if self.use_tensorrt and TENSORRT_AVAILABLE:
                action = self.run_tensorrt_inference(image_array, text_embedding)
            else:
                action = self.run_mock_inference(image_array, text_embedding)
            
            # ì•¡ì…˜ ì‹¤í–‰
            self.execute_action(action)
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error in image callback: {{e}}")
    
    def run_tensorrt_inference(self, image_array, text_embedding):
        """TensorRT ì¶”ë¡  ì‹¤í–‰"""
        try:
            # GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ë³µì‚¬
            cuda.memcpy_htod(self.input_images, image_array)
            cuda.memcpy_htod(self.input_text, text_embedding)
            
            # ì¶”ë¡  ì‹¤í–‰
            start_time = time.time()
            self.context.execute_v2(bindings=[
                int(self.input_images),
                int(self.input_text),
                int(self.output)
            ])
            inference_time = time.time() - start_time
            
            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            result = np.empty((1, 3), dtype=np.float32)
            cuda.memcpy_dtoh(result, self.output)
            
            self.inference_count += 1
            self.last_inference_time = inference_time
            
            # ê²°ê³¼ ë°œí–‰
            self.publish_result(result[0], inference_time)
            
            return result[0]
            
        except Exception as e:
            self.get_logger().error(f"âŒ TensorRT inference error: {{e}}")
            return np.array([0.0, 0.0, 0.0])
    
    def run_mock_inference(self, image_array, text_embedding):
        """Mock ì¶”ë¡  (TensorRT ì—†ì„ ë•Œ)"""
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
        import math
        t = time.time()
        angle = (t * 0.5) % (2 * math.pi)
        
        linear_x = 0.1 * math.cos(angle)
        linear_y = 0.05 * math.sin(angle)
        angular_z = 0.2 * math.sin(angle * 2)
        
        action = np.array([linear_x, linear_y, angular_z], dtype=np.float32)
        
        self.inference_count += 1
        self.last_inference_time = 0.001  # 1ms ì‹œë®¬ë ˆì´ì…˜
        
        self.publish_result(action, 0.001)
        
        return action
    
    def execute_action(self, action):
        """ì•¡ì…˜ ì‹¤í–‰"""
        try:
            twist = Twist()
            twist.linear.x = float(action[0])
            twist.linear.y = float(action[1])
            twist.angular.z = float(action[2])
            
            self.action_pub.publish(twist)
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error executing action: {{e}}")
    
    def publish_result(self, action, inference_time):
        """ê²°ê³¼ ë°œí–‰"""
        try:
            result = {{
                "timestamp": time.time(),
                "inference_time": inference_time,
                "action": action.tolist(),
                "inference_count": self.inference_count,
                "model": "Kosmos2+CLIP_Hybrid_MAE0212",
                "engine": self.engine_path if hasattr(self, 'engine_path') else "mock"
            }}
            
            msg = String()
            msg.data = json.dumps(result)
            self.result_pub.publish(msg)
            
            self.get_logger().info(f"ğŸ† Best Model Inference #{self.inference_count}: {{inference_time*1000:.2f}}ms")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error publishing result: {{e}}")

def main(args=None):
    rclpy.init(args=args)
    node = BestModelTensorRTNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
'''
        
        # ë…¸ë“œ íŒŒì¼ ì €ì¥
        node_path = os.path.join(self.output_dir, "best_model_tensorrt_node.py")
        with open(node_path, "w") as f:
            f.write(node_code)
        
        print(f"âœ… ROS inference node created: {node_path}")
        return node_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting Best Model TensorRT Conversion")
    print("ğŸ¯ Target: Kosmos2 + CLIP Hybrid (MAE 0.212)")
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = BestModelTensorRTConverter()
    
    try:
        # ONNX ë³€í™˜
        print("\nğŸ”¨ Converting to ONNX...")
        onnx_path = converter.convert_to_onnx()
        
        if onnx_path:
            # PyTorch ë²¤ì¹˜ë§ˆí¬
            print("\nğŸ“ˆ Benchmarking PyTorch model...")
            pytorch_results = converter.benchmark_pytorch_model(num_runs=50)
            
            benchmark_results = [pytorch_results]
            
            # TensorRT ë³€í™˜ ë° ë²¤ì¹˜ë§ˆí¬
            if TENSORRT_AVAILABLE:
                # FP16 ë³€í™˜
                print("\nğŸ”¨ Converting to TensorRT FP16...")
                fp16_engine = converter.convert_to_tensorrt(onnx_path, "fp16")
                
                if fp16_engine:
                    print("\nğŸ“ˆ Benchmarking TensorRT FP16...")
                    fp16_results = converter.benchmark_tensorrt_engine(fp16_engine, num_runs=50)
                    if fp16_results:
                        benchmark_results.append(fp16_results)
                
                # INT8 ë³€í™˜ (ì„ íƒì )
                try:
                    print("\nğŸ”¨ Converting to TensorRT INT8...")
                    int8_engine = converter.convert_to_tensorrt(onnx_path, "int8")
                    
                    if int8_engine:
                        print("\nğŸ“ˆ Benchmarking TensorRT INT8...")
                        int8_results = converter.benchmark_tensorrt_engine(int8_engine, num_runs=50)
                        if int8_results:
                            benchmark_results.append(int8_results)
                except Exception as e:
                    print(f"âš ï¸ INT8 conversion failed: {e}")
            
            # ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
            print("\nğŸ“Š Creating performance comparison...")
            converter.create_comparison_report(benchmark_results)
            
            # ROS ì¶”ë¡  ë…¸ë“œ ìƒì„±
            if benchmark_results:
                best_engine = None
                for result in benchmark_results:
                    if "TensorRT" in result["framework"]:
                        best_engine = result.get("engine_path", None)
                        break
                
                if best_engine:
                    print("\nğŸ”§ Creating ROS inference node...")
                    converter.create_ros_inference_node(best_engine)
        
        print("\nâœ… Best Model TensorRT conversion completed!")
        print(f"\nğŸ“ Output directory: {converter.output_dir}")
        print("ğŸ”§ Next steps:")
        print("  1. Check performance_comparison.json for results")
        print("  2. Use the generated TensorRT engines in ROS")
        print("  3. Run: ros2 run mobile_vla_package best_model_tensorrt_node")
        
    except Exception as e:
        print(f"âŒ Best Model TensorRT conversion failed: {e}")
        raise

if __name__ == "__main__":
    main()
