#!/usr/bin/env python3
"""
Step 4: ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„
ì‹¤ì‹œê°„ ì¶”ë¡ , Jetson ìµœì í™”, Docker ì»¨í…Œì´ë„ˆ
"""

import torch
import torch.jit
import cv2
import numpy as np
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import threading
import queue
from dataclasses import dataclass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class InferenceConfig:
    """ì¶”ë¡  ì„¤ì •"""
    model_path: str
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size: int = 1
    max_fps: int = 10
    image_size: Tuple[int, int] = (224, 224)
    memory_limit_gb: float = 14.0
    use_torchscript: bool = True
    use_fp16: bool = True

class MobileVLAInference:
    """Mobile VLA ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.history_memory = []
        self.history_size = 8
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.inference_times = []
        self.memory_usage = []
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
        
        logger.info(f"ğŸš€ Mobile VLA ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - Device: {self.config.device}")
        logger.info(f"  - Batch Size: {self.config.batch_size}")
        logger.info(f"  - Max FPS: {self.config.max_fps}")
        logger.info(f"  - TorchScript: {self.config.use_torchscript}")
        logger.info(f"  - FP16: {self.config.use_fp16}")
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”"""
        try:
            # ëª¨ë¸ ë¡œë“œ
            if self.config.use_torchscript:
                self.model = torch.jit.load(self.config.model_path)
                logger.info("âœ… TorchScript ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                # ì¼ë°˜ PyTorch ëª¨ë¸ ë¡œë“œ
                checkpoint = torch.load(self.config.model_path, map_location=self.config.device)
                self.model = checkpoint['model']
                self.model.eval()
                logger.info("âœ… PyTorch ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model = self.model.to(self.config.device)
            
            # FP16 ìµœì í™”
            if self.config.use_fp16 and self.config.device == "cuda":
                self.model = self.model.half()
                logger.info("âœ… FP16 ìµœì í™” ì ìš©")
            
            # TorchScript ìµœì í™” (ì¶”ê°€)
            if self.config.use_torchscript and not isinstance(self.model, torch.jit.ScriptModule):
                self.model = torch.jit.optimize_for_inference(self.model)
                logger.info("âœ… TorchScript ì¶”ë¡  ìµœì í™” ì ìš©")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            self._check_memory_usage()
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _check_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
        if self.config.device == "cuda":
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            
            logger.info(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            logger.info(f"  - Allocated: {memory_allocated:.2f} GB")
            logger.info(f"  - Reserved: {memory_reserved:.2f} GB")
            
            if memory_allocated > self.config.memory_limit_gb:
                logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {memory_allocated:.2f} GB > {self.config.memory_limit_gb} GB")
        else:
            logger.info("ğŸ“Š CPU ëª¨ë“œì—ì„œ ì‹¤í–‰ ì¤‘")
    
    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # OpenCV ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜
        if len(image.shape) == 3 and image.shape[2] == 3:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ë¦¬ì‚¬ì´ì§•
        image = cv2.resize(image, self.config.image_size)
        
        # ì •ê·œí™” (ImageNet í‘œì¤€)
        image = image.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = (image - mean) / std
        
        # í…ì„œë¡œ ë³€í™˜
        image_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        image_tensor = image_tensor.to(self.config.device)
        
        # FP16 ë³€í™˜
        if self.config.use_fp16 and self.config.device == "cuda":
            image_tensor = image_tensor.half()
        
        return image_tensor
    
    def update_history(self, action: torch.Tensor):
        """íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.history_memory.append(action.cpu().numpy())
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.history_memory) > self.history_size:
            self.history_memory.pop(0)
    
    def predict_action(self, image: np.ndarray, text: str) -> Dict[str, np.ndarray]:
        """ì•¡ì…˜ ì˜ˆì¸¡"""
        start_time = time.time()
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image_tensor = self.preprocess_image(image)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                if self.config.use_torchscript:
                    # TorchScript ëª¨ë¸ ì¶”ë¡ 
                    action_tensor = self.model(image_tensor, text)
                else:
                    # ì¼ë°˜ ëª¨ë¸ ì¶”ë¡ 
                    action_tensor = self.model.get_action(image_tensor, text)
            
            # ì•¡ì…˜ í›„ì²˜ë¦¬
            action = action_tensor.cpu().numpy().squeeze()
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.update_history(action_tensor)
            
            # ì¶”ë¡  ì‹œê°„ ê¸°ë¡
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            
            # FPS ì œí•œ
            target_time = 1.0 / self.config.max_fps
            if inference_time < target_time:
                time.sleep(target_time - inference_time)
            
            return {
                "action": action,
                "inference_time": inference_time,
                "fps": 1.0 / inference_time,
                "history_length": len(self.history_memory)
            }
            
        except Exception as e:
            logger.error(f"âŒ ì•¡ì…˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {
                "action": np.zeros(3),
                "inference_time": 0.0,
                "fps": 0.0,
                "history_length": len(self.history_memory),
                "error": str(e)
            }
    
    def get_performance_stats(self) -> Dict[str, float]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if not self.inference_times:
            return {"avg_fps": 0.0, "avg_inference_time": 0.0}
        
        avg_inference_time = np.mean(self.inference_times)
        avg_fps = 1.0 / avg_inference_time if avg_inference_time > 0 else 0.0
        
        return {
            "avg_fps": avg_fps,
            "avg_inference_time": avg_inference_time,
            "min_inference_time": np.min(self.inference_times),
            "max_inference_time": np.max(self.inference_times),
            "total_inferences": len(self.inference_times)
        }

class RealTimeInferenceServer:
    """ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë²„"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.inference_engine = MobileVLAInference(config)
        self.is_running = False
        self.image_queue = queue.Queue(maxsize=10)
        self.action_queue = queue.Queue(maxsize=10)
        
        # ìŠ¤ë ˆë“œ
        self.inference_thread = None
        self.camera_thread = None
        
        logger.info("ğŸš€ ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def start_camera(self, camera_id: int = 0):
        """ì¹´ë©”ë¼ ì‹œì‘"""
        def camera_worker():
            cap = cv2.VideoCapture(camera_id)
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_FPS, 30)
            
            while self.is_running:
                ret, frame = cap.read()
                if ret:
                    try:
                        self.image_queue.put_nowait(frame)
                    except queue.Full:
                        # íê°€ ê°€ë“ ì°¬ ê²½ìš° ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°
                        try:
                            self.image_queue.get_nowait()
                            self.image_queue.put_nowait(frame)
                        except queue.Empty:
                            pass
                else:
                    logger.warning("ì¹´ë©”ë¼ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                    time.sleep(0.1)
            
            cap.release()
            logger.info("ì¹´ë©”ë¼ ì¢…ë£Œ")
        
        self.camera_thread = threading.Thread(target=camera_worker)
        self.camera_thread.start()
        logger.info(f"ì¹´ë©”ë¼ ì‹œì‘ (ID: {camera_id})")
    
    def start_inference(self, text_command: str = "go to the object"):
        """ì¶”ë¡  ì‹œì‘"""
        def inference_worker():
            while self.is_running:
                try:
                    # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
                    image = self.image_queue.get(timeout=1.0)
                    
                    # ì•¡ì…˜ ì˜ˆì¸¡
                    result = self.inference_engine.predict_action(image, text_command)
                    
                    # ê²°ê³¼ ì €ì¥
                    self.action_queue.put_nowait(result)
                    
                except queue.Empty:
                    continue
                except Exception as e:
                    logger.error(f"ì¶”ë¡  ì˜¤ë¥˜: {e}")
                    time.sleep(0.1)
        
        self.inference_thread = threading.Thread(target=inference_worker)
        self.inference_thread.start()
        logger.info(f"ì¶”ë¡  ì‹œì‘ (ëª…ë ¹: {text_command})")
    
    def start(self, camera_id: int = 0, text_command: str = "go to the object"):
        """ì„œë²„ ì‹œì‘"""
        self.is_running = True
        
        # ì¹´ë©”ë¼ ì‹œì‘
        self.start_camera(camera_id)
        
        # ì¶”ë¡  ì‹œì‘
        self.start_inference(text_command)
        
        logger.info("ğŸš€ ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë²„ ì‹œì‘")
    
    def stop(self):
        """ì„œë²„ ì¤‘ì§€"""
        self.is_running = False
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.camera_thread:
            self.camera_thread.join()
        if self.inference_thread:
            self.inference_thread.join()
        
        logger.info("ğŸ›‘ ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë²„ ì¤‘ì§€")
    
    def get_latest_action(self) -> Optional[Dict]:
        """ìµœì‹  ì•¡ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.action_queue.get_nowait()
        except queue.Empty:
            return None
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ê°€ì ¸ì˜¤ê¸°"""
        return self.inference_engine.get_performance_stats()

class JetsonOptimizer:
    """Jetson ìµœì í™” ë„êµ¬"""
    
    @staticmethod
    def optimize_for_jetson(model: torch.nn.Module) -> torch.nn.Module:
        """Jetsonìš© ëª¨ë¸ ìµœì í™”"""
        logger.info("ğŸ”§ Jetson ìµœì í™” ì ìš© ì¤‘...")
        
        # 1. FP16 ë³€í™˜
        if torch.cuda.is_available():
            model = model.half()
            logger.info("âœ… FP16 ë³€í™˜ ì™„ë£Œ")
        
        # 2. TorchScript ìµœì í™”
        model = torch.jit.script(model)
        model = torch.jit.optimize_for_inference(model)
        logger.info("âœ… TorchScript ìµœì í™” ì™„ë£Œ")
        
        # 3. ë©”ëª¨ë¦¬ ìµœì í™”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(0.9)
            logger.info("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")
        
        return model
    
    @staticmethod
    def benchmark_model(model: torch.nn.Module, input_shape: Tuple[int, ...], num_runs: int = 100) -> Dict[str, float]:
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        logger.info(f"ğŸ“Š ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({num_runs}íšŒ ì‹¤í–‰)")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, *input_shape)
        if torch.cuda.is_available():
            dummy_input = dummy_input.cuda()
            if hasattr(model, 'half'):
                dummy_input = dummy_input.half()
        
        # ì›Œë°ì—…
        for _ in range(10):
            with torch.no_grad():
                _ = model(dummy_input)
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        times = []
        for _ in range(num_runs):
            start_time = time.time()
            with torch.no_grad():
                _ = model(dummy_input)
            times.append(time.time() - start_time)
        
        # í†µê³„ ê³„ì‚°
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        fps = 1.0 / avg_time
        
        results = {
            "avg_inference_time": avg_time,
            "std_inference_time": std_time,
            "min_inference_time": min_time,
            "max_inference_time": max_time,
            "fps": fps
        }
        
        logger.info(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        logger.info(f"  - í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.2f}ms")
        logger.info(f"  - FPS: {fps:.2f}")
        logger.info(f"  - ìµœì†Œ ì‹œê°„: {min_time*1000:.2f}ms")
        logger.info(f"  - ìµœëŒ€ ì‹œê°„: {max_time*1000:.2f}ms")
        
        return results

def test_inference_system():
    """ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª Mobile VLA ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ì„¤ì •
        config = InferenceConfig(
            model_path="test_model.pth",
            device="cpu",  # í…ŒìŠ¤íŠ¸ìš© CPU
            batch_size=1,
            max_fps=10,
            use_torchscript=False,
            use_fp16=False
        )
        
        # ë”ë¯¸ ëª¨ë¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        class DummyModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(3*224*224, 3)
            
            def forward(self, x, text):
                x = x.view(x.size(0), -1)
                return self.linear(x)
        
        dummy_model = DummyModel()
        torch.save(dummy_model.state_dict(), "test_model.pth")
        
        # ì¶”ë¡  ì—”ì§„ ìƒì„±
        inference_engine = MobileVLAInference(config)
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        test_text = "go to the red box"
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        logger.info("ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
        result = inference_engine.predict_action(test_image, test_text)
        
        logger.info(f"âœ… ì•¡ì…˜ ì˜ˆì¸¡ ì„±ê³µ:")
        logger.info(f"  - Action: {result['action']}")
        logger.info(f"  - Inference Time: {result['inference_time']:.4f}s")
        logger.info(f"  - FPS: {result['fps']:.2f}")
        
        # ì„±ëŠ¥ í†µê³„
        stats = inference_engine.get_performance_stats()
        logger.info(f"ğŸ“Š ì„±ëŠ¥ í†µê³„: {stats}")
        
        # ì •ë¦¬
        Path("test_model.pth").unlink()
        
        logger.info("âœ… ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Mobile VLA ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„ ì‹œì‘")
    
    # ì¶”ë¡  ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = test_inference_system()
    
    if success:
        logger.info("âœ… Mobile VLA ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ")
        logger.info("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„")
    else:
        logger.error("âŒ Mobile VLA ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬í˜„ ì‹¤íŒ¨")
        logger.error("ğŸ”§ ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")

if __name__ == "__main__":
    main()
