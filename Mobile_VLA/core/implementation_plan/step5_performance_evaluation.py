#!/usr/bin/env python3
"""
Step 5: ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„
ë²¤ì¹˜ë§ˆí¬, ë©”íŠ¸ë¦­ ì¸¡ì •, ì–‘ìí™” í…ŒìŠ¤íŠ¸
"""

import torch
import torch.nn as nn
import numpy as np
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
import psutil
import GPUtil

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationConfig:
    """í‰ê°€ ì„¤ì •"""
    model_path: str
    test_data_path: str
    output_dir: str = "evaluation_results"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch_sizes: List[int] = None
    num_test_samples: int = 1000
    warmup_runs: int = 10
    benchmark_runs: int = 100

class PerformanceEvaluator:
    """ì„±ëŠ¥ í‰ê°€ê¸°"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥
        self.results = {}
        
        logger.info(f"ğŸš€ ì„±ëŠ¥ í‰ê°€ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - Device: {config.device}")
        logger.info(f"  - Output Dir: {config.output_dir}")
    
    def evaluate_inference_speed(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, float]:
        """ì¶”ë¡  ì†ë„ í‰ê°€"""
        logger.info("ğŸ“Š ì¶”ë¡  ì†ë„ í‰ê°€ ì‹œì‘")
        
        model.eval()
        inference_times = []
        
        with torch.no_grad():
            # ì›Œë°ì—…
            for i in range(self.config.warmup_runs):
                sample = test_data[i % len(test_data)]
                image = sample["image"]
                text = sample["text"]
                
                if isinstance(image, np.ndarray):
                    image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
                
                image = image.to(self.config.device)
                
                # ëª¨ë¸ ì¶”ë¡ 
                start_time = time.time()
                _ = model(image, text)
                inference_times.append(time.time() - start_time)
            
            # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
            for i in range(self.config.benchmark_runs):
                sample = test_data[i % len(test_data)]
                image = sample["image"]
                text = sample["text"]
                
                if isinstance(image, np.ndarray):
                    image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
                
                image = image.to(self.config.device)
                
                # ëª¨ë¸ ì¶”ë¡ 
                start_time = time.time()
                _ = model(image, text)
                inference_times.append(time.time() - start_time)
        
        # í†µê³„ ê³„ì‚°
        avg_time = np.mean(inference_times)
        std_time = np.std(inference_times)
        min_time = np.min(inference_times)
        max_time = np.max(inference_times)
        fps = 1.0 / avg_time
        
        results = {
            "avg_inference_time": avg_time,
            "std_inference_time": std_time,
            "min_inference_time": min_time,
            "max_inference_time": max_time,
            "fps": fps,
            "total_runs": len(inference_times)
        }
        
        logger.info(f"âœ… ì¶”ë¡  ì†ë„ í‰ê°€ ì™„ë£Œ:")
        logger.info(f"  - í‰ê·  ì‹œê°„: {avg_time*1000:.2f}ms")
        logger.info(f"  - FPS: {fps:.2f}")
        logger.info(f"  - í‘œì¤€í¸ì°¨: {std_time*1000:.2f}ms")
        
        return results
    
    def evaluate_memory_usage(self, model: nn.Module) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‰ê°€"""
        logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‰ê°€ ì‹œì‘")
        
        # CPU ë©”ëª¨ë¦¬
        cpu_memory = psutil.virtual_memory()
        cpu_usage = cpu_memory.percent
        
        # GPU ë©”ëª¨ë¦¬
        gpu_usage = 0.0
        gpu_memory_allocated = 0.0
        gpu_memory_reserved = 0.0
        
        if torch.cuda.is_available():
            gpu = GPUtil.getGPUs()[0] if GPUtil.getGPUs() else None
            if gpu:
                gpu_usage = gpu.memoryUtil * 100
            
            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        results = {
            "cpu_memory_usage_percent": cpu_usage,
            "gpu_memory_usage_percent": gpu_usage,
            "gpu_memory_allocated_gb": gpu_memory_allocated,
            "gpu_memory_reserved_gb": gpu_memory_reserved,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / 1024**2  # 4 bytes per float32
        }
        
        logger.info(f"âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‰ê°€ ì™„ë£Œ:")
        logger.info(f"  - CPU ì‚¬ìš©ë¥ : {cpu_usage:.1f}%")
        logger.info(f"  - GPU ì‚¬ìš©ë¥ : {gpu_usage:.1f}%")
        logger.info(f"  - GPU í• ë‹¹: {gpu_memory_allocated:.2f}GB")
        logger.info(f"  - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        
        return results
    
    def evaluate_accuracy(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, float]:
        """ì •í™•ë„ í‰ê°€"""
        logger.info("ğŸ“Š ì •í™•ë„ í‰ê°€ ì‹œì‘")
        
        model.eval()
        correct_predictions = 0
        total_predictions = 0
        movement_errors = []
        gripper_errors = []
        
        with torch.no_grad():
            for sample in test_data:
                image = sample["image"]
                text = sample["text"]
                target_action = sample["target_action"]
                
                if isinstance(image, np.ndarray):
                    image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
                
                image = image.to(self.config.device)
                
                # ëª¨ë¸ ì˜ˆì¸¡
                predicted_action = model(image, text)
                
                if isinstance(predicted_action, dict):
                    predicted_action = predicted_action["action"]
                
                predicted_action = predicted_action.cpu().numpy()
                target_action = np.array(target_action)
                
                # Movement ì •í™•ë„ (MSE)
                movement_error = np.mean((predicted_action[:2] - target_action[:2])**2)
                movement_errors.append(movement_error)
                
                # Gripper ì •í™•ë„ (Binary)
                gripper_pred = 1 if predicted_action[2] > 0.5 else 0
                gripper_target = int(target_action[2])
                gripper_error = 1 if gripper_pred == gripper_target else 0
                gripper_errors.append(gripper_error)
                
                total_predictions += 1
        
        # ì •í™•ë„ ê³„ì‚°
        movement_accuracy = 1.0 - np.mean(movement_errors)
        gripper_accuracy = np.mean(gripper_errors)
        overall_accuracy = (movement_accuracy + gripper_accuracy) / 2
        
        results = {
            "overall_accuracy": overall_accuracy,
            "movement_accuracy": movement_accuracy,
            "gripper_accuracy": gripper_accuracy,
            "movement_mse": np.mean(movement_errors),
            "gripper_error_rate": 1.0 - gripper_accuracy,
            "total_samples": total_predictions
        }
        
        logger.info(f"âœ… ì •í™•ë„ í‰ê°€ ì™„ë£Œ:")
        logger.info(f"  - ì „ì²´ ì •í™•ë„: {overall_accuracy:.4f}")
        logger.info(f"  - Movement ì •í™•ë„: {movement_accuracy:.4f}")
        logger.info(f"  - Gripper ì •í™•ë„: {gripper_accuracy:.4f}")
        
        return results
    
    def evaluate_quantization(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, Dict]:
        """ì–‘ìí™” í‰ê°€"""
        logger.info("ğŸ“Š ì–‘ìí™” í‰ê°€ ì‹œì‘")
        
        results = {}
        
        # FP32 (ì›ë³¸)
        fp32_results = self._evaluate_model_precision(model, test_data, "FP32")
        results["fp32"] = fp32_results
        
        # FP16
        if torch.cuda.is_available():
            fp16_model = model.half()
            fp16_results = self._evaluate_model_precision(fp16_model, test_data, "FP16")
            results["fp16"] = fp16_results
        
        # INT8 (ì‹œë®¬ë ˆì´ì…˜)
        int8_results = self._simulate_int8_evaluation(model, test_data)
        results["int8"] = int8_results
        
        logger.info(f"âœ… ì–‘ìí™” í‰ê°€ ì™„ë£Œ")
        return results
    
    def _evaluate_model_precision(self, model: nn.Module, test_data: List[Dict], precision: str) -> Dict[str, float]:
        """íŠ¹ì • ì •ë°€ë„ë¡œ ëª¨ë¸ í‰ê°€"""
        model.eval()
        inference_times = []
        
        with torch.no_grad():
            for i in range(min(100, len(test_data))):
                sample = test_data[i]
                image = sample["image"]
                text = sample["text"]
                
                if isinstance(image, np.ndarray):
                    image = torch.from_numpy(image).float()
                if len(image.shape) == 3:
                    image = image.unsqueeze(0)
                
                image = image.to(self.config.device)
                if precision == "FP16":
                    image = image.half()
                
                start_time = time.time()
                _ = model(image, text)
                inference_times.append(time.time() - start_time)
        
        avg_time = np.mean(inference_times)
        fps = 1.0 / avg_time
        
        return {
            "avg_inference_time": avg_time,
            "fps": fps,
            "precision": precision
        }
    
    def _simulate_int8_evaluation(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, float]:
        """INT8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜"""
        # ì‹¤ì œ INT8 ì–‘ìí™”ëŠ” TensorRTë‚˜ ONNXë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì†ë„ í–¥ìƒë§Œ ê³„ì‚°
        fp32_time = 0.1  # ê°€ì •ëœ FP32 ì‹œê°„
        int8_speedup = 2.0  # INT8 ì†ë„ í–¥ìƒ ë°°ìˆ˜
        
        return {
            "avg_inference_time": fp32_time / int8_speedup,
            "fps": 1.0 / (fp32_time / int8_speedup),
            "precision": "INT8",
            "speedup": int8_speedup
        }
    
    def run_full_evaluation(self, model: nn.Module, test_data: List[Dict]) -> Dict[str, Dict]:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        logger.info("ğŸš€ ì „ì²´ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        # 1. ì¶”ë¡  ì†ë„ í‰ê°€
        speed_results = self.evaluate_inference_speed(model, test_data)
        self.results["speed"] = speed_results
        
        # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í‰ê°€
        memory_results = self.evaluate_memory_usage(model)
        self.results["memory"] = memory_results
        
        # 3. ì •í™•ë„ í‰ê°€
        accuracy_results = self.evaluate_accuracy(model, test_data)
        self.results["accuracy"] = accuracy_results
        
        # 4. ì–‘ìí™” í‰ê°€
        quantization_results = self.evaluate_quantization(model, test_data)
        self.results["quantization"] = quantization_results
        
        # ê²°ê³¼ ì €ì¥
        self._save_results()
        
        # ì‹œê°í™”
        self._create_visualizations()
        
        logger.info("ğŸ‰ ì „ì²´ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!")
        return self.results
    
    def _save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        results_file = self.output_dir / "evaluation_results.json"
        
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥: {results_file}")
    
    def _create_visualizations(self):
        """ì‹œê°í™” ìƒì„±"""
        # 1. ì¶”ë¡  ì†ë„ ë¶„í¬
        if "speed" in self.results:
            plt.figure(figsize=(10, 6))
            plt.subplot(1, 2, 1)
            plt.bar(["í‰ê· ", "ìµœì†Œ", "ìµœëŒ€"], 
                   [self.results["speed"]["avg_inference_time"]*1000,
                    self.results["speed"]["min_inference_time"]*1000,
                    self.results["speed"]["max_inference_time"]*1000])
            plt.title("ì¶”ë¡  ì‹œê°„ ë¶„í¬ (ms)")
            plt.ylabel("ì‹œê°„ (ms)")
            
            plt.subplot(1, 2, 2)
            plt.bar(["FPS"], [self.results["speed"]["fps"]])
            plt.title("FPS")
            plt.ylabel("FPS")
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "inference_speed.png", dpi=300)
            plt.close()
        
        # 2. ì •í™•ë„ ë¹„êµ
        if "accuracy" in self.results:
            plt.figure(figsize=(8, 6))
            categories = ["ì „ì²´", "Movement", "Gripper"]
            values = [
                self.results["accuracy"]["overall_accuracy"],
                self.results["accuracy"]["movement_accuracy"],
                self.results["accuracy"]["gripper_accuracy"]
            ]
            
            plt.bar(categories, values)
            plt.title("ì •í™•ë„ ë¹„êµ")
            plt.ylabel("ì •í™•ë„")
            plt.ylim(0, 1)
            
            for i, v in enumerate(values):
                plt.text(i, v + 0.01, f"{v:.3f}", ha='center')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "accuracy_comparison.png", dpi=300)
            plt.close()
        
        # 3. ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ
        if "quantization" in self.results:
            plt.figure(figsize=(10, 6))
            
            precisions = list(self.results["quantization"].keys())
            fps_values = [self.results["quantization"][p]["fps"] for p in precisions]
            
            plt.bar(precisions, fps_values)
            plt.title("ì–‘ìí™”ë³„ FPS ë¹„êµ")
            plt.ylabel("FPS")
            
            for i, v in enumerate(fps_values):
                plt.text(i, v + 0.1, f"{v:.1f}", ha='center')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / "quantization_comparison.png", dpi=300)
            plt.close()
        
        logger.info(f"ğŸ“Š ì‹œê°í™” ì €ì¥: {self.output_dir}")

def create_test_data(num_samples: int = 100) -> List[Dict]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘ ({num_samples}ê°œ ìƒ˜í”Œ)")
    
    test_data = []
    for i in range(num_samples):
        # ëœë¤ ì´ë¯¸ì§€ ìƒì„±
        image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        
        # ëœë¤ í…ìŠ¤íŠ¸ ëª…ë ¹
        commands = [
            "go to the red box",
            "pick up the object",
            "move to the corner",
            "navigate to the table",
            "go around the obstacle"
        ]
        text = np.random.choice(commands)
        
        # ëœë¤ íƒ€ê²Ÿ ì•¡ì…˜
        target_action = [
            np.random.uniform(-1, 1),  # X
            np.random.uniform(-1, 1),  # Y
            np.random.randint(0, 2)    # Gripper
        ]
        
        test_data.append({
            "image": image,
            "text": text,
            "target_action": target_action
        })
    
    logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    return test_data

def test_performance_evaluation():
    """ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª Mobile VLA ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ë”ë¯¸ ëª¨ë¸ ìƒì„±
        class DummyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(3*224*224, 3)
            
            def forward(self, x, text):
                x = x.view(x.size(0), -1)
                return self.linear(x)
        
        model = DummyModel()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_data = create_test_data(100)
        
        # í‰ê°€ ì„¤ì •
        config = EvaluationConfig(
            model_path="dummy_model.pth",
            test_data_path="test_data",
            output_dir="test_evaluation_results"
        )
        
        # í‰ê°€ê¸° ìƒì„±
        evaluator = PerformanceEvaluator(config)
        
        # ì „ì²´ í‰ê°€ ì‹¤í–‰
        results = evaluator.run_full_evaluation(model, test_data)
        
        logger.info("âœ… ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
        logger.info(f"  - FPS: {results['speed']['fps']:.2f}")
        logger.info(f"  - ì •í™•ë„: {results['accuracy']['overall_accuracy']:.4f}")
        logger.info(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['memory']['cpu_memory_usage_percent']:.1f}%")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Mobile VLA ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„ ì‹œì‘")
    
    # ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = test_performance_evaluation()
    
    if success:
        logger.info("âœ… Mobile VLA ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ")
        logger.info("ğŸ‰ ëª¨ë“  êµ¬í˜„ ë‹¨ê³„ ì™„ë£Œ!")
    else:
        logger.error("âŒ Mobile VLA ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„ ì‹¤íŒ¨")
        logger.error("ğŸ”§ ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")

if __name__ == "__main__":
    main()
