#!/usr/bin/env python3
"""
Step 2: Mobile VLA ëª¨ë¸ êµ¬ì¡° êµ¬í˜„
2D ì•¡ì…˜ ê³µê°„, LoRA Fine-tuning, LSTM Policy Head
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType
import logging
from typing import Dict, Tuple, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MobileVLAModel(nn.Module):
    """
    Mobile VLA ëª¨ë¸ êµ¬ì¡°
    - 2D ì•¡ì…˜ ê³µê°„ (X, Y, Gripper)
    - LoRA Fine-tuning
    - LSTM Policy Head
    """
    
    def __init__(
        self,
        vlm_model_name: str = "microsoft/kosmos-2-patch14-224",
        action_dim: int = 3,  # X, Y, Gripper
        hidden_size: int = 512,
        lstm_layers: int = 2,
        lora_r: int = 32,
        lora_alpha: int = 16,
        lora_dropout: float = 0.1
    ):
        super().__init__()
        
        self.action_dim = action_dim
        self.hidden_size = hidden_size
        self.lstm_layers = lstm_layers
        
        # VLM ë°±ë³¸ ë¡œë“œ
        self.vlm_model = AutoModel.from_pretrained(vlm_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(vlm_model_name)
        
        # LoRA ì„¤ì •
        self.lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=["query", "value", "key", "dense"]
        )
        
        # VLMì— LoRA ì ìš©
        self.vlm_model = get_peft_model(self.vlm_model, self.lora_config)
        
        # VLM ì¶œë ¥ ì°¨ì›
        self.vlm_output_dim = self.vlm_model.config.hidden_size
        
        # [LRN] í† í° (í•™ìŠµ ê°€ëŠ¥í•œ ì•¡ì…˜ í† í°)
        self.action_token = nn.Parameter(torch.zeros(self.vlm_output_dim))
        
        # LSTM Policy Head
        self.lstm = nn.LSTM(
            input_size=self.vlm_output_dim,
            hidden_size=hidden_size,
            num_layers=lstm_layers,
            batch_first=True,
            dropout=0.1 if lstm_layers > 1 else 0
        )
        
        # Action Head (2D + Gripper)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, action_dim)
        )
        
        # Gripper Head (Binary Classification)
        self.gripper_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )
        
        logger.info(f"Mobile VLA ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"- VLM: {vlm_model_name}")
        logger.info(f"- ì•¡ì…˜ ì°¨ì›: {action_dim}")
        logger.info(f"- LSTM Hidden Size: {hidden_size}")
        logger.info(f"- LoRA r: {lora_r}")
    
    def forward(
        self,
        images: torch.Tensor,
        text: str,
        return_dict: bool = True
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass
        
        Args:
            images: [batch_size, channels, height, width]
            text: ìì—°ì–´ ëª…ë ¹
            return_dict: ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í• ì§€ ì—¬ë¶€
        
        Returns:
            Dict containing:
                - action_logits: [batch_size, action_dim]
                - gripper_logits: [batch_size, 1]
                - vlm_outputs: VLM ì¶œë ¥
        """
        batch_size = images.shape[0]
        
        # 1. í…ìŠ¤íŠ¸ í† í°í™”
        text_inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # 2. VLM Forward Pass
        vlm_outputs = self.vlm_model(
            input_ids=text_inputs.input_ids,
            attention_mask=text_inputs.attention_mask,
            pixel_values=images
        )
        
        # 3. [LRN] í† í° ì¶”ê°€
        # VLM ì¶œë ¥ì˜ ë§ˆì§€ë§‰ í† í°ì— [LRN] í† í° ì¶”ê°€
        last_hidden_states = vlm_outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        
        # [LRN] í† í°ì„ ë°°ì¹˜ë³„ë¡œ ë³µì œ
        action_tokens = self.action_token.unsqueeze(0).unsqueeze(0).expand(
            batch_size, 1, -1
        )  # [batch_size, 1, hidden_size]
        
        # [LRN] í† í°ì„ ì‹œí€€ìŠ¤ ëì— ì¶”ê°€
        lstm_input = torch.cat([last_hidden_states, action_tokens], dim=1)
        
        # 4. LSTM Forward Pass
        lstm_output, (h_n, c_n) = self.lstm(lstm_input)
        
        # ë§ˆì§€ë§‰ LSTM ì¶œë ¥ ì‚¬ìš© (ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ í† í°)
        last_lstm_output = lstm_output[:, -1, :]  # [batch_size, hidden_size]
        
        # 5. Action ì˜ˆì¸¡
        action_logits = self.action_head(last_lstm_output)  # [batch_size, action_dim]
        gripper_logits = self.gripper_head(last_lstm_output)  # [batch_size, 1]
        
        if return_dict:
            return {
                "action_logits": action_logits,
                "gripper_logits": gripper_logits,
                "vlm_outputs": vlm_outputs,
                "lstm_output": lstm_output,
                "hidden_states": (h_n, c_n)
            }
        else:
            return action_logits, gripper_logits
    
    def get_action(self, images: torch.Tensor, text: str) -> torch.Tensor:
        """
        ì•¡ì…˜ ì˜ˆì¸¡ (ì¶”ë¡ ìš©)
        
        Args:
            images: [batch_size, channels, height, width]
            text: ìì—°ì–´ ëª…ë ¹
        
        Returns:
            actions: [batch_size, action_dim] (X, Y, Gripper)
        """
        with torch.no_grad():
            outputs = self.forward(images, text)
            
            # 2D Movement (X, Y) - Tanh í™œì„±í™”
            movement = torch.tanh(outputs["action_logits"][:, :2])
            
            # Gripper - Binary (0 or 1)
            gripper = (outputs["gripper_logits"] > 0.5).float()
            
            # ì•¡ì…˜ ê²°í•©
            actions = torch.cat([movement, gripper], dim=1)
            
            return actions
    
    def get_model_size(self) -> Dict[str, int]:
        """ëª¨ë¸ í¬ê¸° ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "vlm_parameters": sum(p.numel() for p in self.vlm_model.parameters()),
            "lstm_parameters": sum(p.numel() for p in self.lstm.parameters()),
            "action_head_parameters": sum(p.numel() for p in self.action_head.parameters()),
            "gripper_head_parameters": sum(p.numel() for p in self.gripper_head.parameters())
        }

class MobileVLALoss(nn.Module):
    """Mobile VLA Loss í•¨ìˆ˜"""
    
    def __init__(self, movement_weight: float = 1.0, gripper_weight: float = 0.1):
        super().__init__()
        self.movement_weight = movement_weight
        self.gripper_weight = gripper_weight
        
        # MSE Loss for 2D movement
        self.movement_loss = nn.MSELoss()
        
        # BCE Loss for gripper
        self.gripper_loss = nn.BCELoss()
    
    def forward(
        self,
        predictions: Dict[str, torch.Tensor],
        targets: Dict[str, torch.Tensor]
    ) -> Dict[str, torch.Tensor]:
        """
        Loss ê³„ì‚°
        
        Args:
            predictions: ëª¨ë¸ ì˜ˆì¸¡ê°’
            targets: ì •ë‹µê°’
        
        Returns:
            Dict containing losses
        """
        # 2D Movement Loss (MSE)
        movement_loss = self.movement_loss(
            predictions["action_logits"][:, :2],  # X, Y
            targets["movement_targets"]  # [batch_size, 2]
        )
        
        # Gripper Loss (BCE)
        gripper_loss = self.gripper_loss(
            predictions["gripper_logits"].squeeze(-1),  # [batch_size]
            targets["gripper_targets"]  # [batch_size]
        )
        
        # Total Loss
        total_loss = (
            self.movement_weight * movement_loss +
            self.gripper_weight * gripper_loss
        )
        
        return {
            "total_loss": total_loss,
            "movement_loss": movement_loss,
            "gripper_loss": gripper_loss
        }

def create_mobile_vla_model(
    vlm_model_name: str = "microsoft/kosmos-2-patch14-224",
    action_dim: int = 3,
    hidden_size: int = 512,
    lstm_layers: int = 2,
    lora_r: int = 32,
    lora_alpha: int = 16,
    lora_dropout: float = 0.1
) -> Tuple[MobileVLAModel, MobileVLALoss]:
    """
    Mobile VLA ëª¨ë¸ê³¼ Loss í•¨ìˆ˜ ìƒì„±
    
    Args:
        vlm_model_name: VLM ëª¨ë¸ëª…
        action_dim: ì•¡ì…˜ ì°¨ì› (ê¸°ë³¸ 3: X, Y, Gripper)
        hidden_size: LSTM hidden size
        lstm_layers: LSTM ë ˆì´ì–´ ìˆ˜
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
    
    Returns:
        Tuple of (model, loss_function)
    """
    logger.info("ğŸš€ Mobile VLA ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # ëª¨ë¸ ìƒì„±
    model = MobileVLAModel(
        vlm_model_name=vlm_model_name,
        action_dim=action_dim,
        hidden_size=hidden_size,
        lstm_layers=lstm_layers,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout
    )
    
    # Loss í•¨ìˆ˜ ìƒì„±
    loss_fn = MobileVLALoss()
    
    # ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶œë ¥
    size_info = model.get_model_size()
    logger.info("ğŸ“Š ëª¨ë¸ í¬ê¸° ì •ë³´:")
    for key, value in size_info.items():
        logger.info(f"  {key}: {value:,}")
    
    logger.info("âœ… Mobile VLA ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    return model, loss_fn

def test_mobile_vla_model():
    """Mobile VLA ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª Mobile VLA ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ëª¨ë¸ ìƒì„±
        model, loss_fn = create_mobile_vla_model()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        batch_size = 2
        images = torch.randn(batch_size, 3, 224, 224)
        text = "go to the red box"
        
        # Forward pass
        logger.info("Forward pass í…ŒìŠ¤íŠ¸...")
        outputs = model(images, text)
        
        logger.info(f"âœ… Forward pass ì„±ê³µ")
        logger.info(f"  - action_logits shape: {outputs['action_logits'].shape}")
        logger.info(f"  - gripper_logits shape: {outputs['gripper_logits'].shape}")
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        logger.info("ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
        actions = model.get_action(images, text)
        logger.info(f"âœ… ì•¡ì…˜ ì˜ˆì¸¡ ì„±ê³µ: {actions.shape}")
        
        # Loss ê³„ì‚° í…ŒìŠ¤íŠ¸
        logger.info("Loss ê³„ì‚° í…ŒìŠ¤íŠ¸...")
        targets = {
            "movement_targets": torch.randn(batch_size, 2),
            "gripper_targets": torch.randint(0, 2, (batch_size,)).float()
        }
        
        losses = loss_fn(outputs, targets)
        logger.info(f"âœ… Loss ê³„ì‚° ì„±ê³µ")
        logger.info(f"  - total_loss: {losses['total_loss'].item():.4f}")
        logger.info(f"  - movement_loss: {losses['movement_loss'].item():.4f}")
        logger.info(f"  - gripper_loss: {losses['gripper_loss'].item():.4f}")
        
        logger.info("ğŸ‰ Mobile VLA ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ Mobile VLA ëª¨ë¸ êµ¬ì¡° êµ¬í˜„ ì‹œì‘")
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = test_mobile_vla_model()
    
    if success:
        logger.info("âœ… Mobile VLA ëª¨ë¸ êµ¬ì¡° êµ¬í˜„ ì™„ë£Œ")
        logger.info("ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„")
    else:
        logger.error("âŒ Mobile VLA ëª¨ë¸ êµ¬ì¡° êµ¬í˜„ ì‹¤íŒ¨")
        logger.error("ğŸ”§ ë¬¸ì œë¥¼ í•´ê²°í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")

if __name__ == "__main__":
    main()
