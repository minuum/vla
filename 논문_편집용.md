# Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models

**First Author:** HHong-Ju Yang, **Corresponding Author:** In-Yeop Choi  
*HHong-Ju Yang (inpink@kakao.com), Dept. of Computer Science, Kangnam University  
**In-Yeop Choi (billychoi@kangnam.ac.kr), Dept. of Computer Science, Kangnam University*

**Received:** 2024. 00. 00, **Revised:** 2024. 00. 00, **Accepted:** 2024. 00. 00.

Copyright ⓒ 2024 The Korea Society of Computer and Information  
http://www.ksci.re.kr pISSN:1598-849X | eISSN:2383-9945

---

## [Abstract]
This paper proposes a method to improve RoboVLMs' VLA architecture for mobile robot navigation applications.

**Key words:** RoboVLMs, VLA, VLM, Mobile Robot, Navigation

## [요약]
이 논문은 RoboVLMs의 VLA아키텍쳐를 이용하여, RoboVLMs의 액션 헤드를 활용하여 도메인이 로봇팔에 적용되었다. 이 논문은 이를 확장하여 RoboVLMs 프레임워크를 모바일 로봇의 주행에 적용해 보았다.

**주제어:** RoboVLMs, VLA, VLM, 모바일 로봇, 주행

---

## I. Introduction

Vision-Language-Action (VLA) 모델은 딥러닝 연구가 단순한 "시각 인식"과 "언어 인식"을 넘어서 물리적 로봇에게 적용하여 언어 지시를 이해하고 시각을 통한 상황을 인지한 후에 실제 로봇 행동으로 연결하기 위해 등장했다. "컵을 집어줘"라는 언어 정보를 이해한 후에 카메라 이미지와 영상 같은 시각 정보로 상황을 인지한 후에 로봇 팔의 회전, 경로, 그리퍼 동작 같은 실행해야 할 실제 동작 시퀀스의 액션을 직접 학습하는 모델이다.

이러한 연구가 필요하게 된 이유는 전통적인 로봇 구조의 한계에 있다. 기존에 로봇들이 공장에서 프로그래밍된 대로 반복 작업을 하였다. 그러나 서비스 로봇들은 이러한 로봇에게 언어로 지시하면 이를 알아듣고, 현재 상황을 인지하여 명령을 자연스럽게 처리해야 한다. 서비스는 특정 작업만 하는 것이 아니라 다양한 상황에 적용 가능한 범용성이 필요하다.

### 1. VLA에서 VLM을 사용하는 이유

다중 모달을 수행할 수 있는 방법으로 Vision Encoder와 LLM 조합을 생각할 수 있다. 이 조합은 시각 정보를 인코딩한 후 이를 LLM에 입력하여 언어적 추론을 수행하는 방식이다. CLIP 같은 Vision Encoder로 이미지를 벡터 임베딩으로 만든 후에 이를 LLaMa와 같은 LLM에 입력으로 넣어 텍스트 생성 및 이해 하는 구조이다.

이 방법의 장점은 Vision Encoder와 LLM을 독립적으로 훈련하고 교체 가능한 모듈성(Modularity)에 있다. 성능이 좋은 최신 모델을 바로 붙일 수 있다. 이 모듈성은 이미지, OCR, 영상 프레임 같은 다양한 멀티모달 입력을 인코더만 변경하여 확장할 수 있다. 또한 Vision Encoder는 사전 학습된 모델을 사용하고, LLM도 별도로 fine-tune함으로 전체 훈련 비용이 낮아지고, LLM 자체가 그대로 유지되므로 텍스트 처리 능력이 손상되지 않는다.

이 방법의 단점은 Vision Encoder의 출력 벡터가 LLM의 언어 공간과 직접적으로 align되지 않는 표현 격차(Representation Gap)가 발생할 수 있다. 이 경우 별도 projection layer나 alignment 학습이 필요하다. 또한 LLM은 이미지의 "언어화된 특징 벡터"만 받음으로 세밀한 비주얼 reasoning 약하다. 마지막으로 Encoder와 LLM을 동시에 미세 조정하기 어려워 성능 ceiling이 존재한다. Vision Encoder와 LLM의 미세조정의 한계와 비주얼 reasoning이 약한 것은 로봇에게 치명적인 약점이다. 멀티모달 reasoning은 비주얼 reasoning을 통해 이미지 데이터를 분석한 뒤, 텍스트 데이터를 통합해 추론을 수행한다. 그런데 비주얼 reasoning이 약하다는 것은 결국 추론하는 능력이 떨어짐으로 로봇의 성능이 낮아진다. 결론적으로 볼 때 Vision Encoder와 LLM 조합의 장점인 모듈성과 확장성이 추론 능력을 떨어뜨리는 단점으로 작용을 한다.

반면 VLM은 Vision Encoder와 Language Model을 하나의 프레임워크에서 공동 학습하는 구조이다. 처음부터 시각과 언어를 함께 학습하도록 설계되어 의미 공간을 공유함으로 두 모달리티 간의 관계를 더 깊이 이해하고 융합되어 멀티모달 reasoning이 강하다. 이처럼 이미지와 텍스트 간 관계를 세밀하게 학습하여 고품질 Alignment가 가능함으로 멀티모달 추론에서 뛰어나다. 이러한 VLM 모델이 가지는 아키텍쳐 상의 장점이외에도 VLM을 VLA에 사용시 얻어지는 장점이 있다.

첫째는 강력한 다중 모달 표현 학습 능력이다. VLM은 웹 규모의 방대한 데이터로 훈련되어 텍스트와 이미지/비디오와 같은 다양한 모달리티를 통합적으로 이해하고 표현하는 능력이 뛰어나다. 이러한 능력은 로봇이 복잡한 환경에서 시각적 정보와 언어적 지시를 동시에 처리하고 이해하는 데 필수적이다.

둘째는 일반화 및 강건성 (Robustness)이다. VLM은 다양한 오픈 월드 시나리오에서 일반화된 표현을 학습하였다. 그러므로 VLM 기반 VLA는 로봇이 이전에 보지 못했던 새로운 상황, 객체, 배경, 또는 작업 설명에 대해서도 잘 작동할 수 있다. 이는 제한된 로봇 데이터만으로 훈련된 모델보다 훨씬 강력한 일반화 성능을 제공한다.

셋째는 데이터 효율성이다. VLM은 이미 방대한 양의 시각-언어 데이터를 통해 사전 학습되어 있기 때문에, VLA로 미세 조정(fine-tuning)할 때 필요한 로봇 조작 데이터의 양을 줄일 수 있다. 즉, VLM의 사전 학습된 지식을 활용하여 적은 양의 로봇 데이터로도 높은 성능을 달성할 수 있다.

넷째는 복잡한 작업 처리를 잘 수행한다. VLM이 가진 추론 능력은 로봇이 단순히 객체를 인식하는 것을 넘어, 언어적 지시를 바탕으로 복잡한 작업을 계획하고 실행하는 데 도움을 준다. 예를 들어, "컵을 들어서 테이블에 놓으세요"와 같은 지시를 이해하고 단계별로 실행하는 능력은 VLM의 다중 모달 추론 능력 덕분이다.

### 2. VLM 백본 선택

본 연구에서는 VLM 백본으로 Microsoft Kosmos-2 Vision-Language 모델을 선택하였다. Kosmos-2는 멀티모달 대형 언어 모델로서, 시각적 세계와의 텍스트 연결을 강화하여 다양한 다운스트림 작업에서 우수한 성능을 발휘한다. Kosmos-2는 기존 VLM들이 단순히 이미지와 텍스트를 분리하여 처리하는 것과 달리, 시각적 세계와 텍스트를 통합적으로 이해하는 능력을 갖추고 있다.

Kosmos-2 선택의 결정적 요인은 다음과 같다. 첫째, 학술적 검증이다. 선행연구의 8가지 VLM 백본 비교에서 최고 성능을 달성하였다. 둘째, 멀티모달 통합이다. 시각과 언어의 통합적 이해 능력을 제공한다. 셋째, 로봇 제어 적합성이다. 언어 조건부 조작 작업에 최적화된 아키텍처를 가지고 있다. 넷째, 확장성이다. 다양한 다운스트림 태스크에 대한 우수한 일반화 능력을 제공한다. 다섯째, 실용성이다. 실제 로봇 환경에서의 안정적인 성능을 보장한다.

### 3. VLA 구성 방법

연속적인 행동 공간과 과거 관측 정보(history)를 통합하는 Policy Head 구조가 가장 우수하다. 단일 관측 기반(one-step) 모델보다 과거 정보를 활용하는 구조가 일반화와 데이터 효율성 측면에서 뛰어나다.

### 4. Cross-Embodiment 데이터 활용

단순한 사전 학습(pre-training)만으로는 성능 향상이 제한적일 때 사용한다. 사전 학습 후 후속 미세 조정(post-training)이 성능 향상에 효과적이며, 특히 few-shot 학습에서 유리하다.

### 5. 연구 목표

본 연구의 주요 목표는 모바일 환경에 최적화된 VLA 모델을 개발하는 것이다. 구체적으로는 Jetson Orin NX 16GB에서 실시간 동작 가능한 VLA 모델을 구현하여, 추론 속도 750 FPS 이상, 메모리 사용량 2GB 이하, MAE 0.25 이하의 성능을 달성하는 것을 목표로 한다.

---

## II. Preliminaries

### 1. Related Works

여러 가지 VLA 종류를 설명함.
- OpenVLA
- π0
- SayCan

---

## III. The Proposed Scheme

이 논문은 단순한 성능 비교를 넘어, 범용 로봇 정책의 설계 원칙과 실용적 가이드라인을 제공하며, 향후 로봇 연구의 방향성을 제시하는 데 큰 기여를 했다.

### 1. RoboVLMs 분석

이 논문은 Vision-Language-Action 모델(VLA)을 기반으로 한 범용 로봇 정책(generalist robot policies) 구축에 필요한 핵심 요소들을 체계적으로 분석하고, 새로운 프레임워크인 RoboVLMs를 제안한다. 연구는 VLA 아키텍처를 구성하는 방법에 대한 세 가지 필수적인 설계 선택에 중점을 두었다. VLM 백본 선택, VLA 아키텍처 구성 그리고 교차-구현 데이터 추가 시점이다. Kosmos-2 기반으로 기존 Vision-Language Model을 로봇 제어용으로 확장한 end-to-end 학습 프레임워크이다. 언어 명령을 받고 이미지 관찰후에 즉시 로봇 액션을 생성하는 실용적인 실시간 로봇 제어를 보여준다.

#### 1.1 RoboVLMs 프레임워크

다양한 VLM 백본과 VLA 구조를 쉽게 통합할 수 있는 유연하고 개방형(open-source) 프레임워크이다. 시뮬레이션(CALVIN, SimplerEnv)과 실제 로봇 실험에서 최신 성능(state-of-the-art)을 달성하였다.

#### 1.2 VLA 아키텍쳐 구성

Kosmos-2 VLM을 RoboKosmos로 래핑하여 액션 예측 기능을 추가하였다. 훈련 데이터인 CALVIN 데이터셋으로 파인튜닝을 하였고, 정책 헤드를 LSTM 기반으로 7DoF 로봇을 제어하였다.

##### 1.2.1 정책의 기본 개념

정책(Policy)은 강화학습과 로봇공학에서 나온 개념으로, "주어진 상황에서 어떤 행동을 할지 결정하는 규칙 또는 함수"를 의미한다. 언어 명령과 이미지를 보고 로봇 액션으로 변환한다.

이미지와 언어 입력이 주어졌을 때, 로봇 액션을 선택하는 함수는 다음과 같다.
```
π(action | vision, language, history)
```

##### 1.2.2 VLA에서 정책의 역할

VLA 시스템에서 정책은 다음과 같은 역할을 한다.

RoboVLMs에서는 4가지 정책 헤드로 구현되었다. 첫째는 즉시 반응형 정책인 FCDecoder로 직관적이고 빠른 특징이 있다. 단순한 집기나 놓기 등에 사용된다. 두 번째는 시간 순서가 중요한 순차적 정책인 LSTMDecoder로 연속적인 조작이 중요한 상황에서 사용된다. 세 번째는 어텐션 메커니즘을 사용한 GPTDecoder로 강력한 추론이 가능하여 복잡한 멀티태스크 상황에 적합하다. 네 번째는 언어와 액션이 통합된 DiscreteDecoder로 언어와 밀접한 태스크를 수행하는 상황에 적합하다.

##### 1.2.3 RoboVLMs 실행 플로우

RoboVLMs는 6-DOF 팔 + 1-DOF 그리퍼를 제어하는 7-DOF 제어 시스템이다. 비전, 언어, 액션 히스토리, 로봇 상태 융합하여 멀티모달 통합하였다. window-chunk 방식의 과거로 미래를 예측하였다. 액션 예측, 미래 관찰, 캡션 생성 동시 최적화하는 멀티태스크 학습을 수행한다.

##### 1.2.4 시간적 구조화

윈도우-청크 분할을 통해 과거 8프레임 관찰하여, 미래 10프레임을 예측한다. 과거는 미래 예측에만 허용되고, 미래는 과거의 정보 유출을 차단한다.

시간 순서별 동작 시뮬레이션은 아래와 같다.

| 시간 | 동작 |
|------|------|
| t=0 | "pick up the red cup" 명령 입력 |
| t=1~8 | 과거 액션 히스토리와 현재 이미지 관찰 |
| t=9 | 멀티모달 융합으로 상황 이해 |
| t=10 | LSTM을 통한 시퀀스 기반 액션 예측 |
| t=11 | 연속(팔)/이산(그리퍼) 하이브리드 출력 |
| t=12 | 실제 로봇 제어 명령 전송 |

##### 1.2.5 RoboVLM 학습

[학습 내용 추가 예정]

#### 1.3 실험적 기여

600개 이상의 실험을 통해 VLA 설계의 핵심 요소를 정량적으로 분석하였다. 실제 로봇 실험에서 자기 수정(self-correction) 능력까지 관찰되었다.

#### 1.4 다른 VLA와의 차이점

첫 번째는 3가지 태스크를 병렬 처리하는 멀티태스크 동시 학습이다. 두 번째는 window_size와 chunk_size 기반으로 한 시간적 구조화이다. 세 번째는 10가지 마스크로 메모리를 최적화하였다.

### 2. RoboVLMs 한계점

우리가 분석한 RoboVLMs의 한계점이다.

#### 2.1 멀티모달 상호작용 구조의 제한

기존 VLM의 구조(예: attention mask, mixture of experts)를 그대로 유지한 채 VLA를 구성했기 때문에, 행동(action)과의 상호작용을 위한 전용 아키텍처 설계가 부족하다. π0 같은 모델은 이러한 상호작용을 더 정교하게 설계하여 성능 향상을 보여주므로, 향후 연구에서 구조적 개선이 필요하다.

#### 2.2 VLA 구조의 단순화

논문에서 고려한 VLA 구조는 네 가지로 제한되어 있으며, 다양한 구조적 변형이나 세부 설계 요소(예: attention 방식, token 처리 방식 등)에 대한 탐색이 부족하다.

#### 2.3 행동 토크나이징 및 학습 목표의 미탐색

행동을 표현하는 방식(예: VQ-VAE, diffusion models, flow matching 등)에 대한 실험이 부족하며, 정교한 행동 표현 및 예측 방식에 대한 연구가 향후 필요하다.

### 3. RoboVLMs 개선 제안

RoboVLMs의 한계점을 보고 개선점을 제안한다. RoboVLMs 모델을 이용하여,

#### 3.1 환경 설정

Ubuntu 22.04, ROS2, 로봇 구성 소개

#### 3.2 Collect test data

데이터는 [데이터 수집 내용 추가 예정]

#### 3.3 구현 사항

RoboVLMs의 결과는 7-DoF 이지만, 테스트를 수행하는 로봇은 결과로 2-DoF이다.

#### 3.4 학습 수행

[학습 수행 내용 추가 예정]

---

## IV. Experimental Results

테스트 웹사이트 검색창에서 키워드를 입력한 후에 KNU, BM25와 RNTrans BM25의 검색 결과를 비교하였다.

### 1. Search results

[검색 결과 내용 추가 예정]

### 2. 테스트 평가 방법

평가 방법은 [평가 방법 내용 추가 예정]

### 3. Search Performance Optimization Methods

[성능 최적화 방법 내용 추가 예정]

---

## V. Conclusions

RoboVLMs [결론 내용 추가 예정]

---

## REFERENCES

[1] Y. Kim, "Convolution Neural Networks for Sentence Classification," Computer Science and Computation Language, Sep 2014. DOI: https://doi.org/10.48550/arXiv.1408.5882

[2] A. Conneau, H. Schwenk, Y. L. Cun and L. Barrault, "Very Deep Convolutional Networks for Text Classification," Computer Science and Computation Language, Jan 2017. DOI: https://doi.org/10.48550/arXiv.1606.01781

[3] H. Han, X. Bai and J. Liu, "Attention-based ResNet for Chinese Text Sentiment Classification," Advances in Computer Science Research, Vol. 80, Feb 2018. DOI: 10.2991/csece-18.2018.108

[4] K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," The Computer Vision and Pattern Recognition, Dec 2015. https://doi.org/10.48550/arXiv.1512.03385

[5] J. Wang, L. C, Yu, K. R. Lai and X. Zhang, "Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model," 2016 Association for Computational Linguistics, pp. 225–230, Berlin, Germany, Aug 2016.

[6] H. Y. Park and K. J. Kim, "Sentiment Analysis of Movie Review Using Integrated CNN-LSTM Mode," Journal of Intelligence and information system, Vol. 25, pp. 141-154, Dec 2019.

[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin, "Attention Is All You Need," 31st Conference on Neural Information Processing Systems, Long Beach, CA, USA, Jun 2017. https://doi.org/10.48550/arXiv.1706.03762

[8] C. E. Benarab and S. Gui, "CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification," Journal of Korea Design Knowledge, Vol. 33, pp. 401-409, March 2015.

[9] AG News, https://paperswithcode.com/dataset/ag-news

[10] Kangnam University Homepage, https://web.kangnam.ac.kr

[11] Nori Korean morphological analyzer, https://esbook.kimjmin.net/06-text-analysis/6.7-stemming/6.7.2-nori

[12] S. A. Saqqa and A. Awajan, "The Use of Word2vec Model in Sentiment Analysis: A Survey," Association for Computing Machinery, Cairo, Egypt, Dec 2019. DOI:https://doi.org/10.1145/3388218.3388229

[13] Jsoup, https://jsoup.org/

[14] Spring Boot, https://spring.io/projects/spring-boot

[15] Thymeleaf, https://www.thymeleaf.org/

[16] Website on the front-end, https://www.knusearch.site/search

[17] User's Guide of Flask, https://flask.palletsprojects.com/

[18] Representational State Transfer (REST) architecture, https://ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm

[19] REST API Tutorial, https://restfulapi.net/

[20] A website that evaluates search results, https://knusearch.site/research

[21] K. Jarvelin and J. Kekalainen, "Cumulated Gain-based Evaluation of IR Techniques," ACM Transactions on Information System, Vol. 20, No. 4, pp. 422-446, Oct 2002.
