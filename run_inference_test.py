#!/usr/bin/env python3
"""
Mobile VLA Inference Test Script
í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡ (Inference)ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import h5py
from PIL import Image
import json
from pathlib import Path

# RoboVLMs ëª¨ë“ˆ ì„í¬íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.getcwd()
sys.path.append(os.path.join(current_dir, "RoboVLMs_upstream"))

from robovlms.model.backbone.base_backbone import BaseRoboVLM
from robovlms.data.data_utils import get_text_function

def load_model_from_checkpoint(checkpoint_path, config_path):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”„ Loading model from {checkpoint_path}...")
    
    # 1. Config ë¡œë“œ
    with open(config_path, 'r') as f:
        configs = json.load(f)
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”ì— í•„ìš”í•œ ì¸ì êµ¬ì„±
    train_setup_configs = configs.get('train_setup', {})
    train_setup_configs['lora_enable'] = True
    
    # Mobile VLAì— ë§ëŠ” íŒŒë¼ë¯¸í„° ì„¤ì •
    window_size = configs.get('window_size', 8)
    fwd_pred_next_n = configs.get('fwd_pred_next_n', 10)
    
    print("ğŸ—ï¸ Building Mobile VLA Model...")
    try:
        model = BaseRoboVLM(
            configs=configs,
            train_setup_configs=train_setup_configs,
            act_head_configs=configs.get('act_head', None),
            fwd_head_configs=configs.get('fwd_head', None),
            window_size=window_size,
            fwd_pred_next_n=fwd_pred_next_n,
        )
    except Exception as e:
        print(f"âŒ Model initialization failed: {e}")
        raise e

    # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint['state_dict']
        
        # LightningModuleì˜ 'model.' prefix ì œê±° ë° í‚¤ ë§¤í•‘
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('model.'):
                # 'model.' ì œê±° (LightningModule ë˜í¼ ì œê±°)
                new_key = k[6:]
                new_state_dict[new_key] = v
            else:
                new_state_dict[k] = v
        
        # strict=Falseë¡œ ë¡œë“œ (LoRA ë“± ìœ ì—°í•˜ê²Œ ì²˜ë¦¬)
        msg = model.load_state_dict(new_state_dict, strict=False)
        print(f"âš ï¸ Load results: {msg}")
        
    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        raise e
    
    model.eval()
    model.cuda()
    return model

def preprocess_image(image_array, image_size=224):
    """MobileVLAH5Datasetê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©"""
    # (H, W, C) -> PIL Image
    img = Image.fromarray(image_array.astype(np.uint8))
    # Resize (Bilinear)
    img = img.resize((image_size, image_size), Image.BILINEAR)
    # PIL -> numpy -> tensor (0-1 range)
    img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
    # (H, W, C) -> (C, H, W)
    img_tensor = img_tensor.permute(2, 0, 1)
    return img_tensor

def process_input_data(h5_path, model, index=20):
    """H5 íŒŒì¼ì—ì„œ ì…ë ¥ ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬"""
    print(f"ğŸ“‚ Loading data from {h5_path} (Index: {index})...")
    
    with h5py.File(h5_path, 'r') as f:
        if 'images' not in f:
             # try 'observations/images' (calvin format)
             if 'observations' in f and 'images' in f['observations']:
                 images = f['observations']['images'][:]
             else:
                 raise ValueError(f"Cannot find images dataset in {h5_path}. Keys: {list(f.keys())}")
        else:
            images = f['images'][:] # (Total_Frames, H, W, C)
            
        print(f"DEBUG: Total images: {len(images)}")
        
        # ì„ì˜ì˜ ë„¤ë¹„ê²Œì´ì…˜ ëª…ë ¹ì–´
        text_str = "Navigate to the target location" 
        
        # Window Size (8) ë§Œí¼ ê°€ì ¸ì˜¤ê¸°
        window_size = model.window_size
        start_idx = max(0, index - window_size + 1)
        end_idx = index + 1
        
        # Ensure valid range
        if start_idx >= len(images):
            start_idx = max(0, len(images) - window_size)
            end_idx = len(images)
            print(f"WARNING: Index {index} out of bounds. Adjusted to last window.")
            
        img_seq_raw = images[start_idx:end_idx]
        print(f"DEBUG: img_seq_raw length: {len(img_seq_raw)}")
        
        # íŒ¨ë”© ì²˜ë¦¬ (ì•ë¶€ë¶„ì´ ë¶€ì¡±í•  ê²½ìš° ì²« í”„ë ˆì„ ë³µì‚¬)
        if len(img_seq_raw) > 0 and len(img_seq_raw) < window_size:
            pad_len = window_size - len(img_seq_raw)
            padding = np.tile(img_seq_raw[0:1], (pad_len, 1, 1, 1))
            img_seq_raw = np.concatenate([padding, img_seq_raw], axis=0)
            
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    if len(img_seq_raw) == 0:
         raise ValueError("img_seq_raw is empty after processing!")
         
    img_tensors = []
    for img in img_seq_raw:
        img_tensors.append(preprocess_image(img))
    
    # (Window, C, H, W) -> (1, Window, C, H, W) [Batch Dim ì¶”ê°€]
    # img_tensorsëŠ” list of tensors (C, H, W)
    vision_x = torch.stack(img_tensors).unsqueeze(0).cuda()
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (Tokenizer ì‚¬ìš©)
    tokenizer = model.tokenizer
    # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• (RoboVLMs ë°©ì‹)
    tokens = tokenizer(
        text_str, 
        return_tensors="pt", 
        padding="max_length", 
        truncation=True, 
        max_length=256
    )
    
    lang_x = tokens["input_ids"].cuda()
    attention_mask = tokens["attention_mask"].cuda()
    
    return vision_x, lang_x, attention_mask, img_seq_raw

def run_inference(model, vision_x, lang_x, attention_mask):
    """ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
    print("ğŸš€ Running Inference...")
    
    with torch.no_grad():
        # MobileVLA Trainer/Model êµ¬ì¡°ì— ë§ê²Œ í˜¸ì¶œ
        # mode='inference'ë¡œ í˜¸ì¶œí•˜ë©´ logits(actions) ë°˜í™˜
        prediction = model.inference(
            vision_x=vision_x,
            lang_x=lang_x,
            attention_mask=attention_mask
        )
        
    # prediction['action']ì€ (velocities, None) íŠœí”Œì¼ ìˆ˜ ìˆìŒ (MobileVLALSTMDecoder)
    actions = prediction['action']
    if isinstance(actions, tuple) or isinstance(actions, list):
        actions = actions[0] # velocities
    
    # Actions shape: (Batch, Seq_Len, Fwd_Pred, Action_Dim)
    # ìš°ë¦¬ëŠ” ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì˜ˆì¸¡ê°’(Chunk)ì´ í•„ìš”í•¨
    # actions: (1, 8, 10, 2)
    
    # ë§ˆì§€ë§‰ ìœˆë„ìš° ì‹œì ì˜ ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    # (Batch=0, Last_Seq=-1, :)
    pred_chunk = actions[0, -1, :, :].cpu().numpy() # (10, 2)
    
    return pred_chunk

def visualize_results(img_seq_raw, pred_actions, save_path="inference_result.png"):
    """ê²°ê³¼ ì‹œê°í™”"""
    print("ğŸ“Š Visualizing results...")
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # 1. ë§ˆì§€ë§‰ ê´€ì°° ì´ë¯¸ì§€
    last_img = img_seq_raw[-1]
    # BGR to RGB if needed (H5 might be RGB, cv2 uses BGR. PIL uses RGB)
    # Assuming H5 saved as RGB via PIL in collector
    axes[0].imshow(last_img)
    axes[0].set_title("Last Observation (RGB)")
    axes[0].axis('off')
    
    # 2. ì˜ˆì¸¡ ê¶¤ì  (2D Velocity -> Trajectory)
    # linear_x (ì „ì§„), linear_y (ì¢Œìš°)
    # ê°„ë‹¨í•œ ì ë¶„ìœ¼ë¡œ ê²½ë¡œ ì‹œê°í™” (dt=0.4s ê°€ì •)
    dt = 0.4
    x, y = 0, 0
    traj_x = [0]
    traj_y = [0]
    
    # ë¡œë´‡ ì¢Œí‘œê³„: Xê°€ ì „ì§„(Up), Yê°€ ì¢Œì¸¡(Left)
    for vx, vy in pred_actions:
        # ì •ê·œí™”ëœ ì•¡ì…˜(-1~1)ì„ ì‹¤ì œ ë¬¼ë¦¬ëŸ‰ìœ¼ë¡œ ë³µì›í•´ì•¼ í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ê²½í–¥ì„± í™•ì¸ì„ ìœ„í•´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        dx = vx * dt
        dy = vy * dt
        
        # ì „ì—­ ì¢Œí‘œê³„ë¡œ ë³€í™˜ (ëˆ„ì )
        x += dx
        y += dy
        traj_x.append(x)
        traj_y.append(y)
        
    axes[1].plot(traj_y, traj_x, 'b-o', linewidth=2, label='Predicted Path')
    axes[1].plot(0, 0, 'rs', markersize=10, label='Start (Robot)')
    
    # ê·¸ë˜í”„ ë°ì½”ë ˆì´ì…˜
    axes[1].set_title("Predicted 2D Trajectory (Top-Down View)")
    axes[1].set_xlabel("Lateral (Y) - Left(+)/Right(-)")
    axes[1].set_ylabel("Longitudinal (X) - Fwd(+)/Bwd(-)")
    axes[1].grid(True)
    axes[1].legend()
    axes[1].axis('equal')
    
    # ë°©í–¥ í‘œì‹œ (í™”ì‚´í‘œ)
    if len(traj_x) > 1:
        axes[1].arrow(traj_y[-2], traj_x[-2], traj_y[-1]-traj_y[-2], traj_x[-1]-traj_x[-2], 
                     head_width=0.05, head_length=0.1, fc='b', ec='b')

    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ’¾ Saved visualization to {save_path}")

if __name__ == "__main__":
    # ì„¤ì •
    ckpt_path = "RoboVLMs_upstream/runs/mobile_vla_lora_20251114/kosmos/mobile_vla_finetune/2025-11-20/mobile_vla_lora_20251114/epoch_epoch=05-val_loss=val_loss=0.280.ckpt"
    config_path = "Mobile_VLA/configs/mobile_vla_20251114_lora.json"
    # í…ŒìŠ¤íŠ¸í•  ë°ì´í„° íŒŒì¼ (ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì¤‘ í•˜ë‚˜ ì„ íƒ)
    data_path = "ROS_action/mobile_vla_dataset/episode_20251119_170441_1box_hori_left_core_medium.h5"
    
    if not os.path.exists(ckpt_path):
        print(f"âŒ Checkpoint not found: {ckpt_path}")
        sys.exit(1)
        
    if not os.path.exists(data_path):
        print(f"âŒ Data file not found: {data_path}")
        # ëŒ€ì²´ íŒŒì¼ ì°¾ê¸°
        import glob
        files = glob.glob("ROS_action/mobile_vla_dataset/*.h5")
        if files:
            data_path = files[0]
            print(f"âš ï¸ Using alternative data file: {data_path}")
        else:
            print("âŒ No H5 files found.")
            sys.exit(1)

    # 1. ëª¨ë¸ ë¡œë“œ
    model = load_model_from_checkpoint(ckpt_path, config_path)
    
    # 2. ë°ì´í„° ì²˜ë¦¬
    # Index 50: ì—í”¼ì†Œë“œ ì¤‘ê°„ ì¯¤ì—ì„œ í…ŒìŠ¤íŠ¸
    vision_x, lang_x, attention_mask, img_seq_raw = process_input_data(data_path, model, index=50)
    
    # 3. ì¶”ë¡  ì‹¤í–‰
    pred_actions = run_inference(model, vision_x, lang_x, attention_mask)
    
    print("\nğŸ“Š Predicted Actions (First 5 steps):")
    print("   Linear_X (Fwd) | Linear_Y (Left)")
    print("-" * 35)
    for i, (vx, vy) in enumerate(pred_actions[:5]):
        print(f"t+{i}: {vx: .4f}      | {vy: .4f}")
        
    # 4. ì‹œê°í™”
    visualize_results(img_seq_raw, pred_actions)
