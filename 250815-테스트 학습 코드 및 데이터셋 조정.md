# í…ŒìŠ¤íŠ¸ í•™ìŠµ ì½”ë“œ ë° ë°ì´í„°ì…‹ ì¡°ì •
_Exported on 8/15/2025 at 13:57:49 GMT+9 from Cursor (1.4.5)_

---

**User**

ì´ì œ@Mobile_VLA/ ì´ê±¸ ê°€ì§€ê³  í•™ìŠµ ì½”ë“œë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³´ì ì‹¤ì œ ã„·ì´í„°ì…‹ì„ ëª¨ì•˜ìœ¼ë‹ˆê¹Œ í˜•ì‹ì´ë‚˜ ê·¸ëŸ°ê±° ë°”ê¿€ê±° ìˆìœ¼ë©´ í™˜ê°ã…‡ë²—ì´ ì¼ì¹˜ì‹œí‚¤ê³ 

---

**Cursor**

ì§§ê²Œ ì§„í–‰ ìƒí™©: ì‹¤ì œ ìˆ˜ì§‘ë³¸ê³¼ ë¡œë” í˜•ì‹ì´ ì¼ì¹˜í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ê¸°(`mobile_vla_data_collector.py`)ì—ì„œ HDF5ì— ì“°ëŠ” í‚¤/ì†ì„± ì´ë¦„ì„ ìŠ¤ìº”í•˜ê³ , ë§ìœ¼ë©´ ë°ì´í„°ì…‹/ëª¨ë¸/ê°„ì´ íŠ¸ë ˆì´ë„ˆ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì‹¤ì œ ë°ì´í„°ë¡œ 1 ì—í¬í¬ í•™ìŠµì´ ê°€ëŠ¥í•œ ëŸ°ì²˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ê°€í•´ ëŒë ¤ë³´ê² ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë°”ë¡œ í™•ì¸ê³¼ í…ŒìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

ë°©ê¸ˆ ì‹¤ì œ ìˆ˜ì§‘ í¬ë§·ê³¼ ë¡œë”ë¥¼ ëŒ€ì¡°í•´ ë³´ë‹ˆ, HDF5ì˜ `action_event_types`ê°€ UTF-8 ë¬¸ìì—´ë¡œ ì €ì¥ë˜ëŠ”ë° ë¡œë”ëŠ” ë°”ì´íŠ¸ ë¬¸ìì—´ë¡œë§Œ ë§¤í•‘í•´ì„œ ì´ë²¤íŠ¸ ì¸ë±ìŠ¤ê°€ í‹€ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë²„ê·¸ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë˜, Jetsonì—ì„œ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œë¥¼ í”¼í•˜ë ¤ë©´ ì´ë¯¸ì§€ ì¸ì½”ë”ë“¤ì˜ `pretrained=True`ë¥¼ ê¸°ë³¸ `False`ë¡œ ë°”ê¾¸ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤. ì§€ê¸ˆ ë‘ ì ì„ ê³ ì¹˜ê³ , ì‹¤ì œ ë°ì´í„°ë¡œ 1ì—í­ ìŠ¤ëª¨í¬ í•™ìŠµì„ ëŒë¦´ ìˆ˜ ìˆëŠ” ëŸ°ì²˜(`train_mobile_vla.py`)ë¥¼ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤. ìˆ˜ì • í›„ ë°”ë¡œ ë°ì´í„°ì…‹ ì ê²€ê³¼ ì§§ì€ í•™ìŠµì„ ì‹¤í–‰í•´ ë³´ê² ìŠµë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""
Mobile VLA Dataset - mobile_vla_data_collector.py ë°ì´í„° ì§ì ‘ ë¡œë”©
Calvin ì—†ì´ ìˆœìˆ˜ Mobile HDF5 í˜•ì‹ ì‚¬ìš©
"""

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MobileVLADataset(Dataset):
    """
    mobile_vla_data_collector.pyê°€ ìƒì„±í•œ HDF5 íŒŒì¼ì„ ì§ì ‘ ë¡œë”©í•˜ëŠ” ë°ì´í„°ì…‹
    
    ë°ì´í„° í˜•ì‹:
    - images: [T, 720, 1280, 3] - RGB ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
    - actions: [T, 3] - [linear_x, linear_y, angular_z] 
    - action_event_types: [T] - [episode_start, start_action, stop_action]
    - episode_name: str - "episode_20250808_123136_1box_vert_left"
    """
    
    def __init__(
        self, 
        data_dir: str = "/home/soda/vla/ROS_action/mobile_vla_dataset/",
        sequence_length: int = 18,
        image_size: Tuple[int, int] = (224, 224),  # VLM ì…ë ¥ìš© ë¦¬ì‚¬ì´ì¦ˆ
        normalize_actions: bool = True,
        scenario_filter: Optional[List[str]] = None
    ):
        self.data_dir = Path(data_dir)
        self.sequence_length = sequence_length
        self.image_size = image_size
        self.normalize_actions = normalize_actions
        
        # mobile_vla_data_collector.pyì˜ ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘
        self.scenario_instructions = {
            "1box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "1box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "2box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”"
        }
        
        # mobile_vla_data_collector.pyì˜ ì•¡ì…˜ ë²”ìœ„ (WASD_TO_CONTINUOUS ê¸°ì¤€)
        self.action_bounds = {
            "linear_x": 2.0,   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ
            "linear_y": 2.0,   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ  
            "angular_z": 2.0   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ
        }
        
        # ì´ë²¤íŠ¸ íƒ€ì… ë§¤í•‘
        # ë¬¸ìì—´/ë°”ì´íŠ¸ ë¬¸ìì—´ ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        self.event_mapping = {
            'episode_start': 0,
            'start_action': 1,
            'stop_action': 2
        }
        
        # HDF5 íŒŒì¼ ë¡œë“œ ë° í•„í„°ë§
        self.h5_files = self._load_h5_files(scenario_filter)
        self.scenarios = self._extract_scenarios()
        
        # ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥
        self._print_dataset_stats()
        
    def _load_h5_files(self, scenario_filter: Optional[List[str]]) -> List[Path]:
        """HDF5 íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í•„í„°ë§"""
        all_h5_files = list(self.data_dir.glob("*.h5"))
        
        if scenario_filter:
            filtered_files = []
            for h5_file in all_h5_files:
                scenario = self._extract_scenario_from_filename(h5_file.name)
                if scenario in scenario_filter:
                    filtered_files.append(h5_file)
            return filtered_files
        
        return all_h5_files
    
    def _extract_scenario_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ (mobile_vla_data_collector.py ë°©ì‹)"""
        for scenario in self.scenario_instructions.keys():
            if scenario in filename:
                return scenario
        return "unknown"
    
    def _extract_scenarios(self) -> List[str]:
        """ëª¨ë“  íŒŒì¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ"""
        scenarios = []
        for h5_file in self.h5_files:
            scenario = self._extract_scenario_from_filename(h5_file.name)
            scenarios.append(scenario)
        return scenarios
    
    def _print_dataset_stats(self):
        """ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥"""
        scenario_counts = defaultdict(int)
        total_frames = 0
        
        for i, h5_file in enumerate(self.h5_files):
            scenario = self.scenarios[i]
            scenario_counts[scenario] += 1
            
            # í”„ë ˆì„ ìˆ˜ í™•ì¸
            try:
                with h5py.File(h5_file, 'r') as f:
                    num_frames = f.attrs.get('num_frames', 0)
                    total_frames += num_frames
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {h5_file.name}: {e}")
        
        logger.info(f"ğŸ“ Mobile VLA Dataset ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(self.h5_files)}ê°œ ì—í”¼ì†Œë“œ, {total_frames}ê°œ í”„ë ˆì„")
        logger.info(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬: {dict(scenario_counts)}")
        
        # 18í”„ë ˆì„ ì—í”¼ì†Œë“œ íŠ¹ë³„ í‘œì‹œ
        frame_18_count = sum(1 for scenario in scenario_counts.keys() if scenario != "unknown")
        logger.info(f"ğŸ¯ 18í”„ë ˆì„ ì—í”¼ì†Œë“œ: {frame_18_count}ê°œ (í‘œì¤€ ê¸¸ì´)")
    
    def __len__(self) -> int:
        return len(self.h5_files)
    
    def __getitem__(self, idx: int) -> Dict:
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë°ì´í„° ë¡œë“œ"""
        h5_file = self.h5_files[idx]
        scenario = self.scenarios[idx]
        
        try:
            with h5py.File(h5_file, 'r') as f:
                # mobile_vla_data_collector.py ë°ì´í„° ì§ì ‘ ë¡œë“œ
                images = f['images'][:]                    # [T, 720, 1280, 3]
                actions = f['actions'][:]                  # [T, 3] 
                action_events = f['action_event_types'][:]  # [T]
                
                # ë©”íƒ€ë°ì´í„°
                episode_name = f.attrs['episode_name']
                num_frames = f.attrs['num_frames']
                duration = f.attrs['total_duration']
                
        except Exception as e:
            logger.error(f"HDF5 íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {h5_file.name}: {e}")
            # ë¹ˆ ë°ì´í„° ë°˜í™˜
            return self._get_empty_sample(scenario)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self._preprocess_episode(
            images, actions, action_events, scenario, episode_name, num_frames, duration
        )
        
        return processed_data
    
    def _preprocess_episode(
        self, 
        images: np.ndarray, 
        actions: np.ndarray, 
        action_events: np.ndarray,
        scenario: str,
        episode_name: str,
        num_frames: int,
        duration: float
    ) -> Dict:
        """ì—í”¼ì†Œë“œ ë°ì´í„° ì „ì²˜ë¦¬"""
        
        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (720p â†’ 224x224 ë¦¬ì‚¬ì´ì¦ˆ + ì •ê·œí™”)
        processed_images = self._preprocess_images(images)  # [T, 3, 224, 224]
        
        # 2. ì•¡ì…˜ ì •ê·œí™” (mobile_vla_data_collector.py ê¸°ì¤€)
        if self.normalize_actions:
            processed_actions = self._normalize_actions(actions)  # [T, 3] normalized
        else:
            processed_actions = torch.FloatTensor(actions)
        
        # 3. ì´ë²¤íŠ¸ íƒ€ì… ë³€í™˜
        # h5pyê°€ ë°˜í™˜í•˜ëŠ” í˜•ì‹ì´ str ë˜ëŠ” bytes(np.bytes_)ì¼ ìˆ˜ ìˆì–´ í†µí•© ì²˜ë¦¬
        def _to_text(e):
            if isinstance(e, bytes):
                return e.decode('utf-8', errors='ignore')
            try:
                import numpy as _np
                if isinstance(e, _np.bytes_):
                    return e.decode('utf-8', errors='ignore')
            except Exception:
                pass
            return str(e)

        event_indices = np.array([
            self.event_mapping.get(_to_text(event), 1) for event in action_events
        ])
        processed_events = torch.LongTensor(event_indices)  # [T]
        
        # 4. ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (18í”„ë ˆì„ í‘œì¤€)
        if len(processed_images) != self.sequence_length:
            processed_images, processed_actions, processed_events = self._pad_or_truncate_sequence(
                processed_images, processed_actions, processed_events
            )
        
        # 5. í•œêµ­ì–´ ëª…ë ¹ì–´ ì¶”ê°€
        instruction = self.scenario_instructions.get(scenario, "ì»µê¹Œì§€ ê°€ì„¸ìš”")
        
        return {
            "images": processed_images,              # [18, 3, 224, 224]
            "actions": processed_actions,            # [18, 3]
            "action_events": processed_events,       # [18]
            "scenario": scenario,                    # str
            "instruction": instruction,              # str (í•œêµ­ì–´)
            "episode_name": episode_name,            # str
            "num_frames": num_frames,                # int
            "duration": duration,                    # float
            "sequence_mask": torch.ones(self.sequence_length, dtype=torch.bool)  # [18] - ëª¨ë“  í”„ë ˆì„ ìœ íš¨
        }
    
    def _preprocess_images(self, images: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬: 720p â†’ 224x224 ë¦¬ì‚¬ì´ì¦ˆ + ì •ê·œí™”"""
        import torchvision.transforms as transforms
        
        # [T, 720, 1280, 3] â†’ [T, 3, 224, 224]
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.image_size),
            transforms.ToTensor(),  # [0, 1] ì •ê·œí™” + HWCâ†’CHW
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
        ])
        
        processed_images = []
        for i in range(len(images)):
            # uint8 [720, 1280, 3] â†’ normalized [3, 224, 224]
            img_tensor = transform(images[i])
            processed_images.append(img_tensor)
        
        return torch.stack(processed_images)  # [T, 3, 224, 224]
    
    def _normalize_actions(self, actions: np.ndarray) -> torch.Tensor:
        """ì•¡ì…˜ ì •ê·œí™” (mobile_vla_data_collector.py ê¸°ì¤€)"""
        # [T, 3] actions: [linear_x, linear_y, angular_z]
        normalized_actions = actions.copy()
        
        # ê° ì¶•ë³„ë¡œ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        normalized_actions[:, 0] = actions[:, 0] / self.action_bounds["linear_x"]    # linear_x
        normalized_actions[:, 1] = actions[:, 1] / self.action_bounds["linear_y"]    # linear_y  
        normalized_actions[:, 2] = actions[:, 2] / self.action_bounds["angular_z"]   # angular_z
        
        # í´ë¨í•‘ [-1, 1]
        normalized_actions = np.clip(normalized_actions, -1.0, 1.0)
        
        return torch.FloatTensor(normalized_actions)
    
    def _pad_or_truncate_sequence(
        self, 
        images: torch.Tensor, 
        actions: torch.Tensor, 
        events: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ í‘œì¤€ ê¸¸ì´(18)ì— ë§ì¶”ê¸°"""
        current_length = len(images)
        
        if current_length == self.sequence_length:
            return images, actions, events
        elif current_length < self.sequence_length:
            # íŒ¨ë”©: ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
            pad_length = self.sequence_length - current_length
            
            # ì´ë¯¸ì§€ íŒ¨ë”©
            last_image = images[-1:].repeat(pad_length, 1, 1, 1)
            padded_images = torch.cat([images, last_image], dim=0)
            
            # ì•¡ì…˜ íŒ¨ë”© (ì •ì§€ ì•¡ì…˜ìœ¼ë¡œ)
            stop_action = torch.zeros(pad_length, 3)
            padded_actions = torch.cat([actions, stop_action], dim=0)
            
            # ì´ë²¤íŠ¸ íŒ¨ë”© (stop_actionìœ¼ë¡œ)
            stop_events = torch.full((pad_length,), 2, dtype=torch.long)  # stop_action = 2
            padded_events = torch.cat([events, stop_events], dim=0)
            
            return padded_images, padded_actions, padded_events
        else:
            # ìë¥´ê¸°: ì²˜ìŒ sequence_lengthë§Œ ì‚¬ìš©
            return images[:self.sequence_length], actions[:self.sequence_length], events[:self.sequence_length]
    
    def _get_empty_sample(self, scenario: str) -> Dict:
        """ë¹ˆ ìƒ˜í”Œ ë°˜í™˜ (ì—ëŸ¬ ë°œìƒì‹œ)"""
        return {
            "images": torch.zeros(self.sequence_length, 3, *self.image_size),
            "actions": torch.zeros(self.sequence_length, 3),
            "action_events": torch.zeros(self.sequence_length, dtype=torch.long),
            "scenario": scenario,
            "instruction": self.scenario_instructions.get(scenario, "ì»µê¹Œì§€ ê°€ì„¸ìš”"),
            "episode_name": "error_episode",
            "num_frames": 0,
            "duration": 0.0,
            "sequence_mask": torch.zeros(self.sequence_length, dtype=torch.bool)
        }
    
    def denormalize_actions(self, normalized_actions: torch.Tensor) -> torch.Tensor:
        """ì •ê·œí™”ëœ ì•¡ì…˜ì„ ì›ë˜ ë²”ìœ„ë¡œ ë³µì›"""
        # [-1, 1] â†’ mobile_vla_data_collector.py ë²”ìœ„
        denormalized = normalized_actions.clone()
        denormalized[:, 0] *= self.action_bounds["linear_x"]    # linear_x
        denormalized[:, 1] *= self.action_bounds["linear_y"]    # linear_y
        denormalized[:, 2] *= self.action_bounds["angular_z"]   # angular_z
        return denormalized
    
    def get_scenario_statistics(self) -> Dict[str, int]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í†µê³„ ë°˜í™˜"""
        scenario_counts = defaultdict(int)
        for scenario in self.scenarios:
            scenario_counts[scenario] += 1
        return dict(scenario_counts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile VLA Dataset í…ŒìŠ¤íŠ¸")
    
    dataset = MobileVLADataset()
    print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í˜•íƒœ: {sample['images'].shape}")
        print(f"ğŸ® ì•¡ì…˜ í˜•íƒœ: {sample['actions'].shape}")
        print(f"âš¡ ì´ë²¤íŠ¸ í˜•íƒœ: {sample['action_events'].shape}")
        print(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤: {sample['scenario']}")
        print(f"ğŸ—£ï¸ ëª…ë ¹ì–´: {sample['instruction']}")
        print(f"ğŸ“‹ ì—í”¼ì†Œë“œëª…: {sample['episode_name']}")
    
    # ì‹œë‚˜ë¦¬ì˜¤ í†µê³„
    stats = dataset.get_scenario_statistics()
    print(f"ğŸ“ˆ ì‹œë‚˜ë¦¬ì˜¤ í†µê³„: {stats}")

```

```python
#!/usr/bin/env python3
"""
Mobile Image Encoder - mobile_vla_data_collector.pyì˜ 720p ì´ë¯¸ì§€ ì²˜ë¦¬ íŠ¹í™”
RoboVLMsì˜ ì´ë¯¸ì§€ ì¸ì½”ë”© ê¸°ìˆ ì„ Mobile VLAì— ì ìš©
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class MobileImageEncoder(nn.Module):
    """
    Mobile VLA íŠ¹í™” ì´ë¯¸ì§€ ì¸ì½”ë”
    - ì…ë ¥: [B, T, 3, 224, 224] (mobile_vla_data_collector.pyì—ì„œ 720pâ†’224p ë¦¬ì‚¬ì´ì¦ˆë¨)
    - ì¶œë ¥: [B, T, hidden_size] ì‹œê°„ì  íŠ¹ì§•
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_v2_s",
        hidden_size: int = 768,
        num_lstm_layers: int = 2,
        dropout: float = 0.1,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.backbone_name = backbone
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        
        # ë°±ë³¸ CNN (EfficientNet V2 - ëª¨ë°”ì¼ ìµœì í™”)
        if backbone == "efficientnet_v2_s":
            # Jetson/ì˜¤í”„ë¼ì¸ í™˜ê²½ ëŒ€ë¹„: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™”
            self.backbone = models.efficientnet_v2_s(weights=None)
            backbone_output_size = 1000
        elif backbone == "resnet50":
            self.backbone = models.resnet50(weights=None)
            backbone_output_size = 2048
        elif backbone == "mobilenet_v3_large":
            self.backbone = models.mobilenet_v3_large(weights=None)  
            backbone_output_size = 1000
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ë³¸: {backbone}")
        
        # ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì • ì˜µì…˜
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
            logger.info(f"ğŸ”’ {backbone} ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì •ë¨")
        
        # CNN íŠ¹ì§•ì„ hidden_sizeë¡œ ë§¤í•‘
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ (18í”„ë ˆì„ ì‹œí€€ìŠ¤)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,  # ì–‘ë°©í–¥ì´ë¯€ë¡œ ì ˆë°˜
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸ–¼ï¸ Mobile Image Encoder ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë°±ë³¸: {backbone}, Hidden: {hidden_size}, LSTM Layers: {num_lstm_layers}")
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_sequence: [B, T, 3, 224, 224] - ë°°ì¹˜ í¬ê¸° B, ì‹œí€€ìŠ¤ ê¸¸ì´ T
            
        Returns:
            temporal_features: [B, T, hidden_size] - ì‹œê°„ì  ì´ë¯¸ì§€ íŠ¹ì§•
        """
        B, T, C, H, W = image_sequence.shape
        
        # ë°°ì¹˜ì™€ ì‹œê°„ ì°¨ì›ì„ í•©ì³ì„œ CNNì— ì…ë ¥
        images_flat = image_sequence.view(B * T, C, H, W)  # [B*T, 3, 224, 224]
        
        # CNNìœ¼ë¡œ ê° í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ
        with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
            frame_features = self.backbone(images_flat)  # [B*T, backbone_output_size]
        
        # íŠ¹ì§• ì°¨ì›ì„ hidden_sizeë¡œ ë§¤í•‘
        frame_features = self.feature_projection(frame_features)  # [B*T, hidden_size]
        
        # ì‹œê°„ ì°¨ì› ë³µì›
        frame_features = frame_features.view(B, T, self.hidden_size)  # [B, T, hidden_size]
        
        # LSTMìœ¼ë¡œ ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        temporal_features, (hidden, cell) = self.temporal_encoder(frame_features)  # [B, T, hidden_size]
        
        # ë ˆì´ì–´ ì •ê·œí™”
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        # single_image: [B, 3, 224, 224] ë˜ëŠ” [3, 224, 224]
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        with torch.cuda.amp.autocast(enabled=True):
            spatial_features = self.backbone(single_image)  # [B, backbone_output_size]
        
        spatial_features = self.feature_projection(spatial_features)  # [B, hidden_size]
        
        return spatial_features
    
    def get_feature_maps(self, image_sequence: torch.Tensor) -> dict:
        """ì¤‘ê°„ íŠ¹ì§• ë§µë“¤ì„ ë°˜í™˜ (ë””ë²„ê¹…/ë¶„ì„ìš©)"""
        B, T, C, H, W = image_sequence.shape
        images_flat = image_sequence.view(B * T, C, H, W)
        
        features = {}
        
        # EfficientNetì˜ ì¤‘ê°„ íŠ¹ì§•ë“¤ ì¶”ì¶œ
        if self.backbone_name == "efficientnet_v2_s":
            x = images_flat
            for i, layer in enumerate(self.backbone.features):
                x = layer(x)
                if i in [2, 4, 6]:  # ì„ íƒëœ ë ˆì´ì–´ì˜ íŠ¹ì§• ì €ì¥
                    features[f"stage_{i}"] = x.view(B, T, *x.shape[1:])
        
        return features


class MobileImageEncoderLite(nn.Module):
    """
    ê²½ëŸ‰í™”ëœ Mobile Image Encoder (Jetsonìš©)
    ë” ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ í•µì‹¬ ê¸°ëŠ¥ ìœ ì§€
    """
    
    def __init__(
        self,
        hidden_size: int = 512,
        num_lstm_layers: int = 1,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # ê²½ëŸ‰í™”ëœ CNN ë°±ë³¸ (MobileNet V3)
        self.backbone = models.mobilenet_v3_small(pretrained=True)
        backbone_output_size = 1000
        
        # íŠ¹ì§• ë§¤í•‘ (ë” ì‘ì€ hidden_size)
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ê°„ë‹¨í•œ GRU (LSTMë³´ë‹¤ íŒŒë¼ë¯¸í„° ì ìŒ)
        self.temporal_encoder = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸš€ Mobile Image Encoder Lite ì´ˆê¸°í™” (Hidden: {hidden_size})")
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        # ê²½ëŸ‰í™”ëœ CNN
        spatial_features = self.backbone(single_image)
        spatial_features = self.feature_projection(spatial_features)
        
        return spatial_features
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        B, T, C, H, W = image_sequence.shape
        
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # ê²½ëŸ‰í™”ëœ CNN
        frame_features = self.backbone(images_flat)
        frame_features = self.feature_projection(frame_features)
        frame_features = frame_features.view(B, T, self.hidden_size)
        
        # GRUë¡œ ì‹œê°„ì  íŠ¹ì§•
        temporal_features, hidden = self.temporal_encoder(frame_features)
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile Image Encoder í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    encoder = MobileImageEncoder(hidden_size=768)
    encoder_lite = MobileImageEncoderLite(hidden_size=512)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (mobile_vla_data_collector.py í˜•ì‹)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    
    print(f"ğŸ“Š ì…ë ¥ ì´ë¯¸ì§€: {test_images.shape}")
    
    # ì¼ë°˜ ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        features = encoder(test_images)
        print(f"ğŸ–¼ï¸ ì¸ì½”ë” ì¶œë ¥: {features.shape}")
        
        features_lite = encoder_lite(test_images)
        print(f"ğŸš€ Lite ì¸ì½”ë” ì¶œë ¥: {features_lite.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"ğŸ“Š ì¼ë°˜ ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
    print(f"ğŸš€ Lite ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"ğŸ’¡ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
*** Begin Patch
*** Update File: Robo+/Mobile_VLA/models/encoders/korean_text_encoder.py
@@
-            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
-            self.text_encoder = AutoModel.from_pretrained(model_name)
+            # ì˜¤í”„ë¼ì¸/Jetson í™˜ê²½ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ëŒ€ë¹„
+            self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
+            self.text_encoder = AutoModel.from_pretrained(model_name, local_files_only=False)
         except Exception as e:
             logger.warning(f"âš ï¸ KLUE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, DistilBERTë¡œ ëŒ€ì²´: {e}")
-            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased")
-            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased")
+            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
+            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
*** End Patch
```

```python
#!/usr/bin/env python3
"""
Korean Text Encoder - í•œêµ­ì–´ ë„¤ë¹„ê²Œì´ì…˜ ëª…ë ¹ì–´ ì¸ì½”ë”©
mobile_vla_data_collector.pyì˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ í•œêµ­ì–´ ëª…ë ¹ì–´ ì²˜ë¦¬
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)


class KoreanTextEncoder(nn.Module):
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë” (Mobile VLA ì‹œë‚˜ë¦¬ì˜¤ë³„ ëª…ë ¹ì–´ íŠ¹í™”)
    - KLUE RoBERTa ê¸°ë°˜ í•œêµ­ì–´ ì´í•´
    - ì‹œë‚˜ë¦¬ì˜¤ë³„ ëª…ë ¹ì–´ ë§¤í•‘
    - mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ì™€ ì™„ì „ í˜¸í™˜
    """
    
    def __init__(
        self,
        model_name: str = "klue/roberta-base",
        hidden_size: int = 768,
        max_length: int = 128,
        freeze_encoder: bool = False
    ):
        super().__init__()
        
        self.model_name = model_name
        self.hidden_size = hidden_size
        self.max_length = max_length
        
        # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (ì˜¤í”„ë¼ì¸ í™˜ê²½ ëŒ€ë¹„)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
            self.text_encoder = AutoModel.from_pretrained(model_name, local_files_only=False)
            logger.info(f"âœ… í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ KLUE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, DistilBERTë¡œ ëŒ€ì²´: {e}")
            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
        
        # ì¸ì½”ë” ê°€ì¤‘ì¹˜ ê³ ì • ì˜µì…˜
        if freeze_encoder:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
            logger.info("ğŸ”’ í…ìŠ¤íŠ¸ ì¸ì½”ë” ê°€ì¤‘ì¹˜ ê³ ì •ë¨")
        
        # mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ë³„ í•œêµ­ì–´ ëª…ë ¹ì–´
        self.scenario_instructions = {
            "1box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "1box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_hori_left": "ê°€ì¥ ì™¼ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "2box_hori_right": "ê°€ì¥ ì˜¤ë¥¸ìª½ ì™¸ê³½ìœ¼ë¡œ ëŒì•„ ì»µê¹Œì§€ ê°€ì„¸ìš”"
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”© (8ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤)
        self.scenario_embedding = nn.Embedding(8, hidden_size)
        
        # ì‹œë‚˜ë¦¬ì˜¤ ID ë§¤í•‘
        self.scenario_to_id = {
            scenario: idx for idx, scenario in enumerate(self.scenario_instructions.keys())
        }
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• í”„ë¡œì ì…˜ (KLUE RoBERTa: 768 â†’ hidden_size)
        encoder_dim = self.text_encoder.config.hidden_size
        if encoder_dim != hidden_size:
            self.text_projection = nn.Linear(encoder_dim, hidden_size)
        else:
            self.text_projection = nn.Identity()
        
        # ì‹œë‚˜ë¦¬ì˜¤ì™€ í…ìŠ¤íŠ¸ ìœµí•©
        self.fusion_layer = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸ—£ï¸ Korean Text Encoder ì´ˆê¸°í™” ì™„ë£Œ (Hidden: {hidden_size})")
    
    def forward(
        self, 
        instructions: List[str], 
        scenarios: Optional[List[str]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            instructions: í•œêµ­ì–´ ëª…ë ¹ì–´ ë¦¬ìŠ¤íŠ¸ ["ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ...", ...]
            scenarios: ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ["1box_vert_left", ...] (ì˜µì…˜)
            
        Returns:
            dict with:
                - text_features: [B, seq_len, hidden_size] - í…ìŠ¤íŠ¸ íŠ¹ì§•
                - scenario_features: [B, hidden_size] - ì‹œë‚˜ë¦¬ì˜¤ íŠ¹ì§• (scenarios ì œê³µì‹œ)
                - fused_features: [B, hidden_size] - ìœµí•©ëœ íŠ¹ì§•
        """
        batch_size = len(instructions)
        device = next(self.parameters()).device
        
        # 1. í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenizer(
            instructions,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        ).to(device)
        
        # 2. í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        with torch.cuda.amp.autocast(enabled=True):
            text_outputs = self.text_encoder(**tokenized)
        
        # í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ ë° í”„ë¡œì ì…˜
        text_features = text_outputs.last_hidden_state  # [B, seq_len, encoder_dim]
        text_features = self.text_projection(text_features)  # [B, seq_len, hidden_size]
        
        # í…ìŠ¤íŠ¸ í’€ë§ (í‰ê· )
        attention_mask = tokenized['attention_mask'].unsqueeze(-1)  # [B, seq_len, 1]
        text_pooled = (text_features * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)  # [B, hidden_size]
        
        result = {
            "text_features": text_features,
            "text_pooled": text_pooled
        }
        
        # 3. ì‹œë‚˜ë¦¬ì˜¤ ì¸ì½”ë”© (ì œê³µëœ ê²½ìš°)
        if scenarios is not None:
            scenario_ids = []
            for scenario in scenarios:
                scenario_id = self.scenario_to_id.get(scenario, 0)  # unknownì€ 0ë²ˆ
                scenario_ids.append(scenario_id)
            
            scenario_ids = torch.tensor(scenario_ids, device=device)  # [B]
            scenario_features = self.scenario_embedding(scenario_ids)  # [B, hidden_size]
            
            # 4. í…ìŠ¤íŠ¸-ì‹œë‚˜ë¦¬ì˜¤ ìœµí•©
            # ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì¿¼ë¦¬ë¡œ, í…ìŠ¤íŠ¸ë¥¼ í‚¤-ë°¸ë¥˜ë¡œ ì‚¬ìš©
            scenario_query = scenario_features.unsqueeze(1)  # [B, 1, hidden_size]
            fused_features, attention_weights = self.fusion_layer(
                query=scenario_query,      # [B, 1, hidden_size]
                key=text_features,         # [B, seq_len, hidden_size]
                value=text_features,       # [B, seq_len, hidden_size]
                key_padding_mask=~tokenized['attention_mask'].bool()  # íŒ¨ë”© ë§ˆìŠ¤í¬
            )
            
            fused_features = fused_features.squeeze(1)  # [B, hidden_size]
            fused_features = self.layer_norm(fused_features)
            
            result.update({
                "scenario_features": scenario_features,
                "fused_features": fused_features,
                "attention_weights": attention_weights
            })
        else:
            # ì‹œë‚˜ë¦¬ì˜¤ ì—†ì´ëŠ” í…ìŠ¤íŠ¸ í’€ë§ë§Œ ì‚¬ìš©
            result["fused_features"] = self.layer_norm(text_pooled)
        
        return result
    
    def encode_scenarios_only(self, scenarios: List[str]) -> torch.Tensor:
        """ì‹œë‚˜ë¦¬ì˜¤ë§Œìœ¼ë¡œ ì„ë² ë”© ìƒì„± (ë¹ ë¥¸ ì¶”ë¡ ìš©)"""
        device = next(self.parameters()).device
        
        scenario_ids = []
        for scenario in scenarios:
            scenario_id = self.scenario_to_id.get(scenario, 0)
            scenario_ids.append(scenario_id)
        
        scenario_ids = torch.tensor(scenario_ids, device=device)
        scenario_features = self.scenario_embedding(scenario_ids)
        
        return scenario_features
    
    def get_instruction_for_scenario(self, scenario: str) -> str:
        """ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€ì‘í•˜ëŠ” í•œêµ­ì–´ ëª…ë ¹ì–´ ë°˜í™˜"""
        return self.scenario_instructions.get(scenario, "ì»µê¹Œì§€ ê°€ì„¸ìš”")
    
    def batch_encode_scenarios(self, scenarios: List[str]) -> Dict[str, torch.Tensor]:
        """ì‹œë‚˜ë¦¬ì˜¤ ë°°ì¹˜ë¥¼ í•œêµ­ì–´ ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ì—¬ ì¸ì½”ë”©"""
        instructions = [self.get_instruction_for_scenario(scenario) for scenario in scenarios]
        return self.forward(instructions, scenarios)


class KoreanTextEncoderLite(nn.Module):
    """
    ê²½ëŸ‰í™”ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë” (Jetsonìš©)
    ì‚¬ì „ ì •ì˜ëœ ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©ë§Œ ì‚¬ìš©
    """
    
    def __init__(self, hidden_size: int = 512):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‚¬ì „ ì •ì˜ëœ ì„ë² ë”© (í•™ìŠµ ê°€ëŠ¥)
        self.scenario_embedding = nn.Embedding(8, hidden_size)
        
        # mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘
        self.scenario_to_id = {
            "1box_vert_left": 0, "1box_vert_right": 1,
            "1box_hori_left": 2, "1box_hori_right": 3,
            "2box_vert_left": 4, "2box_vert_right": 5,
            "2box_hori_left": 6, "2box_hori_right": 7
        }
        
        logger.info(f"ğŸš€ Korean Text Encoder Lite ì´ˆê¸°í™” (Hidden: {hidden_size})")
    
    def forward(self, scenarios: List[str]) -> torch.Tensor:
        """ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ë§Œìœ¼ë¡œ ì„ë² ë”© ìƒì„±"""
        device = next(self.parameters()).device
        
        scenario_ids = []
        for scenario in scenarios:
            scenario_id = self.scenario_to_id.get(scenario, 0)
            scenario_ids.append(scenario_id)
        
        scenario_ids = torch.tensor(scenario_ids, device=device)
        scenario_features = self.scenario_embedding(scenario_ids)
        
        return scenario_features


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Korean Text Encoder í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    encoder = KoreanTextEncoder(hidden_size=768)
    encoder_lite = KoreanTextEncoderLite(hidden_size=512)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_instructions = [
        "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
        "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì˜¤ë¥¸ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”"
    ]
    test_scenarios = ["1box_vert_left", "2box_vert_right"]
    
    print(f"ğŸ“Š ì…ë ¥ ëª…ë ¹ì–´: {test_instructions}")
    print(f"ğŸ¯ ì…ë ¥ ì‹œë‚˜ë¦¬ì˜¤: {test_scenarios}")
    
    # ì¼ë°˜ ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        result = encoder(test_instructions, test_scenarios)
        print(f"ğŸ—£ï¸ í…ìŠ¤íŠ¸ íŠ¹ì§•: {result['text_features'].shape}")
        print(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ íŠ¹ì§•: {result['scenario_features'].shape}")
        print(f"ğŸ”„ ìœµí•© íŠ¹ì§•: {result['fused_features'].shape}")
        
        # Lite ì¸ì½”ë” í…ŒìŠ¤íŠ¸
        lite_result = encoder_lite(test_scenarios)
        print(f"ğŸš€ Lite íŠ¹ì§•: {lite_result.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"ğŸ“Š ì¼ë°˜ ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
    print(f"ğŸš€ Lite ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"ğŸ’¡ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA Model - Pure Mobile VLM without Calvin dependencies
RoboVLMsì˜ VLM ê¸°ìˆ ì„ mobile_vla_data_collector.pyì— ì™„ì „ ì ì‘
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
import logging

try:
    from .encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from .encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from .policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite
except ImportError:
    # í…ŒìŠ¤íŠ¸ìš© ì ˆëŒ€ ì„í¬íŠ¸
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite

logger = logging.getLogger(__name__)


class MobileVLAModel(nn.Module):
    """
    Pure Mobile VLA Model
    - ì…ë ¥: mobile_vla_data_collector.py ë°ì´í„° í˜•ì‹
    - ì¶œë ¥: 3D ì•¡ì…˜ + ì´ë²¤íŠ¸ ì˜ˆì¸¡
    - Calvin ì˜ì¡´ì„± ì—†ëŠ” ìˆœìˆ˜ Mobile VLM
    """
    
    def __init__(
        self,
        # ëª¨ë¸ í¬ê¸° ì„¤ì •
        hidden_size: int = 768,
        
        # ì´ë¯¸ì§€ ì¸ì½”ë” ì„¤ì •
        image_backbone: str = "efficientnet_v2_s",
        freeze_image_backbone: bool = False,
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” ì„¤ì •  
        text_model: str = "klue/roberta-base",
        freeze_text_encoder: bool = False,
        
        # ì •ì±… í—¤ë“œ ì„¤ì •
        use_policy_lstm: bool = True,
        policy_lstm_layers: int = 2,
        
        # ì¼ë°˜ ì„¤ì •
        dropout: float = 0.1,
        use_lite_mode: bool = False  # Jetsonìš© ê²½ëŸ‰í™” ëª¨ë“œ
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_lite_mode = use_lite_mode
        
        # ê²½ëŸ‰í™” ëª¨ë“œì— ë”°ë¥¸ ì»´í¬ë„ŒíŠ¸ ì„ íƒ
        if use_lite_mode:
            # Jetsonìš© ê²½ëŸ‰í™” ëª¨ë¸
            self.image_encoder = MobileImageEncoderLite(
                hidden_size=hidden_size // 2,  # ë” ì‘ì€ hidden_size
                dropout=dropout
            )
            self.text_encoder = KoreanTextEncoderLite(
                hidden_size=hidden_size // 2
            )
            self.policy_head = MobilePolicyHeadLite(
                hidden_size=hidden_size,  # ìœµí•© í›„ì—ëŠ” ì›ë˜ í¬ê¸°
                dropout=dropout
            )
            logger.info("ğŸš€ Lite ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨ (Jetson ìµœì í™”)")
        else:
            # ì¼ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸
            self.image_encoder = MobileImageEncoder(
                backbone=image_backbone,
                hidden_size=hidden_size,
                dropout=dropout,
                freeze_backbone=freeze_image_backbone
            )
            self.text_encoder = KoreanTextEncoder(
                model_name=text_model,
                hidden_size=hidden_size,
                freeze_encoder=freeze_text_encoder
            )
            self.policy_head = MobilePolicyHead(
                hidden_size=hidden_size,
                dropout=dropout,
                use_lstm=use_policy_lstm,
                lstm_layers=policy_lstm_layers
            )
            logger.info("ğŸ’ª Full ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨ (ê³ ì„±ëŠ¥)")
        
        # ë©€í‹°ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´
        if use_lite_mode:
            # ê²½ëŸ‰í™”ëœ ìœµí•©
            self.multimodal_fusion = nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
        else:
            # ì–´í…ì…˜ ê¸°ë°˜ ìœµí•©
            self.multimodal_fusion = nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # ëª¨ë¸ í†µê³„
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ¤– Mobile VLA Model ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
        logger.info(f"   Hidden Size: {hidden_size}, Lite Mode: {use_lite_mode}")
    
    def forward(
        self,
        images: torch.Tensor,
        scenarios: List[str],
        instructions: Optional[List[str]] = None,
        return_intermediate: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: [B, T, 3, 224, 224] - ì´ë¯¸ì§€ ì‹œí€€ìŠ¤  
            scenarios: List[str] - ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ë“¤ ["1box_vert_left", ...]
            instructions: List[str] - í•œêµ­ì–´ ëª…ë ¹ì–´ (ì˜µì…˜, scenariosì—ì„œ ìë™ ìƒì„± ê°€ëŠ¥)
            return_intermediate: ì¤‘ê°„ íŠ¹ì§•ë“¤ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict with:
                - actions: [B, T, 3] - ì •ê·œí™”ëœ ì•¡ì…˜
                - actions_denorm: [B, T, 3] - ì‹¤ì œ ë²”ìœ„ ì•¡ì…˜
                - event_logits: [B, T, 3] - ì´ë²¤íŠ¸ ë¶„ë¥˜ ë¡œì§“
                - predicted_events: [B, T] - ì˜ˆì¸¡ëœ ì´ë²¤íŠ¸
        """
        batch_size = images.shape[0]
        
        # 1. ì´ë¯¸ì§€ ì¸ì½”ë”©
        image_features = self.image_encoder(images)  # [B, T, hidden_size//2 or hidden_size]
        
        # 2. í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        if self.use_lite_mode:
            # Lite ëª¨ë“œ: ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì‚¬ìš©
            text_features = self.text_encoder(scenarios)  # [B, hidden_size//2]
            # ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í™•ì¥
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size//2]
        else:
            # Full ëª¨ë“œ: í•œêµ­ì–´ ëª…ë ¹ì–´ ì‚¬ìš©
            if instructions is None:
                # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í•œêµ­ì–´ ëª…ë ¹ì–´ ìë™ ìƒì„±
                instructions = [
                    self.text_encoder.get_instruction_for_scenario(scenario) 
                    for scenario in scenarios
                ]
            
            text_result = self.text_encoder(instructions, scenarios)
            text_features = text_result["fused_features"]  # [B, hidden_size]
            # ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í™•ì¥
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size]
        
        # 3. ë©€í‹°ëª¨ë‹¬ ìœµí•©
        if self.use_lite_mode:
            # ê²½ëŸ‰í™”ëœ ìœµí•©: ë‹¨ìˆœ concatenation + MLP
            multimodal_features = torch.cat([image_features, text_features], dim=-1)  # [B, T, hidden_size]
            multimodal_features = self.multimodal_fusion(multimodal_features)
        else:
            # ì–´í…ì…˜ ê¸°ë°˜ ìœµí•©
            # ì´ë¯¸ì§€ë¥¼ ì¿¼ë¦¬ë¡œ, í…ìŠ¤íŠ¸ë¥¼ í‚¤-ë°¸ë¥˜ë¡œ ì‚¬ìš©
            fused_features, attention_weights = self.multimodal_fusion(
                query=image_features,     # [B, T, hidden_size]
                key=text_features,        # [B, T, hidden_size]  
                value=text_features       # [B, T, hidden_size]
            )
            multimodal_features = fused_features
        
        # ì¶œë ¥ ì •ê·œí™”
        multimodal_features = self.output_norm(multimodal_features)  # [B, T, hidden_size]
        
        # 4. ì •ì±… í—¤ë“œë¡œ ì•¡ì…˜ ì˜ˆì¸¡
        policy_output = self.policy_head(multimodal_features)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "actions": policy_output["actions"],                    # [B, T, 3] ì •ê·œí™”ëœ
            "actions_denorm": policy_output["actions_denorm"],      # [B, T, 3] ì‹¤ì œ ë²”ìœ„
            "event_logits": policy_output["event_logits"],          # [B, T, 3]
            "event_probs": policy_output.get("event_probs"),        # [B, T, 3]
            "predicted_events": policy_output["predicted_events"]   # [B, T]
        }
        
        # ì¤‘ê°„ íŠ¹ì§•ë“¤ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        if return_intermediate:
            result.update({
                "image_features": image_features,
                "text_features": text_features,
                "multimodal_features": multimodal_features,
                "attention_weights": attention_weights if (not self.use_lite_mode and 'attention_weights' in locals()) else None
            })
        
        return result
    
    def inference_single_step(
        self,
        current_image: torch.Tensor,
        scenario: str,
        hidden_state: Optional[Tuple] = None
    ) -> Tuple[Dict[str, torch.Tensor], Optional[Tuple]]:
        """
        ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  (ì‹¤ì‹œê°„ mobile_vla_data_collector ì—°ë™ìš©)
        
        Args:
            current_image: [1, 3, 224, 224] - í˜„ì¬ ì´ë¯¸ì§€
            scenario: str - í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤
            hidden_state: LSTM hidden state (ìˆë‹¤ë©´)
            
        Returns:
            (ì•¡ì…˜ ì˜ˆì¸¡ ê²°ê³¼, ìƒˆë¡œìš´ hidden_state)
        """
        with torch.no_grad():
            # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì‹œí€€ìŠ¤ë¡œ í™•ì¥
            image_sequence = current_image.unsqueeze(1)  # [1, 1, 3, 224, 224]
            
            # ì¸ì½”ë”©
            image_features = self.image_encoder.extract_spatial_features(current_image)  # [1, hidden_size]
            
            if self.use_lite_mode:
                text_features = self.text_encoder([scenario])  # [1, hidden_size//2]
                # ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§•ì˜ ì°¨ì›ì„ ë§ì¶¤
                image_features_lite = image_features  # [1, hidden_size//2] (256)
                multimodal_features = torch.cat([image_features_lite, text_features], dim=-1)  # [1, 512]
                multimodal_features = self.multimodal_fusion(multimodal_features)
            else:
                instruction = self.text_encoder.get_instruction_for_scenario(scenario)
                text_result = self.text_encoder([instruction], [scenario])
                text_features = text_result["fused_features"]  # [1, hidden_size]
                
                # ê°„ë‹¨í•œ ìœµí•© (ì–´í…ì…˜ ì—†ì´)
                multimodal_features = (image_features + text_features) / 2
            
            multimodal_features = self.output_norm(multimodal_features)
            
            # ì •ì±… í—¤ë“œë¡œ ë‹¨ì¼ ìŠ¤í… ì˜ˆì¸¡
            if hasattr(self.policy_head, 'predict_single_step') and not self.use_lite_mode:
                action_result, new_hidden_state = self.policy_head.predict_single_step(
                    multimodal_features, hidden_state
                )
            else:
                action_result = self.policy_head(multimodal_features.unsqueeze(1))
                # ë‹¨ì¼ ìŠ¤í…ìœ¼ë¡œ ì••ì¶•
                for key in action_result:
                    if action_result[key].dim() > 1:
                        action_result[key] = action_result[key].squeeze(1)
                new_hidden_state = None
        
        return action_result, new_hidden_state
    
    def get_mobile_vla_action(
        self,
        current_image: torch.Tensor,
        scenario: str
    ) -> Dict[str, float]:
        """
        mobile_vla_data_collector.py í˜¸í™˜ ì•¡ì…˜ ë°˜í™˜
        
        Returns:
            Dict with keys: linear_x, linear_y, angular_z, event_type
        """
        action_result, _ = self.inference_single_step(current_image, scenario)
        
        # ì•¡ì…˜ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°°ì¹˜)
        actions = action_result["actions_denorm"][0].cpu().numpy()  # [3]
        predicted_event = action_result["predicted_events"][0].cpu().item()
        
        # ì´ë²¤íŠ¸ íƒ€ì… ë§¤í•‘
        event_types = ["episode_start", "start_action", "stop_action"]
        event_type = event_types[predicted_event]
        
        return {
            "linear_x": float(actions[0]),
            "linear_y": float(actions[1]),
            "angular_z": float(actions[2]),
            "event_type": event_type
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile VLA Model í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (Full & Lite)
    model_full = MobileVLAModel(hidden_size=768, use_lite_mode=False)
    model_lite = MobileVLAModel(hidden_size=512, use_lite_mode=True)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (mobile_vla_data_collector.py í˜•ì‹)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    test_scenarios = ["1box_vert_left", "2box_hori_right"]
    
    print(f"ğŸ“Š ì…ë ¥ ì´ë¯¸ì§€: {test_images.shape}")
    print(f"ğŸ¯ ì…ë ¥ ì‹œë‚˜ë¦¬ì˜¤: {test_scenarios}")
    
    # Full ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ’ª Full Model í…ŒìŠ¤íŠ¸:")
    with torch.no_grad():
        result_full = model_full(test_images, test_scenarios, return_intermediate=True)
        print(f"   ì•¡ì…˜: {result_full['actions'].shape}")
        print(f"   ì‹¤ì œ ì•¡ì…˜: {result_full['actions_denorm'].shape}")
        print(f"   ì´ë²¤íŠ¸: {result_full['predicted_events'].shape}")
    
    # Lite ëª¨ë¸ í…ŒìŠ¤íŠ¸  
    print("\nğŸš€ Lite Model í…ŒìŠ¤íŠ¸:")
    with torch.no_grad():
        result_lite = model_lite(test_images, test_scenarios)
        print(f"   ì•¡ì…˜: {result_lite['actions'].shape}")
        print(f"   ì‹¤ì œ ì•¡ì…˜: {result_lite['actions_denorm'].shape}")
        print(f"   ì´ë²¤íŠ¸: {result_lite['predicted_events'].shape}")
    
    # ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  í…ŒìŠ¤íŠ¸:")
    single_image = torch.randn(1, 3, 224, 224)
    scenario = "1box_vert_left"
    
    mobile_action = model_full.get_mobile_vla_action(single_image, scenario)
    print(f"   Mobile ì•¡ì…˜: {mobile_action}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
    full_params = sum(p.numel() for p in model_full.parameters())
    lite_params = sum(p.numel() for p in model_lite.parameters())
    
    print(f"\nğŸ“Š ëª¨ë¸ í¬ê¸° ë¹„êµ:")
    print(f"   Full ëª¨ë¸: {full_params:,}ê°œ ({full_params/1e6:.1f}M)")
    print(f"   Lite ëª¨ë¸: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"   ê²½ëŸ‰í™”ìœ¨: {(1 - lite_params/full_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA í•™ìŠµ ëŸ°ì²˜ (ì‹¤ì œ mobile_vla_data_collector ë°ì´í„°ë¡œ ìŠ¤ëª¨í¬ í•™ìŠµ)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader

try:
    from .mobile_trainer_simple import SimpleMobileVLATrainer
    from ..data.mobile_dataset import MobileVLADataset
except ImportError:
    import sys
    from pathlib import Path
    this_dir = Path(__file__).resolve().parent
    sys.path.append(str(this_dir.parent))
    from training.mobile_trainer_simple import SimpleMobileVLATrainer
    from data.mobile_dataset import MobileVLADataset


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='ìŠ¤ëª¨í¬: ëª‡ ìŠ¤í…ë§Œ í•™ìŠµ')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # ë°ì´í„°ì…‹
    dataset = MobileVLADataset(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('âš ï¸ HDF5 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. data_dirë¥¼ í™•ì¸í•˜ì„¸ìš”:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # íŠ¸ë ˆì´ë„ˆ
    trainer = SimpleMobileVLATrainer(configs)

    # ìŠ¤ëª¨í¬ í•™ìŠµ ë£¨í”„
    steps = 0
    for batch in loader:
        # ë°°ì¹˜ dict í…ì„œ ì™¸ í•„ë“œ ìœ ì§€
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('âœ… ìŠ¤ëª¨í¬ í•™ìŠµ ì™„ë£Œ: steps =', steps)


if __name__ == '__main__':
    main()


```

ì§€ê¸ˆ ìŠ¤ëª¨í¬ í•™ìŠµ ì‹¤í–‰ì—ì„œ `lightning` ì˜ì¡´ì„±ìœ¼ë¡œ importê°€ ìƒˆì–´ ë“¤ì–´ì™”ìŠµë‹ˆë‹¤. ëŸ°ì²˜ê°€ íŒ¨í‚¤ì§€ ì„í¬íŠ¸ë¥¼ í”¼í•˜ë„ë¡ íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ ë™ì  ì„í¬íŠ¸ë¡œ ë°”ê¾¸ê³ , `MobileImageEncoderLite`ì˜ ì‚¬ì „ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œë„ ë§‰ì€ ë’¤ ë‹¤ì‹œ í•™ìŠµì„ ëŒë¦¬ê² ìŠµë‹ˆë‹¤.

```python
#!/usr/bin/env python3
"""
Mobile VLA í•™ìŠµ ëŸ°ì²˜ (ì‹¤ì œ mobile_vla_data_collector ë°ì´í„°ë¡œ ìŠ¤ëª¨í¬ í•™ìŠµ)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader
import importlib.util
from pathlib import Path


def _load_attr_from_file(module_name: str, file_path: str, attr: str):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    assert spec and spec.loader
    spec.loader.exec_module(module)  # type: ignore
    return getattr(module, attr)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='ìŠ¤ëª¨í¬: ëª‡ ìŠ¤í…ë§Œ í•™ìŠµ')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # ë°ì´í„°ì…‹
    dataset = MobileVLADataset(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('âš ï¸ HDF5 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. data_dirë¥¼ í™•ì¸í•˜ì„¸ìš”:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # íŠ¸ë ˆì´ë„ˆ (ë™ì  ì„í¬íŠ¸ë¡œ íŒ¨í‚¤ì§€ ì˜ì¡´ì„± íšŒí”¼)
    root = Path(__file__).resolve().parents[1]
    trainer_cls = _load_attr_from_file(
        'mobile_trainer_simple',
        str(root / 'training' / 'mobile_trainer_simple.py'),
        'SimpleMobileVLATrainer'
    )
    trainer = trainer_cls(configs)

    # ìŠ¤ëª¨í¬ í•™ìŠµ ë£¨í”„
    steps = 0
    for batch in loader:
        # ë°°ì¹˜ dict í…ì„œ ì™¸ í•„ë“œ ìœ ì§€
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('âœ… ìŠ¤ëª¨í¬ í•™ìŠµ ì™„ë£Œ: steps =', steps)


if __name__ == '__main__':
    main()



```

```python
#!/usr/bin/env python3
"""
Mobile Image Encoder - mobile_vla_data_collector.pyì˜ 720p ì´ë¯¸ì§€ ì²˜ë¦¬ íŠ¹í™”
RoboVLMsì˜ ì´ë¯¸ì§€ ì¸ì½”ë”© ê¸°ìˆ ì„ Mobile VLAì— ì ìš©
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class MobileImageEncoder(nn.Module):
    """
    Mobile VLA íŠ¹í™” ì´ë¯¸ì§€ ì¸ì½”ë”
    - ì…ë ¥: [B, T, 3, 224, 224] (mobile_vla_data_collector.pyì—ì„œ 720pâ†’224p ë¦¬ì‚¬ì´ì¦ˆë¨)
    - ì¶œë ¥: [B, T, hidden_size] ì‹œê°„ì  íŠ¹ì§•
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_v2_s",
        hidden_size: int = 768,
        num_lstm_layers: int = 2,
        dropout: float = 0.1,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.backbone_name = backbone
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        
        # ë°±ë³¸ CNN (EfficientNet V2 - ëª¨ë°”ì¼ ìµœì í™”)
        if backbone == "efficientnet_v2_s":
            # Jetson/ì˜¤í”„ë¼ì¸ í™˜ê²½ ëŒ€ë¹„: ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™”
            self.backbone = models.efficientnet_v2_s(weights=None)
            backbone_output_size = 1000
        elif backbone == "resnet50":
            self.backbone = models.resnet50(weights=None)
            backbone_output_size = 2048
        elif backbone == "mobilenet_v3_large":
            self.backbone = models.mobilenet_v3_large(weights=None)  
            backbone_output_size = 1000
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ë³¸: {backbone}")
        
        # ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì • ì˜µì…˜
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
            logger.info(f"ğŸ”’ {backbone} ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì •ë¨")
        
        # CNN íŠ¹ì§•ì„ hidden_sizeë¡œ ë§¤í•‘
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ (18í”„ë ˆì„ ì‹œí€€ìŠ¤)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,  # ì–‘ë°©í–¥ì´ë¯€ë¡œ ì ˆë°˜
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸ–¼ï¸ Mobile Image Encoder ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë°±ë³¸: {backbone}, Hidden: {hidden_size}, LSTM Layers: {num_lstm_layers}")
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_sequence: [B, T, 3, 224, 224] - ë°°ì¹˜ í¬ê¸° B, ì‹œí€€ìŠ¤ ê¸¸ì´ T
            
        Returns:
            temporal_features: [B, T, hidden_size] - ì‹œê°„ì  ì´ë¯¸ì§€ íŠ¹ì§•
        """
        B, T, C, H, W = image_sequence.shape
        
        # ë°°ì¹˜ì™€ ì‹œê°„ ì°¨ì›ì„ í•©ì³ì„œ CNNì— ì…ë ¥
        images_flat = image_sequence.view(B * T, C, H, W)  # [B*T, 3, 224, 224]
        
        # CNNìœ¼ë¡œ ê° í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ
        with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
            frame_features = self.backbone(images_flat)  # [B*T, backbone_output_size]
        
        # íŠ¹ì§• ì°¨ì›ì„ hidden_sizeë¡œ ë§¤í•‘
        frame_features = self.feature_projection(frame_features)  # [B*T, hidden_size]
        
        # ì‹œê°„ ì°¨ì› ë³µì›
        frame_features = frame_features.view(B, T, self.hidden_size)  # [B, T, hidden_size]
        
        # LSTMìœ¼ë¡œ ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        temporal_features, (hidden, cell) = self.temporal_encoder(frame_features)  # [B, T, hidden_size]
        
        # ë ˆì´ì–´ ì •ê·œí™”
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        # single_image: [B, 3, 224, 224] ë˜ëŠ” [3, 224, 224]
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        with torch.cuda.amp.autocast(enabled=True):
            spatial_features = self.backbone(single_image)  # [B, backbone_output_size]
        
        spatial_features = self.feature_projection(spatial_features)  # [B, hidden_size]
        
        return spatial_features
    
    def get_feature_maps(self, image_sequence: torch.Tensor) -> dict:
        """ì¤‘ê°„ íŠ¹ì§• ë§µë“¤ì„ ë°˜í™˜ (ë””ë²„ê¹…/ë¶„ì„ìš©)"""
        B, T, C, H, W = image_sequence.shape
        images_flat = image_sequence.view(B * T, C, H, W)
        
        features = {}
        
        # EfficientNetì˜ ì¤‘ê°„ íŠ¹ì§•ë“¤ ì¶”ì¶œ
        if self.backbone_name == "efficientnet_v2_s":
            x = images_flat
            for i, layer in enumerate(self.backbone.features):
                x = layer(x)
                if i in [2, 4, 6]:  # ì„ íƒëœ ë ˆì´ì–´ì˜ íŠ¹ì§• ì €ì¥
                    features[f"stage_{i}"] = x.view(B, T, *x.shape[1:])
        
        return features


class MobileImageEncoderLite(nn.Module):
    """
    ê²½ëŸ‰í™”ëœ Mobile Image Encoder (Jetsonìš©)
    ë” ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ í•µì‹¬ ê¸°ëŠ¥ ìœ ì§€
    """
    
    def __init__(
        self,
        hidden_size: int = 512,
        num_lstm_layers: int = 1,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # ê²½ëŸ‰í™”ëœ CNN ë°±ë³¸ (MobileNet V3)
        self.backbone = models.mobilenet_v3_small(weights=None)
        backbone_output_size = 1000
        
        # íŠ¹ì§• ë§¤í•‘ (ë” ì‘ì€ hidden_size)
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ê°„ë‹¨í•œ GRU (LSTMë³´ë‹¤ íŒŒë¼ë¯¸í„° ì ìŒ)
        self.temporal_encoder = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸš€ Mobile Image Encoder Lite ì´ˆê¸°í™” (Hidden: {hidden_size})")
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        # ê²½ëŸ‰í™”ëœ CNN
        spatial_features = self.backbone(single_image)
        spatial_features = self.feature_projection(spatial_features)
        
        return spatial_features
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        B, T, C, H, W = image_sequence.shape
        
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # ê²½ëŸ‰í™”ëœ CNN
        frame_features = self.backbone(images_flat)
        frame_features = self.feature_projection(frame_features)
        frame_features = frame_features.view(B, T, self.hidden_size)
        
        # GRUë¡œ ì‹œê°„ì  íŠ¹ì§•
        temporal_features, hidden = self.temporal_encoder(frame_features)
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile Image Encoder í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    encoder = MobileImageEncoder(hidden_size=768)
    encoder_lite = MobileImageEncoderLite(hidden_size=512)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (mobile_vla_data_collector.py í˜•ì‹)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    
    print(f"ğŸ“Š ì…ë ¥ ì´ë¯¸ì§€: {test_images.shape}")
    
    # ì¼ë°˜ ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        features = encoder(test_images)
        print(f"ğŸ–¼ï¸ ì¸ì½”ë” ì¶œë ¥: {features.shape}")
        
        features_lite = encoder_lite(test_images)
        print(f"ğŸš€ Lite ì¸ì½”ë” ì¶œë ¥: {features_lite.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"ğŸ“Š ì¼ë°˜ ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
    print(f"ğŸš€ Lite ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"ğŸ’¡ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA í•™ìŠµ ëŸ°ì²˜ (ì‹¤ì œ mobile_vla_data_collector ë°ì´í„°ë¡œ ìŠ¤ëª¨í¬ í•™ìŠµ)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader
import importlib.util
from pathlib import Path


def _load_attr_from_file(module_name: str, file_path: str, attr: str):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    assert spec and spec.loader
    spec.loader.exec_module(module)  # type: ignore
    return getattr(module, attr)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='ìŠ¤ëª¨í¬: ëª‡ ìŠ¤í…ë§Œ í•™ìŠµ')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # ë°ì´í„°ì…‹ (ë™ì  ì„í¬íŠ¸)
    root = Path(__file__).resolve().parents[1]
    dataset_cls = _load_attr_from_file(
        'mobile_dataset',
        str(root / 'data' / 'mobile_dataset.py'),
        'MobileVLADataset'
    )
    dataset = dataset_cls(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('âš ï¸ HDF5 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. data_dirë¥¼ í™•ì¸í•˜ì„¸ìš”:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # íŠ¸ë ˆì´ë„ˆ (ë™ì  ì„í¬íŠ¸ë¡œ íŒ¨í‚¤ì§€ ì˜ì¡´ì„± íšŒí”¼)
    trainer_cls = _load_attr_from_file(
        'mobile_trainer_simple',
        str(root / 'training' / 'mobile_trainer_simple.py'),
        'SimpleMobileVLATrainer'
    )
    trainer = trainer_cls(configs)

    # ìŠ¤ëª¨í¬ í•™ìŠµ ë£¨í”„
    steps = 0
    for batch in loader:
        # ë°°ì¹˜ dict í…ì„œ ì™¸ í•„ë“œ ìœ ì§€
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('âœ… ìŠ¤ëª¨í¬ í•™ìŠµ ì™„ë£Œ: steps =', steps)


if __name__ == '__main__':
    main()



```

### ê³„íš
- ì‹¤ì œ HDF5 êµ¬ì¡°ë¥¼ ìˆ˜ì§‘ê¸°ì™€ ëŒ€ì¡°.
- ì´ë²¤íŠ¸ ë¬¸ìì—´ ì²˜ë¦¬ ë²„ê·¸ ìˆ˜ì •.
- Jetson/ì˜¤í”„ë¼ì¸ ì•ˆì „ì„ ìœ„í•´ ì‚¬ì „ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™”.
- ê°„ë‹¨ ëŸ°ì²˜ ì¶”ê°€ í›„ ì‹¤ì œ ë°ì´í„°ë¡œ ìŠ¤ëª¨í¬ í•™ìŠµ 5ìŠ¤í… ì‹¤í–‰.

### ì§„í–‰
- `action_event_types`ë¥¼ bytesâ†’str ì•ˆì „ ì²˜ë¦¬ë¡œ ìˆ˜ì •í•´ ë§¤í•‘ ì˜¤ë¥˜ ì œê±°.
- `torchvision` ë°±ë³¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ë¹„í™œì„±í™”:
  - `efficientnet_v2_s`, `resnet50`, `mobilenet_v3_large`, `mobilenet_v3_small` ëª¨ë‘ `weights=None`.
- í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë¡œë“œ ê²½ë¡œ ìœ ì§€.
- ë™ì  ì„í¬íŠ¸ ë°©ì‹ì˜ í•™ìŠµ ëŸ°ì²˜ ì¶”ê°€: `Robo+/Mobile_VLA/training/train_mobile_vla.py`.
- ì‹¤ì œ ë°ì´í„°ë¡œ ìŠ¤ëª¨í¬ í•™ìŠµ 5ìŠ¤í… ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë¨.

### ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
- ë°ì´í„°ì…‹: 65 ì—í”¼ì†Œë“œ, 1142 í”„ë ˆì„ ë¡œë“œ.
- Lite ëª¨ë¸(ì•½ 3.7M íŒŒë¼ë¯¸í„°)ë¡œ CPUì—ì„œ ìŠ¤ëª¨í¬ í•™ìŠµ 5ìŠ¤í… ì™„ë£Œ.
- ê° ìŠ¤í… ì¶œë ¥ ì˜ˆ: `total_loss`, `action_loss`, `event_loss`, `action_accuracy`, `event_accuracy` ì •ìƒ ì‚°ì¶œ.

### ì‚¬ìš© ë°©ë²•
- ì•„ë˜ ëª…ë ¹ìœ¼ë¡œ ë°”ë¡œ ì¬í˜„:
```bash
python3 /home/soda/vla/Robo+/Mobile_VLA/training/train_mobile_vla.py \
  --data_dir /home/soda/vla/ROS_action/mobile_vla_dataset \
  --batch_size 2 --max_steps 5 --use_lite_mode --sequence_length 18
```

ì›í•˜ì‹œë©´:
- GPU ì‚¬ìš©, ë°°ì¹˜ í¬ê¸°/ìŠ¤í…/íˆë“  ì‚¬ì´ì¦ˆ ì¡°ì •
- Full ëª¨ë¸ ëª¨ë“œ(`--use_lite_mode` ì œê±°), í•™ìŠµ ë£¨í”„ë¥¼ ì—í­ ë‹¨ìœ„ë¡œ í™•ì¥
- ì²´í¬í¬ì¸íŠ¸/ë¡œê¹… ì¶”ê°€

í•„ìš”í•œ ë‹¤ìŒ ì‘ì—… ìˆìœ¼ì‹œë©´ ì§€ì‹œ ì£¼ì„¸ìš”.

```shellscript
python3 /home/soda/vla/Robo+/Mobile_VLA/training/train_mobile_vla.py \
  --data_dir /home/soda/vla/ROS_action/mobile_vla_dataset \
  --batch_size 2 --max_steps 5 --use_lite_mode --sequence_length 18
```

---

**User**

ì¼ë‹¨ í˜„ì¬ ROS_action/mobile_vla_dataset í´ë” ì•ˆì˜ h5íŒŒì¼ë“¤ì€ gitignoreë¥¼ í’€ì–´ì£¼ê³  lfsì²˜ë¦¬í•˜ê³  ë³€ê²½ë‚´ìš© ìƒì„¸íˆ ì ì–´ì„œ pushí•´ì¤˜

---

**Cursor**

ì§§ê²Œ ì§„í–‰ ìƒí™©: h5 ë¬´ì‹œ ê·œì¹™ì„ ì°¾ì•„ í•´ì œí•˜ê³ , Git LFSë¡œ `ROS_action/mobile_vla_dataset/*.h5`ë¥¼ ì¶”ì í•˜ë„ë¡ ì„¤ì •í•œ ë’¤ ë°ì´í„°ì™€ ê·œì¹™ ë³€ê²½ì„ ìƒì„¸ ë©”ì‹œì§€ë¡œ ì»¤ë°‹/í‘¸ì‹œí•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ì–´ë–¤ `.gitignore`ì—ì„œ h5/í•´ë‹¹ í´ë”ê°€ ë¬´ì‹œë˜ëŠ”ì§€ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.

### ì§„í–‰
- ë£¨íŠ¸ `.gitignore`ì˜ `*.h5` ë¬´ì‹œ ê·œì¹™ í•´ì œ(ì£¼ì„ ì²˜ë¦¬).
- Git LFS ì´ˆê¸°í™” ë° `ROS_action/mobile_vla_dataset/*.h5` ì¶”ì  ì„¤ì •.
- ì‹¤ì œ HDF5 ë°ì´í„°, ë³´ì¡° ë©”íƒ€(`core_patterns.json`), í•™ìŠµ ëŸ°ì²˜ê¹Œì§€ ìƒì„¸ ë©”ì‹œì§€ë¡œ ì»¤ë°‹.
- ì‘ì—… ë¸Œëœì¹˜: `add-mobile-vla-dataset-lfs`.

### ì»¤ë°‹ ë¡œê·¸ ìš”ì§€
- gitignore: stop ignoring `*.h5` (track via LFS)
- lfs: track `ROS_action/mobile_vla_dataset/*.h5` via Git LFS
- data: add Mobile VLA dataset episodes via Git LFS
  - 65ê°œ+ ì—í”¼ì†Œë“œ HDF5 ì¶”ê°€
  - `core_patterns.json` í¬í•¨
  - ëŸ°ì²˜ `Robo+/Mobile_VLA/training/train_mobile_vla.py` ì¶”ê°€
  - HDF5 ìŠ¤í‚¤ë§ˆì™€ ë¡œë” í˜¸í™˜ì„± ë©”ëª¨ í¬í•¨

### ë‹¤ìŒ
- ì›ê²© pushê°€ ì¸ì¦/ê¶Œí•œ ì´ìŠˆë¡œ ë©ˆì¶˜ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì•„ë˜ ì¤‘ ì›í•˜ëŠ” ë°©ì‹ ì•Œë ¤ì£¼ì„¸ìš”:
  - ì›ê²© ì´ë¦„/URL ì§€ì • í›„ push ìˆ˜í–‰
  - í˜„ì¬ origin ìœ ì§€, í† í°/SSH ì„¤ì • í›„ ì¬ì‹œë„
  - ë‹¤ë¥¸ ì›ê²©ìœ¼ë¡œ ìƒˆë¡œ push

ì›í•˜ì‹œëŠ” ì›ê²©(`origin`) ì£¼ì†Œë‚˜ ì ‘ê·¼ ë°©ì‹(HTTPS/SSH)ì„ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ push ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

