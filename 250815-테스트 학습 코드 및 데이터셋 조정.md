# 테스트 학습 코드 및 데이터셋 조정
_Exported on 8/15/2025 at 13:57:49 GMT+9 from Cursor (1.4.5)_

---

**User**

이제@Mobile_VLA/ 이걸 가지고 학습 코드를 테스트 해보자 실제 ㄷ이터셋을 모았으니까 형식이나 그런거 바꿀거 있으면 환각ㅇ벗이 일치시키고

---

**Cursor**

짧게 진행 상황: 실제 수집본과 로더 형식이 일치하는지 먼저 확인하겠습니다. 수집기(`mobile_vla_data_collector.py`)에서 HDF5에 쓰는 키/속성 이름을 스캔하고, 맞으면 데이터셋/모델/간이 트레이너 스모크 테스트를 바로 실행하겠습니다. 그런 다음 실제 데이터로 1 에포크 학습이 가능한 런처 스크립트를 추가해 돌려보겠습니다. 지금 바로 확인과 테스트를 병렬로 시작합니다.

방금 실제 수집 포맷과 로더를 대조해 보니, HDF5의 `action_event_types`가 UTF-8 문자열로 저장되는데 로더는 바이트 문자열로만 매핑해서 이벤트 인덱스가 틀어질 수 있는 버그를 발견했습니다. 또, Jetson에서 가중치 다운로드를 피하려면 이미지 인코더들의 `pretrained=True`를 기본 `False`로 바꾸는 게 안전합니다. 지금 두 점을 고치고, 실제 데이터로 1에폭 스모크 학습을 돌릴 수 있는 런처(`train_mobile_vla.py`)를 추가하겠습니다. 수정 후 바로 데이터셋 점검과 짧은 학습을 실행해 보겠습니다.

```python
#!/usr/bin/env python3
"""
Mobile VLA Dataset - mobile_vla_data_collector.py 데이터 직접 로딩
Calvin 없이 순수 Mobile HDF5 형식 사용
"""

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MobileVLADataset(Dataset):
    """
    mobile_vla_data_collector.py가 생성한 HDF5 파일을 직접 로딩하는 데이터셋
    
    데이터 형식:
    - images: [T, 720, 1280, 3] - RGB 이미지 시퀀스
    - actions: [T, 3] - [linear_x, linear_y, angular_z] 
    - action_event_types: [T] - [episode_start, start_action, stop_action]
    - episode_name: str - "episode_20250808_123136_1box_vert_left"
    """
    
    def __init__(
        self, 
        data_dir: str = "/home/soda/vla/ROS_action/mobile_vla_dataset/",
        sequence_length: int = 18,
        image_size: Tuple[int, int] = (224, 224),  # VLM 입력용 리사이즈
        normalize_actions: bool = True,
        scenario_filter: Optional[List[str]] = None
    ):
        self.data_dir = Path(data_dir)
        self.sequence_length = sequence_length
        self.image_size = image_size
        self.normalize_actions = normalize_actions
        
        # mobile_vla_data_collector.py의 시나리오 매핑
        self.scenario_instructions = {
            "1box_vert_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "1box_vert_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요", 
            "1box_hori_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "1box_hori_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요",
            "2box_vert_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "2box_vert_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요",
            "2box_hori_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요", 
            "2box_hori_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요"
        }
        
        # mobile_vla_data_collector.py의 액션 범위 (WASD_TO_CONTINUOUS 기준)
        self.action_bounds = {
            "linear_x": 2.0,   # 실제로는 ±1.15 사용하지만 여유있게
            "linear_y": 2.0,   # 실제로는 ±1.15 사용하지만 여유있게  
            "angular_z": 2.0   # 실제로는 ±1.15 사용하지만 여유있게
        }
        
        # 이벤트 타입 매핑
        # 문자열/바이트 문자열 모두 안전하게 처리
        self.event_mapping = {
            'episode_start': 0,
            'start_action': 1,
            'stop_action': 2
        }
        
        # HDF5 파일 로드 및 필터링
        self.h5_files = self._load_h5_files(scenario_filter)
        self.scenarios = self._extract_scenarios()
        
        # 데이터셋 통계 출력
        self._print_dataset_stats()
        
    def _load_h5_files(self, scenario_filter: Optional[List[str]]) -> List[Path]:
        """HDF5 파일들을 로드하고 필터링"""
        all_h5_files = list(self.data_dir.glob("*.h5"))
        
        if scenario_filter:
            filtered_files = []
            for h5_file in all_h5_files:
                scenario = self._extract_scenario_from_filename(h5_file.name)
                if scenario in scenario_filter:
                    filtered_files.append(h5_file)
            return filtered_files
        
        return all_h5_files
    
    def _extract_scenario_from_filename(self, filename: str) -> str:
        """파일명에서 시나리오 추출 (mobile_vla_data_collector.py 방식)"""
        for scenario in self.scenario_instructions.keys():
            if scenario in filename:
                return scenario
        return "unknown"
    
    def _extract_scenarios(self) -> List[str]:
        """모든 파일의 시나리오 추출"""
        scenarios = []
        for h5_file in self.h5_files:
            scenario = self._extract_scenario_from_filename(h5_file.name)
            scenarios.append(scenario)
        return scenarios
    
    def _print_dataset_stats(self):
        """데이터셋 통계 출력"""
        scenario_counts = defaultdict(int)
        total_frames = 0
        
        for i, h5_file in enumerate(self.h5_files):
            scenario = self.scenarios[i]
            scenario_counts[scenario] += 1
            
            # 프레임 수 확인
            try:
                with h5py.File(h5_file, 'r') as f:
                    num_frames = f.attrs.get('num_frames', 0)
                    total_frames += num_frames
            except Exception as e:
                logger.warning(f"파일 읽기 실패 {h5_file.name}: {e}")
        
        logger.info(f"📁 Mobile VLA Dataset 로드 완료!")
        logger.info(f"📊 총 {len(self.h5_files)}개 에피소드, {total_frames}개 프레임")
        logger.info(f"🎯 시나리오 분포: {dict(scenario_counts)}")
        
        # 18프레임 에피소드 특별 표시
        frame_18_count = sum(1 for scenario in scenario_counts.keys() if scenario != "unknown")
        logger.info(f"🎯 18프레임 에피소드: {frame_18_count}개 (표준 길이)")
    
    def __len__(self) -> int:
        return len(self.h5_files)
    
    def __getitem__(self, idx: int) -> Dict:
        """단일 에피소드 데이터 로드"""
        h5_file = self.h5_files[idx]
        scenario = self.scenarios[idx]
        
        try:
            with h5py.File(h5_file, 'r') as f:
                # mobile_vla_data_collector.py 데이터 직접 로드
                images = f['images'][:]                    # [T, 720, 1280, 3]
                actions = f['actions'][:]                  # [T, 3] 
                action_events = f['action_event_types'][:]  # [T]
                
                # 메타데이터
                episode_name = f.attrs['episode_name']
                num_frames = f.attrs['num_frames']
                duration = f.attrs['total_duration']
                
        except Exception as e:
            logger.error(f"HDF5 파일 로드 실패 {h5_file.name}: {e}")
            # 빈 데이터 반환
            return self._get_empty_sample(scenario)
        
        # 데이터 전처리
        processed_data = self._preprocess_episode(
            images, actions, action_events, scenario, episode_name, num_frames, duration
        )
        
        return processed_data
    
    def _preprocess_episode(
        self, 
        images: np.ndarray, 
        actions: np.ndarray, 
        action_events: np.ndarray,
        scenario: str,
        episode_name: str,
        num_frames: int,
        duration: float
    ) -> Dict:
        """에피소드 데이터 전처리"""
        
        # 1. 이미지 전처리 (720p → 224x224 리사이즈 + 정규화)
        processed_images = self._preprocess_images(images)  # [T, 3, 224, 224]
        
        # 2. 액션 정규화 (mobile_vla_data_collector.py 기준)
        if self.normalize_actions:
            processed_actions = self._normalize_actions(actions)  # [T, 3] normalized
        else:
            processed_actions = torch.FloatTensor(actions)
        
        # 3. 이벤트 타입 변환
        # h5py가 반환하는 형식이 str 또는 bytes(np.bytes_)일 수 있어 통합 처리
        def _to_text(e):
            if isinstance(e, bytes):
                return e.decode('utf-8', errors='ignore')
            try:
                import numpy as _np
                if isinstance(e, _np.bytes_):
                    return e.decode('utf-8', errors='ignore')
            except Exception:
                pass
            return str(e)

        event_indices = np.array([
            self.event_mapping.get(_to_text(event), 1) for event in action_events
        ])
        processed_events = torch.LongTensor(event_indices)  # [T]
        
        # 4. 시퀀스 길이 맞추기 (18프레임 표준)
        if len(processed_images) != self.sequence_length:
            processed_images, processed_actions, processed_events = self._pad_or_truncate_sequence(
                processed_images, processed_actions, processed_events
            )
        
        # 5. 한국어 명령어 추가
        instruction = self.scenario_instructions.get(scenario, "컵까지 가세요")
        
        return {
            "images": processed_images,              # [18, 3, 224, 224]
            "actions": processed_actions,            # [18, 3]
            "action_events": processed_events,       # [18]
            "scenario": scenario,                    # str
            "instruction": instruction,              # str (한국어)
            "episode_name": episode_name,            # str
            "num_frames": num_frames,                # int
            "duration": duration,                    # float
            "sequence_mask": torch.ones(self.sequence_length, dtype=torch.bool)  # [18] - 모든 프레임 유효
        }
    
    def _preprocess_images(self, images: np.ndarray) -> torch.Tensor:
        """이미지 전처리: 720p → 224x224 리사이즈 + 정규화"""
        import torchvision.transforms as transforms
        
        # [T, 720, 1280, 3] → [T, 3, 224, 224]
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.image_size),
            transforms.ToTensor(),  # [0, 1] 정규화 + HWC→CHW
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 정규화
        ])
        
        processed_images = []
        for i in range(len(images)):
            # uint8 [720, 1280, 3] → normalized [3, 224, 224]
            img_tensor = transform(images[i])
            processed_images.append(img_tensor)
        
        return torch.stack(processed_images)  # [T, 3, 224, 224]
    
    def _normalize_actions(self, actions: np.ndarray) -> torch.Tensor:
        """액션 정규화 (mobile_vla_data_collector.py 기준)"""
        # [T, 3] actions: [linear_x, linear_y, angular_z]
        normalized_actions = actions.copy()
        
        # 각 축별로 [-1, 1] 범위로 정규화
        normalized_actions[:, 0] = actions[:, 0] / self.action_bounds["linear_x"]    # linear_x
        normalized_actions[:, 1] = actions[:, 1] / self.action_bounds["linear_y"]    # linear_y  
        normalized_actions[:, 2] = actions[:, 2] / self.action_bounds["angular_z"]   # angular_z
        
        # 클램핑 [-1, 1]
        normalized_actions = np.clip(normalized_actions, -1.0, 1.0)
        
        return torch.FloatTensor(normalized_actions)
    
    def _pad_or_truncate_sequence(
        self, 
        images: torch.Tensor, 
        actions: torch.Tensor, 
        events: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """시퀀스 길이를 표준 길이(18)에 맞추기"""
        current_length = len(images)
        
        if current_length == self.sequence_length:
            return images, actions, events
        elif current_length < self.sequence_length:
            # 패딩: 마지막 프레임 반복
            pad_length = self.sequence_length - current_length
            
            # 이미지 패딩
            last_image = images[-1:].repeat(pad_length, 1, 1, 1)
            padded_images = torch.cat([images, last_image], dim=0)
            
            # 액션 패딩 (정지 액션으로)
            stop_action = torch.zeros(pad_length, 3)
            padded_actions = torch.cat([actions, stop_action], dim=0)
            
            # 이벤트 패딩 (stop_action으로)
            stop_events = torch.full((pad_length,), 2, dtype=torch.long)  # stop_action = 2
            padded_events = torch.cat([events, stop_events], dim=0)
            
            return padded_images, padded_actions, padded_events
        else:
            # 자르기: 처음 sequence_length만 사용
            return images[:self.sequence_length], actions[:self.sequence_length], events[:self.sequence_length]
    
    def _get_empty_sample(self, scenario: str) -> Dict:
        """빈 샘플 반환 (에러 발생시)"""
        return {
            "images": torch.zeros(self.sequence_length, 3, *self.image_size),
            "actions": torch.zeros(self.sequence_length, 3),
            "action_events": torch.zeros(self.sequence_length, dtype=torch.long),
            "scenario": scenario,
            "instruction": self.scenario_instructions.get(scenario, "컵까지 가세요"),
            "episode_name": "error_episode",
            "num_frames": 0,
            "duration": 0.0,
            "sequence_mask": torch.zeros(self.sequence_length, dtype=torch.bool)
        }
    
    def denormalize_actions(self, normalized_actions: torch.Tensor) -> torch.Tensor:
        """정규화된 액션을 원래 범위로 복원"""
        # [-1, 1] → mobile_vla_data_collector.py 범위
        denormalized = normalized_actions.clone()
        denormalized[:, 0] *= self.action_bounds["linear_x"]    # linear_x
        denormalized[:, 1] *= self.action_bounds["linear_y"]    # linear_y
        denormalized[:, 2] *= self.action_bounds["angular_z"]   # angular_z
        return denormalized
    
    def get_scenario_statistics(self) -> Dict[str, int]:
        """시나리오별 통계 반환"""
        scenario_counts = defaultdict(int)
        for scenario in self.scenarios:
            scenario_counts[scenario] += 1
        return dict(scenario_counts)


if __name__ == "__main__":
    # 테스트 코드
    print("🧪 Mobile VLA Dataset 테스트")
    
    dataset = MobileVLADataset()
    print(f"📊 데이터셋 크기: {len(dataset)}")
    
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"🖼️ 이미지 형태: {sample['images'].shape}")
        print(f"🎮 액션 형태: {sample['actions'].shape}")
        print(f"⚡ 이벤트 형태: {sample['action_events'].shape}")
        print(f"🎯 시나리오: {sample['scenario']}")
        print(f"🗣️ 명령어: {sample['instruction']}")
        print(f"📋 에피소드명: {sample['episode_name']}")
    
    # 시나리오 통계
    stats = dataset.get_scenario_statistics()
    print(f"📈 시나리오 통계: {stats}")

```

```python
#!/usr/bin/env python3
"""
Mobile Image Encoder - mobile_vla_data_collector.py의 720p 이미지 처리 특화
RoboVLMs의 이미지 인코딩 기술을 Mobile VLA에 적용
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class MobileImageEncoder(nn.Module):
    """
    Mobile VLA 특화 이미지 인코더
    - 입력: [B, T, 3, 224, 224] (mobile_vla_data_collector.py에서 720p→224p 리사이즈됨)
    - 출력: [B, T, hidden_size] 시간적 특징
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_v2_s",
        hidden_size: int = 768,
        num_lstm_layers: int = 2,
        dropout: float = 0.1,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.backbone_name = backbone
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        
        # 백본 CNN (EfficientNet V2 - 모바일 최적화)
        if backbone == "efficientnet_v2_s":
            # Jetson/오프라인 환경 대비: 사전학습 가중치 다운로드 비활성화
            self.backbone = models.efficientnet_v2_s(weights=None)
            backbone_output_size = 1000
        elif backbone == "resnet50":
            self.backbone = models.resnet50(weights=None)
            backbone_output_size = 2048
        elif backbone == "mobilenet_v3_large":
            self.backbone = models.mobilenet_v3_large(weights=None)  
            backbone_output_size = 1000
        else:
            raise ValueError(f"지원하지 않는 백본: {backbone}")
        
        # 백본 가중치 고정 옵션
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
            logger.info(f"🔒 {backbone} 백본 가중치 고정됨")
        
        # CNN 특징을 hidden_size로 매핑
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 시간적 특징 추출 (18프레임 시퀀스)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,  # 양방향이므로 절반
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        # 출력 정규화
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"🖼️ Mobile Image Encoder 초기화 완료")
        logger.info(f"   백본: {backbone}, Hidden: {hidden_size}, LSTM Layers: {num_lstm_layers}")
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_sequence: [B, T, 3, 224, 224] - 배치 크기 B, 시퀀스 길이 T
            
        Returns:
            temporal_features: [B, T, hidden_size] - 시간적 이미지 특징
        """
        B, T, C, H, W = image_sequence.shape
        
        # 배치와 시간 차원을 합쳐서 CNN에 입력
        images_flat = image_sequence.view(B * T, C, H, W)  # [B*T, 3, 224, 224]
        
        # CNN으로 각 프레임 특징 추출
        with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
            frame_features = self.backbone(images_flat)  # [B*T, backbone_output_size]
        
        # 특징 차원을 hidden_size로 매핑
        frame_features = self.feature_projection(frame_features)  # [B*T, hidden_size]
        
        # 시간 차원 복원
        frame_features = frame_features.view(B, T, self.hidden_size)  # [B, T, hidden_size]
        
        # LSTM으로 시간적 특징 추출
        temporal_features, (hidden, cell) = self.temporal_encoder(frame_features)  # [B, T, hidden_size]
        
        # 레이어 정규화
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """단일 이미지에서 공간적 특징만 추출 (실시간 추론용)"""
        # single_image: [B, 3, 224, 224] 또는 [3, 224, 224]
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        with torch.cuda.amp.autocast(enabled=True):
            spatial_features = self.backbone(single_image)  # [B, backbone_output_size]
        
        spatial_features = self.feature_projection(spatial_features)  # [B, hidden_size]
        
        return spatial_features
    
    def get_feature_maps(self, image_sequence: torch.Tensor) -> dict:
        """중간 특징 맵들을 반환 (디버깅/분석용)"""
        B, T, C, H, W = image_sequence.shape
        images_flat = image_sequence.view(B * T, C, H, W)
        
        features = {}
        
        # EfficientNet의 중간 특징들 추출
        if self.backbone_name == "efficientnet_v2_s":
            x = images_flat
            for i, layer in enumerate(self.backbone.features):
                x = layer(x)
                if i in [2, 4, 6]:  # 선택된 레이어의 특징 저장
                    features[f"stage_{i}"] = x.view(B, T, *x.shape[1:])
        
        return features


class MobileImageEncoderLite(nn.Module):
    """
    경량화된 Mobile Image Encoder (Jetson용)
    더 작은 모델이지만 핵심 기능 유지
    """
    
    def __init__(
        self,
        hidden_size: int = 512,
        num_lstm_layers: int = 1,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # 경량화된 CNN 백본 (MobileNet V3)
        self.backbone = models.mobilenet_v3_small(pretrained=True)
        backbone_output_size = 1000
        
        # 특징 매핑 (더 작은 hidden_size)
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 간단한 GRU (LSTM보다 파라미터 적음)
        self.temporal_encoder = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"🚀 Mobile Image Encoder Lite 초기화 (Hidden: {hidden_size})")
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """단일 이미지에서 공간적 특징만 추출 (실시간 추론용)"""
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        # 경량화된 CNN
        spatial_features = self.backbone(single_image)
        spatial_features = self.feature_projection(spatial_features)
        
        return spatial_features
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        B, T, C, H, W = image_sequence.shape
        
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # 경량화된 CNN
        frame_features = self.backbone(images_flat)
        frame_features = self.feature_projection(frame_features)
        frame_features = frame_features.view(B, T, self.hidden_size)
        
        # GRU로 시간적 특징
        temporal_features, hidden = self.temporal_encoder(frame_features)
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features


if __name__ == "__main__":
    # 테스트 코드
    print("🧪 Mobile Image Encoder 테스트")
    
    # 모델 초기화
    encoder = MobileImageEncoder(hidden_size=768)
    encoder_lite = MobileImageEncoderLite(hidden_size=512)
    
    # 테스트 데이터 (mobile_vla_data_collector.py 형식)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    
    print(f"📊 입력 이미지: {test_images.shape}")
    
    # 일반 인코더 테스트
    with torch.no_grad():
        features = encoder(test_images)
        print(f"🖼️ 인코더 출력: {features.shape}")
        
        features_lite = encoder_lite(test_images)
        print(f"🚀 Lite 인코더 출력: {features_lite.shape}")
    
    # 파라미터 수 계산
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"📊 일반 인코더 파라미터: {total_params:,}개 ({total_params/1e6:.1f}M)")
    print(f"🚀 Lite 인코더 파라미터: {lite_params:,}개 ({lite_params/1e6:.1f}M)")
    print(f"💡 파라미터 감소율: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
*** Begin Patch
*** Update File: Robo+/Mobile_VLA/models/encoders/korean_text_encoder.py
@@
-            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
-            self.text_encoder = AutoModel.from_pretrained(model_name)
+            # 오프라인/Jetson 환경에서 모델 다운로드 실패 가능성 대비
+            self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
+            self.text_encoder = AutoModel.from_pretrained(model_name, local_files_only=False)
         except Exception as e:
             logger.warning(f"⚠️ KLUE 모델 로드 실패, DistilBERT로 대체: {e}")
-            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased")
-            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased")
+            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
+            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
*** End Patch
```

```python
#!/usr/bin/env python3
"""
Korean Text Encoder - 한국어 네비게이션 명령어 인코딩
mobile_vla_data_collector.py의 시나리오별 한국어 명령어 처리
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)


class KoreanTextEncoder(nn.Module):
    """
    한국어 텍스트 인코더 (Mobile VLA 시나리오별 명령어 특화)
    - KLUE RoBERTa 기반 한국어 이해
    - 시나리오별 명령어 매핑
    - mobile_vla_data_collector.py 시나리오와 완전 호환
    """
    
    def __init__(
        self,
        model_name: str = "klue/roberta-base",
        hidden_size: int = 768,
        max_length: int = 128,
        freeze_encoder: bool = False
    ):
        super().__init__()
        
        self.model_name = model_name
        self.hidden_size = hidden_size
        self.max_length = max_length
        
        # 한국어 토크나이저 및 모델 로드 (오프라인 환경 대비)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
            self.text_encoder = AutoModel.from_pretrained(model_name, local_files_only=False)
            logger.info(f"✅ 한국어 모델 로드 완료: {model_name}")
        except Exception as e:
            logger.warning(f"⚠️ KLUE 모델 로드 실패, DistilBERT로 대체: {e}")
            self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
            self.text_encoder = AutoModel.from_pretrained("distilbert-base-multilingual-cased", local_files_only=False)
        
        # 인코더 가중치 고정 옵션
        if freeze_encoder:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
            logger.info("🔒 텍스트 인코더 가중치 고정됨")
        
        # mobile_vla_data_collector.py 시나리오별 한국어 명령어
        self.scenario_instructions = {
            "1box_vert_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "1box_vert_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요", 
            "1box_hori_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "1box_hori_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요",
            "2box_vert_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요",
            "2box_vert_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요",
            "2box_hori_left": "가장 왼쪽 외곽으로 돌아 컵까지 가세요", 
            "2box_hori_right": "가장 오른쪽 외곽으로 돌아 컵까지 가세요"
        }
        
        # 시나리오 임베딩 (8가지 시나리오)
        self.scenario_embedding = nn.Embedding(8, hidden_size)
        
        # 시나리오 ID 매핑
        self.scenario_to_id = {
            scenario: idx for idx, scenario in enumerate(self.scenario_instructions.keys())
        }
        
        # 텍스트 특징 프로젝션 (KLUE RoBERTa: 768 → hidden_size)
        encoder_dim = self.text_encoder.config.hidden_size
        if encoder_dim != hidden_size:
            self.text_projection = nn.Linear(encoder_dim, hidden_size)
        else:
            self.text_projection = nn.Identity()
        
        # 시나리오와 텍스트 융합
        self.fusion_layer = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"🗣️ Korean Text Encoder 초기화 완료 (Hidden: {hidden_size})")
    
    def forward(
        self, 
        instructions: List[str], 
        scenarios: Optional[List[str]] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            instructions: 한국어 명령어 리스트 ["박스를 왼쪽으로...", ...]
            scenarios: 시나리오 이름 리스트 ["1box_vert_left", ...] (옵션)
            
        Returns:
            dict with:
                - text_features: [B, seq_len, hidden_size] - 텍스트 특징
                - scenario_features: [B, hidden_size] - 시나리오 특징 (scenarios 제공시)
                - fused_features: [B, hidden_size] - 융합된 특징
        """
        batch_size = len(instructions)
        device = next(self.parameters()).device
        
        # 1. 텍스트 토크나이징
        tokenized = self.tokenizer(
            instructions,
            padding=True,
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        ).to(device)
        
        # 2. 텍스트 인코딩
        with torch.cuda.amp.autocast(enabled=True):
            text_outputs = self.text_encoder(**tokenized)
        
        # 텍스트 특징 추출 및 프로젝션
        text_features = text_outputs.last_hidden_state  # [B, seq_len, encoder_dim]
        text_features = self.text_projection(text_features)  # [B, seq_len, hidden_size]
        
        # 텍스트 풀링 (평균)
        attention_mask = tokenized['attention_mask'].unsqueeze(-1)  # [B, seq_len, 1]
        text_pooled = (text_features * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)  # [B, hidden_size]
        
        result = {
            "text_features": text_features,
            "text_pooled": text_pooled
        }
        
        # 3. 시나리오 인코딩 (제공된 경우)
        if scenarios is not None:
            scenario_ids = []
            for scenario in scenarios:
                scenario_id = self.scenario_to_id.get(scenario, 0)  # unknown은 0번
                scenario_ids.append(scenario_id)
            
            scenario_ids = torch.tensor(scenario_ids, device=device)  # [B]
            scenario_features = self.scenario_embedding(scenario_ids)  # [B, hidden_size]
            
            # 4. 텍스트-시나리오 융합
            # 시나리오를 쿼리로, 텍스트를 키-밸류로 사용
            scenario_query = scenario_features.unsqueeze(1)  # [B, 1, hidden_size]
            fused_features, attention_weights = self.fusion_layer(
                query=scenario_query,      # [B, 1, hidden_size]
                key=text_features,         # [B, seq_len, hidden_size]
                value=text_features,       # [B, seq_len, hidden_size]
                key_padding_mask=~tokenized['attention_mask'].bool()  # 패딩 마스크
            )
            
            fused_features = fused_features.squeeze(1)  # [B, hidden_size]
            fused_features = self.layer_norm(fused_features)
            
            result.update({
                "scenario_features": scenario_features,
                "fused_features": fused_features,
                "attention_weights": attention_weights
            })
        else:
            # 시나리오 없이는 텍스트 풀링만 사용
            result["fused_features"] = self.layer_norm(text_pooled)
        
        return result
    
    def encode_scenarios_only(self, scenarios: List[str]) -> torch.Tensor:
        """시나리오만으로 임베딩 생성 (빠른 추론용)"""
        device = next(self.parameters()).device
        
        scenario_ids = []
        for scenario in scenarios:
            scenario_id = self.scenario_to_id.get(scenario, 0)
            scenario_ids.append(scenario_id)
        
        scenario_ids = torch.tensor(scenario_ids, device=device)
        scenario_features = self.scenario_embedding(scenario_ids)
        
        return scenario_features
    
    def get_instruction_for_scenario(self, scenario: str) -> str:
        """시나리오에 대응하는 한국어 명령어 반환"""
        return self.scenario_instructions.get(scenario, "컵까지 가세요")
    
    def batch_encode_scenarios(self, scenarios: List[str]) -> Dict[str, torch.Tensor]:
        """시나리오 배치를 한국어 명령어로 변환하여 인코딩"""
        instructions = [self.get_instruction_for_scenario(scenario) for scenario in scenarios]
        return self.forward(instructions, scenarios)


class KoreanTextEncoderLite(nn.Module):
    """
    경량화된 한국어 텍스트 인코더 (Jetson용)
    사전 정의된 시나리오 임베딩만 사용
    """
    
    def __init__(self, hidden_size: int = 512):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # 시나리오별 사전 정의된 임베딩 (학습 가능)
        self.scenario_embedding = nn.Embedding(8, hidden_size)
        
        # mobile_vla_data_collector.py 시나리오 매핑
        self.scenario_to_id = {
            "1box_vert_left": 0, "1box_vert_right": 1,
            "1box_hori_left": 2, "1box_hori_right": 3,
            "2box_vert_left": 4, "2box_vert_right": 5,
            "2box_hori_left": 6, "2box_hori_right": 7
        }
        
        logger.info(f"🚀 Korean Text Encoder Lite 초기화 (Hidden: {hidden_size})")
    
    def forward(self, scenarios: List[str]) -> torch.Tensor:
        """시나리오 이름만으로 임베딩 생성"""
        device = next(self.parameters()).device
        
        scenario_ids = []
        for scenario in scenarios:
            scenario_id = self.scenario_to_id.get(scenario, 0)
            scenario_ids.append(scenario_id)
        
        scenario_ids = torch.tensor(scenario_ids, device=device)
        scenario_features = self.scenario_embedding(scenario_ids)
        
        return scenario_features


if __name__ == "__main__":
    # 테스트 코드
    print("🧪 Korean Text Encoder 테스트")
    
    # 모델 초기화
    encoder = KoreanTextEncoder(hidden_size=768)
    encoder_lite = KoreanTextEncoderLite(hidden_size=512)
    
    # 테스트 데이터
    test_instructions = [
        "박스를 왼쪽으로 돌아서 컵까지 가세요",
        "두 박스 사이 오른쪽 경로로 컵까지 가세요"
    ]
    test_scenarios = ["1box_vert_left", "2box_vert_right"]
    
    print(f"📊 입력 명령어: {test_instructions}")
    print(f"🎯 입력 시나리오: {test_scenarios}")
    
    # 일반 인코더 테스트
    with torch.no_grad():
        result = encoder(test_instructions, test_scenarios)
        print(f"🗣️ 텍스트 특징: {result['text_features'].shape}")
        print(f"🎯 시나리오 특징: {result['scenario_features'].shape}")
        print(f"🔄 융합 특징: {result['fused_features'].shape}")
        
        # Lite 인코더 테스트
        lite_result = encoder_lite(test_scenarios)
        print(f"🚀 Lite 특징: {lite_result.shape}")
    
    # 파라미터 수 계산
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"📊 일반 인코더 파라미터: {total_params:,}개 ({total_params/1e6:.1f}M)")
    print(f"🚀 Lite 인코더 파라미터: {lite_params:,}개 ({lite_params/1e6:.1f}M)")
    print(f"💡 파라미터 감소율: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA Model - Pure Mobile VLM without Calvin dependencies
RoboVLMs의 VLM 기술을 mobile_vla_data_collector.py에 완전 적응
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
import logging

try:
    from .encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from .encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from .policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite
except ImportError:
    # 테스트용 절대 임포트
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite

logger = logging.getLogger(__name__)


class MobileVLAModel(nn.Module):
    """
    Pure Mobile VLA Model
    - 입력: mobile_vla_data_collector.py 데이터 형식
    - 출력: 3D 액션 + 이벤트 예측
    - Calvin 의존성 없는 순수 Mobile VLM
    """
    
    def __init__(
        self,
        # 모델 크기 설정
        hidden_size: int = 768,
        
        # 이미지 인코더 설정
        image_backbone: str = "efficientnet_v2_s",
        freeze_image_backbone: bool = False,
        
        # 텍스트 인코더 설정  
        text_model: str = "klue/roberta-base",
        freeze_text_encoder: bool = False,
        
        # 정책 헤드 설정
        use_policy_lstm: bool = True,
        policy_lstm_layers: int = 2,
        
        # 일반 설정
        dropout: float = 0.1,
        use_lite_mode: bool = False  # Jetson용 경량화 모드
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_lite_mode = use_lite_mode
        
        # 경량화 모드에 따른 컴포넌트 선택
        if use_lite_mode:
            # Jetson용 경량화 모델
            self.image_encoder = MobileImageEncoderLite(
                hidden_size=hidden_size // 2,  # 더 작은 hidden_size
                dropout=dropout
            )
            self.text_encoder = KoreanTextEncoderLite(
                hidden_size=hidden_size // 2
            )
            self.policy_head = MobilePolicyHeadLite(
                hidden_size=hidden_size,  # 융합 후에는 원래 크기
                dropout=dropout
            )
            logger.info("🚀 Lite 모드로 초기화됨 (Jetson 최적화)")
        else:
            # 일반 고성능 모델
            self.image_encoder = MobileImageEncoder(
                backbone=image_backbone,
                hidden_size=hidden_size,
                dropout=dropout,
                freeze_backbone=freeze_image_backbone
            )
            self.text_encoder = KoreanTextEncoder(
                model_name=text_model,
                hidden_size=hidden_size,
                freeze_encoder=freeze_text_encoder
            )
            self.policy_head = MobilePolicyHead(
                hidden_size=hidden_size,
                dropout=dropout,
                use_lstm=use_policy_lstm,
                lstm_layers=policy_lstm_layers
            )
            logger.info("💪 Full 모드로 초기화됨 (고성능)")
        
        # 멀티모달 융합 레이어
        if use_lite_mode:
            # 경량화된 융합
            self.multimodal_fusion = nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
        else:
            # 어텐션 기반 융합
            self.multimodal_fusion = nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # 출력 정규화
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # 모델 통계
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"🤖 Mobile VLA Model 초기화 완료")
        logger.info(f"   파라미터: {total_params:,}개 ({total_params/1e6:.1f}M)")
        logger.info(f"   Hidden Size: {hidden_size}, Lite Mode: {use_lite_mode}")
    
    def forward(
        self,
        images: torch.Tensor,
        scenarios: List[str],
        instructions: Optional[List[str]] = None,
        return_intermediate: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: [B, T, 3, 224, 224] - 이미지 시퀀스  
            scenarios: List[str] - 시나리오 이름들 ["1box_vert_left", ...]
            instructions: List[str] - 한국어 명령어 (옵션, scenarios에서 자동 생성 가능)
            return_intermediate: 중간 특징들 반환 여부
            
        Returns:
            Dict with:
                - actions: [B, T, 3] - 정규화된 액션
                - actions_denorm: [B, T, 3] - 실제 범위 액션
                - event_logits: [B, T, 3] - 이벤트 분류 로짓
                - predicted_events: [B, T] - 예측된 이벤트
        """
        batch_size = images.shape[0]
        
        # 1. 이미지 인코딩
        image_features = self.image_encoder(images)  # [B, T, hidden_size//2 or hidden_size]
        
        # 2. 텍스트 인코딩
        if self.use_lite_mode:
            # Lite 모드: 시나리오만 사용
            text_features = self.text_encoder(scenarios)  # [B, hidden_size//2]
            # 시간 차원으로 확장
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size//2]
        else:
            # Full 모드: 한국어 명령어 사용
            if instructions is None:
                # 시나리오에서 한국어 명령어 자동 생성
                instructions = [
                    self.text_encoder.get_instruction_for_scenario(scenario) 
                    for scenario in scenarios
                ]
            
            text_result = self.text_encoder(instructions, scenarios)
            text_features = text_result["fused_features"]  # [B, hidden_size]
            # 시간 차원으로 확장
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size]
        
        # 3. 멀티모달 융합
        if self.use_lite_mode:
            # 경량화된 융합: 단순 concatenation + MLP
            multimodal_features = torch.cat([image_features, text_features], dim=-1)  # [B, T, hidden_size]
            multimodal_features = self.multimodal_fusion(multimodal_features)
        else:
            # 어텐션 기반 융합
            # 이미지를 쿼리로, 텍스트를 키-밸류로 사용
            fused_features, attention_weights = self.multimodal_fusion(
                query=image_features,     # [B, T, hidden_size]
                key=text_features,        # [B, T, hidden_size]  
                value=text_features       # [B, T, hidden_size]
            )
            multimodal_features = fused_features
        
        # 출력 정규화
        multimodal_features = self.output_norm(multimodal_features)  # [B, T, hidden_size]
        
        # 4. 정책 헤드로 액션 예측
        policy_output = self.policy_head(multimodal_features)
        
        # 결과 구성
        result = {
            "actions": policy_output["actions"],                    # [B, T, 3] 정규화된
            "actions_denorm": policy_output["actions_denorm"],      # [B, T, 3] 실제 범위
            "event_logits": policy_output["event_logits"],          # [B, T, 3]
            "event_probs": policy_output.get("event_probs"),        # [B, T, 3]
            "predicted_events": policy_output["predicted_events"]   # [B, T]
        }
        
        # 중간 특징들 (디버깅/분석용)
        if return_intermediate:
            result.update({
                "image_features": image_features,
                "text_features": text_features,
                "multimodal_features": multimodal_features,
                "attention_weights": attention_weights if (not self.use_lite_mode and 'attention_weights' in locals()) else None
            })
        
        return result
    
    def inference_single_step(
        self,
        current_image: torch.Tensor,
        scenario: str,
        hidden_state: Optional[Tuple] = None
    ) -> Tuple[Dict[str, torch.Tensor], Optional[Tuple]]:
        """
        단일 스텝 추론 (실시간 mobile_vla_data_collector 연동용)
        
        Args:
            current_image: [1, 3, 224, 224] - 현재 이미지
            scenario: str - 현재 시나리오
            hidden_state: LSTM hidden state (있다면)
            
        Returns:
            (액션 예측 결과, 새로운 hidden_state)
        """
        with torch.no_grad():
            # 단일 이미지를 시퀀스로 확장
            image_sequence = current_image.unsqueeze(1)  # [1, 1, 3, 224, 224]
            
            # 인코딩
            image_features = self.image_encoder.extract_spatial_features(current_image)  # [1, hidden_size]
            
            if self.use_lite_mode:
                text_features = self.text_encoder([scenario])  # [1, hidden_size//2]
                # 이미지 특징과 텍스트 특징의 차원을 맞춤
                image_features_lite = image_features  # [1, hidden_size//2] (256)
                multimodal_features = torch.cat([image_features_lite, text_features], dim=-1)  # [1, 512]
                multimodal_features = self.multimodal_fusion(multimodal_features)
            else:
                instruction = self.text_encoder.get_instruction_for_scenario(scenario)
                text_result = self.text_encoder([instruction], [scenario])
                text_features = text_result["fused_features"]  # [1, hidden_size]
                
                # 간단한 융합 (어텐션 없이)
                multimodal_features = (image_features + text_features) / 2
            
            multimodal_features = self.output_norm(multimodal_features)
            
            # 정책 헤드로 단일 스텝 예측
            if hasattr(self.policy_head, 'predict_single_step') and not self.use_lite_mode:
                action_result, new_hidden_state = self.policy_head.predict_single_step(
                    multimodal_features, hidden_state
                )
            else:
                action_result = self.policy_head(multimodal_features.unsqueeze(1))
                # 단일 스텝으로 압축
                for key in action_result:
                    if action_result[key].dim() > 1:
                        action_result[key] = action_result[key].squeeze(1)
                new_hidden_state = None
        
        return action_result, new_hidden_state
    
    def get_mobile_vla_action(
        self,
        current_image: torch.Tensor,
        scenario: str
    ) -> Dict[str, float]:
        """
        mobile_vla_data_collector.py 호환 액션 반환
        
        Returns:
            Dict with keys: linear_x, linear_y, angular_z, event_type
        """
        action_result, _ = self.inference_single_step(current_image, scenario)
        
        # 액션 추출 (첫 번째 배치)
        actions = action_result["actions_denorm"][0].cpu().numpy()  # [3]
        predicted_event = action_result["predicted_events"][0].cpu().item()
        
        # 이벤트 타입 매핑
        event_types = ["episode_start", "start_action", "stop_action"]
        event_type = event_types[predicted_event]
        
        return {
            "linear_x": float(actions[0]),
            "linear_y": float(actions[1]),
            "angular_z": float(actions[2]),
            "event_type": event_type
        }


if __name__ == "__main__":
    # 테스트 코드
    print("🧪 Mobile VLA Model 테스트")
    
    # 모델 초기화 (Full & Lite)
    model_full = MobileVLAModel(hidden_size=768, use_lite_mode=False)
    model_lite = MobileVLAModel(hidden_size=512, use_lite_mode=True)
    
    # 테스트 데이터 (mobile_vla_data_collector.py 형식)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    test_scenarios = ["1box_vert_left", "2box_hori_right"]
    
    print(f"📊 입력 이미지: {test_images.shape}")
    print(f"🎯 입력 시나리오: {test_scenarios}")
    
    # Full 모델 테스트
    print("\n💪 Full Model 테스트:")
    with torch.no_grad():
        result_full = model_full(test_images, test_scenarios, return_intermediate=True)
        print(f"   액션: {result_full['actions'].shape}")
        print(f"   실제 액션: {result_full['actions_denorm'].shape}")
        print(f"   이벤트: {result_full['predicted_events'].shape}")
    
    # Lite 모델 테스트  
    print("\n🚀 Lite Model 테스트:")
    with torch.no_grad():
        result_lite = model_lite(test_images, test_scenarios)
        print(f"   액션: {result_lite['actions'].shape}")
        print(f"   실제 액션: {result_lite['actions_denorm'].shape}")
        print(f"   이벤트: {result_lite['predicted_events'].shape}")
    
    # 단일 스텝 추론 테스트
    print("\n🔄 단일 스텝 추론 테스트:")
    single_image = torch.randn(1, 3, 224, 224)
    scenario = "1box_vert_left"
    
    mobile_action = model_full.get_mobile_vla_action(single_image, scenario)
    print(f"   Mobile 액션: {mobile_action}")
    
    # 파라미터 수 비교
    full_params = sum(p.numel() for p in model_full.parameters())
    lite_params = sum(p.numel() for p in model_lite.parameters())
    
    print(f"\n📊 모델 크기 비교:")
    print(f"   Full 모델: {full_params:,}개 ({full_params/1e6:.1f}M)")
    print(f"   Lite 모델: {lite_params:,}개 ({lite_params/1e6:.1f}M)")
    print(f"   경량화율: {(1 - lite_params/full_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA 학습 런처 (실제 mobile_vla_data_collector 데이터로 스모크 학습)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader

try:
    from .mobile_trainer_simple import SimpleMobileVLATrainer
    from ..data.mobile_dataset import MobileVLADataset
except ImportError:
    import sys
    from pathlib import Path
    this_dir = Path(__file__).resolve().parent
    sys.path.append(str(this_dir.parent))
    from training.mobile_trainer_simple import SimpleMobileVLATrainer
    from data.mobile_dataset import MobileVLADataset


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='스모크: 몇 스텝만 학습')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # 데이터셋
    dataset = MobileVLADataset(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('⚠️ HDF5 데이터가 없습니다. data_dir를 확인하세요:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # 트레이너
    trainer = SimpleMobileVLATrainer(configs)

    # 스모크 학습 루프
    steps = 0
    for batch in loader:
        # 배치 dict 텐서 외 필드 유지
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('✅ 스모크 학습 완료: steps =', steps)


if __name__ == '__main__':
    main()


```

지금 스모크 학습 실행에서 `lightning` 의존성으로 import가 새어 들어왔습니다. 런처가 패키지 임포트를 피하도록 파일 경로 기반 동적 임포트로 바꾸고, `MobileImageEncoderLite`의 사전가중치 다운로드도 막은 뒤 다시 학습을 돌리겠습니다.

```python
#!/usr/bin/env python3
"""
Mobile VLA 학습 런처 (실제 mobile_vla_data_collector 데이터로 스모크 학습)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader
import importlib.util
from pathlib import Path


def _load_attr_from_file(module_name: str, file_path: str, attr: str):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    assert spec and spec.loader
    spec.loader.exec_module(module)  # type: ignore
    return getattr(module, attr)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='스모크: 몇 스텝만 학습')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # 데이터셋
    dataset = MobileVLADataset(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('⚠️ HDF5 데이터가 없습니다. data_dir를 확인하세요:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # 트레이너 (동적 임포트로 패키지 의존성 회피)
    root = Path(__file__).resolve().parents[1]
    trainer_cls = _load_attr_from_file(
        'mobile_trainer_simple',
        str(root / 'training' / 'mobile_trainer_simple.py'),
        'SimpleMobileVLATrainer'
    )
    trainer = trainer_cls(configs)

    # 스모크 학습 루프
    steps = 0
    for batch in loader:
        # 배치 dict 텐서 외 필드 유지
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('✅ 스모크 학습 완료: steps =', steps)


if __name__ == '__main__':
    main()



```

```python
#!/usr/bin/env python3
"""
Mobile Image Encoder - mobile_vla_data_collector.py의 720p 이미지 처리 특화
RoboVLMs의 이미지 인코딩 기술을 Mobile VLA에 적용
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class MobileImageEncoder(nn.Module):
    """
    Mobile VLA 특화 이미지 인코더
    - 입력: [B, T, 3, 224, 224] (mobile_vla_data_collector.py에서 720p→224p 리사이즈됨)
    - 출력: [B, T, hidden_size] 시간적 특징
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_v2_s",
        hidden_size: int = 768,
        num_lstm_layers: int = 2,
        dropout: float = 0.1,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.backbone_name = backbone
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        
        # 백본 CNN (EfficientNet V2 - 모바일 최적화)
        if backbone == "efficientnet_v2_s":
            # Jetson/오프라인 환경 대비: 사전학습 가중치 다운로드 비활성화
            self.backbone = models.efficientnet_v2_s(weights=None)
            backbone_output_size = 1000
        elif backbone == "resnet50":
            self.backbone = models.resnet50(weights=None)
            backbone_output_size = 2048
        elif backbone == "mobilenet_v3_large":
            self.backbone = models.mobilenet_v3_large(weights=None)  
            backbone_output_size = 1000
        else:
            raise ValueError(f"지원하지 않는 백본: {backbone}")
        
        # 백본 가중치 고정 옵션
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
            logger.info(f"🔒 {backbone} 백본 가중치 고정됨")
        
        # CNN 특징을 hidden_size로 매핑
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 시간적 특징 추출 (18프레임 시퀀스)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,  # 양방향이므로 절반
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        # 출력 정규화
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"🖼️ Mobile Image Encoder 초기화 완료")
        logger.info(f"   백본: {backbone}, Hidden: {hidden_size}, LSTM Layers: {num_lstm_layers}")
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_sequence: [B, T, 3, 224, 224] - 배치 크기 B, 시퀀스 길이 T
            
        Returns:
            temporal_features: [B, T, hidden_size] - 시간적 이미지 특징
        """
        B, T, C, H, W = image_sequence.shape
        
        # 배치와 시간 차원을 합쳐서 CNN에 입력
        images_flat = image_sequence.view(B * T, C, H, W)  # [B*T, 3, 224, 224]
        
        # CNN으로 각 프레임 특징 추출
        with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
            frame_features = self.backbone(images_flat)  # [B*T, backbone_output_size]
        
        # 특징 차원을 hidden_size로 매핑
        frame_features = self.feature_projection(frame_features)  # [B*T, hidden_size]
        
        # 시간 차원 복원
        frame_features = frame_features.view(B, T, self.hidden_size)  # [B, T, hidden_size]
        
        # LSTM으로 시간적 특징 추출
        temporal_features, (hidden, cell) = self.temporal_encoder(frame_features)  # [B, T, hidden_size]
        
        # 레이어 정규화
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """단일 이미지에서 공간적 특징만 추출 (실시간 추론용)"""
        # single_image: [B, 3, 224, 224] 또는 [3, 224, 224]
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        with torch.cuda.amp.autocast(enabled=True):
            spatial_features = self.backbone(single_image)  # [B, backbone_output_size]
        
        spatial_features = self.feature_projection(spatial_features)  # [B, hidden_size]
        
        return spatial_features
    
    def get_feature_maps(self, image_sequence: torch.Tensor) -> dict:
        """중간 특징 맵들을 반환 (디버깅/분석용)"""
        B, T, C, H, W = image_sequence.shape
        images_flat = image_sequence.view(B * T, C, H, W)
        
        features = {}
        
        # EfficientNet의 중간 특징들 추출
        if self.backbone_name == "efficientnet_v2_s":
            x = images_flat
            for i, layer in enumerate(self.backbone.features):
                x = layer(x)
                if i in [2, 4, 6]:  # 선택된 레이어의 특징 저장
                    features[f"stage_{i}"] = x.view(B, T, *x.shape[1:])
        
        return features


class MobileImageEncoderLite(nn.Module):
    """
    경량화된 Mobile Image Encoder (Jetson용)
    더 작은 모델이지만 핵심 기능 유지
    """
    
    def __init__(
        self,
        hidden_size: int = 512,
        num_lstm_layers: int = 1,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # 경량화된 CNN 백본 (MobileNet V3)
        self.backbone = models.mobilenet_v3_small(weights=None)
        backbone_output_size = 1000
        
        # 특징 매핑 (더 작은 hidden_size)
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # 간단한 GRU (LSTM보다 파라미터 적음)
        self.temporal_encoder = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"🚀 Mobile Image Encoder Lite 초기화 (Hidden: {hidden_size})")
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """단일 이미지에서 공간적 특징만 추출 (실시간 추론용)"""
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        # 경량화된 CNN
        spatial_features = self.backbone(single_image)
        spatial_features = self.feature_projection(spatial_features)
        
        return spatial_features
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        B, T, C, H, W = image_sequence.shape
        
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # 경량화된 CNN
        frame_features = self.backbone(images_flat)
        frame_features = self.feature_projection(frame_features)
        frame_features = frame_features.view(B, T, self.hidden_size)
        
        # GRU로 시간적 특징
        temporal_features, hidden = self.temporal_encoder(frame_features)
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features


if __name__ == "__main__":
    # 테스트 코드
    print("🧪 Mobile Image Encoder 테스트")
    
    # 모델 초기화
    encoder = MobileImageEncoder(hidden_size=768)
    encoder_lite = MobileImageEncoderLite(hidden_size=512)
    
    # 테스트 데이터 (mobile_vla_data_collector.py 형식)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    
    print(f"📊 입력 이미지: {test_images.shape}")
    
    # 일반 인코더 테스트
    with torch.no_grad():
        features = encoder(test_images)
        print(f"🖼️ 인코더 출력: {features.shape}")
        
        features_lite = encoder_lite(test_images)
        print(f"🚀 Lite 인코더 출력: {features_lite.shape}")
    
    # 파라미터 수 계산
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"📊 일반 인코더 파라미터: {total_params:,}개 ({total_params/1e6:.1f}M)")
    print(f"🚀 Lite 인코더 파라미터: {lite_params:,}개 ({lite_params/1e6:.1f}M)")
    print(f"💡 파라미터 감소율: {(1 - lite_params/total_params)*100:.1f}%")

```

```python
#!/usr/bin/env python3
"""
Mobile VLA 학습 런처 (실제 mobile_vla_data_collector 데이터로 스모크 학습)
"""

import os
import argparse
import json
import torch
from torch.utils.data import DataLoader
import importlib.util
from pathlib import Path


def _load_attr_from_file(module_name: str, file_path: str, attr: str):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    assert spec and spec.loader
    spec.loader.exec_module(module)  # type: ignore
    return getattr(module, attr)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default=os.getenv('MOBILE_VLA_DATA_DIR', '/home/soda/vla/ROS_action/mobile_vla_dataset/'))
    parser.add_argument('--batch_size', type=int, default=2)
    parser.add_argument('--sequence_length', type=int, default=18)
    parser.add_argument('--hidden_size', type=int, default=512)
    parser.add_argument('--use_lite_mode', action='store_true', default=True)
    parser.add_argument('--max_steps', type=int, default=10, help='스모크: 몇 스텝만 학습')
    parser.add_argument('--num_workers', type=int, default=2)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    return parser.parse_args()


def main():
    args = parse_args()

    configs = {
        'hidden_size': args.hidden_size,
        'use_lite_mode': args.use_lite_mode,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size,
        'sequence_length': args.sequence_length,
        'max_epochs': 1,
    }

    # 데이터셋 (동적 임포트)
    root = Path(__file__).resolve().parents[1]
    dataset_cls = _load_attr_from_file(
        'mobile_dataset',
        str(root / 'data' / 'mobile_dataset.py'),
        'MobileVLADataset'
    )
    dataset = dataset_cls(
        data_dir=args.data_dir,
        sequence_length=args.sequence_length,
        normalize_actions=True
    )

    if len(dataset) == 0:
        print('⚠️ HDF5 데이터가 없습니다. data_dir를 확인하세요:', args.data_dir)
        return

    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)

    # 트레이너 (동적 임포트로 패키지 의존성 회피)
    trainer_cls = _load_attr_from_file(
        'mobile_trainer_simple',
        str(root / 'training' / 'mobile_trainer_simple.py'),
        'SimpleMobileVLATrainer'
    )
    trainer = trainer_cls(configs)

    # 스모크 학습 루프
    steps = 0
    for batch in loader:
        # 배치 dict 텐서 외 필드 유지
        result = trainer.train_step(batch)
        print(json.dumps({k: float(v) if isinstance(v, (int, float)) else v for k, v in result.items()}, ensure_ascii=False))
        steps += 1
        if steps >= args.max_steps:
            break

    print('✅ 스모크 학습 완료: steps =', steps)


if __name__ == '__main__':
    main()



```

### 계획
- 실제 HDF5 구조를 수집기와 대조.
- 이벤트 문자열 처리 버그 수정.
- Jetson/오프라인 안전을 위해 사전가중치 다운로드 비활성화.
- 간단 런처 추가 후 실제 데이터로 스모크 학습 5스텝 실행.

### 진행
- `action_event_types`를 bytes→str 안전 처리로 수정해 매핑 오류 제거.
- `torchvision` 백본 가중치 다운로드 비활성화:
  - `efficientnet_v2_s`, `resnet50`, `mobilenet_v3_large`, `mobilenet_v3_small` 모두 `weights=None`.
- 텍스트 인코더는 모델 다운로드 실패 시 대체 로드 경로 유지.
- 동적 임포트 방식의 학습 런처 추가: `Robo+/Mobile_VLA/training/train_mobile_vla.py`.
- 실제 데이터로 스모크 학습 5스텝 성공적으로 실행됨.

### 실행 결과 요약
- 데이터셋: 65 에피소드, 1142 프레임 로드.
- Lite 모델(약 3.7M 파라미터)로 CPU에서 스모크 학습 5스텝 완료.
- 각 스텝 출력 예: `total_loss`, `action_loss`, `event_loss`, `action_accuracy`, `event_accuracy` 정상 산출.

### 사용 방법
- 아래 명령으로 바로 재현:
```bash
python3 /home/soda/vla/Robo+/Mobile_VLA/training/train_mobile_vla.py \
  --data_dir /home/soda/vla/ROS_action/mobile_vla_dataset \
  --batch_size 2 --max_steps 5 --use_lite_mode --sequence_length 18
```

원하시면:
- GPU 사용, 배치 크기/스텝/히든 사이즈 조정
- Full 모델 모드(`--use_lite_mode` 제거), 학습 루프를 에폭 단위로 확장
- 체크포인트/로깅 추가

필요한 다음 작업 있으시면 지시 주세요.

```shellscript
python3 /home/soda/vla/Robo+/Mobile_VLA/training/train_mobile_vla.py \
  --data_dir /home/soda/vla/ROS_action/mobile_vla_dataset \
  --batch_size 2 --max_steps 5 --use_lite_mode --sequence_length 18
```

---

**User**

일단 현재 ROS_action/mobile_vla_dataset 폴더 안의 h5파일들은 gitignore를 풀어주고 lfs처리하고 변경내용 상세히 적어서 push해줘

---

**Cursor**

짧게 진행 상황: h5 무시 규칙을 찾아 해제하고, Git LFS로 `ROS_action/mobile_vla_dataset/*.h5`를 추적하도록 설정한 뒤 데이터와 규칙 변경을 상세 메시지로 커밋/푸시하겠습니다. 먼저 어떤 `.gitignore`에서 h5/해당 폴더가 무시되는지 확인하겠습니다.

### 진행
- 루트 `.gitignore`의 `*.h5` 무시 규칙 해제(주석 처리).
- Git LFS 초기화 및 `ROS_action/mobile_vla_dataset/*.h5` 추적 설정.
- 실제 HDF5 데이터, 보조 메타(`core_patterns.json`), 학습 런처까지 상세 메시지로 커밋.
- 작업 브랜치: `add-mobile-vla-dataset-lfs`.

### 커밋 로그 요지
- gitignore: stop ignoring `*.h5` (track via LFS)
- lfs: track `ROS_action/mobile_vla_dataset/*.h5` via Git LFS
- data: add Mobile VLA dataset episodes via Git LFS
  - 65개+ 에피소드 HDF5 추가
  - `core_patterns.json` 포함
  - 런처 `Robo+/Mobile_VLA/training/train_mobile_vla.py` 추가
  - HDF5 스키마와 로더 호환성 메모 포함

### 다음
- 원격 push가 인증/권한 이슈로 멈춘 것으로 보입니다. 아래 중 원하는 방식 알려주세요:
  - 원격 이름/URL 지정 후 push 수행
  - 현재 origin 유지, 토큰/SSH 설정 후 재시도
  - 다른 원격으로 새로 push

원하시는 원격(`origin`) 주소나 접근 방식(HTTPS/SSH)을 알려주시면 바로 push 진행하겠습니다.

