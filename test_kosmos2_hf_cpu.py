#!/usr/bin/env python3
"""
Microsoft Kosmos-2 Hugging Face ëª¨ë¸ CPU í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # GPU ë¹„í™œì„±í™”

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import time
import psutil

def get_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024,  # MB
        'percent': process.memory_percent()
    }

def test_kosmos2_loading():
    """Kosmos-2 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ (CPU)"""
    print("ğŸš€ Microsoft Kosmos-2 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘ (CPU)")
    print("=" * 50)
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
    initial_memory = get_memory_usage()
    print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory['rss']:.1f} MB (RSS)")
    
    try:
        # ëª¨ë¸ëª… ì„¤ì • (2B íŒŒë¼ë¯¸í„° ë²„ì „)
        model_name = "microsoft/kosmos-2-patch14-224"
        
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        start_time = time.time()
        
        # í”„ë¡œì„¸ì„œ ë¡œë”©
        print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
        processor = AutoProcessor.from_pretrained(model_name)
        processor_time = time.time() - start_time
        print(f"âœ… í”„ë¡œì„¸ì„œ ë¡œë”© ì™„ë£Œ: {processor_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ë¡œë”© (CPU ìµœì í™”)
        print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_start_time = time.time()
        
        # CPU ìµœì í™” ì„¤ì •
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
            device_map=None,  # CPU ì‚¬ìš©
            low_cpu_mem_usage=True,  # ë‚®ì€ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©
            trust_remote_code=True
        )
        
        model_time = time.time() - model_start_time
        total_time = time.time() - start_time
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_time:.2f}ì´ˆ")
        print(f"â±ï¸ ì´ ë¡œë”© ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
        loaded_memory = get_memory_usage()
        memory_increase = loaded_memory['rss'] - initial_memory['rss']
        print(f"ğŸ“Š ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {loaded_memory['rss']:.1f} MB (RSS)")
        print(f"ğŸ“ˆ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.1f} MB")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"ğŸ“Š í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
        print(f"ğŸ“Š ëª¨ë¸ í¬ê¸° (ì˜ˆìƒ): {total_params * 4 / (1024**3):.2f} GB (FP32)")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def test_kosmos2_inference(model, processor):
    """Kosmos-2 ì¶”ë¡  í…ŒìŠ¤íŠ¸ (CPU)"""
    print("\nğŸ¯ Kosmos-2 ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘ (CPU)")
    print("=" * 50)
    
    try:
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ë”ë¯¸ ì´ë¯¸ì§€)
        print("ğŸ–¼ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        dummy_image = torch.randn(3, 224, 224)  # RGB ì´ë¯¸ì§€
        
        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        text_prompt = "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {text_prompt}")
        
        # ì¶”ë¡  ì‹œì‘
        print("ğŸ§  ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
        inference_start = time.time()
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = processor(
            images=dummy_image,
            text=text_prompt,
            return_tensors="pt"
        )
        
        print("ğŸ’» CPU ì‚¬ìš© ì¤‘")
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=50,  # CPUì—ì„œëŠ” ë” ì§§ê²Œ
                num_beams=2,    # CPUì—ì„œëŠ” ë” ì ì€ beam
                early_stopping=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        inference_time = time.time() - inference_start
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"âœ… ì¶”ë¡  ì™„ë£Œ: {inference_time:.2f}ì´ˆ")
        print(f"ğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = get_memory_usage()
        print(f"ğŸ“Š ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory['rss']:.1f} MB (RSS)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return False

def test_simple_inference():
    """ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        model_name = "microsoft/kosmos-2-patch14-224"
        
        print("ğŸ”§ ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()
        
        # ë” ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ë¡œë”©
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(model_name)
        
        load_time = time.time() - start_time
        print(f"âœ… ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = get_memory_usage()
        print(f"ğŸ“Š ê°„ë‹¨í•œ ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory['rss']:.1f} MB (RSS)")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        dummy_image = torch.randn(3, 224, 224)
        text_prompt = "ì´ë¯¸ì§€ ì„¤ëª…"
        
        inputs = processor(
            images=dummy_image,
            text=text_prompt,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=30,
                num_beams=1,
                do_sample=False
            )
        
        generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ“ ê°„ë‹¨í•œ ëª¨ë¸ ê²°ê³¼: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ê°„ë‹¨í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ Microsoft Kosmos-2 Hugging Face ëª¨ë¸ CPU í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print(f"ğŸ’» CPU ì½”ì–´ ìˆ˜: {psutil.cpu_count()}")
    print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    print("ğŸ’» CPU ì „ìš© ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    model, processor = test_kosmos2_loading()
    
    if model is not None and processor is not None:
        # 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
        inference_success = test_kosmos2_inference(model, processor)
        
        if inference_success:
            print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print("\nâš ï¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    # 3. ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    test_simple_inference()
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
