#!/usr/bin/env python3
"""
ì˜ë¯¸ ë²¡í„° ë¹„êµ: Frozen VLM vs LoRA VLM

ëª©ì :
1. Case 1 (LoRA + Action Head): VLMì´ íƒœìŠ¤í¬ì— ì ì‘í•œ ì˜ë¯¸ ë²¡í„°
2. Case 2 (Frozen + Action Head): VLM ì›ë³¸ ì˜ë¯¸ ë²¡í„°

ë¹„êµ ë©”íŠ¸ë¦­:
- Cosine Similarity
- L2 Distance
- CKA (Centered Kernel Alignment)
"""

import torch
import torch.nn.functional as F
import numpy as np
import h5py
from pathlib import Path
from PIL import Image
import torchvision.transforms as T
import sys
import argparse

sys.path.insert(0, str(Path(__file__).parent.parent / "RoboVLMs_upstream"))

from robovlms.train.mobile_vla_trainer import MobileVLATrainer


def compute_cka(X, Y):
    """
    Centered Kernel Alignment (CKA) - í‘œí˜„ ìœ ì‚¬ë„ ì¸¡ì •
    
    CKAëŠ” ë‘ í‘œí˜„ ê³µê°„ì˜ êµ¬ì¡°ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•œ í‘œí˜„
    """
    def centering(K):
        n = K.shape[0]
        unit = np.ones([n, n])
        I = np.eye(n)
        H = I - unit / n
        return np.dot(np.dot(H, K), H)
    
    def linear_HSIC(X, Y):
        L_X = np.dot(X, X.T)
        L_Y = np.dot(Y, Y.T)
        return np.sum(centering(L_X) * centering(L_Y))
    
    hsic_xy = linear_HSIC(X, Y)
    hsic_xx = linear_HSIC(X, X)
    hsic_yy = linear_HSIC(Y, Y)
    
    return hsic_xy / (np.sqrt(hsic_xx) * np.sqrt(hsic_yy) + 1e-10)


def extract_semantic_vectors(model, h5_files, transform, device, max_samples=50):
    """
    ëª¨ë¸ì—ì„œ ì˜ë¯¸ ë²¡í„° ì¶”ì¶œ
    
    Returns:
        vectors: (N, D) í˜•íƒœì˜ numpy array
        labels: Left/Right ë¼ë²¨
    """
    vectors = []
    labels = []
    
    with torch.no_grad():
        for i, h5_file in enumerate(h5_files[:max_samples]):
            with h5py.File(h5_file, 'r') as f:
                # ì´ë¯¸ì§€ ë¡œë“œ
                images = []
                for t in range(min(8, len(f['images']))):
                    img = Image.fromarray(f['images'][t].astype(np.uint8))
                    images.append(transform(img))
                
                while len(images) < 8:
                    images.append(torch.zeros(3, 224, 224))
                
                images_tensor = torch.stack(images).unsqueeze(0).to(device)
                
                # ì˜ë¯¸ ë²¡í„° ì¶”ì¶œ (encode_images)
                context = model.model.encode_images(images_tensor)
                
                # Flatten: (1, 8, 64, 2048) -> (1, D)
                vector = context.view(1, -1).cpu().numpy()
                vectors.append(vector)
                
                # ë¼ë²¨
                label = 'left' if 'left' in str(h5_file) else 'right'
                labels.append(label)
            
            if (i + 1) % 10 == 0:
                print(f"  Processed {i+1}/{min(len(h5_files), max_samples)}")
    
    return np.vstack(vectors), labels


def compare_vectors(vectors_frozen, vectors_lora):
    """
    ë‘ ëª¨ë¸ì˜ ì˜ë¯¸ ë²¡í„° ë¹„êµ
    """
    results = {}
    
    # 1. Cosine Similarity (ìƒ˜í”Œë³„)
    cosine_sims = []
    for v1, v2 in zip(vectors_frozen, vectors_lora):
        sim = F.cosine_similarity(
            torch.tensor(v1).unsqueeze(0),
            torch.tensor(v2).unsqueeze(0)
        ).item()
        cosine_sims.append(sim)
    
    results['cosine_mean'] = np.mean(cosine_sims)
    results['cosine_std'] = np.std(cosine_sims)
    
    # 2. L2 Distance
    l2_dists = np.linalg.norm(vectors_frozen - vectors_lora, axis=1)
    results['l2_mean'] = np.mean(l2_dists)
    results['l2_std'] = np.std(l2_dists)
    
    # 3. CKA (Centered Kernel Alignment)
    results['cka'] = compute_cka(vectors_frozen, vectors_lora)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Frozen vs LoRA ì˜ë¯¸ ë²¡í„° ë¹„êµ")
    parser.add_argument("--frozen_ckpt", type=str, required=True, help="Frozen ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸")
    parser.add_argument("--lora_ckpt", type=str, default=None, help="LoRA ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ (ì—†ìœ¼ë©´ ë¹„êµ ìŠ¤í‚µ)")
    parser.add_argument("--data_dir", type=str, default="ROS_action/mobile_vla_dataset")
    parser.add_argument("--max_samples", type=int, default=50)
    args = parser.parse_args()
    
    print("="*70)
    print("ì˜ë¯¸ ë²¡í„° ë¹„êµ: Frozen VLM vs LoRA VLM")
    print("="*70)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    transform = T.Compose([T.Resize((224, 224)), T.ToTensor()])
    
    # ë°ì´í„° ë¡œë“œ
    h5_files = sorted(list(Path(args.data_dir).glob("episode_*.h5")))
    print(f"\nì´ ì—í”¼ì†Œë“œ: {len(h5_files)}")
    
    # Frozen ëª¨ë¸ ë¡œë“œ
    print(f"\n[1/3] Frozen ëª¨ë¸ ë¡œë“œ: {args.frozen_ckpt}")
    model_frozen = MobileVLATrainer.load_from_checkpoint(args.frozen_ckpt, map_location='cpu')
    model_frozen.eval()
    model_frozen.to(device)
    
    # Frozen ë²¡í„° ì¶”ì¶œ
    print("\n[2/3] Frozen ì˜ë¯¸ ë²¡í„° ì¶”ì¶œ...")
    vectors_frozen, labels = extract_semantic_vectors(
        model_frozen, h5_files, transform, device, args.max_samples
    )
    print(f"  Shape: {vectors_frozen.shape}")
    
    # LoRA ëª¨ë¸ (ìˆìœ¼ë©´)
    if args.lora_ckpt and Path(args.lora_ckpt).exists():
        print(f"\n[3/3] LoRA ëª¨ë¸ ë¡œë“œ: {args.lora_ckpt}")
        model_lora = MobileVLATrainer.load_from_checkpoint(args.lora_ckpt, map_location='cpu')
        model_lora.eval()
        model_lora.to(device)
        
        print("\n  LoRA ì˜ë¯¸ ë²¡í„° ì¶”ì¶œ...")
        vectors_lora, _ = extract_semantic_vectors(
            model_lora, h5_files, transform, device, args.max_samples
        )
        
        # ë¹„êµ
        print("\n" + "="*70)
        print("ë¹„êµ ê²°ê³¼")
        print("="*70)
        
        results = compare_vectors(vectors_frozen, vectors_lora)
        
        print(f"\nğŸ“Š Cosine Similarity: {results['cosine_mean']:.4f} Â± {results['cosine_std']:.4f}")
        print(f"ğŸ“Š L2 Distance: {results['l2_mean']:.4f} Â± {results['l2_std']:.4f}")
        print(f"ğŸ“Š CKA: {results['cka']:.4f}")
        
        # í•´ì„
        print("\nğŸ“ í•´ì„:")
        if results['cosine_mean'] > 0.9:
            print("  - Cosine > 0.9: ë§¤ìš° ìœ ì‚¬í•œ ë°©í–¥ â†’ LoRAê°€ ë²¡í„° ë°©í–¥ í¬ê²Œ ë³€ê²½ ì•ˆ í•¨")
        elif results['cosine_mean'] > 0.7:
            print("  - 0.7 < Cosine < 0.9: ì¤‘ê°„ ì •ë„ ìœ ì‚¬ â†’ LoRAê°€ ì¼ë¶€ ì ì‘")
        else:
            print("  - Cosine < 0.7: ìƒë‹¹íˆ ë‹¤ë¦„ â†’ LoRAê°€ í‘œí˜„ í¬ê²Œ ë³€ê²½")
        
        if results['cka'] > 0.8:
            print("  - CKA > 0.8: êµ¬ì¡°ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‘œí˜„ ê³µê°„")
        else:
            print("  - CKA < 0.8: êµ¬ì¡°ì ìœ¼ë¡œ ë‹¤ë¥¸ í‘œí˜„ ê³µê°„")
    else:
        print("\n[3/3] LoRA ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ - ë¹„êµ ìŠ¤í‚µ")
        print("  â†’ Case 1 (LoRA) í•™ìŠµ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”")
    
    # Left vs Right ë¶„ì„ (Frozenë§Œ)
    print("\n" + "="*70)
    print("Frozen ëª¨ë¸ ë‚´ Left vs Right ë¹„êµ")
    print("="*70)
    
    left_mask = np.array([l == 'left' for l in labels])
    right_mask = ~left_mask
    
    vectors_left = vectors_frozen[left_mask]
    vectors_right = vectors_frozen[right_mask]
    
    # í‰ê·  ë²¡í„° ê°„ ìœ ì‚¬ë„
    mean_left = vectors_left.mean(axis=0)
    mean_right = vectors_right.mean(axis=0)
    
    cosine_lr = F.cosine_similarity(
        torch.tensor(mean_left).unsqueeze(0),
        torch.tensor(mean_right).unsqueeze(0)
    ).item()
    
    l2_lr = np.linalg.norm(mean_left - mean_right)
    
    print(f"\nğŸ“Š Left vs Right (Frozen ëª¨ë¸):")
    print(f"  Cosine Similarity: {cosine_lr:.4f}")
    print(f"  L2 Distance: {l2_lr:.4f}")
    
    if cosine_lr > 0.95:
        print("  â†’ ì˜ë¯¸ ë²¡í„°ì—ì„œ Left/Right êµ¬ë¶„ ì•½í•¨ (ì´ë¯¸ì§€ ê¸°ë°˜)")
    else:
        print("  â†’ ì˜ë¯¸ ë²¡í„°ì—ì„œ Left/Right ì–´ëŠ ì •ë„ êµ¬ë¶„ë¨")
    
    print("\nì™„ë£Œ!")


if __name__ == "__main__":
    main()
