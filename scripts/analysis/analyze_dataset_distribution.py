"""
Mobile-VLA ë°ì´í„°ì…‹ ê¶¤ì  ë¶„í¬ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë¶„ì„ í•­ëª©:
1. ì—í”¼ì†Œë“œ ìˆ˜ ë° ê¸¸ì´ ë¶„í¬
2. ì•¡ì…˜ ë¶„í¬ (linear_x, angular_z)
3. ê¶¤ì  íŒ¨í„´ (ì§ì§„, íšŒì „, ì •ì§€ ë“±)
4. ì†ë„ ë²”ìœ„ ë° í†µê³„
5. ë°ì´í„°ì…‹ ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­
"""

import h5py
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ì´ ì‹¤í–‰
import matplotlib.pyplot as plt
from tqdm import tqdm
import json

class MobileVLADatasetAnalyzer:
    """Mobile-VLA ë°ì´í„°ì…‹ ë¶„ì„ê¸°"""
    
    def __init__(self, dataset_dir: str):
        self.dataset_dir = Path(dataset_dir)
        self.h5_files = sorted(list(self.dataset_dir.glob("*.h5")))
        print(f"ğŸ“‚ Found {len(self.h5_files)} H5 files")
        
        self.analysis_results = {
            'episodes': [],
            'actions': [],
            'statistics': {}
        }
    
    def analyze_single_episode(self, h5_path: Path) -> Dict:
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë¶„ì„"""
        with h5py.File(h5_path, 'r') as f:
            # ë°ì´í„° ë¡œë“œ
            actions = f['actions'][:]  # (N, 2) - [linear_x, angular_z]
            
            # ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ë¡œë“œ
            images = f['observations']['images'][:] if 'observations' in f and 'images' in f['observations'] else None
            
            episode_info = {
                'file': h5_path.name,
                'length': len(actions),
                'linear_x': actions[:, 0],
                'angular_z': actions[:, 1],
                'has_images': images is not None
            }
            
            # í†µê³„ ê³„ì‚°
            episode_info['stats'] = {
                'linear_mean': np.mean(actions[:, 0]),
                'linear_std': np.std(actions[:, 0]),
                'linear_min': np.min(actions[:, 0]),
                'linear_max': np.max(actions[:, 0]),
                'angular_mean': np.mean(actions[:, 1]),
                'angular_std': np.std(actions[:, 1]),
                'angular_min': np.min(actions[:, 1]),
                'angular_max': np.max(actions[:, 1]),
            }
            
            return episode_info
    
    def classify_action_type(self, linear_x: float, angular_z: float) -> str:
        """ì•¡ì…˜ íƒ€ì… ë¶„ë¥˜"""
        LINEAR_THRESHOLD = 0.1  # m/s
        ANGULAR_THRESHOLD = 0.2  # rad/s
        
        is_moving = abs(linear_x) > LINEAR_THRESHOLD
        is_turning = abs(angular_z) > ANGULAR_THRESHOLD
        
        if not is_moving and not is_turning:
            return 'STOP'
        elif is_moving and not is_turning:
            return 'FORWARD' if linear_x > 0 else 'BACKWARD'
        elif not is_moving and is_turning:
            return 'TURN_LEFT' if angular_z > 0 else 'TURN_RIGHT'
        else:
            # ë™ì‹œ ì´ë™
            if angular_z > 0:
                return 'FORWARD_LEFT' if linear_x > 0 else 'BACKWARD_LEFT'
            else:
                return 'FORWARD_RIGHT' if linear_x > 0 else 'BACKWARD_RIGHT'
    
    def analyze_all_episodes(self):
        """ì „ì²´ ì—í”¼ì†Œë“œ ë¶„ì„"""
        print("\nğŸ” Analyzing all episodes...")
        
        all_actions = []
        episode_lengths = []
        action_type_counts = {
            'STOP': 0,
            'FORWARD': 0,
            'BACKWARD': 0,
            'TURN_LEFT': 0,
            'TURN_RIGHT': 0,
            'FORWARD_LEFT': 0,
            'FORWARD_RIGHT': 0,
            'BACKWARD_LEFT': 0,
            'BACKWARD_RIGHT': 0
        }
        
        for h5_file in tqdm(self.h5_files, desc="Processing episodes"):
            try:
                ep_info = self.analyze_single_episode(h5_file)
                self.analysis_results['episodes'].append(ep_info)
                
                episode_lengths.append(ep_info['length'])
                
                # ì•¡ì…˜ ìˆ˜ì§‘
                for linear_x, angular_z in zip(ep_info['linear_x'], ep_info['angular_z']):
                    all_actions.append([linear_x, angular_z])
                    action_type = self.classify_action_type(linear_x, angular_z)
                    action_type_counts[action_type] += 1
                
            except Exception as e:
                print(f"âš ï¸  Error processing {h5_file.name}: {e}")
        
        # ì „ì²´ í†µê³„
        all_actions = np.array(all_actions)
        
        self.analysis_results['statistics'] = {
            'total_episodes': len(self.h5_files),
            'total_timesteps': len(all_actions),
            'avg_episode_length': np.mean(episode_lengths),
            'std_episode_length': np.std(episode_lengths),
            'min_episode_length': np.min(episode_lengths),
            'max_episode_length': np.max(episode_lengths),
            
            'linear_x_mean': np.mean(all_actions[:, 0]),
            'linear_x_std': np.std(all_actions[:, 0]),
            'linear_x_min': np.min(all_actions[:, 0]),
            'linear_x_max': np.max(all_actions[:, 0]),
            
            'angular_z_mean': np.mean(all_actions[:, 1]),
            'angular_z_std': np.std(all_actions[:, 1]),
            'angular_z_min': np.min(all_actions[:, 1]),
            'angular_z_max': np.max(all_actions[:, 1]),
            
            'action_type_counts': action_type_counts
        }
        
        self.all_actions = all_actions
        self.episode_lengths = episode_lengths
    
    def generate_summary_table(self) -> pd.DataFrame:
        """ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        stats = self.analysis_results['statistics']
        
        summary_data = {
            'í•­ëª©': [
                'ì´ ì—í”¼ì†Œë“œ ìˆ˜',
                'ì´ íƒ€ì„ìŠ¤í… ìˆ˜',
                'í‰ê·  ì—í”¼ì†Œë“œ ê¸¸ì´',
                'ì—í”¼ì†Œë“œ ê¸¸ì´ (ìµœì†Œ/ìµœëŒ€)',
                '',
                'Linear Velocity (m/s)',
                '  - í‰ê·  Â± í‘œì¤€í¸ì°¨',
                '  - ë²”ìœ„ (ìµœì†Œ/ìµœëŒ€)',
                '',
                'Angular Velocity (rad/s)',
                '  - í‰ê·  Â± í‘œì¤€í¸ì°¨',
                '  - ë²”ìœ„ (ìµœì†Œ/ìµœëŒ€)',
            ],
            'ê°’': [
                f"{stats['total_episodes']}ê°œ",
                f"{stats['total_timesteps']:,}ê°œ",
                f"{stats['avg_episode_length']:.1f} Â± {stats['std_episode_length']:.1f}",
                f"{stats['min_episode_length']} / {stats['max_episode_length']}",
                '',
                '',
                f"{stats['linear_x_mean']:.3f} Â± {stats['linear_x_std']:.3f}",
                f"{stats['linear_x_min']:.3f} / {stats['linear_x_max']:.3f}",
                '',
                '',
                f"{stats['angular_z_mean']:.3f} Â± {stats['angular_z_std']:.3f}",
                f"{stats['angular_z_min']:.3f} / {stats['angular_z_max']:.3f}",
            ]
        }
        
        return pd.DataFrame(summary_data)
    
    def generate_action_distribution_table(self) -> pd.DataFrame:
        """ì•¡ì…˜ ë¶„í¬ í…Œì´ë¸” ìƒì„±"""
        action_counts = self.analysis_results['statistics']['action_type_counts']
        total = sum(action_counts.values())
        
        action_data = {
            'ì•¡ì…˜ íƒ€ì…': [],
            'ê°œìˆ˜': [],
            'ë¹„ìœ¨ (%)': [],
            'ì„¤ëª…': []
        }
        
        action_descriptions = {
            'FORWARD': 'ì§ì§„',
            'BACKWARD': 'í›„ì§„',
            'TURN_LEFT': 'ì œìë¦¬ ì¢ŒíšŒì „',
            'TURN_RIGHT': 'ì œìë¦¬ ìš°íšŒì „',
            'FORWARD_LEFT': 'ì „ì§„ + ì¢ŒíšŒì „',
            'FORWARD_RIGHT': 'ì „ì§„ + ìš°íšŒì „',
            'BACKWARD_LEFT': 'í›„ì§„ + ì¢ŒíšŒì „',
            'BACKWARD_RIGHT': 'í›„ì§„ + ìš°íšŒì „',
            'STOP': 'ì •ì§€'
        }
        
        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_actions = sorted(action_counts.items(), key=lambda x: x[1], reverse=True)
        
        for action_type, count in sorted_actions:
            action_data['ì•¡ì…˜ íƒ€ì…'].append(action_type)
            action_data['ê°œìˆ˜'].append(f"{count:,}")
            action_data['ë¹„ìœ¨ (%)'].append(f"{count/total*100:.1f}%")
            action_data['ì„¤ëª…'].append(action_descriptions.get(action_type, '-'))
        
        return pd.DataFrame(action_data)
    
    def plot_distributions(self, save_path: str = None):
        """ë¶„í¬ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        
        # 1. ì—í”¼ì†Œë“œ ê¸¸ì´ ë¶„í¬
        axes[0, 0].hist(self.episode_lengths, bins=20, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('Episode Length Distribution', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Episode Length (steps)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].axvline(np.mean(self.episode_lengths), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(self.episode_lengths):.1f}')
        axes[0, 0].legend()
        
        # 2. Linear Velocity ë¶„í¬
        axes[0, 1].hist(self.all_actions[:, 0], bins=50, color='lightgreen', edgecolor='black')
        axes[0, 1].set_title('Linear Velocity Distribution', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Linear Velocity (m/s)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].axvline(0, color='black', linestyle='-', alpha=0.3)
        
        # 3. Angular Velocity ë¶„í¬
        axes[0, 2].hist(self.all_actions[:, 1], bins=50, color='lightcoral', edgecolor='black')
        axes[0, 2].set_title('Angular Velocity Distribution', fontsize=14, fontweight='bold')
        axes[0, 2].set_xlabel('Angular Velocity (rad/s)')
        axes[0, 2].set_ylabel('Frequency')
        axes[0, 2].axvline(0, color='black', linestyle='-', alpha=0.3)
        
        # 4. 2D Joint Distribution
        axes[1, 0].scatter(self.all_actions[:, 0], self.all_actions[:, 1], 
                          alpha=0.1, s=1, color='navy')
        axes[1, 0].set_title('Action Space Distribution (2D)', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Linear Velocity (m/s)')
        axes[1, 0].set_ylabel('Angular Velocity (rad/s)')
        axes[1, 0].axhline(0, color='gray', linestyle='--', alpha=0.5)
        axes[1, 0].axvline(0, color='gray', linestyle='--', alpha=0.5)
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Action Type Distribution (íŒŒì´ ì°¨íŠ¸)
        action_counts = self.analysis_results['statistics']['action_type_counts']
        # 5% ì´ìƒë§Œ í‘œì‹œ
        total = sum(action_counts.values())
        filtered_counts = {k: v for k, v in action_counts.items() if v/total > 0.05}
        other_count = sum(v for k, v in action_counts.items() if k not in filtered_counts)
        if other_count > 0:
            filtered_counts['OTHER'] = other_count
        
        axes[1, 1].pie(filtered_counts.values(), labels=filtered_counts.keys(), 
                      autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('Action Type Distribution', fontsize=14, fontweight='bold')
        
        # 6. Velocity Magnitude Distribution
        velocity_magnitudes = np.sqrt(self.all_actions[:, 0]**2 + self.all_actions[:, 1]**2)
        axes[1, 2].hist(velocity_magnitudes, bins=50, color='orange', edgecolor='black')
        axes[1, 2].set_title('Velocity Magnitude Distribution', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('|v| = âˆš(linearÂ² + angularÂ²)')
        axes[1, 2].set_ylabel('Frequency')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… Plot saved to: {save_path}")
        
        return fig
    
    def export_analysis(self, output_dir: str = "analysis_results"):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # 1. Summary Table
        summary_table = self.generate_summary_table()
        summary_table.to_csv(output_path / "dataset_summary.csv", index=False)
        print(f"âœ… Summary table saved to: {output_path / 'dataset_summary.csv'}")
        
        # 2. Action Distribution Table
        action_table = self.generate_action_distribution_table()
        action_table.to_csv(output_path / "action_distribution.csv", index=False)
        print(f"âœ… Action distribution saved to: {output_path / 'action_distribution.csv'}")
        
        # 3. Statistics JSON
        with open(output_path / "statistics.json", 'w') as f:
            json.dump(self.analysis_results['statistics'], f, indent=2)
        print(f"âœ… Statistics JSON saved to: {output_path / 'statistics.json'}")
        
        # 4. Plots
        self.plot_distributions(save_path=output_path / "distributions.png")
        
        return summary_table, action_table

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_dir = "/Users/minu/dev/vla/ROS_action/mobile_vla_dataset"
    
    print("=" * 60)
    print("ğŸ“Š Mobile-VLA Dataset Trajectory Analysis")
    print("=" * 60)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = MobileVLADatasetAnalyzer(dataset_dir)
    
    # ì „ì²´ ë¶„ì„ ìˆ˜í–‰
    analyzer.analyze_all_episodes()
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ Exporting analysis results...")
    summary_table, action_table = analyzer.export_analysis(
        output_dir="/Users/minu/dev/vla/docs/research/data_augmentation/analysis_results"
    )
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ DATASET SUMMARY")
    print("=" * 60)
    print(summary_table.to_string(index=False))
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ACTION TYPE DISTRIBUTION")
    print("=" * 60)
    print(action_table.to_string(index=False))
    
    print("\nâœ… Analysis complete!")

if __name__ == "__main__":
    main()
