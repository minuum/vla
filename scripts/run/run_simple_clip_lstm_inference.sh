#!/bin/bash

# π€ Simple CLIP LSTM λ¨λΈ μ²΄ν¬ν¬μΈνΈ κΈ°λ° μ¶”λ΅  μ¤ν¬λ¦½νΈ
# μ²΄ν¬ν¬μΈνΈ: vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth

set -e

echo "π€ Simple CLIP LSTM λ¨λΈ μ²΄ν¬ν¬μΈνΈ κΈ°λ° μ¶”λ΅  μ‹μ‘"
echo "π“… μ²΄ν¬ν¬μΈνΈ: best_simple_clip_lstm_model.pth"
echo ""

# μ²΄ν¬ν¬μΈνΈ νμΌ μ΅΄μ¬ ν™•μΈ
CHECKPOINT_PATH="vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth"
if [ ! -f "$CHECKPOINT_PATH" ]; then
    echo "β μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: $CHECKPOINT_PATH"
    exit 1
fi

echo "β… μ²΄ν¬ν¬μΈνΈ νμΌ ν™•μΈλ¨: $(ls -lh $CHECKPOINT_PATH)"
echo ""

# κΈ°μ΅΄ μ»¨ν…μ΄λ„ μ •λ¦¬
echo "π§Ή κΈ°μ΅΄ μ»¨ν…μ΄λ„ μ •λ¦¬ μ¤‘..."
docker stop simple_clip_lstm_inference 2>/dev/null || true
docker rm simple_clip_lstm_inference 2>/dev/null || true

# μ»¨ν…μ΄λ„ μ‹¤ν–‰
echo "π€ Simple CLIP LSTM μ¶”λ΅  μ»¨ν…μ΄λ„ μ‹¤ν–‰ μ¤‘..."
docker run -d \
    --name simple_clip_lstm_inference \
    --gpus all \
    -v /dev/bus/usb:/dev/bus/usb \
    -v /usr/local/cuda:/usr/local/cuda \
    -v /usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu \
    -v $(pwd)/vla:/workspace/vla \
    -e NVIDIA_VISIBLE_DEVICES=all \
    -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    -e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
    -e CUDA_VISIBLE_DEVICES=0 \
    mobile_vla:pytorch-2.3.0-cuda \
    sleep infinity

# μ»¨ν…μ΄λ„ μƒνƒ ν™•μΈ
echo "π“‹ μ»¨ν…μ΄λ„ μƒνƒ ν™•μΈ:"
sleep 3
docker ps | grep simple_clip_lstm_inference

# μ„±κ³µν•λ©΄ μ¶”λ΅  μ‹μ‘
if docker ps | grep -q simple_clip_lstm_inference; then
    echo "β… μ»¨ν…μ΄λ„κ°€ μ‹¤ν–‰ μ¤‘μ…λ‹λ‹¤. Simple CLIP LSTM μ¶”λ΅ μ„ μ‹μ‘ν•©λ‹λ‹¤..."
    echo ""
    
    # μ¶”λ΅  μ‹¤ν–‰
    docker exec -it simple_clip_lstm_inference bash -c "
        echo 'π― Simple CLIP LSTM λ¨λΈ μ²΄ν¬ν¬μΈνΈ κΈ°λ° μ¶”λ΅ '
        echo 'π“… μ²΄ν¬ν¬μΈνΈ: best_simple_clip_lstm_model.pth'
        echo ''
        
        # μ‘μ—… λ””λ ‰ν† λ¦¬λ΅ μ΄λ™
        cd /workspace
        
        # 1. μ‹μ¤ν… μƒνƒ ν™•μΈ
        echo '1οΈβƒ£ μ‹μ¤ν… μƒνƒ ν™•μΈ:'
        echo '   CUDA μƒνƒ:'
        python3 -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')\"
        echo '   GPU μ •λ³΄:'
        nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader,nounits
        echo ''
        
        # 2. μ²΄ν¬ν¬μΈνΈ νμΌ ν™•μΈ
        echo '2οΈβƒ£ μ²΄ν¬ν¬μΈνΈ νμΌ ν™•μΈ:'
        ls -lh vla/Robo+/Mobile_VLA/simple_clip_lstm_model/
        echo ''
        
        # 3. Simple CLIP LSTM λ¨λΈ μ •μ λ° μ²΄ν¬ν¬μΈνΈ λ΅λ“
        echo '3οΈβƒ£ Simple CLIP LSTM λ¨λΈ λ΅λ“:'
        python3 -c \"
import torch
import torch.nn as nn
import sys
import os
import time
import numpy as np

# Simple CLIP LSTM λ¨λΈ ν΄λμ¤ μ •μ
class SimpleCLIPLSTMModel(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Vision feature encoder
        self.vision_encoder = nn.Linear(input_size, hidden_size)
        
        # Text feature encoder  
        self.text_encoder = nn.Linear(input_size, hidden_size)
        
        # LSTM layer
        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True)
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, output_size)
        )
        
    def forward(self, vision_features, text_features, hidden=None):
        # Encode features
        vision_encoded = self.vision_encoder(vision_features)
        text_encoded = self.text_encoder(text_features)
        
        # Concatenate features
        combined = torch.cat([vision_encoded, text_encoded], dim=-1)
        
        # Add sequence dimension if needed
        if combined.dim() == 2:
            combined = combined.unsqueeze(1)  # [batch, 1, features]
        
        # LSTM forward pass
        lstm_out, hidden = self.lstm(combined, hidden)
        
        # Take the last output
        last_output = lstm_out[:, -1, :]
        
        # Predict actions
        actions = self.action_head(last_output)
        
        return actions, hidden

print('Simple CLIP LSTM μ²΄ν¬ν¬μΈνΈ λ΅λ”© μ¤‘...')
try:
    # μ²΄ν¬ν¬μΈνΈ κ²½λ΅ μ„¤μ •
    checkpoint_path = './vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth'
    
    if os.path.exists(checkpoint_path):
        # λ¨λΈ μΈμ¤ν„΄μ¤ μƒμ„±
        model = SimpleCLIPLSTMModel()
        
        # μ²΄ν¬ν¬μΈνΈ λ΅λ“
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° ν™•μΈ
        print(f'μ²΄ν¬ν¬μΈνΈ ν‚¤: {list(checkpoint.keys())}')
        
        # λ¨λΈ μƒνƒ λ΅λ“ (λ‹¤μ–‘ν• ν‚¤ μ΄λ¦„ μ‹λ„)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        elif 'state_dict' in checkpoint:
            model.load_state_dict(checkpoint['state_dict'])
        elif 'model' in checkpoint:
            model.load_state_dict(checkpoint['model'])
        else:
            # μ§μ ‘ λ΅λ“ μ‹λ„
            model.load_state_dict(checkpoint)
        
        print('β… Simple CLIP LSTM μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ„±κ³µ')
        print(f'μ²΄ν¬ν¬μΈνΈ κ²½λ΅: {checkpoint_path}')
        print(f'λ¨λΈ νλΌλ―Έν„° μ: {sum(p.numel() for p in model.parameters()):,}')
        
        # μ²΄ν¬ν¬μΈνΈ λ©”νƒ€λ°μ΄ν„° μ¶λ ¥
        for key, value in checkpoint.items():
            if key not in ['model_state_dict', 'state_dict', 'model']:
                print(f'{key}: {value}')
                
    else:
        print(f'β μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}')
        print('μ‚¬μ© κ°€λ¥ν• μ²΄ν¬ν¬μΈνΈ:')
        os.system('find . -name \"*.pth\" -type f | head -10')
        
except Exception as e:
    print(f'β μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}')
    import traceback
    traceback.print_exc()
\"
        echo ''
        
        # 4. μ‹¤μ  μ¶”λ΅  μ„±λ¥ ν…μ¤νΈ
        echo '4οΈβƒ£ Simple CLIP LSTM μ¶”λ΅  μ„±λ¥ ν…μ¤νΈ:'
        python3 -c \"
import time
import torch
import torch.nn as nn
import numpy as np

# Simple CLIP LSTM λ¨λΈ ν΄λμ¤ μ •μ (μ¶”λ΅ μ©)
class SimpleCLIPLSTMModel(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.vision_encoder = nn.Linear(input_size, hidden_size)
        self.text_encoder = nn.Linear(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, output_size)
        )
        
    def forward(self, vision_features, text_features, hidden=None):
        vision_encoded = self.vision_encoder(vision_features)
        text_encoded = self.text_encoder(text_features)
        combined = torch.cat([vision_encoded, text_encoded], dim=-1)
        
        if combined.dim() == 2:
            combined = combined.unsqueeze(1)
        
        lstm_out, hidden = self.lstm(combined, hidden)
        last_output = lstm_out[:, -1, :]
        actions = self.action_head(last_output)
        return actions, hidden

print('Simple CLIP LSTM μ¶”λ΅  μ„±λ¥ μΈ΅μ • μ¤‘...')
try:
    # μ²΄ν¬ν¬μΈνΈ λ΅λ“
    checkpoint_path = './vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth'
    model = SimpleCLIPLSTMModel()
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # λ¨λΈ μƒνƒ λ΅λ“
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
    elif 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    
    # GPUλ΅ μ΄λ™ (κ°€λ¥ν• κ²½μ°)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    # λ”λ―Έ μ…λ ¥ μƒμ„±
    batch_size = 1
    vision_features = torch.randn(batch_size, 2048).to(device)
    text_features = torch.randn(batch_size, 2048).to(device)
    
    # μ¶”λ΅  μ‹κ°„ μΈ΅μ • (100ν ν‰κ· )
    num_runs = 100
    times = []
    
    with torch.no_grad():
        for i in range(num_runs):
            start_time = time.time()
            actions, hidden = model(vision_features, text_features)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
    
    avg_time = sum(times) / len(times)
    fps = 1000 / avg_time
    
    print(f'β… Simple CLIP LSTM μ¶”λ΅  μ„±λ¥:')
    print(f'   λ””λ°”μ΄μ¤: {device}')
    print(f'   ν‰κ·  μ¶”λ΅  μ‹κ°„: {avg_time:.3f}ms')
    print(f'   FPS: {fps:.0f}')
    print(f'   λ¨λΈ νλΌλ―Έν„° μ: {sum(p.numel() for p in model.parameters()):,}')
    print(f'   β… μ‹¤μ‹κ°„ λ΅λ΄‡ μ μ–΄ κ°€λ¥')
    
    # μƒν” μ¶”λ΅  κ²°κ³Ό μ¶λ ¥
    print(f'   μƒν” μ¶”λ΅  κ²°κ³Ό: {actions[0].cpu().numpy()}')
    
except Exception as e:
    print(f'β μ¶”λ΅  ν…μ¤νΈ μ‹¤ν¨: {e}')
    import traceback
    traceback.print_exc()
\"
        echo ''
        
        # 5. λ€ν™”ν• μ¶”λ΅  λ¨λ“
        echo '5οΈβƒ£ λ€ν™”ν• μ¶”λ΅  λ¨λ“:'
        echo '   μ‚¬μ© κ°€λ¥ν• λ…λ Ήμ–΄:'
        echo '     - python3 inference_demo.py: λ€ν™”ν• μ¶”λ΅  λ°λ¨'
        echo '     - python3 batch_inference.py: λ°°μΉ μ¶”λ΅  ν…μ¤νΈ'
        echo '     - nvidia-smi: GPU μƒνƒ ν™•μΈ'
        echo '     - htop: μ‹μ¤ν… λ¦¬μ†μ¤ ν™•μΈ'
        echo ''
        echo 'π’¬ λ€ν™”ν• λ¨λ“λ΅ μ „ν™ν•©λ‹λ‹¤. λ…λ Ήμ–΄λ¥Ό μ…λ ¥ν•μ„Έμ”:'
        exec bash
    "
else
    echo "β μ»¨ν…μ΄λ„ μ‹¤ν–‰ μ‹¤ν¨"
    docker logs simple_clip_lstm_inference
    exit 1
fi
