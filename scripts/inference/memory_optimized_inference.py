#!/usr/bin/env python3
"""
ğŸ¯ ë©”ëª¨ë¦¬ ìµœì í™”ëœ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
ì²´í¬í¬ì¸íŠ¸: best_simple_clip_lstm_model.pth
ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™” í¬í•¨
"""

import torch
import torch.nn as nn
import numpy as np
import time
import os
import sys
import psutil
import gc
from typing import Optional, Tuple
import subprocess

def get_memory_info():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        system_info = {
            'total': memory.total / (1024**3),  # GB
            'available': memory.available / (1024**3),  # GB
            'used': memory.used / (1024**3),  # GB
            'percent': memory.percent
        }
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_info = {}
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_stats()
            gpu_info = {
                'allocated': torch.cuda.memory_allocated() / (1024**3),  # GB
                'cached': torch.cuda.memory_reserved() / (1024**3),  # GB
                'max_allocated': torch.cuda.max_memory_allocated() / (1024**3),  # GB
                'max_cached': torch.cuda.max_memory_reserved() / (1024**3)  # GB
            }
        
        return system_info, gpu_info
    except Exception as e:
        print(f"ë©”ëª¨ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return {}, {}

def print_memory_status(title="ë©”ëª¨ë¦¬ ìƒíƒœ"):
    """ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    system_info, gpu_info = get_memory_info()
    
    print(f"\nğŸ“Š {title}")
    print("-" * 50)
    
    if system_info:
        print(f"ğŸ’¾ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬:")
        print(f"   ì „ì²´: {system_info['total']:.2f} GB")
        print(f"   ì‚¬ìš© ì¤‘: {system_info['used']:.2f} GB ({system_info['percent']:.1f}%)")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {system_info['available']:.2f} GB")
    
    if gpu_info:
        print(f"ğŸ® GPU ë©”ëª¨ë¦¬:")
        print(f"   í• ë‹¹ë¨: {gpu_info['allocated']:.2f} GB")
        print(f"   ìºì‹œë¨: {gpu_info['cached']:.2f} GB")
        print(f"   ìµœëŒ€ í• ë‹¹: {gpu_info['max_allocated']:.2f} GB")
        print(f"   ìµœëŒ€ ìºì‹œ: {gpu_info['max_cached']:.2f} GB")

def clear_memory():
    """ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
    print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    
    # PyTorch ìºì‹œ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    gc.collect()
    
    print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

class MemoryOptimizedKosmosCLIPModel(nn.Module):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸"""
    
    def __init__(self, 
                 kosmos_hidden_size=4096,
                 clip_hidden_size=768,
                 fusion_size=2048,
                 lstm_hidden_size=512,
                 lstm_layers=4,
                 action_size=2):
        super().__init__()
        
        print("ğŸ”¨ ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ë” ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘ (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.kosmos_hidden_size = kosmos_hidden_size
        self.clip_hidden_size = clip_hidden_size
        
        # Kosmos2 ëª¨ë¸ (ë ˆì´ì–´ ìˆ˜ ì¤„ì„)
        self.kosmos_text_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=kosmos_hidden_size,
                nhead=16,
                dim_feedforward=8192,  # ì¤„ì„
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6  # 24 â†’ 6ìœ¼ë¡œ ì¤„ì„
        )
        
        self.kosmos_vision_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=kosmos_hidden_size,
                nhead=16,
                dim_feedforward=8192,  # ì¤„ì„
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6  # 24 â†’ 6ìœ¼ë¡œ ì¤„ì„
        )
        
        # CLIP ëª¨ë¸ (ë ˆì´ì–´ ìˆ˜ ì¤„ì„)
        self.clip_text_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=clip_hidden_size,
                nhead=12,
                dim_feedforward=1536,  # ì¤„ì„
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6  # 12 â†’ 6ìœ¼ë¡œ ì¤„ì„
        )
        
        self.clip_vision_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=clip_hidden_size,
                nhead=12,
                dim_feedforward=1536,  # ì¤„ì„
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6  # 12 â†’ 6ìœ¼ë¡œ ì¤„ì„
        )
        
        # Feature fusion layer
        self.feature_fusion = nn.Linear(
            kosmos_hidden_size + clip_hidden_size, 
            fusion_size
        )
        
        # LSTM layer (ë ˆì´ì–´ ìˆ˜ ì¤„ì„)
        self.lstm = nn.LSTM(
            input_size=fusion_size,
            hidden_size=lstm_hidden_size,
            num_layers=2,  # 4 â†’ 2ë¡œ ì¤„ì„
            batch_first=True,
            dropout=0.1
        )
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(lstm_hidden_size, lstm_hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(lstm_hidden_size // 2, action_size)
        )
        
        # Embeddings (í¬ê¸° ì¤„ì„)
        self.kosmos_text_embedding = nn.Embedding(32000, kosmos_hidden_size)
        self.kosmos_vision_embedding = nn.Linear(768, kosmos_hidden_size)
        self.clip_text_embedding = nn.Embedding(49408, clip_hidden_size)
        self.clip_vision_embedding = nn.Linear(768, clip_hidden_size)
        
        # Position embeddings (í¬ê¸° ì¤„ì„)
        self.kosmos_text_pos_embedding = nn.Embedding(512, kosmos_hidden_size)  # 2048 â†’ 512
        self.kosmos_vision_pos_embedding = nn.Embedding(257, kosmos_hidden_size)
        self.clip_text_pos_embedding = nn.Embedding(77, clip_hidden_size)
        self.clip_vision_pos_embedding = nn.Embedding(257, clip_hidden_size)
        
        # Layer norms
        self.kosmos_text_norm = nn.LayerNorm(kosmos_hidden_size)
        self.kosmos_vision_norm = nn.LayerNorm(kosmos_hidden_size)
        self.clip_text_norm = nn.LayerNorm(clip_hidden_size)
        self.clip_vision_norm = nn.LayerNorm(clip_hidden_size)
        
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
    def forward(self, 
                kosmos_text_input: Optional[torch.Tensor] = None,
                kosmos_vision_input: Optional[torch.Tensor] = None,
                clip_text_input: Optional[torch.Tensor] = None,
                clip_vision_input: Optional[torch.Tensor] = None,
                hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):
        """Forward pass with memory optimization"""
        
        # Kosmos2 text processing
        if kosmos_text_input is not None:
            kosmos_text_emb = self.kosmos_text_embedding(kosmos_text_input)
            kosmos_text_pos = self.kosmos_text_pos_embedding(
                torch.arange(kosmos_text_input.size(1), device=kosmos_text_input.device)
            ).unsqueeze(0)
            kosmos_text_emb = kosmos_text_emb + kosmos_text_pos
            kosmos_text_emb = self.kosmos_text_norm(kosmos_text_emb)
            kosmos_text_features = self.kosmos_text_model(kosmos_text_emb)
            kosmos_text_features = kosmos_text_features.mean(dim=1)
            del kosmos_text_emb, kosmos_text_pos  # ë©”ëª¨ë¦¬ í•´ì œ
        else:
            kosmos_text_features = torch.zeros(
                kosmos_vision_input.size(0), self.kosmos_hidden_size, 
                device=kosmos_vision_input.device
            )
        
        # Kosmos2 vision processing
        if kosmos_vision_input is not None:
            kosmos_vision_emb = self.kosmos_vision_embedding(kosmos_vision_input)
            kosmos_vision_pos = self.kosmos_vision_pos_embedding(
                torch.arange(kosmos_vision_input.size(1), device=kosmos_vision_input.device)
            ).unsqueeze(0)
            kosmos_vision_emb = kosmos_vision_emb + kosmos_vision_pos
            kosmos_vision_emb = self.kosmos_vision_norm(kosmos_vision_emb)
            kosmos_vision_features = self.kosmos_vision_model(kosmos_vision_emb)
            kosmos_vision_features = kosmos_vision_features.mean(dim=1)
            del kosmos_vision_emb, kosmos_vision_pos  # ë©”ëª¨ë¦¬ í•´ì œ
        else:
            kosmos_vision_features = torch.zeros(
                kosmos_text_input.size(0), self.kosmos_hidden_size,
                device=kosmos_text_input.device
            )
        
        # CLIP text processing
        if clip_text_input is not None:
            clip_text_emb = self.clip_text_embedding(clip_text_input)
            clip_text_pos = self.clip_text_pos_embedding(
                torch.arange(clip_text_input.size(1), device=clip_text_input.device)
            ).unsqueeze(0)
            clip_text_emb = clip_text_emb + clip_text_pos
            clip_text_emb = self.clip_text_norm(clip_text_emb)
            clip_text_features = self.clip_text_model(clip_text_emb)
            clip_text_features = clip_text_features.mean(dim=1)
            del clip_text_emb, clip_text_pos  # ë©”ëª¨ë¦¬ í•´ì œ
        else:
            clip_text_features = torch.zeros(
                kosmos_vision_input.size(0), self.clip_hidden_size,
                device=kosmos_vision_input.device
            )
        
        # CLIP vision processing
        if clip_vision_input is not None:
            clip_vision_emb = self.clip_vision_embedding(clip_vision_input)
            clip_vision_pos = self.clip_vision_pos_embedding(
                torch.arange(clip_vision_input.size(1), device=clip_vision_input.device)
            ).unsqueeze(0)
            clip_vision_emb = clip_vision_emb + clip_vision_pos
            clip_vision_emb = self.clip_vision_norm(clip_vision_emb)
            clip_vision_features = self.clip_vision_model(clip_vision_emb)
            clip_vision_features = clip_vision_features.mean(dim=1)
            del clip_vision_emb, clip_vision_pos  # ë©”ëª¨ë¦¬ í•´ì œ
        else:
            clip_vision_features = torch.zeros(
                kosmos_text_input.size(0), self.clip_hidden_size,
                device=kosmos_text_input.device
            )
        
        # Feature fusion
        kosmos_features = (kosmos_text_features + kosmos_vision_features) / 2
        clip_features = (clip_text_features + clip_vision_features) / 2
        
        combined_features = torch.cat([kosmos_features, clip_features], dim=-1)
        fused_features = self.feature_fusion(combined_features)
        
        # Add sequence dimension for LSTM
        if fused_features.dim() == 2:
            fused_features = fused_features.unsqueeze(1)
        
        # LSTM processing
        lstm_out, hidden = self.lstm(fused_features, hidden)
        
        # Take the last output
        last_output = lstm_out[:, -1, :]
        
        # Action prediction
        actions = self.action_head(last_output)
        
        return actions, hidden

def load_model_with_memory_check(checkpoint_path):
    """ë©”ëª¨ë¦¬ ì²´í¬ì™€ í•¨ê»˜ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ ë¡œë”© ì¤‘: {checkpoint_path}")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    print_memory_status("ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ")
    
    try:
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        print("ğŸ”¨ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        model = MemoryOptimizedKosmosCLIPModel()
        
        print_memory_status("ëª¨ë¸ ìƒì„± í›„ ë©”ëª¨ë¦¬ ìƒíƒœ")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (CPUì—ì„œ ë¨¼ì €)
        print("ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤ ìˆ˜: {len(checkpoint.keys())}")
        print(f"ğŸ“‹ ì£¼ìš” í‚¤ë“¤: {list(checkpoint.keys())[:5]}...")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        print_memory_status("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ")
        
        # ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì‹œë„
        print("ğŸ”§ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì¤‘...")
        try:
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            elif 'model' in checkpoint:
                model.load_state_dict(checkpoint['model'])
            else:
                model.load_state_dict(checkpoint)
            print("âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‘ìœ„ ê°€ì¤‘ì¹˜ ì‚¬ìš©): {e}")
        
        # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì¤‘...")
        
        model = model.to(device)
        model.eval()
        
        print_memory_status("GPU ì´ë™ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ")
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
        
        return model, device
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        clear_memory()
        raise e

def generate_small_dummy_inputs(batch_size=1, device='cpu'):
    """ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ì‘ì€ ë”ë¯¸ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # ë” ì‘ì€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì‚¬ìš©
    kosmos_text = torch.randint(0, 32000, (batch_size, 64)).to(device)  # 128 â†’ 64
    kosmos_vision = torch.randn(batch_size, 129, 768).to(device)  # 257 â†’ 129
    clip_text = torch.randint(0, 49408, (batch_size, 38)).to(device)  # 77 â†’ 38
    clip_vision = torch.randn(batch_size, 129, 768).to(device)  # 257 â†’ 129
    
    return kosmos_text, kosmos_vision, clip_text, clip_vision

def run_memory_optimized_inference(model, device, inputs=None):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if inputs is None:
        inputs = generate_small_dummy_inputs(device=device)
    
    kosmos_text, kosmos_vision, clip_text, clip_vision = inputs
    
    print_memory_status("ì¶”ë¡  ì‹œì‘ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ")
    
    start_time = time.time()
    
    with torch.no_grad():
        actions, hidden = model(
            kosmos_text_input=kosmos_text,
            kosmos_vision_input=kosmos_vision,
            clip_text_input=clip_text,
            clip_vision_input=clip_vision
        )
    
    inference_time = (time.time() - start_time) * 1000  # ms
    fps = 1000 / inference_time
    
    print_memory_status("ì¶”ë¡  ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_memory()
    
    return actions, hidden, inference_time, fps

def main():
    print("ğŸ¯ ë©”ëª¨ë¦¬ ìµœì í™”ëœ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¶”ë¡  ë°ëª¨")
    print("=" * 70)
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
    checkpoint_path = './vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth'
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, device = load_model_with_memory_check(checkpoint_path)
        
        print("\nğŸš€ ëŒ€í™”í˜• ì¶”ë¡  ëª¨ë“œ ì‹œì‘")
        print("ëª…ë ¹ì–´:")
        print("  'infer': ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰")
        print("  'benchmark': ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (5íšŒ)")
        print("  'memory': ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
        print("  'clear': ë©”ëª¨ë¦¬ ì •ë¦¬")
        print("  'quit': ì¢…ë£Œ")
        print("-" * 70)
        
        while True:
            try:
                command = input("\nğŸ’¬ ëª…ë ¹ì–´ ì…ë ¥: ").strip().lower()
                
                if command == 'quit':
                    print("ğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                elif command == 'infer':
                    print("ğŸ”„ ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
                    actions, hidden, inference_time, fps = run_memory_optimized_inference(model, device)
                    
                    print(f"âœ… ì¶”ë¡  ì™„ë£Œ:")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ms")
                    print(f"   FPS: {fps:.0f}")
                    print(f"   ì•¡ì…˜ ê²°ê³¼: {actions[0].cpu().numpy()}")
                
                elif command == 'benchmark':
                    print("ğŸ”„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ (5íšŒ)...")
                    
                    times = []
                    for i in range(5):
                        print(f"   {i+1}/5 ì‹¤í–‰ ì¤‘...")
                        _, _, inference_time, _ = run_memory_optimized_inference(model, device)
                        times.append(inference_time)
                        print(f"   {i+1}/5 ì™„ë£Œ: {inference_time:.3f}ms")
                    
                    avg_time = sum(times) / len(times)
                    fps = 1000 / avg_time
                    min_time = min(times)
                    max_time = max(times)
                    
                    print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ:")
                    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.3f}ms")
                    print(f"   ìµœì†Œ ì¶”ë¡  ì‹œê°„: {min_time:.3f}ms")
                    print(f"   ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {max_time:.3f}ms")
                    print(f"   í‰ê·  FPS: {fps:.0f}")
                
                elif command == 'memory':
                    print_memory_status("í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ")
                
                elif command == 'clear':
                    clear_memory()
                    print_memory_status("ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ìƒíƒœ")
                
                else:
                    print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. 'infer', 'benchmark', 'memory', 'clear', 'quit' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                clear_memory()
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        clear_memory()
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
