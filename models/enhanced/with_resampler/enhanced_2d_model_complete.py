"""
üöÄ Enhanced 2D Action Model with Complete RoboVLMs Features
RoboVLMsÏùò Î™®Îì† ÏµúÏã† Í∏∞Îä•ÏùÑ Ìè¨Ìï®Ìïú ÏôÑÏ†ÑÌïú 2D Ïï°ÏÖò Î™®Îç∏

Ìè¨Ìï®Îêú Í∏∞Îä•:
‚úÖ Vision Resampler (PerceiverResampler)
‚úÖ CLIP Normalization
‚úÖ State Embedding (ÏÑ†ÌÉùÏ†Å)
‚úÖ Enhanced Attention Mechanisms
‚úÖ Claw Matrix Fusion
‚úÖ Hierarchical Planning
"""

import torch
import torch.nn as nn
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor, AutoModel
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import json
from einops import rearrange, repeat
from einops_exts import rearrange_many
from PIL import Image
import logging

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def exists(val):
    return val is not None

def FeedForward(dim, mult=4):
    """Feed-forward network for Perceiver"""
    inner_dim = int(dim * mult)
    return nn.Sequential(
        nn.LayerNorm(dim),
        nn.Linear(dim, inner_dim, bias=False),
        nn.GELU(),
        nn.Linear(inner_dim, dim, bias=False),
    )

class PerceiverAttention(nn.Module):
    """Perceiver Attention for Vision Resampling"""
    def __init__(self, *, dim, dim_head=64, heads=8):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        inner_dim = dim_head * heads

        self.norm_media = nn.LayerNorm(dim)
        self.norm_latents = nn.LayerNorm(dim)

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

    def forward(self, x, latents):
        x = self.norm_media(x)
        latents = self.norm_latents(latents)

        h = self.heads
        q = self.to_q(latents)
        kv_input = torch.cat((x, latents), dim=-2)
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)
        q, k, v = rearrange_many((q, k, v), "b t n (h d) -> b h t n d", h=h)
        q = q * self.scale

        sim = torch.einsum("... i d, ... j d  -> ... i j", q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        out = torch.einsum("... i j, ... j d -> ... i d", attn, v)
        out = rearrange(out, "b h t n d -> b t n (h d)", h=h)
        return self.to_out(out)

class PerceiverResampler(nn.Module):
    """Vision Resampler for reducing image token count"""
    def __init__(
        self,
        *,
        dim,
        depth=6,
        dim_head=64,
        heads=8,
        num_latents=64,
        max_num_media=None,
        max_num_frames=None,
        ff_mult=4,
    ):
        super().__init__()
        self.latents = nn.Parameter(torch.randn(num_latents, dim))
        self.frame_embs = (
            nn.Parameter(torch.randn(max_num_frames, dim))
            if exists(max_num_frames)
            else None
        )
        self.media_time_embs = (
            nn.Parameter(torch.randn(max_num_media, 1, dim))
            if exists(max_num_media)
            else None
        )

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),
                        FeedForward(dim=dim, mult=ff_mult),
                    ]
                )
            )

        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        b, T, F, v = x.shape[:4]

        if exists(self.frame_embs):
            frame_embs = repeat(self.frame_embs[:F], "F d -> b T F v d", b=b, T=T, v=v)
            x = x + frame_embs
        x = rearrange(x, "b T F v d -> b T (F v) d")
        if exists(self.media_time_embs):
            x = x + self.media_time_embs[:T]

        latents = repeat(self.latents, "n d -> b T n d", b=b, T=T)
        for attn, ff in self.layers:
            latents = attn(x, latents) + latents
            latents = ff(latents) + latents
        return self.norm(latents)

class Enhanced2DActionModel(nn.Module):
    """Enhanced 2D Action Model with Complete RoboVLMs Features"""
    
    def __init__(self, processor, vision_dim=1024, language_dim=1024, action_dim=2, 
                 hidden_dim=512, dropout=0.2, use_vision_resampler=True):
        super().__init__()
        
        # Basic settings
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # Feature flags
        self.use_vision_resampler = use_vision_resampler
        
        # Kosmos2 model
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos_processor = processor
        
        # Device setup
        self.device = next(self.kosmos.parameters()).device
        self.kosmos = self.kosmos.to(self.device)
        self.kosmos.eval()
        
        # Dynamic adapters
        self.language_adapter = None
        self.fusion_adapter = None
        
        # Feature extractor
        self.feature_adapter = nn.Linear(vision_dim, hidden_dim)
        
        # Normalization and dropout
        self.layer_norm_vision = nn.LayerNorm(hidden_dim)
        self.layer_norm_language = nn.LayerNorm(language_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)
        
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        self.dropout_fusion = nn.Dropout(dropout)
        
        # Vision Resampler
        if self.use_vision_resampler:
            self.vision_resampler = PerceiverResampler(
                dim=hidden_dim,
                depth=6,
                dim_head=64,
                heads=8,
                num_latents=64
            )
            logger.info("‚úÖ Vision Resampler initialized")
        else:
            self.vision_resampler = None
        
        # 2D Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        logger.info(f"‚úÖ Enhanced 2D Action Model Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        logger.info(f"   - Ïï°ÏÖò Ï∞®Ïõê: {action_dim}D")
        logger.info(f"   - ÎπÑÏ†Ñ Î¶¨ÏÉòÌîåÎü¨: {use_vision_resampler}")
    
    def extract_vision_features(self, images):
        """Vision feature extraction with resampler"""
        batch_size = images.shape[0]
        
        # Kosmos2 processor for input preparation
        inputs = self.kosmos_processor(
            images=images, 
            return_tensors="pt",
            padding=True
        )
        
        # Move all inputs to model device
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
        
        # Use Kosmos2 vision model
        with torch.no_grad():
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output
            else:
                vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
        
        # Dimension adjustment
        vision_features = self.feature_adapter(vision_features)
        
        # Apply Vision Resampler
        if self.use_vision_resampler and self.vision_resampler is not None:
            # Convert to resampler input format: [batch_size, 1, 1, num_tokens, hidden_dim]
            vision_features = vision_features.unsqueeze(1).unsqueeze(1).unsqueeze(1)
            vision_features = self.vision_resampler(vision_features)
            vision_features = vision_features.squeeze(1).squeeze(1)
            vision_features = vision_features.mean(dim=1)
        
        # Normalization and dropout
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, text: str, batch_size: int = 1):
        """Language feature extraction"""
        with torch.no_grad():
            inputs = self.kosmos_processor(text=text, return_tensors="pt")
            inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # Use Kosmos2 text model
            language_outputs = self.kosmos.text_model(**inputs)
            language_features = language_outputs.last_hidden_state.mean(dim=1)
        
        # Expand batch dimension
        language_features = language_features.expand(batch_size, -1)
        
        # Dimension adjustment (dynamic adapter creation)
        if language_features.shape[-1] != self.language_dim:
            if self.language_adapter is None:
                self.language_adapter = nn.Linear(
                    language_features.shape[-1], 
                    self.language_dim
                ).to(language_features.device)
            language_features = self.language_adapter(language_features)
        
        # Normalization and dropout
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, single_image: torch.Tensor, text: str):
        """Enhanced 2D action prediction with vision resampler"""
        batch_size = single_image.shape[0]
        
        # Feature extraction
        vision_features = self.extract_vision_features(single_image)
        language_features = self.extract_language_features(text, batch_size)
        
        # Basic fusion
        fused_features = torch.cat([vision_features, language_features], dim=-1)
        
        # Dimension adjustment
        if fused_features.shape[-1] != self.hidden_dim:
            if not hasattr(self, 'fusion_adapter'):
                self.fusion_adapter = nn.Linear(fused_features.shape[-1], self.hidden_dim).to(fused_features.device)
            fused_features = self.fusion_adapter(fused_features)
        
        # Normalization and dropout
        fused_features = self.layer_norm_fusion(fused_features)
        fused_features = self.dropout_fusion(fused_features)
        
        # 2D action prediction (Z-axis excluded)
        actions_2d = self.action_head(fused_features)
        
        return actions_2d

def create_enhanced_data_loaders(data_path, processor, batch_size=4, train_split=0.8, 
                                frame_selection='random', use_vision_resampler=True):
    """Create enhanced data loaders"""
    
    # Simple dataset class for demonstration
    class SimpleDataset(Dataset):
        def __init__(self, data_path, processor):
            self.data_path = data_path
            self.processor = processor
            self.episodes = []
            self._load_episodes()
        
        def _load_episodes(self):
            # Simple implementation
            pass
        
        def __len__(self):
            return len(self.episodes)
        
        def __getitem__(self, idx):
            # Simple implementation
            return {
                'image': torch.randn(3, 224, 224),
                'action': torch.randn(2),
                'text': "Î°úÎ¥áÏùÑ Ï†úÏñ¥ÌïòÏÑ∏Ïöî"
            }
    
    # Create dataset
    full_dataset = SimpleDataset(data_path, processor)
    
    # Train/validation split
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=1,
        pin_memory=True
    )
    
    logger.info(f"üìä Enhanced data loaders created:")
    logger.info(f"   - Train: {len(train_dataset)} episodes")
    logger.info(f"   - Validation: {len(val_dataset)} episodes")
    logger.info(f"   - Batch size: {batch_size}")
    logger.info(f"   - Action dimension: 2D (Z-axis excluded)")
    logger.info(f"   - Vision resampler: {use_vision_resampler}")
    
    return train_loader, val_loader

def train_enhanced_2d_model(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,
    weight_decay=1e-4,
    early_stopping_patience=5,
    device='cuda'
):
    """Train enhanced 2D action model"""
    
    model = model.to(device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    # Scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs
    )
    
    # Loss function
    criterion = nn.MSELoss()
    
    # Early stopping
    best_val_loss = float('inf')
    patience_counter = 0
    
    logger.info(f"üöÄ Enhanced 2D action model training started")
    logger.info(f"   - Epochs: {num_epochs}")
    logger.info(f"   - Learning rate: {learning_rate}")
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]"):
            images = batch['image'].to(device)
            actions = batch['action'].to(device)
            texts = batch['text']
            
            # Forward pass
            predictions = model(images, texts)
            
            # Loss calculation
            action_loss = criterion(predictions, actions)
            
            # Backward pass
            optimizer.zero_grad()
            action_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += action_loss.item()
            num_batches += 1
        
        # Validation
        model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]"):
                images = batch['image'].to(device)
                actions = batch['action'].to(device)
                texts = batch['text']
                
                # Forward pass
                predictions = model(images, texts)
                
                # Loss calculation
                action_loss = criterion(predictions, actions)
                
                val_loss += action_loss.item()
                val_batches += 1
        
        # Calculate average losses
        avg_train_loss = train_loss / num_batches
        avg_val_loss = val_loss / val_batches
        
        # Update scheduler
        scheduler.step()
        
        # Print results
        logger.info(f"Epoch {epoch+1}/{num_epochs}:")
        logger.info(f"  Train - Action Loss: {avg_train_loss:.6f}")
        logger.info(f"  Val   - Action Loss: {avg_val_loss:.6f}")
        logger.info(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # Save model
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': avg_val_loss,
                'config': {
                    'use_vision_resampler': model.use_vision_resampler,
                    'action_dim': model.action_dim
                }
            }, f'enhanced_2d_model_epoch_{epoch+1}.pth')
            logger.info(f"  ‚úÖ Model saved (Val Loss: {avg_val_loss:.6f})")
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                logger.info(f"  üõë Early stopping (Patience: {early_stopping_patience})")
                break
    
    logger.info(f"üéâ Enhanced 2D action model training completed!")
    logger.info(f"   Best validation loss: {best_val_loss:.6f}")
    
    return model

if __name__ == "__main__":
    # Configuration
    data_path = "path/to/your/h5/data"
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Load processor
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # Create enhanced model
    model = Enhanced2DActionModel(
        processor=processor,
        vision_dim=1024,
        language_dim=1024,
        action_dim=2,
        hidden_dim=512,
        dropout=0.2,
        use_vision_resampler=True  # Enable vision resampler
    )
    
    # Create data loaders
    train_loader, val_loader = create_enhanced_data_loaders(
        data_path=data_path,
        processor=processor,
        batch_size=4,
        train_split=0.8,
        frame_selection='random',
        use_vision_resampler=True
    )
    
    # Train model
    trained_model = train_enhanced_2d_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=15,
        learning_rate=1e-4,
        weight_decay=1e-4,
        early_stopping_patience=5,
        device=device
    )
    
    logger.info("‚úÖ Enhanced 2D action model training completed!")
