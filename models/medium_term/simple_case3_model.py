#!/usr/bin/env python3
"""
Case 3: ì¤‘ê¸° ì ìš© - ê°„ë‹¨í•œ ê³ ê¸‰ ëª¨ë¸ (Case 1 ê¸°ë°˜)
ëª©í‘œ: MAE 0.3 â†’ 0.2, ì •í™•ë„ 35% â†’ 55%
íŠ¹ì§•: Case 1ì˜ ì•ˆì •ì ì¸ êµ¬ì¡° ì‚¬ìš©
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoProcessor
from PIL import Image
import logging

logger = logging.getLogger(__name__)

class SimpleCase3Model(nn.Module):
    """
    Case 3 ëª¨ë¸ - Case 1 ê¸°ë°˜
    - Case 1ì˜ ì•ˆì •ì ì¸ êµ¬ì¡° ì‚¬ìš©
    - 2D ì•¡ì…˜ ì¶œë ¥
    """
    
    def __init__(self, processor, vision_dim=1024, language_dim=2048, action_dim=2, 
                 hidden_dim=256, dropout=0.4, use_vision_resampler=False):
        super().__init__()
        
        self.processor = processor
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.dropout_rate = dropout
        self.use_vision_resampler = use_vision_resampler
        
        # ë‹¨ìˆœí™”ëœ íŠ¹ì§• ì–´ëŒ‘í„° (hidden_dim 256ìœ¼ë¡œ ê°ì†Œ)
        self.feature_adapter = nn.Linear(vision_dim, hidden_dim)
        self.language_adapter = nn.Linear(language_dim, hidden_dim)  # 2048 â†’ 256
        
        # ì •ê·œí™” ê°•í™” (dropout 0.4)
        self.layer_norm_vision = nn.LayerNorm(hidden_dim)
        self.layer_norm_language = nn.LayerNorm(hidden_dim)
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        
        # ê°œì„ ëœ ì•¡ì…˜ í—¤ë“œ (ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # ì¶”ê°€ ì •ê·œí™” ë ˆì´ì–´
        self.final_norm = nn.LayerNorm(action_dim)
        
        logger.info(f"âœ… Case 3 Model ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - hidden_dim: {hidden_dim}")
        logger.info(f"   - action_dim: {action_dim}")
        logger.info(f"   - dropout: {dropout}")
        logger.info(f"   - action_head layers: 4")
    
    def extract_vision_features(self, images):
        """ì‹œê° íŠ¹ì§• ì¶”ì¶œ (Case 1ê³¼ ë™ì¼)"""
        batch_size = len(images)  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        
        # PIL ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì²˜ë¦¬
        inputs = self.processor(images=images, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
        
        # Kosmos2 vision ëª¨ë¸ ì‚¬ìš©
        with torch.no_grad():
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output
            else:
                vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
        
        # ë‹¨ìˆœí™”ëœ íŠ¹ì§• ì²˜ë¦¬
        vision_features = self.feature_adapter(vision_features)
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, texts):
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ (Case 1ê³¼ ë™ì¼)"""
        batch_size = len(texts)
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        inputs = self.processor(text=texts, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
        
        # Kosmos2 text ëª¨ë¸ ì‚¬ìš©
        with torch.no_grad():
            if 'input_ids' in inputs:
                text_outputs = self.kosmos.text_model(inputs['input_ids'])
                # pooler_outputì´ ì—†ìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ hidden stateì˜ í‰ê·  ì‚¬ìš©
                language_features = text_outputs.last_hidden_state.mean(dim=1)
            else:
                language_features = torch.zeros(batch_size, 2048).to(self.kosmos.device)
        
        # ë‹¨ìˆœí™”ëœ íŠ¹ì§• ì²˜ë¦¬
        language_features = self.language_adapter(language_features)
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, images, texts):
        """ìˆœì „íŒŒ (Case 1ê³¼ ë™ì¼)"""
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(images)
        language_features = self.extract_language_features(texts)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([vision_features, language_features], dim=1)
        
        # ë‹¨ìˆœí™”ëœ ì•¡ì…˜ ì˜ˆì¸¡ (4ì¸µ)
        actions = self.action_head(combined_features)
        actions = self.final_norm(actions)
        
        return actions

class SimpleCase3Trainer:
    """Case 3 í›ˆë ¨ê¸° - Case 1ê³¼ ë™ì¼"""
    
    def __init__(self, model, device, learning_rate=5e-5, weight_decay=1e-3):
        self.model = model.to(device)
        self.device = device
        
        # ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100, eta_min=1e-6
        )
        
        # Huber Loss (ì´ìƒì¹˜ì— ê°•í•¨)
        self.criterion = nn.HuberLoss(delta=0.1)
        
        logger.info(f"âœ… Case 3 Trainer ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - learning_rate: {learning_rate}")
        logger.info(f"   - weight_decay: {weight_decay}")
        logger.info(f"   - scheduler: CosineAnnealingLR")
        logger.info(f"   - criterion: HuberLoss")
    
    def train_step(self, batch):
        """ë‹¨ì¼ í›ˆë ¨ ìŠ¤í…"""
        self.model.train()
        
        images = batch['image']  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        actions = batch['action'].to(self.device)
        texts = batch['text']
        
        # ìˆœì „íŒŒ
        predicted_actions = self.model(images, texts)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = self.criterion(predicted_actions, actions)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def validate_step(self, batch):
        """ë‹¨ì¼ ê²€ì¦ ìŠ¤í…"""
        self.model.eval()
        
        with torch.no_grad():
            images = batch['image']  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            actions = batch['action'].to(self.device)
            texts = batch['text']
            
            predicted_actions = self.model(images, texts)
            loss = self.criterion(predicted_actions, actions)
            
            # MAE ê³„ì‚°
            mae = torch.mean(torch.abs(predicted_actions - actions))
            
            return loss.item(), mae.item()
    
    def save_checkpoint(self, path, epoch, loss, mae):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'mae': mae
        }, path)
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")
    
    def load_checkpoint(self, path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {path}")
        return checkpoint['epoch'], checkpoint['loss'], checkpoint['mae']

def create_simple_case3_model(processor, device=None):
    """Case 3 ëª¨ë¸ ìƒì„±"""
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = SimpleCase3Model(
        processor=processor,
        vision_dim=1024,
        language_dim=2048,
        action_dim=2,
        hidden_dim=256,
        dropout=0.4
    )
    
    trainer = SimpleCase3Trainer(
        model=model,
        device=device,
        learning_rate=5e-5,
        weight_decay=1e-3
    )
    
    return model, trainer
