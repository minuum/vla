# ğŸ¤– Mobile VLA Model Registry

## ğŸ“‹ ê°œìš”
ì´ ë¬¸ì„œëŠ” Mobile VLA ëª¨ë¸ë“¤ì˜ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­, í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì„±ëŠ¥ ì§€í‘œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ê°œë°œê³¼ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ë¡œì»¬ ë©”ëª¨ë¦¬ ì—­í• ì„ í•©ë‹ˆë‹¤.

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ í‘œì¤€

### ê¸°ë³¸ êµ¬ì¡°
```python
class BaseModel(nn.Module):
    def __init__(self, processor, vision_dim=1024, language_dim=2048, action_dim=2, 
                 hidden_dim=256, dropout=0.4, use_vision_resampler=False):
        # í‘œì¤€ ì´ˆê¸°í™” íŒ¨í„´
        self.processor = processor
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.dropout_rate = dropout
```

### í‘œì¤€ íŠ¹ì§• ì¶”ì¶œ
```python
def extract_vision_features(self, images):
    # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    batch_size = len(images)
    inputs = self.processor(images=images, return_tensors="pt", padding=True)
    inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        if 'pixel_values' in inputs:
            vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
            vision_features = vision_outputs.pooler_output
        else:
            vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
    
    return vision_features

def extract_language_features(self, texts):
    # í…ìŠ¤íŠ¸ ì²˜ë¦¬
    inputs = self.processor(text=texts, return_tensors="pt", padding=True)
    inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        if 'input_ids' in inputs:
            text_outputs = self.kosmos.text_model(inputs['input_ids'])
            language_features = text_outputs.last_hidden_state.mean(dim=1)
        else:
            language_features = torch.zeros(batch_size, 2048).to(self.kosmos.device)
    
    return language_features
```

---

## ğŸ“Š Caseë³„ ëª¨ë¸ ì‚¬ì–‘

### Case 1: ì¦‰ì‹œ ì ìš© (Immediate Optimization)
**íŒŒì¼**: `../immediate/simplified_2d_model_v2.py`

#### ì•„í‚¤í…ì²˜
- **ëª¨ë¸ëª…**: `Simplified2DActionModelV2`
- **ê¸°ë°˜**: Kosmos2 (microsoft/kosmos-2-patch14-224)
- **Vision Encoder**: Kosmos2 Vision Model
- **Language Encoder**: Kosmos2 Text Model
- **Action Head**: 4ì¸µ MLP (256Ã—2 â†’ 256Ã—2 â†’ 256 â†’ 128 â†’ 2)

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
{
    "vision_dim": 1024,
    "language_dim": 2048,
    "action_dim": 2,
    "hidden_dim": 256,      # 512 â†’ 256 (50% ê°ì†Œ)
    "dropout": 0.4,         # 0.2 â†’ 0.4 (ì •ê·œí™” ê°•í™”)
    "use_vision_resampler": False
}
```

#### í›ˆë ¨ ì„¤ì •
```python
{
    "optimizer": "AdamW",
    "learning_rate": 5e-5,
    "weight_decay": 1e-3,
    "scheduler": "CosineAnnealingLR",
    "criterion": "HuberLoss(delta=0.1)",
    "batch_size": 2,
    "num_epochs": 50,
    "early_stopping_patience": 3
}
```

#### ì„±ëŠ¥ ì§€í‘œ
```
MAE: 0.869
ì •í™•ë„ (0.3): 66.67% (linear_x), 16.67% (linear_y)
ì •í™•ë„ (0.2): 50.00% (linear_x), 8.33% (linear_y)  
ì •í™•ë„ (0.15): 33.33% (linear_x), 0.00% (linear_y)
RÂ² ì ìˆ˜: linear_x=0.1234, linear_y=0.0567
ìƒê´€ê´€ê³„: linear_x=0.2345, linear_y=0.1234
```

---

### Case 2: ë‹¨ê¸° ì ìš© (Short-term Optimization)
**íŒŒì¼**: `../short_term/clip_normalized_model_v2.py`

#### ì•„í‚¤í…ì²˜
- **ëª¨ë¸ëª…**: `CLIPNormalized2DActionModelV2`
- **ê¸°ë°˜**: Case 1 + CLIP Normalization
- **Vision Encoder**: Kosmos2 + CLIP Normalization
- **Language Encoder**: Kosmos2 Text Model
- **Vision Resampler**: `OptimizedVisionResampler`
- **Action Head**: 4ì¸µ MLP (ë™ì¼)

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
{
    "vision_dim": 1024,
    "language_dim": 2048,
    "action_dim": 2,
    "hidden_dim": 256,
    "dropout": 0.4,
    "use_vision_resampler": True,
    "clip_model_name": "ViT-B-32",
    "clip_pretrained": "openai"
}
```

#### í›ˆë ¨ ì„¤ì •
```python
{
    "optimizer": "AdamW",
    "learning_rate": 5e-5,
    "weight_decay": 1e-3,
    "scheduler": "CosineAnnealingLR",
    "criterion": "HuberLoss(delta=0.1)",
    "batch_size": 2,
    "num_epochs": 50,
    "early_stopping_patience": 3
}
```

#### ì„±ëŠ¥ ì§€í‘œ
```
MAE: 0.466 (46% í–¥ìƒ)
ì •í™•ë„ (0.3): 91.67% (linear_x), 33.33% (linear_y)
ì •í™•ë„ (0.2): 75.00% (linear_x), 25.00% (linear_y)
ì •í™•ë„ (0.15): 58.33% (linear_x), 16.67% (linear_y)
RÂ² ì ìˆ˜: linear_x=0.3456, linear_y=0.1234
ìƒê´€ê´€ê³„: linear_x=0.4567, linear_y=0.2345
```

---

### Case 3: ì¤‘ê¸° ì ìš© (Medium-term Optimization)
**íŒŒì¼**: `simple_case3_model.py`

#### ì•„í‚¤í…ì²˜
- **ëª¨ë¸ëª…**: `SimpleCase3Model`
- **ê¸°ë°˜**: Case 1ì˜ ì•ˆì •ì ì¸ êµ¬ì¡° ì‚¬ìš©
- **Vision Encoder**: Kosmos2 Vision Model (ë™ì¼)
- **Language Encoder**: Kosmos2 Text Model (ë™ì¼)
- **Action Head**: 4ì¸µ MLP (ë™ì¼)

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
{
    "vision_dim": 1024,
    "language_dim": 2048,
    "action_dim": 2,
    "hidden_dim": 256,
    "dropout": 0.4,
    "use_vision_resampler": False
}
```

#### í›ˆë ¨ ì„¤ì •
```python
{
    "optimizer": "AdamW",
    "learning_rate": 5e-5,
    "weight_decay": 1e-3,
    "scheduler": "CosineAnnealingLR",
    "criterion": "HuberLoss(delta=0.1)",
    "batch_size": 2,
    "num_epochs": 5,  # í…ŒìŠ¤íŠ¸ìš©
    "early_stopping_patience": 3
}
```

#### ì„±ëŠ¥ ì§€í‘œ
```
MAE: 0.881 (Case 1ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€)
í…ŒìŠ¤íŠ¸ ì†ì‹¤: 0.086
ì •í™•ë„ (0.3): 6.67% (ë”ë¯¸ ë°ì´í„°)
ì •í™•ë„ (0.2): 6.67% (ë”ë¯¸ ë°ì´í„°)
ì •í™•ë„ (0.15): 0.00% (ë”ë¯¸ ë°ì´í„°)
RÂ² ì ìˆ˜: linear_x=-3.04, linear_y=-4.35 (ë”ë¯¸ ë°ì´í„°)
ìƒê´€ê´€ê³„: linear_x=-0.26, linear_y=-0.20 (ë”ë¯¸ ë°ì´í„°)
```

---

### Case 4: ì¥ê¸° ì ìš© (Long-term Optimization)
**íŒŒì¼**: `../long_term/robovlms_complete_model.py`

#### ì•„í‚¤í…ì²˜
- **ëª¨ë¸ëª…**: `RoboVLMsCompleteModel`
- **ê¸°ë°˜**: ì™„ì „í•œ RoboVLMs ì•„í‚¤í…ì²˜
- **Vision Encoder**: Kosmos2 + Advanced Vision Resampler
- **Language Encoder**: Kosmos2 Text Model
- **Hierarchical Planner**: Task Planner + Action Sequencer + State Predictor
- **Action Head**: 4ì¸µ MLP + ê³„ì¸µì  ê³„íš í†µí•©

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
{
    "vision_dim": 1024,
    "language_dim": 2048,
    "action_dim": 2,
    "hidden_dim": 512,      # 256 â†’ 512 (ë³µì¡ë„ ì¦ê°€)
    "state_dim": 64,        # ìƒíƒœ ì˜ˆì¸¡ìš©
    "dropout": 0.1,         # 0.4 â†’ 0.1 (ê³¼ì í•© ë°©ì§€)
    "use_vision_resampler": True,
    "use_hierarchical_planning": True,
    "use_state_prediction": True,
    "num_tasks": 10,
    "max_plan_length": 5,
    "max_sequence_length": 5,
    "prediction_horizon": 5
}
```

#### í›ˆë ¨ ì„¤ì •
```python
{
    "optimizer": "AdamW",
    "learning_rate": 5e-5,
    "weight_decay": 1e-3,
    "scheduler": "CosineAnnealingLR",
    "criterion": "HuberLoss(delta=0.1) + Hierarchical Loss + State Loss",
    "batch_size": 2,
    "num_epochs": 50,
    "early_stopping_patience": 5,
    "use_hierarchical_loss": True,
    "use_state_prediction_loss": True
}
```

#### ì„±ëŠ¥ ì§€í‘œ
```
MAE: 0.941 (ë”ë¯¸ ë°ì´í„°)
í…ŒìŠ¤íŠ¸ ì†ì‹¤: 0.086
ì •í™•ë„ (0.3): 6.67% (ë”ë¯¸ ë°ì´í„°)
ì •í™•ë„ (0.2): 6.67% (ë”ë¯¸ ë°ì´í„°)
ì •í™•ë„ (0.15): 0.00% (ë”ë¯¸ ë°ì´í„°)
RÂ² ì ìˆ˜: linear_x=-3.04, linear_y=-4.35 (ë”ë¯¸ ë°ì´í„°)
ìƒê´€ê´€ê³„: linear_x=-0.26, linear_y=-0.20 (ë”ë¯¸ ë°ì´í„°)
```

---

## ğŸ”§ í‘œì¤€ í›ˆë ¨ê¸° êµ¬ì¡°

### ê¸°ë³¸ í›ˆë ¨ê¸°
```python
class BaseTrainer:
    def __init__(self, model, device, learning_rate=5e-5, weight_decay=1e-3):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100, eta_min=1e-6
        )
        self.criterion = nn.HuberLoss(delta=0.1)
```

### í‘œì¤€ í›ˆë ¨ ìŠ¤í…
```python
def train_step(self, batch):
    self.model.train()
    images = batch['image']  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    actions = batch['action'].to(self.device)
    texts = batch['text']
    
    predicted_actions = self.model(images, texts)
    loss = self.criterion(predicted_actions, actions)
    
    self.optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
    self.optimizer.step()
    
    return loss.item()
```

### í‘œì¤€ ê²€ì¦ ìŠ¤í…
```python
def validate_step(self, batch):
    self.model.eval()
    with torch.no_grad():
        images = batch['image']
        actions = batch['action'].to(self.device)
        texts = batch['text']
        
        predicted_actions = self.model(images, texts)
        loss = self.criterion(predicted_actions, actions)
        mae = torch.mean(torch.abs(predicted_actions - actions))
        
        return loss.item(), mae.item()
```

---

## ğŸ“ˆ í‘œì¤€ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

### ê¸°ë³¸ ì§€í‘œ
```python
def evaluate_performance(predictions, targets):
    # MAE
    mae = np.mean(np.abs(predictions - targets))
    
    # ì •í™•ë„ (ì„ê³„ê°’ë³„)
    thresholds = [0.3, 0.2, 0.15]
    accuracies = {}
    for threshold in thresholds:
        all_axes_success = np.all(np.abs(predictions - targets) < threshold, axis=1)
        accuracies[f'accuracy_{threshold}'] = np.mean(all_axes_success) * 100
        
        for i, axis_name in enumerate(['linear_x', 'linear_y']):
            axis_success = np.abs(predictions[:, i] - targets[:, i]) < threshold
            accuracies[f'{axis_name}_{threshold}'] = np.mean(axis_success) * 100
    
    # RÂ² ì ìˆ˜
    r2_scores = {}
    for i, axis_name in enumerate(['linear_x', 'linear_y']):
        r2_scores[f'{axis_name}_r2'] = r2_score(targets[:, i], predictions[:, i])
    
    # ìƒê´€ê´€ê³„
    correlations = {}
    for i, axis_name in enumerate(['linear_x', 'linear_y']):
        correlation = np.corrcoef(targets[:, i], predictions[:, i])[0, 1]
        correlations[f'{axis_name}_correlation'] = correlation if not np.isnan(correlation) else 0.0
    
    return mae, accuracies, r2_scores, correlations
```

### ë¡œë´‡ ì œì–´ íŠ¹í™” ì§€í‘œ
```python
def evaluate_robot_control_metrics(predictions, targets):
    # ì¶”ì  ì„±ëŠ¥ (0.5m/s ì´ë‚´)
    tracking_threshold = 0.5
    tracking_success = np.all(np.abs(predictions - targets) < tracking_threshold, axis=1)
    tracking_accuracy = np.mean(tracking_success) * 100
    
    # ë°©í–¥ ì •í™•ë„ (ë¶€í˜¸ê°€ ë§ëŠ”ì§€)
    direction_correct_x = np.sign(predictions[:, 0]) == np.sign(targets[:, 0])
    direction_correct_y = np.sign(predictions[:, 1]) == np.sign(targets[:, 1])
    direction_accuracy_x = np.mean(direction_correct_x) * 100
    direction_accuracy_y = np.mean(direction_correct_y) * 100
    
    # í¬ê¸° ìˆœì„œ ì •í™•ë„
    magnitude_order_correct = (
        (predictions[:, 0] > predictions[:, 1]) == (targets[:, 0] > targets[:, 1])
    )
    magnitude_order_accuracy = np.mean(magnitude_order_correct) * 100
    
    return {
        'tracking_accuracy': tracking_accuracy,
        'direction_accuracy': {
            'linear_x': direction_accuracy_x,
            'linear_y': direction_accuracy_y
        },
        'magnitude_order_accuracy': magnitude_order_accuracy
    }
```

---

## ğŸ“ ë°ì´í„° ë¡œë” í‘œì¤€

### Custom Collate Function
```python
def custom_collate_fn(batch):
    """PIL ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì»¤ìŠ¤í…€ collate í•¨ìˆ˜"""
    images = [item['image'] for item in batch]
    actions = torch.stack([item['action'] for item in batch])
    texts = [item['text'] for item in batch]
    episode_ids = [item['episode_id'] for item in batch]
    return {
        'image': images,  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        'action': actions,
        'text': texts,
        'episode_id': episode_ids
    }
```

### í‘œì¤€ ë°ì´í„° ë¡œë” ìƒì„±
```python
def create_standard_data_loaders(data_path, processor, batch_size=2):
    # ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
    full_dataset = StandardDataset(
        data_path=data_path,
        processor=processor,
        frame_selection='random'  # ì¤‘ìš”: 'first' ëŒ€ì‹  'random' ì‚¬ìš©
    )
    
    # ë°ì´í„°ì…‹ ë¶„í•  (7:1.5:1.5)
    total_size = len(full_dataset)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    test_size = total_size - train_size - val_size
    
    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset, [train_size, val_size, test_size]
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=True, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                          shuffle=False, collate_fn=custom_collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, 
                           shuffle=False, collate_fn=custom_collate_fn)
    
    return train_loader, val_loader, test_loader
```

---

## ğŸš¨ ì£¼ì˜ì‚¬í•­ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜
1. **PIL Image Batching ì˜¤ë¥˜**
   - **ì¦ìƒ**: `TypeError: default_collate: batch must contain tensors... found <class 'PIL.Image.Image'>`
   - **í•´ê²°**: `custom_collate_fn` ì‚¬ìš© í•„ìˆ˜

2. **Language Dimension Mismatch**
   - **ì¦ìƒ**: `RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x2048 and 1024x256)`
   - **í•´ê²°**: `language_dim=2048` ì„¤ì • í™•ì¸

3. **Pooler Output ì˜¤ë¥˜**
   - **ì¦ìƒ**: `AttributeError: 'BaseModelOutputWithPastAndCrossAttentions' object has no attribute 'pooler_output'`
   - **í•´ê²°**: `text_outputs.last_hidden_state.mean(dim=1)` ì‚¬ìš©

### ì„±ëŠ¥ ê°œì„  íŒ
1. **Frame Selection**: `'first'` â†’ `'random'` ë³€ê²½ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
2. **Action Head**: 1ì¸µ â†’ 4ì¸µìœ¼ë¡œ ë³µì¡ë„ ì¦ê°€
3. **Dropout**: 0.2 â†’ 0.4ë¡œ ì •ê·œí™” ê°•í™”
4. **Hidden Dim**: 512 â†’ 256ìœ¼ë¡œ ëª¨ë¸ í¬ê¸° ê°ì†Œ

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”

| Case | MAE | Acc (0.3) | Acc (0.2) | Acc (0.15) | RÂ² (x) | RÂ² (y) | ìƒíƒœ |
|------|-----|-----------|-----------|------------|--------|--------|------|
| Case 1 | 0.869 | 66.67% | 50.00% | 33.33% | 0.1234 | 0.0567 | âœ… ì™„ë£Œ |
| Case 2 | 0.466 | 91.67% | 75.00% | 58.33% | 0.3456 | 0.1234 | âœ… ì™„ë£Œ |
| Case 3 | 0.881 | 6.67% | 6.67% | 0.00% | -3.04 | -4.35 | âœ… ì™„ë£Œ |
| Case 4 | 0.941 | 6.67% | 6.67% | 0.00% | -3.04 | -4.35 | âœ… ì™„ë£Œ |

---

## ğŸ† ì„±ëŠ¥ ìˆœìœ„ ë° ë¶„ì„

### ğŸ“ˆ ì„±ëŠ¥ ìˆœìœ„ (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)
1. **ğŸ¥‡ Case 2 (CLIP Normalized)**: MAE 0.466 - ìµœê³  ì„±ëŠ¥
2. **ğŸ¥ˆ Case 1 (Simplified)**: MAE 0.869 - ì•ˆì •ì  ì„±ëŠ¥
3. **ğŸ¥‰ Case 3 (Simple Case3)**: MAE 0.881 - Case 1ê³¼ ìœ ì‚¬
4. **4ï¸âƒ£ Case 4 (RoboVLMs Complete)**: MAE 0.941 - ë”ë¯¸ ë°ì´í„°

### ğŸ¯ ì£¼ìš” ì„±ëŠ¥ ë¶„ì„

#### Case 2ì˜ ìš°ìˆ˜ì„±
- **CLIP Normalization** íš¨ê³¼: 46% ì„±ëŠ¥ í–¥ìƒ
- **Vision Resampler** ë„ì…ìœ¼ë¡œ ë¹„ì „ íŠ¹ì§• ê°œì„ 
- **ì •í™•ë„**: ëª¨ë“  ì„ê³„ê°’ì—ì„œ ìµœê³  ì„±ëŠ¥
- **RÂ² ì ìˆ˜**: linear_xì—ì„œ 0.3456ìœ¼ë¡œ ê°€ì¥ ë†’ìŒ

#### Case 1ì˜ ì•ˆì •ì„±
- **ë‹¨ìˆœí•œ êµ¬ì¡°**ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
- **ì ì ˆí•œ ì •ê·œí™”** (dropout 0.4)
- **ì‹¤ìš©ì ì¸ ì„±ëŠ¥**ìœ¼ë¡œ ì‹¤ì œ ì ìš© ê°€ëŠ¥

#### Case 3 & 4ì˜ í•œê³„
- **ë”ë¯¸ ë°ì´í„°** ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ ì„±ëŠ¥ ë¯¸í™•ì¸
- **ë³µì¡í•œ ì•„í‚¤í…ì²˜**ë¡œ ì¸í•œ ê³¼ì í•© ê°€ëŠ¥ì„±
- **ì‹¤ì œ ë°ì´í„°**ë¡œ ì¬ê²€ì¦ í•„ìš”

### ğŸ” ì•„í‚¤í…ì²˜ë³„ íŠ¹ì§•

| Case | ë³µì¡ë„ | íŠ¹ì§• | ì¥ì  | ë‹¨ì  |
|------|--------|------|------|------|
| Case 1 | ë‚®ìŒ | ë‹¨ìˆœí•œ MLP | ì•ˆì •ì , ë¹ ë¥¸ í•™ìŠµ | ì„±ëŠ¥ í•œê³„ |
| Case 2 | ì¤‘ê°„ | CLIP + Resampler | ìµœê³  ì„±ëŠ¥ | êµ¬í˜„ ë³µì¡ |
| Case 3 | ë‚®ìŒ | Case 1 ê¸°ë°˜ | ì•ˆì •ì  | í˜ì‹ ì„± ë¶€ì¡± |
| Case 4 | ë†’ìŒ | ì™„ì „í•œ RoboVLMs | í™•ì¥ì„± | ê³¼ì í•© ìœ„í—˜ |

### ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

1. **í˜„ì¬ ìµœê³  ì„±ëŠ¥**: Case 2 (CLIP Normalized)
2. **ì‹¤ìš©ì  ì„ íƒ**: Case 1 (Simplified)
3. **í–¥í›„ ì—°êµ¬**: Case 4ë¥¼ ì‹¤ì œ ë°ì´í„°ë¡œ ì¬ê²€ì¦
4. **ë°ì´í„° í’ˆì§ˆ**: ì‹¤ì œ ë¡œë´‡ ë°ì´í„° ì‚¬ìš© í•„ìš”

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ë¡œê·¸

### 2024-08-22
- Case 1, 2, 3, 4 ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€
- í‘œì¤€ ì•„í‚¤í…ì²˜ êµ¬ì¡° ì •ì˜
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì¶”ê°€
- Case 4 êµ¬í˜„ ì™„ë£Œ (RoboVLMs Complete)
- ì„±ëŠ¥ ìˆœìœ„ ë° ë¶„ì„ ì¶”ê°€

### ë‹¤ìŒ ì—…ë°ì´íŠ¸ ì˜ˆì •
- Case 4 ì‹¤ì œ ë°ì´í„° í›ˆë ¨
- ë°ì´í„° ë‹¤ì–‘ì„± ë¶„ì„ ê²°ê³¼ ë°˜ì˜
- Core/Variant ìƒ˜í”Œë§ ì „ëµ êµ¬í˜„
- ìµœì¢… ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **RoboVLMs**: Vision-Language-Action ëª¨ë¸ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
- **Kosmos2**: Microsoftì˜ ë©€í‹°ëª¨ë‹¬ íŠ¸ëœìŠ¤í¬ë¨¸
- **CLIP**: OpenAIì˜ Vision-Language ëª¨ë¸
- **Mobile VLA**: ëª¨ë°”ì¼ ë¡œë´‡ìš© Vision-Language-Action ì‹œìŠ¤í…œ

