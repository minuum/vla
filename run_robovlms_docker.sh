#!/bin/bash

# ğŸš€ RoboVLMs Docker ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (SOTA ëª¨ë¸)
# ğŸ† Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ (MAE 0.212) ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©

set -e

echo "ğŸš€ RoboVLMs Docker í™˜ê²½ ì‹œì‘ (SOTA ëª¨ë¸)"
echo "ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ (MAE 0.212)"

# ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬ ì¤‘..."
docker stop mobile_vla_robovlms_final 2>/dev/null || true
docker rm mobile_vla_robovlms_final 2>/dev/null || true

# ì´ë¯¸ì§€ ë¹Œë“œ (CUDA Trueë¡œ ê²€ì¦ëœ ë²„ì „ ì‚¬ìš©)
echo "ğŸ”¨ CUDA Trueë¡œ ê²€ì¦ëœ ì´ë¯¸ì§€ ì‚¬ìš©..."
# docker build -f Dockerfile.mobile-vla -t mobile_vla:robovlms-final .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (CUDA Trueë¡œ ê²€ì¦ëœ ì„¤ì •)
echo "ğŸš€ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì¤‘ (CUDA True ì„¤ì •)..."
docker run -d \
    --name mobile_vla_robovlms_final \
    --gpus all \
    -v /dev/bus/usb:/dev/bus/usb \
    -v /usr/local/cuda:/usr/local/cuda \
    -v /usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu \
    -e NVIDIA_VISIBLE_DEVICES=all \
    -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    -e PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512 \
    mobile_vla:pytorch-2.3.0-cuda \
    sleep infinity

echo "âœ… ì»¨í…Œì´ë„ˆê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."
echo ""
echo "ğŸš€ ì»¨í…Œì´ë„ˆì— ë°”ë¡œ ì ‘ì†í•©ë‹ˆë‹¤..."
echo ""
echo "ğŸ† CUDA True + Mobile VLA ëª¨ë¸ í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:"
echo "   cuda-test          # PyTorch/CUDA ìƒíƒœ í™•ì¸"
echo "   torch_cuda_test    # ìƒì„¸ PyTorch CUDA í…ŒìŠ¤íŠ¸"
echo "   mobile-vla-test    # Mobile VLA ì¹´ë©”ë¼ í…ŒìŠ¤íŠ¸"
echo "   robovlms-test      # Mobile VLA ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"
echo "   mobile-vla-model   # minium/mobile-vla-omniwheel ëª¨ë¸ í…ŒìŠ¤íŠ¸"
echo ""
echo "ğŸš€ ìµœì í™” ì˜µì…˜:"
echo "   --ros-args -p optimization_mode:=test    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ë©”ëª¨ë¦¬ ìµœì†Œí™”)"
echo "   --ros-args -p optimization_mode:=auto    # ìë™ ìµœì í™” (ê¶Œì¥)"
echo "   --ros-args -p optimization_mode:=fp16    # FP16 ì–‘ìí™”"
echo ""
echo "ğŸ“Š CUDA True + Mobile VLA ëª¨ë¸ ì •ë³´:"
echo "   ğŸ† PyTorch 2.3.0 + CUDA 12.2 - ê²€ì¦ë¨"
echo "   ğŸ¥ˆ Jetson Orin GPU - ì™„ë²½ ì§€ì›"
echo "   ğŸ¥‰ minium/mobile-vla-omniwheel (MAE 0.222) - ìµœì‹  ëª¨ë¸"
echo "   âš¡ CUDA Available: True âœ…"
echo ""
echo "ğŸ® CUDA + Mobile VLA í…ŒìŠ¤íŠ¸:"
echo "   cuda-test: PyTorch CUDA ìƒíƒœ í™•ì¸"
echo "   torch_cuda_test: ìƒì„¸ GPU ì •ë³´"
echo "   robovlms-test: Mobile VLA ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸"
echo "   mobile-vla-model: minium/mobile-vla-omniwheel ëª¨ë¸ í…ŒìŠ¤íŠ¸"
echo "   python3 -c 'from transformers import AutoModel; model=AutoModel.from_pretrained(\"minium/mobile-vla-omniwheel\")': ì§ì ‘ ëª¨ë¸ í…ŒìŠ¤íŠ¸"
echo ""

# ì»¨í…Œì´ë„ˆì— ë°”ë¡œ ì ‘ì†
docker exec -it mobile_vla_robovlms_final bash
