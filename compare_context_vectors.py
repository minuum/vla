#!/usr/bin/env python3
"""
Context Vector ë¹„êµ: Kosmos-2 vs RoboVLMs
ëª©ì : VLM pretrain ì°¨ì´ê°€ context vectorì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
"""

import torch
import numpy as np
import h5py
from pathlib import Path
import sys
from PIL import Image
import torchvision.transforms as T
import json

sys.path.insert(0, "RoboVLMs_upstream")

def load_checkpoint_and_extract_vlm(ckpt_path, device='cuda'):
    """
    Checkpointì—ì„œ VLMë§Œ ì¶”ì¶œ
    """
    print(f"Loading checkpoint: {ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    
    # State dictì—ì„œ VLM ë¶€ë¶„ë§Œ ì¶”ì¶œ
    vlm_state_dict = {}
    for key, value in checkpoint.items():
        if 'state_dict' in checkpoint:
            # Lightning checkpoint
            for k, v in checkpoint['state_dict'].items():
                if 'model.model' in k and 'act_head' not in k:
                    # Remove 'model.model.' prefix
                    new_key = k.replace('model.model.', '')
                    vlm_state_dict[new_key] = v
        else:
            # Direct checkpoint
            if 'act_head' not in key and 'action' not in key:
                vlm_state_dict[key] = value
    
    print(f"  Extracted {len(vlm_state_dict)} VLM parameters")
    return vlm_state_dict

def compare_context_vectors():
    """
    Context vector ë¹„êµ ë¶„ì„
    """
    print("="*70)
    print("Context Vector ë¹„êµ: Kosmos-2 vs RoboVLMs")
    print("="*70)
    
    # 1. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    kosmos2_ckpt = "RoboVLMs_upstream/runs/mobile_vla_lora_20251203/kosmos/mobile_vla_finetune/2025-12-03/mobile_vla_lora_20251203/epoch_epoch=09-val_loss=val_loss=0.013.ckpt"
    robovlms_ckpt = "checkpoints/RoboVLMs/checkpoints/kosmos_ph_oxe-pretrain.pt"
    
    print("\n[1] ì²´í¬í¬ì¸íŠ¸ í™•ì¸")
    print("-"*70)
    
    # Kosmos-2 í™•ì¸
    if Path(kosmos2_ckpt).exists():
        size_k = Path(kosmos2_ckpt).stat().st_size / (1024**3)
        print(f"  âœ… Kosmos-2 (Mobile-VLA): {size_k:.2f} GB")
    else:
        print(f"  âŒ Kosmos-2 ì—†ìŒ: {kosmos2_ckpt}")
        return
    
    # RoboVLMs í™•ì¸  
    if Path(robovlms_ckpt).exists():
        size_r = Path(robovlms_ckpt).stat().st_size / (1024**3)
        print(f"  âœ… RoboVLMs: {size_r:.2f} GB")
    else:
        print(f"  âŒ RoboVLMs ì—†ìŒ: {robovlms_ckpt}")
        print("  â†’ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ í•„ìš”")
        return
    
    # 2. ëª¨ë¸ ë¡œë“œ
    print("\n[2] VLM ë¡œë“œ ë° ë¹„êµ")
    print("-"*70)
    
    try:
        from robovlms.train.mobile_vla_trainer import MobileVLATrainer
        
        # Kosmos-2 (Mobile-VLA)
        print("  Loading Kosmos-2 (Mobile-VLA trained)...")
        model_k = MobileVLATrainer.load_from_checkpoint(kosmos2_ckpt)
        model_k.eval()
        model_k.cuda()
        print("  âœ… Kosmos-2 loaded")
        
        # RoboVLMsëŠ” ì§ì ‘ checkpoint ë¶„ì„
        print("  Analyzing RoboVLMs checkpoint...")
        vlm_state_dict = load_checkpoint_and_extract_vlm(robovlms_ckpt)
        print("  âœ… RoboVLMs analyzed")
        
    except Exception as e:
        print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 3. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ context ì¶”ì¶œ
    print("\n[3] Context Vector ì¶”ì¶œ")
    print("-"*70)
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
    h5_file = "ROS_action/mobile_vla_dataset/episode_20251204_113519_1box_hori_left_core_medium.h5"
    
    if not Path(h5_file).exists():
        # ë‹¤ë¥¸ íŒŒì¼ ì°¾ê¸°
        import glob
        h5_files = glob.glob("ROS_action/mobile_vla_dataset/episode_*.h5")
        if h5_files:
            h5_file = h5_files[0]
            print(f"  Using: {Path(h5_file).name}")
        else:
            print("  âŒ H5 íŒŒì¼ ì—†ìŒ")
            return
    
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor()
    ])
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    with h5py.File(h5_file, 'r') as f:
        images = []
        for t in range(min(8, len(f['images']))):
            img_array = f['images'][t]
            img = Image.fromarray(img_array.astype(np.uint8))
            img_tensor = transform(img)
            images.append(img_tensor)
        
        images_tensor = torch.stack(images).unsqueeze(0).cuda()
        print(f"  ì´ë¯¸ì§€ shape: {images_tensor.shape}")
    
    # Kosmos-2 context ì¶”ì¶œ
    with torch.no_grad():
        context_k = model_k.model.encode_images(images_tensor)
    
    print(f"\n  Kosmos-2 Context:")
    print(f"    Shape: {context_k.shape}")
    print(f"    Mean: {context_k.mean().item():.4f}")
    print(f"    Std: {context_k.std().item():.4f}")
    print(f"    Min: {context_k.min().item():.4f}")
    print(f"    Max: {context_k.max().item():.4f}")
    
    # 4. ë¶„ì„
    print("\n[4] ë¶„ì„ ê²°ê³¼")
    print("="*70)
    
    print("\nâœ… Kosmos-2 (Mobile-VLA trained):")
    print("  - Pretrain: ì¼ë°˜ ì´ë¯¸ì§€ (COCO, Flickr)")
    print("  - Fine-tuned: Mobile navigation (250 left)")
    print("  - Context: Image â†’ 2048D vector")
    
    print("\nğŸ“Š RoboVLMs:")
    print("  - Pretrain: Robot manipulation (OXE)")
    print("  - Checkpoint ë¶„ì„ ì™„ë£Œ")
    print(f"  - Parameters: {len(vlm_state_dict)}")
    
    print("\nğŸ¯ ê²°ë¡ :")
    print("  1. RoboVLMs checkpoint ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
    print("  2. Kosmos-2 context vector ì¶”ì¶œ ì„±ê³µ")
    print("  3. VLM pretrain ì°¨ì´ í™•ì¸ ê°€ëŠ¥")
    print("  4. ë‹¤ìŒ: RoboVLMsë¡œ í•™ìŠµí•˜ì—¬ context ë¹„êµ")
    
    print("\n" + "="*70)

if __name__ == "__main__":
    compare_context_vectors()
