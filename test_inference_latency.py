#!/usr/bin/env python3
"""
Inference Latency ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸
ëª©ì : Best checkpointì˜ ì‹¤ì œ ì¶”ë¡  ì†ë„ ì¸¡ì •
"""

import torch
import time
import numpy as np
from pathlib import Path
import argparse
import sys

# RoboVLMs ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "RoboVLMs_upstream"))

from robovlms.train.mobile_vla_trainer import MobileVLATrainer


def measure_inference_latency(checkpoint_path, num_iterations=100):
    """
    Inference latency ì¸¡ì •
    
    Args:
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        num_iterations: ì¸¡ì • ë°˜ë³µ íšŸìˆ˜
    """
    print("="*60)
    print("Inference Latency ì¸¡ì •")
    print("="*60)
    print(f"Checkpoint: {checkpoint_path}")
    print(f"Iterations: {num_iterations}")
    print()
    
    # 1. ëª¨ë¸ ë¡œë“œ
    print("[1/4] ëª¨ë¸ ë¡œë”©...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    try:
        model = MobileVLATrainer.load_from_checkpoint(checkpoint_path)
        model = model.to(device)
        model.eval()
        print(f"  âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {device})")
    except Exception as e:
        print(f"  âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 2. ë”ë¯¸ ì…ë ¥ ìƒì„±
    print("\n[2/4] ë”ë¯¸ ì…ë ¥ ìƒì„±...")
    window_size = 8
    image_size = 224
    batch_size = 1
    
    dummy_images = torch.randn(batch_size, window_size, 3, image_size, image_size).to(device)
    dummy_text = torch.zeros(batch_size, 256, dtype=torch.long).to(device)
    dummy_attention_mask = torch.ones(batch_size, 256, dtype=torch.long).to(device)
    
    print(f"  Images shape: {dummy_images.shape}")
    print(f"  Text shape: {dummy_text.shape}")
    
    # 3. Warm-up (GPU ì¤€ë¹„)
    print("\n[3/4] Warm-up (10 iterations)...")
    with torch.no_grad():
        for _ in range(10):
            _ = model.model(dummy_images, dummy_text, dummy_attention_mask)
    print("  âœ… Warm-up ì™„ë£Œ")
    
    # 4. ì‹¤ì œ ì¸¡ì •
    print(f"\n[4/4] Latency ì¸¡ì • ({num_iterations} iterations)...")
    
    vlm_times = []
    action_head_times = []
    total_times = []
    
    with torch.no_grad():
        for i in range(num_iterations):
            # Total time
            t_total_start = time.time()
            
            # VLM forward
            t_vlm_start = time.time()
            context = model.model.encode_images(dummy_images)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            t_vlm_end = time.time()
            
            # Action Head forward
            t_action_start = time.time()
            actions = model.model.act_head(context)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            t_action_end = time.time()
            
            t_total_end = time.time()
            
            vlm_times.append((t_vlm_end - t_vlm_start) * 1000)  # ms
            action_head_times.append((t_action_end - t_action_start) * 1000)
            total_times.append((t_total_end - t_total_start) * 1000)
            
            if (i + 1) % 20 == 0:
                print(f"  Progress: {i+1}/{num_iterations}")
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ì¸¡ì • ê²°ê³¼")
    print("="*60)
    
    print(f"\nğŸ“Š VLM Forward:")
    print(f"  Mean: {np.mean(vlm_times):.2f} ms")
    print(f"  Std:  {np.std(vlm_times):.2f} ms")
    print(f"  Min:  {np.min(vlm_times):.2f} ms")
    print(f"  Max:  {np.max(vlm_times):.2f} ms")
    
    print(f"\nğŸ“Š Action Head Forward:")
    print(f"  Mean: {np.mean(action_head_times):.2f} ms")
    print(f"  Std:  {np.std(action_head_times):.2f} ms")
    print(f"  Min:  {np.min(action_head_times):.2f} ms")
    print(f"  Max:  {np.max(action_head_times):.2f} ms")
    
    print(f"\nğŸ“Š Total Inference:")
    print(f"  Mean: {np.mean(total_times):.2f} ms")
    print(f"  Std:  {np.std(total_times):.2f} ms")
    print(f"  Min:  {np.min(total_times):.2f} ms")
    print(f"  Max:  {np.max(total_times):.2f} ms")
    
    # 6. ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    print("\n" + "="*60)
    print("ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
    print("="*60)
    
    target_latency = 200.0  # ms
    mean_total = np.mean(total_times)
    
    if mean_total < target_latency:
        print(f"  âœ… ëª©í‘œ ë‹¬ì„±! ({mean_total:.2f} ms < {target_latency} ms)")
        print(f"  â†’ 0.4ì´ˆ ê°„ê²© ì¶”ë¡ ì— ì¶©ë¶„í•©ë‹ˆë‹¤!")
    else:
        print(f"  âš ï¸  ëª©í‘œ ë¯¸ë‹¬ ({mean_total:.2f} ms >= {target_latency} ms)")
        print(f"  â†’ ì¶”ë¡  ê°„ê²© ì¡°ì • í•„ìš”")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Inference Latency ì¸¡ì •")
    parser.add_argument("--checkpoint", type=str, 
                       default="RoboVLMs_upstream/runs/mobile_vla_lora_20251203/kosmos/mobile_vla_finetune/2025-12-03/mobile_vla_lora_20251203/epoch_epoch=09-val_loss=val_loss=0.013.ckpt",
                       help="Checkpoint ê²½ë¡œ")
    parser.add_argument("--iterations", type=int, default=100,
                       help="ì¸¡ì • ë°˜ë³µ íšŸìˆ˜")
    
    args = parser.parse_args()
    
    measure_inference_latency(args.checkpoint, args.iterations)
