# ğŸ“š RoboVLMs ë…¼ë¬¸ Introduction ì„¹ì…˜ ë¶„ì„

> **ì¸ìš©**: ë…¼ë¬¸ 1í˜ì´ì§€ 2ë²ˆì§¸ ì¤„ë¶€í„° 2í˜ì´ì§€ 1ë²ˆì§¸ ì¤„ê¹Œì§€ì˜ Introduction ì„¹ì…˜

## ğŸ¯ **1. ì—°êµ¬ ë°°ê²½ ë° ë™ê¸°**

### **ë¡œë´‡ ì •ì±…ì˜ ì¥ê¸°ì  ë„ì „ê³¼ì œ**
> **ì¸ìš©**: "Building generalizable robot policies capable of perceiving, reasoning, and interacting with the physical environment given human instructions has been a long-standing challenge in robotics [4, 5, 7, 35]." (1í˜ì´ì§€ 2-3ë²ˆì§¸ ì¤„)

- **ëª©í‘œ**: ì¸ê°„ ì§€ì‹œì— ë”°ë¼ ë¬¼ë¦¬ì  í™˜ê²½ì„ ì¸ì§€, ì¶”ë¡ , ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì¼ë°˜í™” ê°€ëŠ¥í•œ ë¡œë´‡ ì •ì±… êµ¬ì¶•
- **ê¸°ì¡´ ì ‘ê·¼ë²•**: ë‹¤ì–‘í•œ ì¼ë°˜í™” ì •ì±…ë“¤ (ë¹„ë””ì˜¤ ëª¨ë¸ ê¸°ë°˜, ì²˜ìŒë¶€í„° í•™ìŠµ ë“±)
- **ìƒˆë¡œìš´ ë°©í–¥**: Vision-Language Models (VLMs)ë¥¼ ë¡œë´‡ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ì—¬ Vision-Language-Action Models (VLAs) êµ¬ì¶•

### **VLA ì„ íƒì˜ ê·¼ê±°**
> **ì¸ìš©**: "Recently, there has been an active exploration into learning robot foundation models by fine-tuning the Vision-Language Models (VLMs) on robot data with certain architectural adjustments. The resulting models, also referred to as Vision-Language-Action Models (VLAs), show promising results in both simulated and real-world tasks [7, 22, 24]." (1í˜ì´ì§€ 3-4ë²ˆì§¸ ì¤„)

- **VLMsì˜ ê°•ì **: ì›¹ ê·œëª¨ ë°ì´í„°ë¡œ í•™ìŠµëœ ë‹¤ì¤‘ ëª¨ë‹¬ ë°ì´í„°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€/ë¹„ë””ì˜¤)ì˜ ì¼ë°˜í™”ë˜ê³  ê°•ê±´í•œ í‘œí˜„ í•™ìŠµ ëŠ¥ë ¥
- **í•µì‹¬ ê°€ì¹˜**: ë‹¤ì–‘í•œ ì˜¤í”ˆì›”ë“œ ì¥ë©´ê³¼ ì œí•œëœ ë¡œë´‡ ë°ì´í„° ê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ì ì‘ ëŠ¥ë ¥
- **ì ì¬ë ¥**: ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ì´ ë¡œë´‡ ì¡°ì‘ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ íƒêµ¬ í•„ìš”

## ğŸ” **2. í•µì‹¬ ì—°êµ¬ ì§ˆë¬¸ë“¤**

### **ì§ˆë¬¸ 1: Why do we prefer VLAs?**
> **ì¸ìš©**: "Therefore, a natural question arises: Why do we prefer VLAs built upon large-scale pre-trained VLMs? Compared with other generalist policies, a mostly believed reason for utilizing VLM-based VLAs is that VLMs have demonstrated strong capabilities in learning generalized and robust representations of multi-modal data, such as text, images/videos, through extensive training on web-scale data." (1í˜ì´ì§€ 4-5ë²ˆì§¸ ì¤„)

- **ë°°ê²½**: ë‹¤ì–‘í•œ ì¼ë°˜í™” ì •ì±… ì¤‘ VLAë¥¼ ì„ í˜¸í•˜ëŠ” ì´ìœ 
- **ê°€ì„¤**: ëŒ€ê·œëª¨ ë¹„ì „-ì–¸ì–´ ì‚¬ì „ í›ˆë ¨ì´ ì¼ë°˜í™” ë¡œë´‡ ì •ì±…ì— ì–´ëŠ ì •ë„ ê¸°ì—¬í•˜ëŠ”ê°€?
- **ê²€ì¦ í•„ìš”**: VLMsì˜ í‘œí˜„ í•™ìŠµ ëŠ¥ë ¥ì´ ì‹¤ì œ ë¡œë´‡ ì¡°ì‘ì— ì–¼ë§ˆë‚˜ íš¨ê³¼ì ì¸ì§€
- **ì—°êµ¬ ê°­**: VLMì—ì„œ VLAë¡œì˜ ì „í™˜ ê³¼ì •ì—ì„œì˜ í•µì‹¬ ì„¤ê³„ ìš”ì†Œ ë¯¸í•´ëª…

### **ì§ˆë¬¸ 2: Which backbone to select?**
> **ì¸ìš©**: "Moreover, a large and diverse set of different VLMs emerged rapidly with different kinds of LLM backbone, training data, model sizes, architectures, and training recipes. Which kind of VLM backbones is more suitable for robot manipulation is also a crucial issue for the development of successful VLAs." (1í˜ì´ì§€ 5-6ë²ˆì§¸ ì¤„)

- **ë¬¸ì œ**: ë‹¤ì–‘í•œ VLM ë°±ë³¸ë“¤ì˜ ë“±ì¥ (ë‹¤ë¥¸ LLM ë°±ë³¸, í›ˆë ¨ ë°ì´í„°, ëª¨ë¸ í¬ê¸°, ì•„í‚¤í…ì²˜, í›ˆë ¨ ë°©ë²•)
- **í•µì‹¬ ì´ìŠˆ**: ì–´ë–¤ ì¢…ë¥˜ì˜ VLM ë°±ë³¸ì´ ë¡œë´‡ ì¡°ì‘ì— ë” ì í•©í•œê°€?
- **ë³µì¡ì„±**: ë°±ë³¸ ì„ íƒì´ VLA ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„ í•„ìš”

### **ì§ˆë¬¸ 3: How to formulate VLAs?**
> **ì¸ìš©**: "Beyond the diversity of different backbones, for generalist robot policies, including VLAs, the structures are more complex and vary in form. Based on the most prevalent existing work [4, 7, 20, 22, 24, 34, 35, 39, 47, 55], we propose a categorization based on 1) how the history and action information are incorporated in VLAs and 2) whether the action space is continuous or discrete." (1í˜ì´ì§€ 6-7ë²ˆì§¸ ì¤„)

- **ë³µì¡ì„±**: ì¼ë°˜í™” ë¡œë´‡ ì •ì±…ì˜ êµ¬ì¡°ê°€ ë³µì¡í•˜ê³  í˜•íƒœê°€ ë‹¤ì–‘í•¨
- **ë¶„ë¥˜ ê¸°ì¤€**: 
  1. íˆìŠ¤í† ë¦¬ì™€ ì•¡ì…˜ ì •ë³´ê°€ VLAì— ì–´ë–»ê²Œ í†µí•©ë˜ëŠ”ê°€?
  2. ì•¡ì…˜ ê³µê°„ì´ ì—°ì†ì ì¸ê°€ ì´ì‚°ì ì¸ê°€?
- **ì‹¤ìš©ì  ì¤‘ìš”ì„±**: VLMì˜ í˜ì„ ì¶©ë¶„íˆ í™œìš©í•  ìˆ˜ ìˆëŠ” VLA êµ¬ì„± ë°©ë²•

### **ì§ˆë¬¸ 4: When to use cross-embodiment data?**
> **ì¸ìš©**: "In addition to the VLA itself, the quality and diversity of the training data used to develop VLAs are equally critical. With recent progress achieved by well-known VLAs [4, 7, 22, 35, 39], large-scale data from different sources is important to further improve performance in terms of robustness and generalization against out-of-distribution tasks and environments." (1í˜ì´ì§€ 7-8ë²ˆì§¸ ì¤„)

- **ë°ì´í„° ì¤‘ìš”ì„±**: VLA ê°œë°œì— ì‚¬ìš©ë˜ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±
- **ì „ëµ ì°¨ì´**: 
  - ì¶”ê°€ ë°ì´í„°ë¡œ VLMs ì‚¬ì „ í›ˆë ¨ (í‘œí˜„ì„ ë¡œë´‡ ì¡°ì‘ ì‘ì—…ì— ê°€ê¹ê²Œ ì •ì œ)
  - ë„ë©”ì¸ ë‚´ ì‘ì—…ê³¼ í•¨ê»˜ VLA ê³µë™ í›ˆë ¨
- **í•µì‹¬ ì§ˆë¬¸**: ì–¸ì œ ëŒ€ê·œëª¨ êµì°¨-ì—”ë°”ë””ë¨¼íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•´ì•¼ í•˜ëŠ”ê°€?

## ğŸ—ï¸ **3. VLA êµ¬ì¡° ë¶„ë¥˜ ì²´ê³„ (Figure 2 ê¸°ë°˜)**

> **ì¸ìš©**: "As shown in Fig.2, four types of structure formulations are considered. For history information modeling, two forms are identified: 1) one-step modeling, which utilizes only the current state or observation to produce actions; and 2) history modeling, which processes a sliding window of historical states or observations." (1í˜ì´ì§€ 8-9ë²ˆì§¸ ì¤„)

### **ë¶„ë¥˜ ê¸°ì¤€ 1: íˆìŠ¤í† ë¦¬ ì •ë³´ ëª¨ë¸ë§**

#### **One-step modeling (ì¼ë‹¨ê³„ ëª¨ë¸ë§)**
- **íŠ¹ì§•**: í˜„ì¬ ìƒíƒœë‚˜ ê´€ì¸¡ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì•¡ì…˜ ìƒì„±
- **ì¥ì **: ë‹¨ìˆœí•œ êµ¬ì¡°, ë¹ ë¥¸ ì²˜ë¦¬
- **ë‹¨ì **: ì‹œê°„ì  ë§¥ë½ ì •ë³´ ë¶€ì¡±

#### **History modeling (íˆìŠ¤í† ë¦¬ ëª¨ë¸ë§)**
- **íŠ¹ì§•**: íˆìŠ¤í† ë¦¬ ìƒíƒœë‚˜ ê´€ì¸¡ì˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²˜ë¦¬
- **ì¥ì **: ì‹œê°„ì  ë§¥ë½ ê³ ë ¤, ë” ë³µì¡í•œ ì˜ì‚¬ê²°ì •
- **ë‹¨ì **: ê³„ì‚° ë³µì¡ë„ ì¦ê°€

### **ë¶„ë¥˜ ê¸°ì¤€ 2: íˆìŠ¤í† ë¦¬ ì •ë³´ ì§‘ê³„ ë°©ë²•**

> **ì¸ìš©**: "Regarding the aggregation of history information, we classify it into two approaches: a) interleaved modeling, which integrates historical observation and action sequences in an interleaved format; and b) policy head, which separately processes each historical step and fuses the information at a distinct policy head for action prediction." (1í˜ì´ì§€ 9-10ë²ˆì§¸ ì¤„)

#### **Interleaved modeling (êµì°¨ ëª¨ë¸ë§)**
- **íŠ¹ì§•**: íˆìŠ¤í† ë¦¬ ê´€ì¸¡ê³¼ ì•¡ì…˜ ì‹œí€€ìŠ¤ë¥¼ êµì°¨ í˜•ì‹ìœ¼ë¡œ í†µí•©
- **ì¥ì **: ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì²˜ë¦¬
- **ë‹¨ì **: ë³µì¡í•œ ì‹œí€€ìŠ¤ ì²˜ë¦¬ í•„ìš”

#### **Policy head (ì •ì±… í—¤ë“œ)**
- **íŠ¹ì§•**: ê° íˆìŠ¤í† ë¦¬ ë‹¨ê³„ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
- **ì¥ì **: ë³„ë„ì˜ ì •ì±… í—¤ë“œì—ì„œ ì •ë³´ë¥¼ ìœµí•©í•˜ì—¬ ì•¡ì…˜ ì˜ˆì¸¡
- **ë‹¨ì **: ì •ë³´ ìœµí•© ê³¼ì •ì˜ ë³µì¡ì„±

## ğŸ”¬ **4. ì‹¤í—˜ ì„¤ê³„ ë° ë°©ë²•ë¡ **

> **ì¸ìš©**: "To thoroughly study the aforementioned issues and find the most effective solution for VLAs, our study chose 4 VLA structures, 8 various backbones, and 3 different training data recipes to train the VLA models." (1í˜ì´ì§€ 10-11ë²ˆì§¸ ì¤„)

### **ì‹¤í—˜ êµ¬ì„±**
- **VLA êµ¬ì¡°**: 4ê°€ì§€ (Figure 2 ê¸°ë°˜ ë¶„ë¥˜)
- **ë°±ë³¸**: 8ê°€ì§€ ë‹¤ì–‘í•œ VLM
- **í›ˆë ¨ ë°ì´í„° ë ˆì‹œí”¼**: 3ê°€ì§€ (Pre-training, Fine-tuning, Post-training)

### **í‰ê°€ í™˜ê²½**

> **ì¸ìš©**: "We evaluate these models on two popular robot manipulation benchmarks in simulation: CALVIN [32] and SimplerEnv [37]. Moreover, we also trained and evaluated the built VLAs on a self-collected real-world robot manipulation dataset, consisting of 100 manipulation tasks and a total of 74K trajectories." (1í˜ì´ì§€ 11-12ë²ˆì§¸ ì¤„)

#### **ì‹œë®¬ë ˆì´ì…˜ ë²¤ì¹˜ë§ˆí¬**
- **CALVIN [32]**: ëŒ€ê·œëª¨ ë¡œë´‡ ì¡°ì‘ ë²¤ì¹˜ë§ˆí¬
- **SimplerEnv [37]**: ë‹¨ìˆœí™”ëœ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ í‰ê°€

#### **ì‹¤ì œ ë¡œë´‡ ë°ì´í„°ì…‹**
- **ê·œëª¨**: 100ê°œ ì¡°ì‘ ì‘ì—…, ì´ 74K ê¶¤ì 
- **ë‹¤ì–‘ì„±**: ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ í¬í•¨

---

*ë¶„ì„ ì‘ì„±ì¼: 2024ë…„ 12ì›”*  
*ì›ë³¸ ë…¼ë¬¸: "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models"*  
*ë¶„ì„ì: Mobile VLA í”„ë¡œì íŠ¸ íŒ€*