#!/usr/bin/env python3
"""
Simplified ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import torch.nn as nn
from transformers import AutoProcessor, CLIPVisionModel, CLIPTextModel
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

class SimpleTestModel(nn.Module):
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ëª¨ë¸"""
    
    def __init__(self):
        super().__init__()
        
        # CLIP ëª¨ë¸ë“¤
        self.clip_vision = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_text = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Feature Fusion
        self.fusion = nn.Sequential(
            nn.Linear(768 + 512, 512),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Action Head
        self.action_head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 2)
        )
    
    def forward(self, images, text_inputs):
        # CLIP Vision
        vision_outputs = self.clip_vision(images)
        vision_features = vision_outputs.pooler_output
        
        # CLIP Text
        text_outputs = self.clip_text(**text_inputs)
        text_features = text_outputs.pooler_output
        
        # Feature Fusion
        combined = torch.cat([vision_features, text_features], dim=1)
        fused = self.fusion(combined)
        
        # Action Prediction
        actions = self.action_head(fused)
        
        return actions

def test_model():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Simplified ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    # ëª¨ë¸ ìƒì„±
    model = SimpleTestModel().to(device)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 2
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    images = torch.randn(batch_size, 3, 224, 224).to(device)
    
    # ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
    text_inputs = processor(
        text=["move forward", "turn left"],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    ).to(device)
    
    # ë”ë¯¸ ì•¡ì…˜ ìƒì„±
    actions = torch.randn(batch_size, 2).to(device)
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {images.shape}")
    print(f"   í…ìŠ¤íŠ¸ ì…ë ¥ í¬ê¸°: {text_inputs['input_ids'].shape}")
    print(f"   ì•¡ì…˜ í¬ê¸°: {actions.shape}")
    
    # Forward pass í…ŒìŠ¤íŠ¸
    try:
        with torch.no_grad():
            predicted_actions = model(images, text_inputs)
            print("âœ… Forward pass ì„±ê³µ!")
            print(f"   ì˜ˆì¸¡ ì•¡ì…˜ í¬ê¸°: {predicted_actions.shape}")
            
            # ì†ì‹¤ ê³„ì‚° í…ŒìŠ¤íŠ¸
            criterion = nn.MSELoss()
            loss = criterion(predicted_actions, actions)
            print(f"   í…ŒìŠ¤íŠ¸ ì†ì‹¤: {loss.item():.6f}")
            
    except Exception as e:
        print(f"âŒ Forward pass ì‹¤íŒ¨: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = test_model()
    if success:
        print("ğŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("ğŸ’¥ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
