#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ Mobile VLA TensorRT ë³€í™˜ê¸°
- ONNXë¥¼ í†µí•œ TensorRT ë³€í™˜
- í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì„ ê³ ì„±ëŠ¥ìœ¼ë¡œ ë³€í™˜
"""

import torch
from transformers import AutoProcessor, AutoModel
import numpy as np
import os
import json
import time
from PIL import Image

class SimpleTensorRTConverter:
    """ê°„ë‹¨í•œ TensorRT ë³€í™˜ê¸°"""
    
    def __init__(self, model_name: str = "minium/mobile-vla-omniwheel"):
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = "Robo+/Mobile_VLA/tensorrt_quantized"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_original_model()
        
    def load_original_model(self):
        """ì›ë³¸ ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ”„ Loading original model: {self.model_name}")
        
        try:
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            
            if self.device.type == 'cuda':
                self.model = self.model.cuda()
            
            self.model.eval()
            print("âœ… Original model loaded successfully")
            
        except Exception as e:
            print(f"âŒ Failed to load original model: {e}")
            raise
    
    def prepare_sample_inputs(self):
        """ìƒ˜í”Œ ì…ë ¥ ë°ì´í„° ì¤€ë¹„"""
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
        pil_image = Image.fromarray(image)
        
        # í”„ë¡œì„¸ì„œë¡œ ì „ì²˜ë¦¬
        inputs = self.processor(
            images=pil_image,
            text="Navigate around obstacles to track the target cup",
            return_tensors="pt"
        )
        
        # GPUë¡œ ì´ë™
        if self.device.type == 'cuda':
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        return inputs['pixel_values'], inputs['input_ids']
    
    def convert_to_onnx(self):
        """ONNX ëª¨ë¸ë¡œ ë³€í™˜"""
        print("ğŸ”¨ Converting to ONNX model")
        
        # ìƒ˜í”Œ ì…ë ¥ ì¤€ë¹„
        sample_images, sample_text = self.prepare_sample_inputs()
        
        # ONNX ëª¨ë¸ ì €ì¥
        onnx_path = os.path.join(self.output_dir, "mobile_vla_model.onnx")
        
        try:
            torch.onnx.export(
                self.model,
                (sample_images, sample_text),
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['pixel_values', 'input_ids'],
                output_names=['action_logits'],
                dynamic_axes={
                    'pixel_values': {0: 'batch_size'},
                    'input_ids': {0: 'batch_size'},
                    'action_logits': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNX model saved: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ONNX conversion failed: {e}")
            return None
    
    def create_trtexec_script(self, onnx_path: str):
        """trtexec ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        print("ğŸ”§ Creating trtexec conversion script")
        
        # FP16 ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
        fp16_script = f"""#!/bin/bash
# TensorRT FP16 ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸ”¨ Converting to TensorRT FP16..."

# FP16 ì—”ì§„ ìƒì„±
trtexec \\
    --onnx={onnx_path} \\
    --saveEngine={self.output_dir}/mobile_vla_fp16.engine \\
    --fp16 \\
    --workspace=1024 \\
    --verbose \\
    --minShapes=pixel_values:1x3x224x224,input_ids:1x512 \\
    --optShapes=pixel_values:1x3x224x224,input_ids:1x512 \\
    --maxShapes=pixel_values:4x3x224x224,input_ids:4x512

echo "âœ… FP16 engine created: {self.output_dir}/mobile_vla_fp16.engine"
"""
        
        # INT8 ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
        int8_script = f"""#!/bin/bash
# TensorRT INT8 ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸ”¨ Converting to TensorRT INT8..."

# INT8 ì—”ì§„ ìƒì„±
trtexec \\
    --onnx={onnx_path} \\
    --saveEngine={self.output_dir}/mobile_vla_int8.engine \\
    --int8 \\
    --workspace=1024 \\
    --verbose \\
    --minShapes=pixel_values:1x3x224x224,input_ids:1x512 \\
    --optShapes=pixel_values:1x3x224x224,input_ids:1x512 \\
    --maxShapes=pixel_values:4x3x224x224,input_ids:4x512

echo "âœ… INT8 engine created: {self.output_dir}/mobile_vla_int8.engine"
"""
        
        # ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
        fp16_script_path = os.path.join(self.output_dir, "convert_to_fp16.sh")
        int8_script_path = os.path.join(self.output_dir, "convert_to_int8.sh")
        
        with open(fp16_script_path, "w") as f:
            f.write(fp16_script)
        
        with open(int8_script_path, "w") as f:
            f.write(int8_script)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        os.chmod(fp16_script_path, 0o755)
        os.chmod(int8_script_path, 0o755)
        
        print(f"âœ… Conversion scripts created:")
        print(f"  FP16: {fp16_script_path}")
        print(f"  INT8: {int8_script_path}")
        
        return fp16_script_path, int8_script_path
    
    def create_tensorrt_inference_node(self):
        """TensorRT ì¶”ë¡  ë…¸ë“œ ìƒì„±"""
        print("ğŸ”§ Creating TensorRT inference node")
        
        node_code = '''#!/usr/bin/env python3
"""
TensorRT ì¶”ë¡  ë…¸ë“œ (ìë™ ìƒì„±)
- ê³ ì„±ëŠ¥ ì‹¤ì‹œê°„ ì¶”ë¡ 
- FP16/INT8 ì—”ì§„ ì§€ì›
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import CompressedImage
from geometry_msgs.msg import Twist
from std_msgs.msg import String
import cv2
import numpy as np
from PIL import Image as PILImage
import json
import time
import os

# TensorRT import (ì„ íƒì )
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    print("Warning: TensorRT not available. Using mock inference.")
    TENSORRT_AVAILABLE = False

class TensorRTInferenceNode(Node):
    def __init__(self):
        super().__init__('tensorrt_inference_node')
        
        # ëª¨ë¸ ì„¤ì •
        self.engine_path = self.declare_parameter('engine_path', '').value
        self.use_tensorrt = self.declare_parameter('use_tensorrt', True).value
        
        # TensorRT ì—”ì§„ ë¡œë“œ
        if self.use_tensorrt and TENSORRT_AVAILABLE:
            self.load_tensorrt_engine()
        else:
            self.get_logger().info("Using mock TensorRT inference")
        
        # ROS ì„¤ì •
        self.setup_ros()
        
        # ìƒíƒœ ë³€ìˆ˜
        self.inference_count = 0
        self.last_inference_time = 0.0
        
    def load_tensorrt_engine(self):
        """TensorRT ì—”ì§„ ë¡œë“œ"""
        if not self.engine_path or not os.path.exists(self.engine_path):
            self.get_logger().warn("No valid TensorRT engine path provided")
            return
        
        try:
            with open(self.engine_path, "rb") as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            self.engine = runtime.deserialize_cuda_engine(engine_data)
            self.context = self.engine.create_execution_context()
            
            # ë©”ëª¨ë¦¬ í• ë‹¹
            self.input_images = cuda.mem_alloc(1 * 3 * 224 * 224 * 4)  # float32
            self.input_text = cuda.mem_alloc(1 * 512 * 4)  # float32
            self.output = cuda.mem_alloc(1 * 3 * 4)  # float32
            
            self.get_logger().info(f"âœ… TensorRT engine loaded: {self.engine_path}")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Failed to load TensorRT engine: {e}")
            self.use_tensorrt = False
    
    def setup_ros(self):
        """ROS ì„¤ì •"""
        # ì´ë¯¸ì§€ ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„
        self.image_sub = self.create_subscription(
            CompressedImage,
            '/camera/image/compressed',
            self.image_callback,
            10
        )
        
        # ì•¡ì…˜ í¼ë¸”ë¦¬ì…”
        self.action_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # ê²°ê³¼ í¼ë¸”ë¦¬ì…”
        self.result_pub = self.create_publisher(
            String,
            '/tensorrt/inference_result',
            10
        )
        
    def image_callback(self, msg):
        """ì´ë¯¸ì§€ ì½œë°±"""
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            np_arr = np.frombuffer(msg.data, np.uint8)
            image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # PIL Imageë¡œ ë³€í™˜ ë° ë¦¬ì‚¬ì´ì¦ˆ
            pil_image = PILImage.fromarray(image_rgb)
            pil_image = pil_image.resize((224, 224))
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            image_array = np.array(pil_image, dtype=np.float32) / 255.0
            image_array = np.transpose(image_array, (2, 0, 1))  # HWC -> CHW
            image_array = np.expand_dims(image_array, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”© (ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„°)
            text_embedding = np.random.randn(1, 512).astype(np.float32)
            
            # ì¶”ë¡  ì‹¤í–‰
            if self.use_tensorrt and TENSORRT_AVAILABLE:
                action = self.run_tensorrt_inference(image_array, text_embedding)
            else:
                action = self.run_mock_inference(image_array, text_embedding)
            
            # ì•¡ì…˜ ì‹¤í–‰
            self.execute_action(action)
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error in image callback: {e}")
    
    def run_tensorrt_inference(self, image_array, text_embedding):
        """TensorRT ì¶”ë¡  ì‹¤í–‰"""
        try:
            # GPU ë©”ëª¨ë¦¬ë¡œ ë°ì´í„° ë³µì‚¬
            cuda.memcpy_htod(self.input_images, image_array)
            cuda.memcpy_htod(self.input_text, text_embedding)
            
            # ì¶”ë¡  ì‹¤í–‰
            start_time = time.time()
            self.context.execute_v2(bindings=[
                int(self.input_images),
                int(self.input_text),
                int(self.output)
            ])
            inference_time = time.time() - start_time
            
            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            result = np.empty((1, 3), dtype=np.float32)
            cuda.memcpy_dtoh(result, self.output)
            
            self.inference_count += 1
            self.last_inference_time = inference_time
            
            # ê²°ê³¼ ë°œí–‰
            self.publish_result(result[0], inference_time)
            
            return result[0]
            
        except Exception as e:
            self.get_logger().error(f"âŒ TensorRT inference error: {e}")
            return np.array([0.0, 0.0, 0.0])
    
    def run_mock_inference(self, image_array, text_embedding):
        """Mock ì¶”ë¡  (TensorRT ì—†ì„ ë•Œ)"""
        # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
        import math
        t = time.time()
        angle = (t * 0.5) % (2 * math.pi)
        
        linear_x = 0.1 * math.cos(angle)
        linear_y = 0.05 * math.sin(angle)
        angular_z = 0.2 * math.sin(angle * 2)
        
        action = np.array([linear_x, linear_y, angular_z], dtype=np.float32)
        
        self.inference_count += 1
        self.last_inference_time = 0.001  # 1ms ì‹œë®¬ë ˆì´ì…˜
        
        self.publish_result(action, 0.001)
        
        return action
    
    def execute_action(self, action):
        """ì•¡ì…˜ ì‹¤í–‰"""
        try:
            twist = Twist()
            twist.linear.x = float(action[0])
            twist.linear.y = float(action[1])
            twist.angular.z = float(action[2])
            
            self.action_pub.publish(twist)
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error executing action: {e}")
    
    def publish_result(self, action, inference_time):
        """ê²°ê³¼ ë°œí–‰"""
        try:
            result = {
                "timestamp": time.time(),
                "inference_time": inference_time,
                "action": action.tolist(),
                "inference_count": self.inference_count,
                "engine": self.engine_path if hasattr(self, 'engine_path') else "mock"
            }
            
            msg = String()
            msg.data = json.dumps(result)
            self.result_pub.publish(msg)
            
            self.get_logger().info(f"ğŸ¯ TensorRT Inference #{self.inference_count}: {inference_time*1000:.2f}ms")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Error publishing result: {e}")

def main(args=None):
    rclpy.init(args=args)
    node = TensorRTInferenceNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
'''
        
        # ë…¸ë“œ íŒŒì¼ ì €ì¥
        node_path = os.path.join(self.output_dir, "tensorrt_inference_node.py")
        with open(node_path, "w") as f:
            f.write(node_code)
        
        print(f"âœ… TensorRT inference node created: {node_path}")
        return node_path
    
    def create_usage_guide(self):
        """ì‚¬ìš© ê°€ì´ë“œ ìƒì„±"""
        print("ğŸ“– Creating usage guide")
        
        guide = f"""# Mobile VLA TensorRT ë³€í™˜ ê°€ì´ë“œ

## ê°œìš”
í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ `{self.model_name}` ëª¨ë¸ì„ TensorRTë¡œ ë³€í™˜í•˜ì—¬ ê³ ì„±ëŠ¥ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ë³€í™˜ëœ íŒŒì¼ë“¤
- `mobile_vla_model.onnx`: ONNX ëª¨ë¸
- `convert_to_fp16.sh`: FP16 TensorRT ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
- `convert_to_int8.sh`: INT8 TensorRT ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
- `tensorrt_inference_node.py`: TensorRT ì¶”ë¡  ë…¸ë“œ

## ì‚¬ìš© ë°©ë²•

### 1. TensorRT ì—”ì§„ ìƒì„±
```bash
# FP16 ì—”ì§„ ìƒì„±
cd {self.output_dir}
./convert_to_fp16.sh

# INT8 ì—”ì§„ ìƒì„± (ì„ íƒì )
./convert_to_int8.sh
```

### 2. ROS ë…¸ë“œ ì‹¤í–‰
```bash
# FP16 ì—”ì§„ ì‚¬ìš©
ros2 run mobile_vla_package tensorrt_inference_node --ros-args -p engine_path:={self.output_dir}/mobile_vla_fp16.engine

# INT8 ì—”ì§„ ì‚¬ìš©
ros2 run mobile_vla_package tensorrt_inference_node --ros-args -p engine_path:={self.output_dir}/mobile_vla_int8.engine

# Mock ëª¨ë“œ (TensorRT ì—†ì„ ë•Œ)
ros2 run mobile_vla_package tensorrt_inference_node --ros-args -p use_tensorrt:=false
```

### 3. ì„±ëŠ¥ ë¹„êµ
- ì›ë³¸ PyTorch: ~50-100ms
- TensorRT FP16: ~10-20ms (2-5x ì†ë„ í–¥ìƒ)
- TensorRT INT8: ~5-10ms (5-10x ì†ë„ í–¥ìƒ)

## ìš”êµ¬ì‚¬í•­
- NVIDIA GPU
- TensorRT 8.x
- CUDA 11.x ì´ìƒ
- PyTorch 2.x

## ë¬¸ì œ í•´ê²°
1. TensorRT ì„¤ì¹˜: `pip install tensorrt`
2. CUDA ì„¤ì¹˜: NVIDIA ë“œë¼ì´ë²„ì™€ í•¨ê»˜ ì„¤ì¹˜
3. ê¶Œí•œ ë¬¸ì œ: `chmod +x convert_to_*.sh`
"""
        
        guide_path = os.path.join(self.output_dir, "README.md")
        with open(guide_path, "w") as f:
            f.write(guide)
        
        print(f"âœ… Usage guide created: {guide_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting Simple Mobile VLA TensorRT Conversion")
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = SimpleTensorRTConverter()
    
    try:
        # ONNX ë³€í™˜
        print("\nğŸ”¨ Converting to ONNX...")
        onnx_path = converter.convert_to_onnx()
        
        if onnx_path:
            # ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
            print("\nğŸ”§ Creating conversion scripts...")
            converter.create_trtexec_script(onnx_path)
            
            # ì¶”ë¡  ë…¸ë“œ ìƒì„±
            print("\nğŸ”§ Creating inference node...")
            converter.create_tensorrt_inference_node()
            
            # ì‚¬ìš© ê°€ì´ë“œ ìƒì„±
            print("\nğŸ“– Creating usage guide...")
            converter.create_usage_guide()
        
        print("\nâœ… Simple TensorRT conversion completed!")
        print(f"\nğŸ“ Output directory: {converter.output_dir}")
        print("ğŸ”§ Next steps:")
        print("  1. cd Robo+/Mobile_VLA/tensorrt_quantized")
        print("  2. ./convert_to_fp16.sh")
        print("  3. Use the generated TensorRT inference node")
        
    except Exception as e:
        print(f"âŒ Simple TensorRT conversion failed: {e}")
        raise

if __name__ == "__main__":
    main()
