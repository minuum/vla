#!/usr/bin/env python3
"""
Simple Mobile VLA Trainer - PyTorch Lightning ì—†ì´ ê¸°ë³¸ PyTorch ì‚¬ìš©
í…ŒìŠ¤íŠ¸ ë° ê°„ë‹¨í•œ í•™ìŠµìš©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
import logging

# Mobile VLA ëª¨ë“ˆë“¤
try:
    from ..models.mobile_vla_model import MobileVLAModel
    from ..data.mobile_dataset import MobileVLADataset
except ImportError:
    # í…ŒìŠ¤íŠ¸ìš© ì ˆëŒ€ ì„í¬íŠ¸
    import sys
    import os
    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(parent_dir)
    from models.mobile_vla_model import MobileVLAModel
    from data.mobile_dataset import MobileVLADataset

logger = logging.getLogger(__name__)


class SimpleMobileVLATrainer:
    """
    ê°„ë‹¨í•œ Mobile VLA íŠ¸ë ˆì´ë„ˆ (PyTorch Lightning ì—†ìŒ)
    í…ŒìŠ¤íŠ¸ ë° í”„ë¡œí† íƒ€ì´í•‘ìš©
    """
    
    def __init__(self, configs: Dict[str, Any]):
        self.configs = configs
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Mobile VLA ëª¨ë¸ ì´ˆê¸°í™”
        self.model = MobileVLAModel(
            hidden_size=configs.get("hidden_size", 768),
            image_backbone=configs.get("image_backbone", "efficientnet_v2_s"),
            text_model=configs.get("text_model", "klue/roberta-base"),
            use_lite_mode=configs.get("use_lite_mode", False),
            dropout=configs.get("dropout", 0.1)
        ).to(self.device)
        
        # mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜
        self.scenario_weights = configs.get("scenario_weights", {
            "1box_vert_left": 1.0,
            "1box_vert_right": 1.0,
            "1box_hori_left": 1.2,
            "1box_hori_right": 1.1,
            "2box_vert_left": 1.5,
            "2box_vert_right": 1.4,
            "2box_hori_left": 1.8,
            "2box_hori_right": 1.6
        })
        
        # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
        self.action_loss_weight = configs.get("action_loss_weight", 1.0)
        self.event_loss_weight = configs.get("event_loss_weight", 0.5)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=configs.get("learning_rate", 1e-4),
            weight_decay=configs.get("weight_decay", 0.01)
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=configs.get("max_epochs", 100),
            eta_min=configs.get("learning_rate", 1e-4) * 0.1
        )
        
        logger.info("ğŸ¤– Simple Mobile VLA Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   ëª¨ë¸ í¬ê¸°: {sum(p.numel() for p in self.model.parameters()):,}ê°œ")
    
    def compute_loss(self, predictions: Dict, targets: Dict, scenarios: List[str]) -> Dict:
        """ì†ì‹¤ ê³„ì‚°"""
        device = predictions["actions"].device
        
        # 1. ì•¡ì…˜ ì†ì‹¤ (MSE)
        action_loss = F.mse_loss(predictions["actions"], targets["actions"])
        
        # 2. ì´ë²¤íŠ¸ ì†ì‹¤ (Cross-entropy)
        B, T, num_classes = predictions["event_logits"].shape
        event_loss = F.cross_entropy(
            predictions["event_logits"].view(B * T, num_classes),
            targets["action_events"].view(B * T)
        )
        
        # 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜
        scenario_weights = torch.tensor([
            self.scenario_weights.get(scenario, 1.0) for scenario in scenarios
        ], device=device, dtype=torch.float32)
        
        scenario_weight = scenario_weights.mean()
        
        # ì´ ì†ì‹¤
        total_loss = (
            self.action_loss_weight * action_loss * scenario_weight +
            self.event_loss_weight * event_loss * scenario_weight
        )
        
        return {
            "total_loss": total_loss,
            "action_loss": action_loss,
            "event_loss": event_loss,
            "scenario_weight": scenario_weight
        }
    
    def compute_metrics(self, predictions: Dict, targets: Dict) -> Dict:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # ì•¡ì…˜ ì •í™•ë„ (í—ˆìš© ì˜¤ì°¨ 0.1 ì´ë‚´)
        action_diff = torch.abs(predictions["actions"] - targets["actions"])
        accurate_actions = (action_diff < 0.1).all(dim=-1)
        action_accuracy = accurate_actions.float().mean()
        
        # ì´ë²¤íŠ¸ ì •í™•ë„
        correct_events = (predictions["predicted_events"] == targets["action_events"])
        event_accuracy = correct_events.float().mean()
        
        return {
            "action_accuracy": action_accuracy,
            "event_accuracy": event_accuracy
        }
    
    def train_step(self, batch: Dict) -> Dict:
        """í•™ìŠµ ìŠ¤í…"""
        self.model.train()
        
        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                for k, v in batch.items()}
        
        # ìˆœì „íŒŒ
        predictions = self.model(
            images=batch["images"],
            scenarios=batch["scenario"],
            instructions=batch.get("instruction")
        )
        
        # íƒ€ê²Ÿ ì¤€ë¹„
        targets = {
            "actions": batch["actions"],
            "action_events": batch["action_events"]
        }
        
        # ì†ì‹¤ ê³„ì‚°
        losses = self.compute_loss(predictions, targets, batch["scenario"])
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        losses["total_loss"].backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        with torch.no_grad():
            metrics = self.compute_metrics(predictions, targets)
        
        # ê²°ê³¼ ë°˜í™˜
        result = {**losses, **metrics}
        return {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in result.items()}
    
    def val_step(self, batch: Dict) -> Dict:
        """ê²€ì¦ ìŠ¤í…"""
        self.model.eval()
        
        with torch.no_grad():
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # ìˆœì „íŒŒ
            predictions = self.model(
                images=batch["images"],
                scenarios=batch["scenario"],
                instructions=batch.get("instruction")
            )
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = {
                "actions": batch["actions"],
                "action_events": batch["action_events"]
            }
            
            # ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
            losses = self.compute_loss(predictions, targets, batch["scenario"])
            metrics = self.compute_metrics(predictions, targets)
            
            result = {**losses, **metrics}
            return {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in result.items()}
    
    def predict_mobile_action(self, current_image: torch.Tensor, scenario: str) -> Dict[str, float]:
        """mobile_vla_data_collector.py í˜¸í™˜ ì•¡ì…˜ ì˜ˆì¸¡"""
        self.model.eval()
        
        current_image = current_image.to(self.device)
        
        with torch.no_grad():
            mobile_action = self.model.get_mobile_vla_action(current_image, scenario)
        
        return mobile_action
    
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'configs': self.configs
        }, path)
        logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
    
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Simple Mobile VLA Trainer í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    test_configs = {
        "hidden_size": 512,
        "use_lite_mode": True,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Lite ëª¨ë“œ
        "learning_rate": 1e-4,
        "batch_size": 2,
        "sequence_length": 18,
        "max_epochs": 5
    }
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = SimpleMobileVLATrainer(test_configs)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, seq_len = 2, 18
    test_batch = {
        "images": torch.randn(batch_size, seq_len, 3, 224, 224),
        "actions": torch.randn(batch_size, seq_len, 3),  # ì •ê·œí™”ëœ ì•¡ì…˜
        "action_events": torch.randint(0, 3, (batch_size, seq_len)),  # ì´ë²¤íŠ¸ ì¸ë±ìŠ¤
        "scenario": ["1box_vert_left", "2box_hori_right"],
        "instruction": ["ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”", "ë‘ ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”"]
    }
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°°ì¹˜:")
    print(f"   ì´ë¯¸ì§€: {test_batch['images'].shape}")
    print(f"   ì•¡ì…˜: {test_batch['actions'].shape}")
    print(f"   ì´ë²¤íŠ¸: {test_batch['action_events'].shape}")
    print(f"   ì‹œë‚˜ë¦¬ì˜¤: {test_batch['scenario']}")
    
    # í•™ìŠµ ìŠ¤í… í…ŒìŠ¤íŠ¸
    train_result = trainer.train_step(test_batch)
    print(f"\nğŸ“ˆ í•™ìŠµ ê²°ê³¼:")
    for key, value in train_result.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    # ê²€ì¦ ìŠ¤í… í…ŒìŠ¤íŠ¸
    val_result = trainer.val_step(test_batch)
    print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
    for key, value in val_result.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.4f}")
        else:
            print(f"   {key}: {value}")
    
    # ë‹¨ì¼ ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    single_image = torch.randn(1, 3, 224, 224)
    mobile_action = trainer.predict_mobile_action(single_image, "1box_vert_left")
    print(f"\nğŸ¯ Mobile ì•¡ì…˜ ì˜ˆì¸¡: {mobile_action}")
    
    print(f"\nâœ… Simple Mobile VLA Trainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
