#!/usr/bin/env python3
"""
Original 72 Episodes CLIP ëª¨ë¸ì„ ì¦ê°• ë°ì´í„°ë¡œ í•™ìŠµ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from transformers import AutoProcessor, CLIPVisionModel, CLIPTextModel
import numpy as np
import logging
import argparse
from pathlib import Path
import json
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import h5py
from PIL import Image
import torchvision.transforms as transforms

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AugmentedDataset(Dataset):
    """ì¦ê°• ë°ì´í„°ì…‹"""
    
    def __init__(self, data_dir, processor, transform=None):
        self.data_dir = data_dir
        self.processor = processor
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])
        
        # ì¦ê°•ëœ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        self.episode_dirs = []
        for item in os.listdir(data_dir):
            item_path = os.path.join(data_dir, item)
            if os.path.isdir(item_path):
                self.episode_dirs.append(item_path)
        
        logger.info(f"ğŸ“Š ë¡œë“œëœ ì¦ê°• ì—í”¼ì†Œë“œ ìˆ˜: {len(self.episode_dirs)}")
        
        # ëª¨ë“  ë°ì´í„° ë¡œë“œ
        self.all_data = []
        for episode_dir in self.episode_dirs:
            episode_name = os.path.basename(episode_dir)
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
            image_files = []
            for file in os.listdir(episode_dir):
                if file.endswith('.jpg') or file.endswith('.png'):
                    image_files.append(os.path.join(episode_dir, file))
            
            image_files = sorted(image_files)
            
            # ì•¡ì…˜ íŒŒì¼ ë¡œë“œ
            actions_file = os.path.join(episode_dir, 'actions.npy')
            if os.path.exists(actions_file):
                actions = np.load(actions_file)
                
                # ê° ì´ë¯¸ì§€ì™€ ì•¡ì…˜ì„ ë§¤ì¹­
                for i, image_file in enumerate(image_files):
                    if i < len(actions):
                        # ì´ë¯¸ì§€ ë¡œë“œ
                        img = Image.open(image_file).convert('RGB')
                        
                        # ì•¡ì…˜ (linear_x, linear_yë§Œ ì‚¬ìš©)
                        action = actions[i][:2]  # 2D ì•¡ì…˜ë§Œ
                        
                        # í…ìŠ¤íŠ¸ ëª…ë ¹
                        text_command = f"Navigate around obstacle in {episode_name}"
                        
                        self.all_data.append({
                            'image': img,
                            'action': action,
                            'text': text_command
                        })
        
        logger.info(f"ğŸ“Š ì´ ì¦ê°• ìƒ˜í”Œ ìˆ˜: {len(self.all_data)}")
    
    def __len__(self):
        return len(self.all_data)
    
    def __getitem__(self, idx):
        data = self.all_data[idx]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = self.transform(data['image'])
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (CLIP ê¸°ë³¸ ê¸¸ì´ ì‚¬ìš©)
        text_inputs = self.processor(
            text=data['text'],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=77  # CLIP ê¸°ë³¸ ìµœëŒ€ ê¸¸ì´
        )
        
        # í…ì„œì—ì„œ ìŠ¤ì¹¼ë¼ ì¶”ì¶œ
        for key in text_inputs:
            text_inputs[key] = text_inputs[key].squeeze(0)
        
        # ì•¡ì…˜ì„ í…ì„œë¡œ ë³€í™˜
        action = torch.tensor(data['action'], dtype=torch.float32)
        
        return {
            'image': image,
            'text_inputs': text_inputs,
            'action': action
        }

class OriginalCLIPModel(nn.Module):
    """Original 72 Episodes CLIP ëª¨ë¸"""
    
    def __init__(self, processor, hidden_dim=512, dropout=0.2):
        super().__init__()
        self.processor = processor
        
        # CLIP ëª¨ë¸ë“¤
        self.clip_vision = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_text = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Feature Fusion
        self.fusion = nn.Sequential(
            nn.Linear(768 + 512, hidden_dim),  # CLIP Vision + Text
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Action Head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2)  # 2D action
        )
    
    def forward(self, images, text_inputs):
        # CLIP Vision
        vision_outputs = self.clip_vision(images)
        vision_features = vision_outputs.pooler_output  # [batch, 768]
        
        # CLIP Text
        text_outputs = self.clip_text(**text_inputs)
        text_features = text_outputs.pooler_output  # [batch, 512]
        
        # Feature Fusion
        combined = torch.cat([vision_features, text_features], dim=1)
        fused = self.fusion(combined)
        
        # Action Prediction
        actions = self.action_head(fused)
        
        return actions

class Trainer:
    """í›ˆë ¨ê¸°"""
    
    def __init__(self, model, device, learning_rate=5e-5, weight_decay=1e-3):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.5)
        self.criterion = nn.MSELoss()
    
    def train_step(self, batch):
        self.model.train()
        
        images = batch['image'].to(self.device)
        text_inputs = {k: v.to(self.device) for k, v in batch['text_inputs'].items()}
        targets = batch['action'].to(self.device)
        
        self.optimizer.zero_grad()
        
        predictions = self.model(images, text_inputs)
        loss = self.criterion(predictions, targets)
        
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def validate(self, dataloader):
        self.model.eval()
        total_loss = 0.0
        total_mae = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in dataloader:
                images = batch['image'].to(self.device)
                text_inputs = {k: v.to(self.device) for k, v in batch['text_inputs'].items()}
                targets = batch['action'].to(self.device)
                
                predictions = self.model(images, text_inputs)
                loss = self.criterion(predictions, targets)
                
                # MAE ê³„ì‚°
                mae = torch.mean(torch.abs(predictions - targets))
                
                total_loss += loss.item()
                total_mae += mae.item()
                num_batches += 1
        
        return total_loss / num_batches, total_mae / num_batches

def train_original_clip_augmented(data_path, output_dir, num_epochs=10, batch_size=4, 
                                learning_rate=5e-5, weight_decay=1e-3):
    """Original CLIP ëª¨ë¸ì„ ì¦ê°• ë°ì´í„°ë¡œ í›ˆë ¨"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    logger.info("ğŸ“¥ CLIP í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    logger.info("ğŸ“Š ì¦ê°• ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    dataset = AugmentedDataset(data_path, processor)
    
    # ì»¤ìŠ¤í…€ collate í•¨ìˆ˜
    def custom_collate(batch):
        images = torch.stack([item['image'] for item in batch])
        actions = torch.stack([item['action'] for item in batch])
        
        # í…ìŠ¤íŠ¸ ì…ë ¥ë“¤ì„ ë™ì¼í•œ í¬ê¸°ë¡œ íŒ¨ë”©
        text_inputs = {}
        for key in batch[0]['text_inputs'].keys():
            max_len = max(item['text_inputs'][key].size(0) for item in batch)
            padded_tensors = []
            for item in batch:
                tensor = item['text_inputs'][key]
                if tensor.size(0) < max_len:
                    # íŒ¨ë”©
                    padding = torch.zeros(max_len - tensor.size(0), dtype=tensor.dtype)
                    tensor = torch.cat([tensor, padding])
                padded_tensors.append(tensor)
            text_inputs[key] = torch.stack(padded_tensors)
        
        return {
            'image': images,
            'text_inputs': text_inputs,
            'action': actions
        }
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=custom_collate)
    
    # ëª¨ë¸ ìƒì„±
    logger.info("ğŸ¤– Original CLIP ëª¨ë¸ ìƒì„± ì¤‘...")
    model = OriginalCLIPModel(processor, hidden_dim=512, dropout=0.2)
    
    # í›ˆë ¨ê¸° ìƒì„±
    trainer = Trainer(model, device, learning_rate, weight_decay)
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses = []
    val_losses = []
    val_maes = []
    best_mae = float('inf')
    
    logger.info(f"ğŸ¯ í›ˆë ¨ ì„¤ì •:")
    logger.info(f"   - ëª¨ë¸: Original CLIP (ì¦ê°• ë°ì´í„°)")
    logger.info(f"   - ì—í¬í¬: {num_epochs}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í•™ìŠµë¥ : {learning_rate}")
    logger.info(f"   - Weight Decay: {weight_decay}")
    
    # í›ˆë ¨ ë£¨í”„
    for epoch in range(num_epochs):
        logger.info(f"\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        train_pbar = tqdm(dataloader, desc=f"Training Epoch {epoch+1}")
        for batch in train_pbar:
            loss = trainer.train_step(batch)
            train_loss += loss
            train_batches += 1
            train_pbar.set_postfix({'Loss': f'{loss:.6f}'})
        
        avg_train_loss = train_loss / train_batches
        train_losses.append(avg_train_loss)
        
        # ê²€ì¦ ë‹¨ê³„
        val_loss, val_mae = trainer.validate(dataloader)
        val_losses.append(val_loss)
        val_maes.append(val_mae)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        trainer.scheduler.step()
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"   ğŸ“Š í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.6f}")
        logger.info(f"   ğŸ“Š ê²€ì¦ ì†ì‹¤: {val_loss:.6f}")
        logger.info(f"   ğŸ“Š ê²€ì¦ MAE: {val_mae:.6f}")
        logger.info(f"   ğŸ“Š í•™ìŠµë¥ : {trainer.optimizer.param_groups[0]['lr']:.2e}")
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if val_mae < best_mae:
            best_mae = val_mae
            best_checkpoint_path = output_path / f"best_original_clip_augmented_epoch_{epoch+1}.pth"
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'epoch': epoch + 1,
                'val_loss': val_loss,
                'val_mae': val_mae,
                'train_loss': avg_train_loss
            }, best_checkpoint_path)
            logger.info(f"   ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! MAE: {best_mae:.6f}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_checkpoint_path = output_path / "final_original_clip_augmented.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': trainer.optimizer.state_dict(),
        'epoch': epoch + 1,
        'val_loss': val_loss,
        'val_mae': val_mae,
        'train_loss': avg_train_loss
    }, final_checkpoint_path)
    
    # í›ˆë ¨ ê²°ê³¼ ì €ì¥
    training_results = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_maes': val_maes,
        'best_mae': best_mae,
        'final_epoch': epoch + 1
    }
    
    with open(output_path / 'training_results.json', 'w') as f:
        json.dump(training_results, f, indent=2)
    
    logger.info(f"âœ… Original CLIP (ì¦ê°• ë°ì´í„°) í›ˆë ¨ ì™„ë£Œ!")
    logger.info(f"   ìµœê³  MAE: {best_mae:.6f}")
    logger.info(f"   ìµœì¢… MAE: {val_mae:.6f}")
    
    return model, trainer

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Train Original CLIP Model with Augmented Data')
    parser.add_argument('--data_path', type=str, default='legacy/augmented_dataset', help='Path to augmented dataset')
    parser.add_argument('--output_dir', type=str, default='original_clip_augmented_results', help='Output directory')
    parser.add_argument('--num_epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch_size', type=int, default=4, help='Batch size')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-3, help='Weight decay')
    
    args = parser.parse_args()
    
    # í›ˆë ¨ ì‹¤í–‰
    model, trainer = train_original_clip_augmented(
        data_path=args.data_path,
        output_dir=args.output_dir,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay
    )

if __name__ == "__main__":
    main()
