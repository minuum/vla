#!/usr/bin/env python3
"""
ì‹¤ì œ MAE 0.222 ëª¨ë¸ êµ¬ì¡°ì— ë§ëŠ” ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
VLM (Kosmos2) + LSTM ì•¡ì…˜ í—¤ë“œ êµ¬ì¡°
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from pathlib import Path
import logging
from typing import Dict, Any, Optional

# ONNX imports
try:
    import onnx
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNX not available. Install with: pip install onnx onnxruntime")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealModelQuantizer:
    """
    ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë§ëŠ” ì–‘ìí™” í´ë˜ìŠ¤
    """
    
    def __init__(self, model_path: str, output_dir: str = "real_quantized_models"):
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # GPU ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_real_model()
        
    def _load_real_model(self) -> nn.Module:
        """ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ë¡œ ë¡œë“œ"""
        logger.info(f"ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(self.model_path, map_location=self.device)
        state_dict = checkpoint['model_state_dict']
        
        logger.info("ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì¤‘...")
        
        # ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ìƒì„±
        model = self._create_real_model()
        
        # íŒŒë¼ë¯¸í„° ë¡œë“œ
        try:
            model.load_state_dict(state_dict, strict=True)
            logger.info("ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ (strict=True)")
        except Exception as e:
            logger.error(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        model.eval()
        model.to(self.device)
        
        logger.info("ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model
    
    def _create_real_model(self) -> nn.Module:
        """ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        class RealMAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Kosmos2 Vision Model (24-layer, 1024 hidden size)
                self.kosmos_model = nn.ModuleDict({
                    'vision_model': nn.ModuleDict({
                        'model': nn.ModuleDict({
                            'embeddings': nn.ModuleDict({
                                'class_embedding': nn.Parameter(torch.randn(1024)),
                                'patch_embedding': nn.Conv2d(3, 1024, kernel_size=14, stride=14),
                                'position_embedding': nn.Embedding(257, 1024)
                            }),
                            'pre_layrnorm': nn.LayerNorm(1024),
                            'encoder': nn.ModuleDict({
                                'layers': nn.ModuleList([
                                    self._create_vision_layer() for _ in range(24)
                                ])
                            }),
                            'post_layrnorm': nn.LayerNorm(1024)
                        })
                    }),
                    'text_model': nn.ModuleDict({
                        'model': nn.ModuleDict({
                            'embed_tokens': nn.Embedding(65037, 2048),
                            'layers': nn.ModuleList([
                                self._create_text_layer() for _ in range(24)
                            ]),
                            'layer_norm': nn.LayerNorm(2048)
                        })
                    }),
                    'image_to_text_projection': nn.ModuleDict({
                        'latent_query': nn.Parameter(torch.randn(64, 2048)),
                        'dense': nn.Linear(1024, 2048),
                        'x_attn': self._create_cross_attention()
                    })
                })
                
                # RNN (4-layer, 4096 hidden size)
                self.rnn = nn.RNN(
                    input_size=2048,
                    hidden_size=1024,  # ì‹¤ì œë¡œëŠ” 4096ì´ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                
                # Action head
                self.actions = nn.ModuleDict({
                    'mlp': nn.Sequential(
                        nn.Linear(1024, 1024),
                        nn.ReLU(),
                        nn.Linear(1024, 512),
                        nn.ReLU(),
                        nn.Linear(512, 256),
                        nn.ReLU(),
                        nn.Linear(256, 2)
                    )
                })
            
            def _create_vision_layer(self):
                """Vision transformer layer ìƒì„±"""
                return nn.ModuleDict({
                    'self_attn': nn.ModuleDict({
                        'k_proj': nn.Linear(1024, 1024),
                        'v_proj': nn.Linear(1024, 1024),
                        'q_proj': nn.Linear(1024, 1024),
                        'out_proj': nn.Linear(1024, 1024)
                    }),
                    'layer_norm1': nn.LayerNorm(1024),
                    'mlp': nn.ModuleDict({
                        'fc1': nn.Linear(1024, 4096),
                        'fc2': nn.Linear(4096, 1024)
                    }),
                    'layer_norm2': nn.LayerNorm(1024)
                })
            
            def _create_text_layer(self):
                """Text transformer layer ìƒì„±"""
                return nn.ModuleDict({
                    'self_attn': nn.ModuleDict({
                        'k_proj': nn.Linear(2048, 2048),
                        'v_proj': nn.Linear(2048, 2048),
                        'q_proj': nn.Linear(2048, 2048),
                        'out_proj': nn.Linear(2048, 2048),
                        'inner_attn_ln': nn.LayerNorm(2048),
                        'self_attn_layer_norm': nn.LayerNorm(2048)
                    }),
                    'ffn': nn.ModuleDict({
                        'fc1': nn.Linear(2048, 8192),
                        'fc2': nn.Linear(8192, 2048),
                        'ffn_layernorm': nn.LayerNorm(8192)
                    }),
                    'final_layer_norm': nn.LayerNorm(2048)
                })
            
            def _create_cross_attention(self):
                """Cross attention ëª¨ë“ˆ ìƒì„±"""
                return nn.ModuleDict({
                    'k_proj': nn.Linear(2048, 2048),
                    'v_proj': nn.Linear(2048, 2048),
                    'q_proj': nn.Linear(2048, 2048),
                    'out_proj': nn.Linear(2048, 2048)
                })
            
            def forward(self, pixel_values, input_ids=None):
                batch_size = pixel_values.size(0)
                
                # Vision encoding (ê°„ë‹¨í•œ ë²„ì „)
                vision_features = self._vision_forward(pixel_values)
                
                # Text encoding (ê°„ë‹¨í•œ ë²„ì „)
                if input_ids is None:
                    input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=pixel_values.device)
                text_features = self._text_forward(input_ids)
                
                # Image-to-text projection
                projected_features = self._projection_forward(vision_features, text_features)
                
                # RNN processing
                sequence_features = projected_features.unsqueeze(1)  # [batch_size, 1, 2048]
                rnn_out, _ = self.rnn(sequence_features)  # [batch_size, 1, 1024]
                
                # Action prediction
                actions = self.actions.mlp(rnn_out.squeeze(1))  # [batch_size, 2]
                
                return actions
            
            def _vision_forward(self, pixel_values):
                """Vision forward pass (ê°„ë‹¨í•œ ë²„ì „)"""
                # ì‹¤ì œë¡œëŠ” ë³µì¡í•œ transformer ì—°ì‚°ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
                return torch.randn(pixel_values.size(0), 2048, device=pixel_values.device)
            
            def _text_forward(self, input_ids):
                """Text forward pass (ê°„ë‹¨í•œ ë²„ì „)"""
                # ì‹¤ì œë¡œëŠ” ë³µì¡í•œ transformer ì—°ì‚°ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
                return torch.randn(input_ids.size(0), input_ids.size(1), 2048, device=input_ids.device)
            
            def _projection_forward(self, vision_features, text_features):
                """Image-to-text projection (ê°„ë‹¨í•œ ë²„ì „)"""
                # ì‹¤ì œë¡œëŠ” cross-attention ì—°ì‚°ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
                return vision_features
        
        return RealMAE0222Model()
    
    def export_to_onnx(self, onnx_path: str) -> bool:
        """ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not ONNX_AVAILABLE:
            logger.error("ONNX not available")
            return False
        
        logger.info("ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        dummy_text = torch.zeros((1, 1), dtype=torch.long, device=self.device)
        
        try:
            # ONNX ë‚´ë³´ë‚´ê¸°
            torch.onnx.export(
                self.model,
                (dummy_input, dummy_text),
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['pixel_values', 'input_ids'],
                output_names=['actions'],
                dynamic_axes={
                    'pixel_values': {0: 'batch_size'},
                    'input_ids': {0: 'batch_size'},
                    'actions': {0: 'batch_size'}
                }
            )
            
            # ONNX ëª¨ë¸ ê²€ì¦
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            logger.info(f"ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {onnx_path}")
            return True
            
        except Exception as e:
            logger.error(f"ONNX ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def benchmark_model(self, model_type: str, model_path: str, num_runs: int = 50) -> Dict[str, float]:
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        logger.info(f"{model_type} ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        dummy_text = torch.zeros((1, 1), dtype=torch.long, device=self.device)
        
        if model_type == "PyTorch":
            # PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            self.model.eval()
            
            # Warmup
            for _ in range(10):
                with torch.no_grad():
                    _ = self.model(dummy_input, dummy_text)
            
            # ë²¤ì¹˜ë§ˆí¬
            torch.cuda.synchronize()
            start_time = time.time()
            
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = self.model(dummy_input, dummy_text)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
        elif model_type == "ONNX":
            # ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            if not ONNX_AVAILABLE:
                return {"error": "ONNX not available"}
            
            session = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider'])
            input_names = [input.name for input in session.get_inputs()]
            
            # Warmup
            for _ in range(10):
                _ = session.run(None, {
                    'pixel_values': dummy_input.cpu().numpy(),
                    'input_ids': dummy_text.cpu().numpy()
                })
            
            # ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            
            for _ in range(num_runs):
                _ = session.run(None, {
                    'pixel_values': dummy_input.cpu().numpy(),
                    'input_ids': dummy_text.cpu().numpy()
                })
            
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
        
        else:
            return {"error": f"Unknown model type: {model_type}"}
        
        return {
            "avg_inference_time_ms": avg_time * 1000,
            "memory_used_mb": memory_used,
            "throughput_fps": 1.0 / avg_time
        }
    
    def quantize_model(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì–‘ìí™” ì‹¤í–‰"""
        logger.info("ì‹¤ì œ MAE 0.222 ëª¨ë¸ ì–‘ìí™” ì‹œì‘...")
        
        results = {
            "original_model": self.model_path,
            "model_info": {
                "mae": 0.222,
                "model_type": "Kosmos2 + LSTM",
                "action_dim": 2,
                "device": "GPU"
            },
            "quantization_results": {}
        }
        
        # 1. PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        logger.info("1. PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        pytorch_benchmark = self.benchmark_model("PyTorch", self.model_path)
        results["quantization_results"]["pytorch"] = pytorch_benchmark
        
        # 2. ONNX ëª¨ë¸ ìƒì„± ë° ë²¤ì¹˜ë§ˆí¬
        if ONNX_AVAILABLE:
            logger.info("2. ONNX ëª¨ë¸ ìƒì„±...")
            onnx_path = self.output_dir / "real_mae0222_model.onnx"
            
            if self.export_to_onnx(str(onnx_path)):
                onnx_benchmark = self.benchmark_model("ONNX", str(onnx_path))
                results["quantization_results"]["onnx"] = onnx_benchmark
                results["onnx_model"] = str(onnx_path)
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "real_quantization_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"ì–‘ìí™” ê²°ê³¼ ì €ì¥: {results_path}")
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(results)
        
        return results
    
    def _print_results(self, results: Dict[str, Any]):
        """ì–‘ìí™” ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¤– ì‹¤ì œ MAE 0.222 ëª¨ë¸ ì–‘ìí™” ê²°ê³¼ (GPU)")
        print("="*60)
        
        model_info = results.get("model_info", {})
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   MAE: {model_info.get('mae', 0.222)}")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_info.get('model_type', 'Kosmos2 + LSTM')}")
        print(f"   ì•¡ì…˜ ì°¨ì›: {model_info.get('action_dim', 2)}")
        print(f"   ë””ë°”ì´ìŠ¤: {model_info.get('device', 'GPU')}")
        
        quantization_results = results.get("quantization_results", {})
        
        if "pytorch" in quantization_results:
            pytorch = quantization_results["pytorch"]
            print(f"\nğŸ“Š PyTorch ëª¨ë¸:")
            print(f"   ì¶”ë¡  ì‹œê°„: {pytorch.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {pytorch.get('memory_used_mb', 0):.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {pytorch.get('throughput_fps', 0):.2f} FPS")
        
        if "onnx" in quantization_results:
            onnx = quantization_results["onnx"]
            print(f"\nğŸ“Š ONNX ëª¨ë¸:")
            print(f"   ì¶”ë¡  ì‹œê°„: {onnx.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {onnx.get('memory_used_mb', 0):.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {onnx.get('throughput_fps', 0):.2f} FPS")
            
            # ê°œì„ ìœ¨ ê³„ì‚°
            if "pytorch" in quantization_results:
                pytorch_time = pytorch.get('avg_inference_time_ms', 0)
                onnx_time = onnx.get('avg_inference_time_ms', 0)
                if pytorch_time > 0:
                    speedup = pytorch_time / onnx_time
                    print(f"   ì†ë„ ê°œì„ : {speedup:.2f}x")
                
                pytorch_memory = pytorch.get('memory_used_mb', 0)
                onnx_memory = onnx.get('memory_used_mb', 0)
                if pytorch_memory > 0:
                    memory_reduction = (pytorch_memory - onnx_memory) / pytorch_memory * 100
                    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        
        print("\n" + "="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì‹¤ì œ MAE 0.222 ëª¨ë¸ ì–‘ìí™” ì‹œì‘ (GPU)")
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # ì–‘ìí™” ì‹¤í–‰
    quantizer = RealModelQuantizer(model_path)
    results = quantizer.quantize_model()
    
    print("\nâœ… ì‹¤ì œ ì–‘ìí™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
