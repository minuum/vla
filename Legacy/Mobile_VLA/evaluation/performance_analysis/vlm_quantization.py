#!/usr/bin/env python3
"""
ğŸ¯ VLM + Action Head í†µí•© ì–‘ìí™”
VLM(Kosmos2)ê³¼ Action Head ëª¨ë‘ì— ì–‘ìí™” ì ìš©
"""

import torch
import torch.nn as nn
import torch.quantization as quantization
import time
import psutil
import os
import json
import logging
from pathlib import Path
from transformers import AutoProcessor, AutoModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VLMQuantizedModel(nn.Module):
    """VLMê³¼ Action Head ëª¨ë‘ ì–‘ìí™”ëœ ëª¨ë¸"""
    
    def __init__(self, model_path: str):
        super().__init__()
        
        # Kosmos2 ëª¨ë¸ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # Action Head (LSTM + MLP)
        self.rnn = nn.RNN(
            input_size=2048,  # Kosmos2 hidden size
            hidden_size=4096,
            num_layers=4,
            batch_first=True,
            dropout=0.1
        )
        
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # linear_x, linear_y
        )
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ Action Headë§Œ ë¡œë“œ
        self._load_action_head(model_path)
        
        # VLMì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (í›ˆë ¨í•˜ì§€ ì•ŠìŒ)
        self.kosmos.eval()
        for param in self.kosmos.parameters():
            param.requires_grad = False
    
    def _load_action_head(self, model_path: str):
        """Action Headë§Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dictì—ì„œ Action Head ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
            action_state_dict = {}
            for key, value in checkpoint['model_state_dict'].items():
                if 'rnn' in key or 'actions' in key:
                    action_state_dict[key] = value
            
            # Action Headë§Œ ë¡œë“œ
            self.load_state_dict(action_state_dict, strict=False)
            logger.info(f"âœ… Action Head ë¡œë“œ ì™„ë£Œ: {len(action_state_dict)} íŒŒë¼ë¯¸í„°")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Action Head ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ Action Head ì‚¬ìš©")
    
    def forward(self, x):
        """ìˆœì „íŒŒ (VLM + Action Head)"""
        batch_size = x.size(0)
        
        # 1. VLMìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        with torch.no_grad():
            # ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
            dummy_text = ["<image>"] * batch_size
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = self.processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(x.device)
            
            # Kosmos2ë¡œ íŠ¹ì§• ì¶”ì¶œ
            vision_outputs = self.kosmos(
                pixel_values=x,
                input_ids=text_inputs['input_ids'],
                attention_mask=text_inputs['attention_mask']
            )
            
            # íŠ¹ì§• ì¶”ì¶œ (ì²« ë²ˆì§¸ í† í°)
            vision_features = vision_outputs.last_hidden_state[:, 0]  # [batch_size, 2048]
        
        # 2. Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡
        sequence_features = vision_features.unsqueeze(1)  # [batch_size, 1, 2048]
        rnn_out, _ = self.rnn(sequence_features)  # [batch_size, 1, 4096]
        actions = self.actions(rnn_out.squeeze(1))  # [batch_size, 2]
        
        return actions

class VLMQuantizer:
    """VLM + Action Head í†µí•© ì–‘ìí™”ê¸°"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        self.original_model = VLMQuantizedModel(model_path).to(self.device)
        self.original_model.eval()
        
        logger.info(f"ğŸš€ VLM Quantizer ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    def benchmark_model(self, model, name: str, num_runs: int = 100):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
        
        # ì›Œë°ì—…
        for _ in range(10):
            _ = model(dummy_input)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        for _ in range(num_runs):
            with torch.no_grad():
                _ = model(dummy_input)
        
        end_time = time.time()
        inference_time = (end_time - start_time) / num_runs * 1000  # ms
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = memory_after - memory_before
        
        fps = 1000 / inference_time if inference_time > 0 else 0
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': inference_time,
            'memory_usage_mb': memory_usage,
            'fps': fps
        }
    
    def quantize_to_fp16(self):
        """FP16 ì–‘ìí™” (VLM + Action Head)"""
        logger.info("ğŸ”§ FP16 ì–‘ìí™” ì‹œì‘...")
        
        try:
            # ëª¨ë¸ì„ FP16ìœ¼ë¡œ ë³€í™˜
            fp16_model = self.original_model.half()
            fp16_model.eval()
            
            # ë²¤ì¹˜ë§ˆí¬
            benchmark = self.benchmark_model(fp16_model, "FP16")
            
            # ëª¨ë¸ ì €ì¥
            torch.save(fp16_model.state_dict(), 'vlm_fp16_model.pth')
            logger.info("ğŸ’¾ FP16 ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
            return fp16_model, benchmark
            
        except Exception as e:
            logger.error(f"âŒ FP16 ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return None, None
    
    def quantize_to_int8(self):
        """INT8 ì–‘ìí™” (Action Headë§Œ)"""
        logger.info("ğŸ”§ INT8 ì–‘ìí™” ì‹œì‘...")
        
        try:
            # Action Headë§Œ INT8 ì–‘ìí™” (VLMì€ ë³µì¡í•´ì„œ ì œì™¸)
            int8_model = quantization.quantize_dynamic(
                self.original_model,
                {nn.Linear, nn.RNN},  # Action Headì˜ Linearì™€ RNNë§Œ ì–‘ìí™”
                dtype=torch.qint8
            )
            int8_model.eval()
            
            # ë²¤ì¹˜ë§ˆí¬
            benchmark = self.benchmark_model(int8_model, "INT8")
            
            # ëª¨ë¸ ì €ì¥
            torch.save(int8_model.state_dict(), 'vlm_int8_model.pth')
            logger.info("ğŸ’¾ INT8 ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
            return int8_model, benchmark
            
        except Exception as e:
            logger.error(f"âŒ INT8 ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return None, None
    
    def quantize_model(self):
        """ì „ì²´ ì–‘ìí™” í”„ë¡œì„¸ìŠ¤"""
        logger.info("ğŸ¯ VLM + Action Head í†µí•© ì–‘ìí™” ì‹œì‘!")
        
        results = {}
        
        # 1. FP32 ë²¤ì¹˜ë§ˆí¬ (ì›ë³¸)
        logger.info("1. FP32 ë²¤ì¹˜ë§ˆí¬...")
        fp32_benchmark = self.benchmark_model(self.original_model, "FP32")
        results['fp32'] = fp32_benchmark
        
        # 2. FP16 ì–‘ìí™”
        logger.info("2. FP16 ì–‘ìí™”...")
        fp16_model, fp16_benchmark = self.quantize_to_fp16()
        if fp16_benchmark:
            results['fp16'] = fp16_benchmark
        
        # 3. INT8 ì–‘ìí™”
        logger.info("3. INT8 ì–‘ìí™”...")
        int8_model, int8_benchmark = self.quantize_to_int8()
        if int8_benchmark:
            results['int8'] = int8_benchmark
        
        # 4. ê²°ê³¼ ë¹„êµ
        logger.info("4. ê²°ê³¼ ë¹„êµ...")
        self._compare_results(results)
        
        # 5. ê²°ê³¼ ì €ì¥
        with open('vlm_quantization_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: vlm_quantization_results.json")
        
        return results
    
    def _compare_results(self, results):
        """ê²°ê³¼ ë¹„êµ ë° ë¶„ì„"""
        logger.info("\nğŸ“Š ì–‘ìí™” ê²°ê³¼ ë¹„êµ:")
        logger.info("=" * 60)
        
        fp32 = results.get('fp32', {})
        fp16 = results.get('fp16', {})
        int8 = results.get('int8', {})
        
        if fp32 and fp16:
            speedup_fp16 = fp32['inference_time_ms'] / fp16['inference_time_ms']
            memory_save_fp16 = (fp32['memory_usage_mb'] - fp16['memory_usage_mb']) / fp32['memory_usage_mb'] * 100
            
            logger.info(f"FP16 vs FP32:")
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup_fp16:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save_fp16:.1f}%")
        
        if fp32 and int8:
            speedup_int8 = fp32['inference_time_ms'] / int8['inference_time_ms']
            memory_save_int8 = (fp32['memory_usage_mb'] - int8['memory_usage_mb']) / fp32['memory_usage_mb'] * 100
            
            logger.info(f"INT8 vs FP32:")
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup_int8:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save_int8:.1f}%")
        
        # ìµœì  ê¶Œì¥ì‚¬í•­
        logger.info("\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
        if fp16 and int8:
            if fp16['fps'] > int8['fps']:
                logger.info("   ğŸ† FP16 ê¶Œì¥: ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„")
            else:
                logger.info("   ğŸ† INT8 ê¶Œì¥: ë” ì‘ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        elif fp16:
            logger.info("   ğŸ† FP16 ê¶Œì¥: ì•ˆì •ì ì¸ ì„±ëŠ¥")
        elif int8:
            logger.info("   ğŸ† INT8 ê¶Œì¥: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ VLM + Action Head í†µí•© ì–‘ìí™” ì‹œì‘")
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # ì–‘ìí™” ì‹¤í–‰
    quantizer = VLMQuantizer(model_path)
    results = quantizer.quantize_model()
    
    logger.info("ğŸ‰ VLM + Action Head í†µí•© ì–‘ìí™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
