#!/usr/bin/env python3
"""
ì‹¤ì œ ì„±ëŠ¥ ê°œì„  ì¸¡ì • í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
MAE 0.222 ëª¨ë¸ê³¼ ì–‘ìí™”ëœ ëª¨ë¸ì˜ ì •í™•í•œ ë¹„êµ
"""

import torch
import torch.nn as nn
import time
import json
import logging
from transformers import CLIPProcessor, CLIPModel
import os
import gc

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ActualPerformanceVerification:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_runs = 50
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def _load_actual_mae0222_model(self):
        """ì‹¤ì œ MAE 0.222 ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        try:
            # ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            checkpoint_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
            
            if not os.path.exists(checkpoint_path):
                logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
                return None, None
            
            logger.info(f"ğŸ“ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
            logger.info(f"   - ì—í¬í¬: {checkpoint.get('epoch', 'N/A')}")
            logger.info(f"   - ê²€ì¦ MAE: {checkpoint.get('val_mae', 'N/A')}")
            logger.info(f"   - ëª¨ë¸ í‚¤ ìˆ˜: {len(checkpoint.get('model_state_dict', {}))}")
            
            return checkpoint, checkpoint.get('val_mae', None)
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _create_actual_model_structure(self):
        """ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ìƒì„± (MAE 0.222 ëª¨ë¸ê³¼ ë™ì¼)"""
        class ActualMAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                # ì‹¤ì œ MAE 0.222 ëª¨ë¸ êµ¬ì¡° (simple_lstm_model.py ê¸°ë°˜)
                self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.clip.eval()
                for param in self.clip.parameters():
                    param.requires_grad = False
                
                # ì‹¤ì œ Action Head êµ¬ì¡° (MAE 0.222 ëª¨ë¸ê³¼ ë™ì¼)
                self.rnn = nn.RNN(
                    input_size=512,  # CLIP ì¶œë ¥ í¬ê¸°
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    # CLIP ì´ë¯¸ì§€ ì¸ì½”ë”©
                    image_features = self.clip.get_image_features(pixel_values=x)
                    
                sequence_features = image_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        return ActualMAE0222Model()
    
    def _create_quantized_model_structure(self):
        """ì–‘ìí™”ëœ ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        class QuantizedMAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                # CLIPì„ FP16ìœ¼ë¡œ ì–‘ìí™”
                self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.clip = self.clip.half()  # FP16 ì–‘ìí™”
                self.clip.eval()
                for param in self.clip.parameters():
                    param.requires_grad = False
                
                # Action HeadëŠ” FP32 ìœ ì§€ (ì‹¤ì œ êµ¬ì¡°ì™€ ë™ì¼)
                self.rnn = nn.RNN(
                    input_size=512,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    x_fp16 = x.half()  # ì…ë ¥ì„ FP16ìœ¼ë¡œ
                    image_features = self.clip.get_image_features(pixel_values=x_fp16)
                
                image_features_fp32 = image_features.float()  # FP32ë¡œ ë³€í™˜
                sequence_features = image_features_fp32.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        return QuantizedMAE0222Model()
    
    def _benchmark_model(self, model, name, checkpoint=None):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
        if checkpoint:
            try:
                model.load_state_dict(checkpoint['model_state_dict'])
                logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ (MAE: {checkpoint.get('val_mae', 'N/A')})")
            except Exception as e:
                logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.info("   ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸")
        
        model = model.to(self.device)
        model.eval()
        
        # ì…ë ¥ ë°ì´í„° ìƒì„±
        batch_size = 1
        input_data = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        # ì›Œë°ì—…
        for _ in range(5):
            with torch.no_grad():
                _ = model(input_data)
        
        # ì •í™•í•œ ë©”ëª¨ë¦¬ ì¸¡ì •
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            start_memory = torch.cuda.memory_allocated()
        else:
            start_memory = 0
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        outputs = []
        
        for _ in range(self.num_runs):
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            outputs.append(output.detach().cpu())
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            end_memory = torch.cuda.memory_allocated()
            peak_memory = torch.cuda.max_memory_allocated()
            
            memory_used = (end_memory - start_memory) / (1024 ** 2)  # MB
            peak_memory_used = peak_memory / (1024 ** 2)  # MB
        else:
            memory_used = 0
            peak_memory_used = 0
        
        avg_time = sum(times) / len(times)
        fps = 1000 / avg_time
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {avg_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {peak_memory_used:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': avg_time,
            'memory_usage_mb': memory_used,
            'peak_memory_mb': peak_memory_used,
            'fps': fps,
            'outputs': outputs
        }
    
    def verify_actual_performance(self):
        """ì‹¤ì œ ì„±ëŠ¥ ê°œì„  ì¸¡ì • í™•ì¸"""
        logger.info("ğŸš€ ì‹¤ì œ ì„±ëŠ¥ ê°œì„  ì¸¡ì • í™•ì¸ ì‹œì‘")
        
        # 1. ì‹¤ì œ MAE 0.222 ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint, actual_mae = self._load_actual_mae0222_model()
        
        if actual_mae:
            logger.info(f"âœ… ì‹¤ì œ MAE 0.222 ëª¨ë¸ í™•ì¸: {actual_mae:.6f}")
        else:
            logger.warning("âš ï¸ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨, êµ¬ì¡°ë§Œ ë¹„êµ")
        
        # 2. ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° ìƒì„±
        actual_model = self._create_actual_model_structure()
        quantized_model = self._create_quantized_model_structure()
        
        # 3. ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
        actual_results = self._benchmark_model(actual_model, "ì‹¤ì œ MAE 0.222 ëª¨ë¸", checkpoint)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del actual_model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        quantized_results = self._benchmark_model(quantized_model, "ì–‘ìí™”ëœ ëª¨ë¸ (FP16)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del quantized_model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # 4. ì„±ëŠ¥ ë¹„êµ
        speedup = actual_results['inference_time_ms'] / quantized_results['inference_time_ms']
        memory_save = 0
        if actual_results['memory_usage_mb'] > 0:
            memory_save = (actual_results['memory_usage_mb'] - quantized_results['memory_usage_mb']) / actual_results['memory_usage_mb'] * 100
        
        # 5. ì¶œë ¥ ë¹„êµ (ì •í™•ë„ í™•ì¸)
        accuracy_info = self._compare_outputs(actual_results['outputs'], quantized_results['outputs'])
        
        # 6. ê²°ê³¼ ì¶œë ¥
        logger.info("\nğŸ“Š ì‹¤ì œ ì„±ëŠ¥ ê°œì„  ì¸¡ì • ê²°ê³¼:")
        logger.info("=" * 60)
        logger.info(f"ì‹¤ì œ MAE: {actual_mae:.6f}" if actual_mae else "ì‹¤ì œ MAE: í™•ì¸ ë¶ˆê°€")
        logger.info(f"ì†ë„ í–¥ìƒ: {speedup:.2f}x")
        logger.info(f"ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save:.1f}%")
        logger.info(f"ì¶œë ¥ ì •í™•ë„: {accuracy_info}")
        
        # 7. ê²°ê³¼ ì €ì¥
        results = {
            'actual_mae': actual_mae,
            'actual_model': actual_results,
            'quantized_model': quantized_results,
            'improvement': {
                'speedup': speedup,
                'memory_save': memory_save,
                'accuracy_info': accuracy_info
            },
            'verification': {
                'checkpoint_loaded': checkpoint is not None,
                'actual_structure_used': True,
                'quantization_applied': True
            }
        }
        
        with open('actual_performance_verification.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("âœ… ê²°ê³¼ê°€ actual_performance_verification.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return results
    
    def _compare_outputs(self, actual_outputs, quantized_outputs):
        """ì¶œë ¥ ì •í™•ë„ ë¹„êµ"""
        if not actual_outputs or not quantized_outputs:
            return "ë¹„êµ ë¶ˆê°€"
        
        try:
            actual_tensor = torch.stack(actual_outputs)
            quantized_tensor = torch.stack(quantized_outputs)
            
            mae = torch.mean(torch.abs(actual_tensor - quantized_tensor)).item()
            correlation = torch.corrcoef(torch.stack([actual_tensor.flatten(), quantized_tensor.flatten()]))[0, 1].item()
            accuracy_01 = torch.mean((torch.abs(actual_tensor - quantized_tensor) < 0.1).float()).item()
            
            return f"MAE: {mae:.6f}, ìƒê´€ê³„ìˆ˜: {correlation:.6f}, 0.1ì´ë‚´: {accuracy_01:.2%}"
            
        except Exception as e:
            return f"ë¹„êµ ì˜¤ë¥˜: {e}"

def main():
    verifier = ActualPerformanceVerification()
    results = verifier.verify_actual_performance()
    
    return results

if __name__ == "__main__":
    main()
