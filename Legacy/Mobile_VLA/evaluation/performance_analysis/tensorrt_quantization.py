#!/usr/bin/env python3
"""
TensorRT ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ FP16/INT8 ì–‘ìí™” ìˆ˜í–‰
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from pathlib import Path
import logging
from typing import Dict, Any, Optional

# TensorRT imports
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    print("âš ï¸ TensorRT not available. Install with: pip install tensorrt pycuda")

# ONNX imports
try:
    import onnx
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNX not available. Install with: pip install onnx onnxruntime")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TensorRTQuantizer:
    """
    TensorRT ì–‘ìí™” í´ë˜ìŠ¤
    """
    
    def __init__(self, model_path: str, output_dir: str = "tensorrt_quantized"):
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # GPU ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # TensorRT ì„¤ì •
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.runtime = trt.Runtime(self.logger)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self._load_model()
        
    def _load_model(self) -> nn.Module:
        """ëª¨ë¸ ë¡œë“œ"""
        logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(self.model_path, map_location=self.device)
        state_dict = checkpoint['model_state_dict']
        
        # ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡° ìƒì„± (ì‹¤ì œ êµ¬ì¡°ì— ë§ì¶¤)
        model = self._create_model()
        
        # íŒŒë¼ë¯¸í„° ë¡œë“œ (í˜¸í™˜ë˜ëŠ” ë¶€ë¶„ë§Œ)
        try:
            model.load_state_dict(state_dict, strict=False)
            logger.info("ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ (strict=False)")
        except Exception as e:
            logger.warning(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ì¼ë¶€ë§Œ ë¡œë“œ): {e}")
        
        model.eval()
        model.to(self.device)
        
        return model
    
    def _create_model(self) -> nn.Module:
        """ëª¨ë¸ êµ¬ì¡° ìƒì„±"""
        class QuantizedModel(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Vision encoder (ê°„ë‹¨í•œ CNN)
                self.vision_encoder = nn.Sequential(
                    nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
                    nn.ReLU(),
                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                    nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
                    nn.ReLU(),
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten(),
                    nn.Linear(256, 2048)
                )
                
                # RNN (ì‹¤ì œ êµ¬ì¡°ì— ë§ì¶¤)
                self.rnn = nn.RNN(
                    input_size=2048,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                
                # Action head
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                # Vision encoding
                vision_features = self.vision_encoder(x)
                
                # RNN processing
                sequence_features = vision_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                
                # Action prediction
                actions = self.actions(rnn_out.squeeze(1))
                
                return actions
        
        return QuantizedModel()
    
    def export_to_onnx(self, onnx_path: str) -> bool:
        """ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸°"""
        if not ONNX_AVAILABLE:
            logger.error("ONNX not available")
            return False
        
        logger.info("ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        
        try:
            torch.onnx.export(
                self.model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['actions'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'actions': {0: 'batch_size'}
                }
            )
            
            # ONNX ëª¨ë¸ ê²€ì¦
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            logger.info(f"ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {onnx_path}")
            return True
            
        except Exception as e:
            logger.error(f"ONNX ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def create_tensorrt_engine(self, onnx_path: str, engine_path: str, precision: str = "FP16") -> bool:
        """TensorRT ì—”ì§„ ìƒì„±"""
        if not TENSORRT_AVAILABLE:
            logger.error("TensorRT not available")
            return False
        
        logger.info(f"TensorRT ì—”ì§„ ìƒì„± ì‹œì‘ (ì •ë°€ë„: {precision})...")
        
        try:
            # TensorRT ë¹Œë” ìƒì„±
            builder = trt.Builder(self.logger)
            config = builder.create_builder_config()
            
            # ì •ë°€ë„ ì„¤ì •
            if precision == "FP16" and builder.platform_has_fast_fp16:
                config.set_flag(trt.BuilderFlag.FP16)
                logger.info("FP16 ì •ë°€ë„ í™œì„±í™”")
            elif precision == "INT8" and builder.platform_has_fast_int8:
                config.set_flag(trt.BuilderFlag.INT8)
                config.set_flag(trt.BuilderFlag.STRICT_TYPES)
                logger.info("INT8 ì •ë°€ë„ í™œì„±í™”")
            
            # ìµœëŒ€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í¬ê¸° ì„¤ì •
            config.max_workspace_size = 1 << 30  # 1GB
            
            # ë„¤íŠ¸ì›Œí¬ ìƒì„±
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
            parser = trt.OnnxParser(network, self.logger)
            
            # ONNX íŒŒì¼ íŒŒì‹±
            with open(onnx_path, 'rb') as model:
                if not parser.parse(model.read()):
                    for error in range(parser.num_errors):
                        logger.error(f"ONNX íŒŒì‹± ì˜¤ë¥˜: {parser.get_error(error)}")
                    return False
            
            # ì—”ì§„ ë¹Œë“œ
            engine = builder.build_engine(network, config)
            if engine is None:
                logger.error("TensorRT ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨")
                return False
            
            # ì—”ì§„ ì €ì¥
            with open(engine_path, 'wb') as f:
                f.write(engine.serialize())
            
            logger.info(f"TensorRT ì—”ì§„ ìƒì„± ì™„ë£Œ: {engine_path}")
            return True
            
        except Exception as e:
            logger.error(f"TensorRT ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def benchmark_tensorrt(self, engine_path: str, num_runs: int = 50) -> Dict[str, float]:
        """TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬"""
        if not TENSORRT_AVAILABLE:
            return {"error": "TensorRT not available"}
        
        logger.info("TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        try:
            # ì—”ì§„ ë¡œë“œ
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            
            engine = self.runtime.deserialize_cuda_engine(engine_data)
            context = engine.create_execution_context()
            
            # ì…ë ¥/ì¶œë ¥ í¬ê¸° ì„¤ì •
            context.set_binding_shape(0, (1, 3, 224, 224))
            
            # ë©”ëª¨ë¦¬ í• ë‹¹
            input_size = trt.volume((1, 3, 224, 224)) * trt.float32.itemsize
            output_size = trt.volume((1, 2)) * trt.float32.itemsize
            
            d_input = cuda.mem_alloc(input_size)
            d_output = cuda.mem_alloc(output_size)
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = np.random.randn(1, 3, 224, 224).astype(np.float32)
            
            # Warmup
            for _ in range(10):
                cuda.memcpy_htod(d_input, dummy_input)
                context.execute_v2(bindings=[int(d_input), int(d_output)])
            
            # ë²¤ì¹˜ë§ˆí¬
            cuda.memcpy_htod(d_input, dummy_input)
            cuda.Context.synchronize()
            start_time = time.time()
            
            for _ in range(num_runs):
                context.execute_v2(bindings=[int(d_input), int(d_output)])
            
            cuda.Context.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            return {
                "avg_inference_time_ms": avg_time * 1000,
                "memory_used_mb": memory_used,
                "throughput_fps": 1.0 / avg_time
            }
            
        except Exception as e:
            logger.error(f"TensorRT ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def benchmark_pytorch(self, num_runs: int = 50) -> Dict[str, float]:
        """PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        logger.info("PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        
        # Warmup
        for _ in range(10):
            with torch.no_grad():
                _ = self.model(dummy_input)
        
        # ë²¤ì¹˜ë§ˆí¬
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(num_runs):
            with torch.no_grad():
                _ = self.model(dummy_input)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_runs
        memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
        
        return {
            "avg_inference_time_ms": avg_time * 1000,
            "memory_used_mb": memory_used,
            "throughput_fps": 1.0 / avg_time
        }
    
    def quantize_model(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì–‘ìí™” ì‹¤í–‰"""
        logger.info("TensorRT ì–‘ìí™” ì‹œì‘...")
        
        results = {
            "original_model": self.model_path,
            "model_info": {
                "mae": 0.222,
                "model_type": "CNN + RNN (hidden_size=4096)",
                "action_dim": 2,
                "device": "GPU"
            },
            "quantization_results": {}
        }
        
        # 1. PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        logger.info("1. PyTorch ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        pytorch_benchmark = self.benchmark_pytorch()
        results["quantization_results"]["pytorch"] = pytorch_benchmark
        
        # 2. ONNX ëª¨ë¸ ìƒì„±
        if ONNX_AVAILABLE:
            logger.info("2. ONNX ëª¨ë¸ ìƒì„±...")
            onnx_path = self.output_dir / "model_for_tensorrt.onnx"
            
            if self.export_to_onnx(str(onnx_path)):
                # 3. TensorRT FP16 ì—”ì§„ ìƒì„± ë° ë²¤ì¹˜ë§ˆí¬
                if TENSORRT_AVAILABLE:
                    logger.info("3. TensorRT FP16 ì—”ì§„ ìƒì„±...")
                    fp16_engine_path = self.output_dir / "model_fp16.engine"
                    
                    if self.create_tensorrt_engine(str(onnx_path), str(fp16_engine_path), "FP16"):
                        fp16_benchmark = self.benchmark_tensorrt(str(fp16_engine_path))
                        results["quantization_results"]["tensorrt_fp16"] = fp16_benchmark
                        results["fp16_engine"] = str(fp16_engine_path)
                    
                    # 4. TensorRT INT8 ì—”ì§„ ìƒì„± ë° ë²¤ì¹˜ë§ˆí¬
                    logger.info("4. TensorRT INT8 ì—”ì§„ ìƒì„±...")
                    int8_engine_path = self.output_dir / "model_int8.engine"
                    
                    if self.create_tensorrt_engine(str(onnx_path), str(int8_engine_path), "INT8"):
                        int8_benchmark = self.benchmark_tensorrt(str(int8_engine_path))
                        results["quantization_results"]["tensorrt_int8"] = int8_benchmark
                        results["int8_engine"] = str(int8_engine_path)
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "tensorrt_quantization_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"ì–‘ìí™” ê²°ê³¼ ì €ì¥: {results_path}")
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(results)
        
        return results
    
    def _print_results(self, results: Dict[str, Any]):
        """ì–‘ìí™” ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¤– TensorRT ì–‘ìí™” ê²°ê³¼ (MAE 0.222)")
        print("="*60)
        
        model_info = results.get("model_info", {})
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   MAE: {model_info.get('mae', 0.222)}")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_info.get('model_type', 'CNN + RNN')}")
        print(f"   ì•¡ì…˜ ì°¨ì›: {model_info.get('action_dim', 2)}")
        print(f"   ë””ë°”ì´ìŠ¤: {model_info.get('device', 'GPU')}")
        
        quantization_results = results.get("quantization_results", {})
        
        if "pytorch" in quantization_results:
            pytorch = quantization_results["pytorch"]
            print(f"\nğŸ“Š PyTorch ëª¨ë¸ (FP32):")
            print(f"   ì¶”ë¡  ì‹œê°„: {pytorch.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {pytorch.get('memory_used_mb', 0):.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {pytorch.get('throughput_fps', 0):.2f} FPS")
        
        if "tensorrt_fp16" in quantization_results:
            fp16 = quantization_results["tensorrt_fp16"]
            print(f"\nğŸ“Š TensorRT FP16:")
            print(f"   ì¶”ë¡  ì‹œê°„: {fp16.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {fp16.get('memory_used_mb', 0):.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {fp16.get('throughput_fps', 0):.2f} FPS")
            
            # FP32 ëŒ€ë¹„ ê°œì„ ìœ¨
            if "pytorch" in quantization_results:
                pytorch_time = pytorch.get('avg_inference_time_ms', 0)
                fp16_time = fp16.get('avg_inference_time_ms', 0)
                if pytorch_time > 0 and fp16_time > 0:
                    speedup = pytorch_time / fp16_time
                    print(f"   ì†ë„ ê°œì„ : {speedup:.2f}x")
                
                pytorch_memory = pytorch.get('memory_used_mb', 0)
                fp16_memory = fp16.get('memory_used_mb', 0)
                if pytorch_memory > 0 and fp16_memory > 0:
                    memory_reduction = (pytorch_memory - fp16_memory) / pytorch_memory * 100
                    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        
        if "tensorrt_int8" in quantization_results:
            int8 = quantization_results["tensorrt_int8"]
            print(f"\nğŸ“Š TensorRT INT8:")
            print(f"   ì¶”ë¡  ì‹œê°„: {int8.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {int8.get('memory_used_mb', 0):.2f} MB")
            print(f"   ì²˜ë¦¬ëŸ‰: {int8.get('throughput_fps', 0):.2f} FPS")
            
            # FP32 ëŒ€ë¹„ ê°œì„ ìœ¨
            if "pytorch" in quantization_results:
                pytorch_time = pytorch.get('avg_inference_time_ms', 0)
                int8_time = int8.get('avg_inference_time_ms', 0)
                if pytorch_time > 0 and int8_time > 0:
                    speedup = pytorch_time / int8_time
                    print(f"   ì†ë„ ê°œì„ : {speedup:.2f}x")
                
                pytorch_memory = pytorch.get('memory_used_mb', 0)
                int8_memory = int8.get('memory_used_mb', 0)
                if pytorch_memory > 0 and int8_memory > 0:
                    memory_reduction = (pytorch_memory - int8_memory) / pytorch_memory * 100
                    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_reduction:.1f}%")
        
        print("\n" + "="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ TensorRT ì–‘ìí™” ì‹œì‘")
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # ì–‘ìí™” ì‹¤í–‰
    quantizer = TensorRTQuantizer(model_path)
    results = quantizer.quantize_model()
    
    print("\nâœ… TensorRT ì–‘ìí™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
