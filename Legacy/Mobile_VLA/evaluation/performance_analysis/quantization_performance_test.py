#!/usr/bin/env python3
"""
ğŸ¯ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
ì›ë³¸ MAE 0.222 ëª¨ë¸ vs ì–‘ìí™”ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ
"""

import torch
import torch.nn as nn
import time
import psutil
import os
import json
import logging
import numpy as np
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OriginalMAE0222Model(nn.Module):
    """ì›ë³¸ MAE 0.222 ëª¨ë¸ (ì •í™•í•œ êµ¬ì¡° ì¬í˜„)"""
    
    def __init__(self, model_path: str):
        super().__init__()
        
        # Kosmos2 ëª¨ë¸ ë¡œë“œ (ì›ë³¸ FP32)
        from transformers import AutoProcessor, AutoModel
        
        self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos.eval()
        for param in self.kosmos.parameters():
            param.requires_grad = False
        
        # Action Head (ì›ë³¸ êµ¬ì¡°)
        self.rnn = nn.RNN(
            input_size=2048,  # Kosmos2 hidden size
            hidden_size=4096,
            num_layers=4,
            batch_first=True,
            dropout=0.1
        )
        
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # linear_x, linear_y
        )
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ Action Head ë¡œë“œ
        self._load_action_head(model_path)
        
        logger.info("âœ… ì›ë³¸ MAE 0.222 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_action_head(self, model_path: str):
        """Action Headë§Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dictì—ì„œ Action Head ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
            action_state_dict = {}
            for key, value in checkpoint['model_state_dict'].items():
                if 'rnn' in key or 'actions' in key:
                    action_state_dict[key] = value
            
            # Action Headë§Œ ë¡œë“œ
            self.load_state_dict(action_state_dict, strict=False)
            logger.info(f"âœ… Action Head ë¡œë“œ ì™„ë£Œ: {len(action_state_dict)} íŒŒë¼ë¯¸í„°")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Action Head ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ Action Head ì‚¬ìš©")
    
    def forward(self, x):
        """ìˆœì „íŒŒ (ì›ë³¸ FP32)"""
        batch_size = x.size(0)
        
        # 1. VLMìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (FP32)
        with torch.no_grad():
            # ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
            dummy_text = ["<image>"] * batch_size
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = self.processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(x.device)
            
            try:
                # Kosmos2ë¡œ íŠ¹ì§• ì¶”ì¶œ (FP32)
                vision_outputs = self.kosmos(
                    pixel_values=x,
                    input_ids=text_inputs['input_ids'],
                    attention_mask=text_inputs['attention_mask']
                )
                vision_features = vision_outputs.last_hidden_state[:, 0]  # [batch_size, 2048]
            except Exception as e:
                logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                vision_features = torch.randn(batch_size, 2048).to(x.device)
        
        # 2. Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡ (FP32)
        sequence_features = vision_features.unsqueeze(1)  # [batch_size, 1, 2048]
        rnn_out, _ = self.rnn(sequence_features)  # [batch_size, 1, 4096]
        actions = self.actions(rnn_out.squeeze(1))  # [batch_size, 2]
        
        return actions

class QuantizedMAE0222Model(nn.Module):
    """ì–‘ìí™”ëœ MAE 0.222 ëª¨ë¸ (VLM FP16)"""
    
    def __init__(self, model_path: str):
        super().__init__()
        
        # Kosmos2 ëª¨ë¸ ë¡œë“œ (FP16ìœ¼ë¡œ ë³€í™˜)
        from transformers import AutoProcessor, AutoModel
        
        self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # VLMì„ FP16ìœ¼ë¡œ ë³€í™˜
        self.kosmos = self.kosmos.half()
        self.kosmos.eval()
        for param in self.kosmos.parameters():
            param.requires_grad = False
        
        # Action Head (FP32 ìœ ì§€)
        self.rnn = nn.RNN(
            input_size=2048,  # Kosmos2 hidden size
            hidden_size=4096,
            num_layers=4,
            batch_first=True,
            dropout=0.1
        )
        
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # linear_x, linear_y
        )
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ Action Head ë¡œë“œ
        self._load_action_head(model_path)
        
        logger.info("âœ… ì–‘ìí™”ëœ MAE 0.222 ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (VLM FP16)")
    
    def _load_action_head(self, model_path: str):
        """Action Headë§Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dictì—ì„œ Action Head ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
            action_state_dict = {}
            for key, value in checkpoint['model_state_dict'].items():
                if 'rnn' in key or 'actions' in key:
                    action_state_dict[key] = value
            
            # Action Headë§Œ ë¡œë“œ
            self.load_state_dict(action_state_dict, strict=False)
            logger.info(f"âœ… Action Head ë¡œë“œ ì™„ë£Œ: {len(action_state_dict)} íŒŒë¼ë¯¸í„°")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Action Head ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ Action Head ì‚¬ìš©")
    
    def forward(self, x):
        """ìˆœì „íŒŒ (VLM FP16 + Action Head FP32)"""
        batch_size = x.size(0)
        
        # 1. VLMìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (FP16)
        with torch.no_grad():
            # ì…ë ¥ì„ FP16ìœ¼ë¡œ ë³€í™˜
            x_fp16 = x.half()
            
            # ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
            dummy_text = ["<image>"] * batch_size
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = self.processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(x.device)
            
            try:
                # Kosmos2ë¡œ íŠ¹ì§• ì¶”ì¶œ (FP16)
                vision_outputs = self.kosmos(
                    pixel_values=x_fp16,
                    input_ids=text_inputs['input_ids'],
                    attention_mask=text_inputs['attention_mask']
                )
                vision_features = vision_outputs.last_hidden_state[:, 0]  # [batch_size, 2048]
            except Exception as e:
                logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                vision_features = torch.randn(batch_size, 2048).half().to(x.device)
        
        # 2. Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡ (FP32)
        # FP16ì—ì„œ FP32ë¡œ ë³€í™˜
        vision_features_fp32 = vision_features.float()
        
        sequence_features = vision_features_fp32.unsqueeze(1)  # [batch_size, 1, 2048]
        rnn_out, _ = self.rnn(sequence_features)  # [batch_size, 1, 4096]
        actions = self.actions(rnn_out.squeeze(1))  # [batch_size, 2]
        
        return actions

class QuantizationPerformanceTester:
    """ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤í„°"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        self.original_model = OriginalMAE0222Model(model_path).to(self.device)
        self.original_model.eval()
        
        # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
        self.quantized_model = QuantizedMAE0222Model(model_path).to(self.device)
        self.quantized_model.eval()
        
        logger.info(f"ğŸš€ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    def benchmark_model(self, model, name: str, num_runs: int = 100):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
        
        # ì›Œë°ì—…
        for _ in range(10):
            _ = model(dummy_input)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        outputs = []
        for _ in range(num_runs):
            with torch.no_grad():
                output = model(dummy_input)
                outputs.append(output)
        
        end_time = time.time()
        inference_time = (end_time - start_time) / num_runs * 1000  # ms
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = memory_after - memory_before
        
        fps = 1000 / inference_time if inference_time > 0 else 0
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': inference_time,
            'memory_usage_mb': memory_usage,
            'fps': fps,
            'outputs': outputs
        }
    
    def compare_outputs(self, original_outputs, quantized_outputs):
        """ì¶œë ¥ ë¹„êµ (ì •í™•ë„ ì¸¡ì •)"""
        logger.info("ğŸ” ì¶œë ¥ ì •í™•ë„ ë¹„êµ...")
        
        if not original_outputs or not quantized_outputs:
            logger.warning("âš ï¸ ì¶œë ¥ ë¹„êµ ë¶ˆê°€")
            return None
        
        # ì¶œë ¥ì„ í…ì„œë¡œ ë³€í™˜
        original_tensor = torch.stack(original_outputs)
        quantized_tensor = torch.stack(quantized_outputs)
        
        # MAE ê³„ì‚°
        mae = torch.mean(torch.abs(original_tensor - quantized_tensor)).item()
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        original_flat = original_tensor.flatten()
        quantized_flat = quantized_tensor.flatten()
        correlation = torch.corrcoef(torch.stack([original_flat, quantized_flat]))[0, 1].item()
        
        # ì •í™•ë„ ê³„ì‚° (0.1 ì´ë‚´ ì˜¤ì°¨)
        accuracy_01 = torch.mean((torch.abs(original_tensor - quantized_tensor) < 0.1).float()).item()
        
        # ì •í™•ë„ ê³„ì‚° (0.05 ì´ë‚´ ì˜¤ì°¨)
        accuracy_005 = torch.mean((torch.abs(original_tensor - quantized_tensor) < 0.05).float()).item()
        
        logger.info(f"   MAE: {mae:.6f}")
        logger.info(f"   ìƒê´€ê³„ìˆ˜: {correlation:.6f}")
        logger.info(f"   0.1 ì´ë‚´ ì •í™•ë„: {accuracy_01:.2%}")
        logger.info(f"   0.05 ì´ë‚´ ì •í™•ë„: {accuracy_005:.2%}")
        
        return {
            'mae': mae,
            'correlation': correlation,
            'accuracy_01': accuracy_01,
            'accuracy_005': accuracy_005
        }
    
    def test_performance(self):
        """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        
        results = {}
        
        # 1. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        logger.info("1. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        original_benchmark = self.benchmark_model(self.original_model, "ì›ë³¸ MAE 0.222")
        results['original'] = original_benchmark
        
        # 2. ì–‘ìí™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        logger.info("2. ì–‘ìí™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        quantized_benchmark = self.benchmark_model(self.quantized_model, "ì–‘ìí™”ëœ MAE 0.222")
        results['quantized'] = quantized_benchmark
        
        # 3. ì¶œë ¥ ì •í™•ë„ ë¹„êµ
        logger.info("3. ì¶œë ¥ ì •í™•ë„ ë¹„êµ...")
        accuracy_comparison = self.compare_outputs(
            original_benchmark['outputs'], 
            quantized_benchmark['outputs']
        )
        results['accuracy'] = accuracy_comparison
        
        # 4. ê²°ê³¼ ë¹„êµ
        logger.info("4. ê²°ê³¼ ë¹„êµ...")
        self._compare_performance_results(results)
        
        # 5. ê²°ê³¼ ì €ì¥
        with open('quantization_performance_results.json', 'w') as f:
            # outputsëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ ì œì™¸
            save_results = {
                'original': {k: v for k, v in results['original'].items() if k != 'outputs'},
                'quantized': {k: v for k, v in results['quantized'].items() if k != 'outputs'},
                'accuracy': results['accuracy']
            }
            json.dump(save_results, f, indent=2)
        
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: quantization_performance_results.json")
        
        return results
    
    def _compare_performance_results(self, results):
        """ì„±ëŠ¥ ê²°ê³¼ ë¹„êµ"""
        logger.info("\nğŸ“Š ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        logger.info("=" * 60)
        
        original = results.get('original', {})
        quantized = results.get('quantized', {})
        accuracy = results.get('accuracy', {})
        
        if original and quantized:
            speedup = original['inference_time_ms'] / quantized['inference_time_ms']
            memory_save = (original['memory_usage_mb'] - quantized['memory_usage_mb']) / original['memory_usage_mb'] * 100
            
            logger.info(f"ì„±ëŠ¥ ë¹„êµ:")
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save:.1f}%")
            logger.info(f"   FPS í–¥ìƒ: {quantized['fps']:.1f} â†’ {original['fps']:.1f}")
        
        if accuracy:
            logger.info(f"\nì •í™•ë„ ë¹„êµ:")
            logger.info(f"   MAE: {accuracy['mae']:.6f}")
            logger.info(f"   ìƒê´€ê³„ìˆ˜: {accuracy['correlation']:.6f}")
            logger.info(f"   0.1 ì´ë‚´ ì •í™•ë„: {accuracy['accuracy_01']:.2%}")
            logger.info(f"   0.05 ì´ë‚´ ì •í™•ë„: {accuracy['accuracy_005']:.2%}")
        
        # ìµœì¢… ê¶Œì¥ì‚¬í•­
        logger.info("\nğŸ¯ ì–‘ìí™” ê¶Œì¥ì‚¬í•­:")
        if original and quantized and accuracy:
            if speedup > 1.5 and accuracy['correlation'] > 0.95:
                logger.info("   ğŸ† ì–‘ìí™” ê°•ë ¥ ê¶Œì¥: ì†ë„ í–¥ìƒ + ë†’ì€ ì •í™•ë„")
            elif speedup > 1.2 and accuracy['correlation'] > 0.9:
                logger.info("   ğŸŸ¢ ì–‘ìí™” ê¶Œì¥: ì ë‹¹í•œ ì†ë„ í–¥ìƒ + ì–‘í˜¸í•œ ì •í™•ë„")
            elif accuracy['correlation'] < 0.8:
                logger.info("   ğŸŸ¡ ì–‘ìí™” ì£¼ì˜: ì •í™•ë„ ì €í•˜ ìš°ë ¤")
            else:
                logger.info("   ğŸŸ¡ ì›ë³¸ ëª¨ë¸ ìœ ì§€ ê¶Œì¥: ì–‘ìí™” íš¨ê³¼ ë¯¸ë¯¸")
        else:
            logger.info("   âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ê¶Œì¥ì‚¬í•­ ì œê³µ ë¶ˆê°€")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = QuantizationPerformanceTester(model_path)
    results = tester.test_performance()
    
    logger.info("ğŸ‰ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
