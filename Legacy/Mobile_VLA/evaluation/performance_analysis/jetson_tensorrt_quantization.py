#!/usr/bin/env python3
"""
Jetson Orin NXìš© TensorRT ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
MAE 0.222 ëª¨ë¸ì„ TensorRTë¡œ ìµœì í™”
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from pathlib import Path
import logging
from typing import Dict, Any, Optional

# TensorRT imports (Jetsonì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥)
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    print("âš ï¸ TensorRT not available. This script is for Jetson Orin NX")

# ONNX imports
try:
    import onnx
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNX not available. Install with: pip install onnx onnxruntime")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JetsonTensorRTQuantizer:
    """
    Jetson Orin NXìš© TensorRT ì–‘ìí™” í´ë˜ìŠ¤
    """
    
    def __init__(self, onnx_path: str, output_dir: str = "jetson_tensorrt_models"):
        self.onnx_path = onnx_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Jetson Orin NX ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # TensorRT ì„¤ì •
        if TENSORRT_AVAILABLE:
            self.trt_logger = trt.Logger(trt.Logger.WARNING)
            self.max_batch_size = 1
            self.max_workspace_size = 1 << 30  # 1GB
        
    def create_tensorrt_engine(self, engine_path: str, precision: str = "fp16") -> bool:
        """TensorRT ì—”ì§„ ìƒì„±"""
        if not TENSORRT_AVAILABLE:
            logger.error("TensorRT not available")
            return False
        
        logger.info(f"TensorRT ì—”ì§„ ìƒì„± ì‹œì‘ (precision: {precision})...")
        
        try:
            # TensorRT ë¹Œë” ìƒì„±
            builder = trt.Builder(self.trt_logger)
            config = builder.create_builder_config()
            config.max_workspace_size = self.max_workspace_size
            
            # Precision ì„¤ì •
            if precision == "fp16" and builder.platform_has_fast_fp16:
                config.set_flag(trt.BuilderFlag.FP16)
                logger.info("FP16 precision enabled")
            elif precision == "int8" and builder.platform_has_fast_int8:
                config.set_flag(trt.BuilderFlag.INT8)
                config.set_flag(trt.BuilderFlag.STRICT_TYPES)
                logger.info("INT8 precision enabled")
            
            # ë„¤íŠ¸ì›Œí¬ ìƒì„±
            network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
            parser = trt.OnnxParser(network, self.trt_logger)
            
            # ONNX íŒŒì¼ íŒŒì‹±
            with open(self.onnx_path, 'rb') as model:
                if not parser.parse(model.read()):
                    for error in range(parser.num_errors):
                        logger.error(f"ONNX íŒŒì‹± ì˜¤ë¥˜: {parser.get_error(error)}")
                    return False
            
            # ì—”ì§„ ë¹Œë“œ
            engine = builder.build_engine(network, config)
            if engine is None:
                logger.error("TensorRT ì—”ì§„ ë¹Œë“œ ì‹¤íŒ¨")
                return False
            
            # ì—”ì§„ ì €ì¥
            with open(engine_path, 'wb') as f:
                f.write(engine.serialize())
            
            logger.info(f"TensorRT ì—”ì§„ ìƒì„± ì™„ë£Œ: {engine_path}")
            return True
            
        except Exception as e:
            logger.error(f"TensorRT ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def benchmark_tensorrt(self, engine_path: str, num_runs: int = 50) -> Dict[str, float]:
        """TensorRT ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        if not TENSORRT_AVAILABLE:
            return {"error": "TensorRT not available"}
        
        logger.info("TensorRT ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        try:
            # TensorRT ì—”ì§„ ë¡œë“œ
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            
            runtime = trt.Runtime(self.trt_logger)
            engine = runtime.deserialize_cuda_engine(engine_data)
            context = engine.create_execution_context()
            
            # ë©”ëª¨ë¦¬ í• ë‹¹
            input_shape = (1, 3, 224, 224)
            output_shape = (1, 2)  # action_dim = 2
            
            d_input = cuda.mem_alloc(input_shape[0] * input_shape[1] * input_shape[2] * input_shape[3] * 4)
            d_output = cuda.mem_alloc(output_shape[0] * output_shape[1] * 4)
            
            bindings = [int(d_input), int(d_output)]
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = torch.randn(input_shape, device=self.device)
            
            # Warmup
            for _ in range(10):
                cuda.memcpy_htod(d_input, dummy_input.cpu().numpy())
                context.execute_v2(bindings)
                output = np.empty(output_shape, dtype=np.float32)
                cuda.memcpy_dtoh(output, d_output)
            
            # ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            
            for _ in range(num_runs):
                cuda.memcpy_htod(d_input, dummy_input.cpu().numpy())
                context.execute_v2(bindings)
                output = np.empty(output_shape, dtype=np.float32)
                cuda.memcpy_dtoh(output, d_output)
            
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            return {
                "avg_inference_time_ms": avg_time * 1000,
                "memory_used_mb": memory_used,
                "throughput_fps": 1.0 / avg_time
            }
            
        except Exception as e:
            logger.error(f"TensorRT ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def quantize_for_jetson(self) -> Dict[str, Any]:
        """Jetsonìš© ì–‘ìí™” ì‹¤í–‰"""
        logger.info("Jetson Orin NXìš© TensorRT ì–‘ìí™” ì‹œì‘...")
        
        results = {
            "onnx_model": self.onnx_path,
            "model_info": {
                "mae": 0.222,
                "model_type": "Simple CNN + RNN",
                "action_dim": 2,
                "target_device": "Jetson Orin NX"
            },
            "tensorrt_results": {}
        }
        
        # 1. TensorRT FP16 ì—”ì§„ ìƒì„± ë° ë²¤ì¹˜ë§ˆí¬
        if TENSORRT_AVAILABLE:
            logger.info("1. TensorRT FP16 ì—”ì§„ ìƒì„±...")
            fp16_engine_path = self.output_dir / "mae0222_model_fp16.trt"
            
            if self.create_tensorrt_engine(str(fp16_engine_path), "fp16"):
                fp16_benchmark = self.benchmark_tensorrt(str(fp16_engine_path))
                results["tensorrt_results"]["fp16"] = fp16_benchmark
                results["fp16_engine"] = str(fp16_engine_path)
        
        # 2. TensorRT INT8 ì—”ì§„ ìƒì„± ë° ë²¤ì¹˜ë§ˆí¬
        if TENSORRT_AVAILABLE:
            logger.info("2. TensorRT INT8 ì—”ì§„ ìƒì„±...")
            int8_engine_path = self.output_dir / "mae0222_model_int8.trt"
            
            if self.create_tensorrt_engine(str(int8_engine_path), "int8"):
                int8_benchmark = self.benchmark_tensorrt(str(int8_engine_path))
                results["tensorrt_results"]["int8"] = int8_benchmark
                results["int8_engine"] = str(int8_engine_path)
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / "jetson_tensorrt_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Jetson ì–‘ìí™” ê²°ê³¼ ì €ì¥: {results_path}")
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(results)
        
        return results
    
    def _print_results(self, results: Dict[str, Any]):
        """ì–‘ìí™” ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¤– Jetson Orin NX TensorRT ì–‘ìí™” ê²°ê³¼ (MAE 0.222)")
        print("="*60)
        
        model_info = results.get("model_info", {})
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   MAE: {model_info.get('mae', 0.222)}")
        print(f"   ëª¨ë¸ íƒ€ì…: {model_info.get('model_type', 'Simple CNN + RNN')}")
        print(f"   ì•¡ì…˜ ì°¨ì›: {model_info.get('action_dim', 2)}")
        print(f"   íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: {model_info.get('target_device', 'Jetson Orin NX')}")
        
        tensorrt_results = results.get("tensorrt_results", {})
        
        if "fp16" in tensorrt_results:
            fp16 = tensorrt_results["fp16"]
            if "error" not in fp16:
                print(f"\nğŸ“Š TensorRT FP16:")
                print(f"   ì¶”ë¡  ì‹œê°„: {fp16.get('avg_inference_time_ms', 0):.2f} ms")
                print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {fp16.get('memory_used_mb', 0):.2f} MB")
                print(f"   ì²˜ë¦¬ëŸ‰: {fp16.get('throughput_fps', 0):.2f} FPS")
            else:
                print(f"\nâŒ TensorRT FP16 ì˜¤ë¥˜: {fp16['error']}")
        
        if "int8" in tensorrt_results:
            int8 = tensorrt_results["int8"]
            if "error" not in int8:
                print(f"\nğŸ“Š TensorRT INT8:")
                print(f"   ì¶”ë¡  ì‹œê°„: {int8.get('avg_inference_time_ms', 0):.2f} ms")
                print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {int8.get('memory_used_mb', 0):.2f} MB")
                print(f"   ì²˜ë¦¬ëŸ‰: {int8.get('throughput_fps', 0):.2f} FPS")
            else:
                print(f"\nâŒ TensorRT INT8 ì˜¤ë¥˜: {int8['error']}")
        
        print("\n" + "="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Jetson Orin NX TensorRT ì–‘ìí™” ì‹œì‘")
    
    # ONNX ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    onnx_path = "quantized_models_cpu/mae0222_model_cpu.onnx"
    
    if not os.path.exists(onnx_path):
        print(f"âŒ ONNX ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {onnx_path}")
        print("ë¨¼ì € CPU ì–‘ìí™”ë¥¼ ì‹¤í–‰í•˜ì—¬ ONNX ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # Jetson ì–‘ìí™” ì‹¤í–‰
    quantizer = JetsonTensorRTQuantizer(onnx_path)
    results = quantizer.quantize_for_jetson()
    
    print("\nâœ… Jetson ì–‘ìí™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
