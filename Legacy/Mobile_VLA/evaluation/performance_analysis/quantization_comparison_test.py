#!/usr/bin/env python3
"""
ë‘ ëª¨ë¸ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
MAE 0.222 (Kosmos2) vs MAE 0.212 (CLIP) ì–‘ìí™” ë¹„êµ
"""

import torch
import torch.nn as nn
import time
import json
import logging
from transformers import CLIPProcessor, CLIPModel
import os
import gc
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuantizationComparisonTest:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_runs = 100  # ë” ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´ ì¦ê°€
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def _create_kosmos2_model(self):
        """MAE 0.222 ëª¨ë¸ êµ¬ì¡° (Kosmos2 Vision ê¸°ë°˜)"""
        class Kosmos2VisionModel(nn.Module):
            def __init__(self):
                super().__init__()
                # Kosmos2 Vision ëª¨ë¸ë§Œ ì‚¬ìš© (ì´ë¯¸ì§€ ì¸ì½”ë”©)
                from transformers import AutoProcessor, AutoModel
                self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos2 = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos2.eval()
                for param in self.kosmos2.parameters():
                    param.requires_grad = False
                
                # ì‹¤ì œ MAE 0.222 ëª¨ë¸ì˜ Action Head êµ¬ì¡°
                self.rnn = nn.RNN(
                    input_size=2048,  # Kosmos2 ì¶œë ¥ í¬ê¸°
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    # Kosmos2 Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ì¸ì½”ë”©
                    # ë”ë¯¸ í…ìŠ¤íŠ¸ ì…ë ¥ ìƒì„± (Kosmos2 ìš”êµ¬ì‚¬í•­)
                    dummy_text = torch.zeros(batch_size, 1, dtype=torch.long).to(x.device)
                    
                    outputs = self.kosmos2(
                        pixel_values=x,
                        input_ids=dummy_text,
                        attention_mask=torch.ones_like(dummy_text)
                    )
                    
                    # Vision features ì¶”ì¶œ (ë§ˆì§€ë§‰ hidden stateì˜ ì²« ë²ˆì§¸ í† í°)
                    image_features = outputs.last_hidden_state[:, 0, :]
                    
                sequence_features = image_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        return Kosmos2VisionModel()
    
    def _create_clip_model(self):
        """MAE 0.212 ëª¨ë¸ êµ¬ì¡° (CLIP ê¸°ë°˜)"""
        class CLIPModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                self.clip = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.clip.eval()
                for param in self.clip.parameters():
                    param.requires_grad = False
                
                # CLIP ê¸°ë°˜ Action Head êµ¬ì¡°
                self.rnn = nn.RNN(
                    input_size=512,  # CLIP ì¶œë ¥ í¬ê¸°
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    # CLIP ì´ë¯¸ì§€ ì¸ì½”ë”©
                    image_features = self.clip.get_image_features(pixel_values=x)
                    
                sequence_features = image_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        return CLIPModel()
    
    def _create_quantized_model(self, base_model, quantization_type="fp16"):
        """ì–‘ìí™”ëœ ëª¨ë¸ ìƒì„±"""
        class QuantizedModel(nn.Module):
            def __init__(self, base_model, quantization_type):
                super().__init__()
                self.base_model = base_model
                self.quantization_type = quantization_type
                
                # ì–‘ìí™” ì ìš©
                if quantization_type == "fp16":
                    self.base_model = self.base_model.half()
                elif quantization_type == "int8":
                    # ë™ì  ì–‘ìí™”
                    self.base_model = torch.quantization.quantize_dynamic(
                        self.base_model, {nn.Linear, nn.RNN}, dtype=torch.qint8
                    )
            
            def forward(self, x):
                if self.quantization_type == "fp16":
                    x = x.half()
                return self.base_model(x)
        
        return QuantizedModel(base_model, quantization_type)
    
    def _benchmark_model(self, model, name, input_data):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        model = model.to(self.device)
        model.eval()
        
        # ì›Œë°ì—…
        for _ in range(10):
            with torch.no_grad():
                _ = model(input_data)
        
        # ì •í™•í•œ ë©”ëª¨ë¦¬ ì¸¡ì •
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            start_memory = torch.cuda.memory_allocated()
        else:
            start_memory = 0
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        outputs = []
        
        for _ in range(self.num_runs):
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            outputs.append(output.detach().cpu())
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            end_memory = torch.cuda.memory_allocated()
            peak_memory = torch.cuda.max_memory_allocated()
            
            memory_used = (end_memory - start_memory) / (1024 ** 2)  # MB
            peak_memory_used = peak_memory / (1024 ** 2)  # MB
        else:
            memory_used = 0
            peak_memory_used = 0
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1000 / avg_time
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {peak_memory_used:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': avg_time,
            'inference_time_std': std_time,
            'memory_usage_mb': memory_used,
            'peak_memory_mb': peak_memory_used,
            'fps': fps,
            'outputs': outputs
        }
    
    def _compare_outputs(self, original_outputs, quantized_outputs):
        """ì¶œë ¥ ì •í™•ë„ ë¹„êµ"""
        if not original_outputs or not quantized_outputs:
            return "ë¹„êµ ë¶ˆê°€"
        
        try:
            original_tensor = torch.stack(original_outputs)
            quantized_tensor = torch.stack(quantized_outputs)
            
            mae = torch.mean(torch.abs(original_tensor - quantized_tensor)).item()
            correlation = torch.corrcoef(torch.stack([original_tensor.flatten(), quantized_tensor.flatten()]))[0, 1].item()
            accuracy_01 = torch.mean((torch.abs(original_tensor - quantized_tensor) < 0.1).float()).item()
            accuracy_001 = torch.mean((torch.abs(original_tensor - quantized_tensor) < 0.01).float()).item()
            
            return {
                'mae': mae,
                'correlation': correlation,
                'accuracy_01': accuracy_01,
                'accuracy_001': accuracy_001
            }
            
        except Exception as e:
            return f"ë¹„êµ ì˜¤ë¥˜: {e}"
    
    def run_comparison(self):
        """ë‘ ëª¨ë¸ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰"""
        logger.info("ğŸš€ ë‘ ëª¨ë¸ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        
        # ì…ë ¥ ë°ì´í„° ìƒì„±
        batch_size = 1
        input_data = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        results = {}
        
        # 1. Kosmos2 ëª¨ë¸ (MAE 0.222) í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ MAE 0.222 ëª¨ë¸ (Kosmos2) ì–‘ìí™” í…ŒìŠ¤íŠ¸")
        logger.info("="*60)
        
        kosmos2_original = self._create_kosmos2_model()
        kosmos2_fp16 = self._create_quantized_model(self._create_kosmos2_model(), "fp16")
        
        # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        kosmos2_original_results = self._benchmark_model(kosmos2_original, "Kosmos2 ì›ë³¸", input_data)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del kosmos2_original
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # FP16 ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        kosmos2_fp16_results = self._benchmark_model(kosmos2_fp16, "Kosmos2 FP16", input_data)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del kosmos2_fp16
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Kosmos2 ê²°ê³¼ ì €ì¥
        results['kosmos2'] = {
            'original': kosmos2_original_results,
            'fp16': kosmos2_fp16_results,
            'improvement': {
                'speedup': kosmos2_original_results['inference_time_ms'] / kosmos2_fp16_results['inference_time_ms'],
                'memory_save': 0 if kosmos2_original_results['memory_usage_mb'] == 0 else 
                    (kosmos2_original_results['memory_usage_mb'] - kosmos2_fp16_results['memory_usage_mb']) / kosmos2_original_results['memory_usage_mb'] * 100,
                'fps_improvement': kosmos2_fp16_results['fps'] / kosmos2_original_results['fps']
            }
        }
        
        # 2. CLIP ëª¨ë¸ (MAE 0.212) í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ MAE 0.212 ëª¨ë¸ (CLIP) ì–‘ìí™” í…ŒìŠ¤íŠ¸")
        logger.info("="*60)
        
        clip_original = self._create_clip_model()
        clip_fp16 = self._create_quantized_model(self._create_clip_model(), "fp16")
        
        # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        clip_original_results = self._benchmark_model(clip_original, "CLIP ì›ë³¸", input_data)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del clip_original
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # FP16 ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        clip_fp16_results = self._benchmark_model(clip_fp16, "CLIP FP16", input_data)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del clip_fp16
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # CLIP ê²°ê³¼ ì €ì¥
        results['clip'] = {
            'original': clip_original_results,
            'fp16': clip_fp16_results,
            'improvement': {
                'speedup': clip_original_results['inference_time_ms'] / clip_fp16_results['inference_time_ms'],
                'memory_save': 0 if clip_original_results['memory_usage_mb'] == 0 else 
                    (clip_original_results['memory_usage_mb'] - clip_fp16_results['memory_usage_mb']) / clip_original_results['memory_usage_mb'] * 100,
                'fps_improvement': clip_fp16_results['fps'] / clip_original_results['fps']
            }
        }
        
        # 3. ê²°ê³¼ ì¶œë ¥
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        logger.info("="*80)
        
        # Kosmos2 ê²°ê³¼
        logger.info(f"\nğŸ¥‡ MAE 0.222 ëª¨ë¸ (Kosmos2):")
        logger.info(f"   ì›ë³¸: {kosmos2_original_results['inference_time_ms']:.2f}ms, {kosmos2_original_results['fps']:.1f} FPS")
        logger.info(f"   FP16: {kosmos2_fp16_results['inference_time_ms']:.2f}ms, {kosmos2_fp16_results['fps']:.1f} FPS")
        logger.info(f"   ì†ë„ í–¥ìƒ: {results['kosmos2']['improvement']['speedup']:.2f}x")
        logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {results['kosmos2']['improvement']['memory_save']:.1f}%")
        
        # CLIP ê²°ê³¼
        logger.info(f"\nğŸ¥ˆ MAE 0.212 ëª¨ë¸ (CLIP):")
        logger.info(f"   ì›ë³¸: {clip_original_results['inference_time_ms']:.2f}ms, {clip_original_results['fps']:.1f} FPS")
        logger.info(f"   FP16: {clip_fp16_results['inference_time_ms']:.2f}ms, {clip_fp16_results['fps']:.1f} FPS")
        logger.info(f"   ì†ë„ í–¥ìƒ: {results['clip']['improvement']['speedup']:.2f}x")
        logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {results['clip']['improvement']['memory_save']:.1f}%")
        
        # ëª¨ë¸ ê°„ ë¹„êµ
        logger.info(f"\nğŸ† ëª¨ë¸ ê°„ ë¹„êµ:")
        kosmos2_fp16_fps = kosmos2_fp16_results['fps']
        clip_fp16_fps = clip_fp16_results['fps']
        logger.info(f"   Kosmos2 FP16: {kosmos2_fp16_fps:.1f} FPS")
        logger.info(f"   CLIP FP16: {clip_fp16_fps:.1f} FPS")
        logger.info(f"   CLIPì´ Kosmos2ë³´ë‹¤ {clip_fp16_fps/kosmos2_fp16_fps:.2f}x ë¹ ë¦„")
        
        # 4. ê²°ê³¼ ì €ì¥
        with open('quantization_comparison_results.json', 'w') as f:
            # Tensor ê°ì²´ ì œê±° í›„ ì €ì¥
            clean_results = {}
            for model_name, model_results in results.items():
                clean_results[model_name] = {}
                for test_name, test_results in model_results.items():
                    if test_name == 'improvement':
                        clean_results[model_name][test_name] = test_results
                    else:
                        clean_results[model_name][test_name] = {
                            'inference_time_ms': test_results['inference_time_ms'],
                            'inference_time_std': test_results['inference_time_std'],
                            'memory_usage_mb': test_results['memory_usage_mb'],
                            'peak_memory_mb': test_results['peak_memory_mb'],
                            'fps': test_results['fps']
                        }
            
            json.dump(clean_results, f, indent=2)
        
        logger.info("\nâœ… ê²°ê³¼ê°€ quantization_comparison_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return results

def main():
    tester = QuantizationComparisonTest()
    results = tester.run_comparison()
    
    return results

if __name__ == "__main__":
    main()
