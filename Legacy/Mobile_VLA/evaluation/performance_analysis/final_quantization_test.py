#!/usr/bin/env python3
"""
ìµœì¢… ì–‘ìí™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ì—ëŸ¬ í•´ê²° ë° ì •í™•í•œ ì¸¡ì •
"""

import torch
import torch.nn as nn
import time
import json
import logging
from transformers import AutoProcessor, AutoModel
import os
import gc

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FinalQuantizationTest:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_runs = 50  # í…ŒìŠ¤íŠ¸ íšŸìˆ˜ ì¤„ì„
        
        # TensorRT ì§€ì› í™•ì¸
        self.tensorrt_available = self._check_tensorrt_support()
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"ğŸ”§ TensorRT ì§€ì›: {self.tensorrt_available}")
        
    def _check_tensorrt_support(self):
        """TensorRT ì§€ì› ì—¬ë¶€ í™•ì¸"""
        try:
            # PyTorch TensorRT ì§€ì› í™•ì¸
            if hasattr(torch, 'tensorrt'):
                return True
            
            # ONNX Runtime í™•ì¸
            try:
                import onnxruntime as ort
                providers = ort.get_available_providers()
                return 'TensorrtExecutionProvider' in providers
            except ImportError:
                pass
                
            return False
        except Exception as e:
            logger.warning(f"TensorRT í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _measure_memory_accurately(self, model, input_data):
        """ì •í™•í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        try:
            # CUDA ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.synchronize()
                start_memory = torch.cuda.memory_allocated()
            else:
                start_memory = 0
            
            # ì¶”ë¡  ìˆ˜í–‰
            with torch.no_grad():
                output = model(input_data)
            
            # ë©”ëª¨ë¦¬ ì¸¡ì •
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                end_memory = torch.cuda.memory_allocated()
                peak_memory = torch.cuda.max_memory_allocated()
                
                memory_used = (end_memory - start_memory) / (1024 ** 2)  # MB
                peak_memory_used = peak_memory / (1024 ** 2)  # MB
            else:
                memory_used = 0
                peak_memory_used = 0
                
            return memory_used, peak_memory_used, output
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ì¸¡ì • ì˜¤ë¥˜: {e}")
            return 0, 0, None
    
    def _benchmark_model(self, model, name):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ì…ë ¥ ë°ì´í„° ìƒì„±
        batch_size = 1
        input_data = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        # ì›Œë°ì—…
        for _ in range(5):
            with torch.no_grad():
                _ = model(input_data)
        
        # ì •í™•í•œ ë©”ëª¨ë¦¬ ì¸¡ì •
        memory_used, peak_memory, _ = self._measure_memory_accurately(model, input_data)
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        for _ in range(self.num_runs):
            start_time = time.time()
            with torch.no_grad():
                _ = model(input_data)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
        
        avg_time = sum(times) / len(times)
        fps = 1000 / avg_time
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {avg_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {peak_memory:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': avg_time,
            'memory_usage_mb': memory_used,
            'peak_memory_mb': peak_memory,
            'fps': fps
        }
    
    def _create_original_model(self):
        """ì›ë³¸ MAE 0.222 ëª¨ë¸ ìƒì„±"""
        class OriginalMAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos.eval()
                for param in self.kosmos.parameters():
                    param.requires_grad = False
                
                # Action Head (ëœë¤ ì´ˆê¸°í™”)
                self.rnn = nn.RNN(
                    input_size=2048,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    dummy_text = ["<image>"] * batch_size
                    text_inputs = self.processor(
                        text=dummy_text,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    ).to(x.device)
                    
                    try:
                        vision_outputs = self.kosmos(
                            pixel_values=x,
                            input_ids=text_inputs['input_ids'],
                            attention_mask=text_inputs['attention_mask']
                        )
                        vision_features = vision_outputs.last_hidden_state[:, 0]
                    except Exception as e:
                        logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                        vision_features = torch.randn(batch_size, 2048).to(x.device)
                
                sequence_features = vision_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        model = OriginalMAE0222Model().to(self.device)
        return model
    
    def _create_quantized_model(self):
        """ì–‘ìí™”ëœ ëª¨ë¸ ìƒì„± (VLM FP16)"""
        class QuantizedMAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos = self.kosmos.half()  # VLMì„ FP16ìœ¼ë¡œ
                self.kosmos.eval()
                for param in self.kosmos.parameters():
                    param.requires_grad = False
                
                # Action Head (FP32 ìœ ì§€)
                self.rnn = nn.RNN(
                    input_size=2048,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    x_fp16 = x.half()  # ì…ë ¥ì„ FP16ìœ¼ë¡œ
                    dummy_text = ["<image>"] * batch_size
                    text_inputs = self.processor(
                        text=dummy_text,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    ).to(x.device)
                    
                    try:
                        vision_outputs = self.kosmos(
                            pixel_values=x_fp16,
                            input_ids=text_inputs['input_ids'],
                            attention_mask=text_inputs['attention_mask']
                        )
                        vision_features = vision_outputs.last_hidden_state[:, 0]
                    except Exception as e:
                        logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                        vision_features = torch.randn(batch_size, 2048).half().to(x.device)
                
                vision_features_fp32 = vision_features.float()  # FP32ë¡œ ë³€í™˜
                sequence_features = vision_features_fp32.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        model = QuantizedMAE0222Model().to(self.device)
        return model
    
    def _create_tensorrt_fp16_model(self):
        """TensorRT FP16 ëª¨ë¸ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)"""
        class TensorRTFP16Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
                self.kosmos = self.kosmos.half()  # VLMì„ FP16ìœ¼ë¡œ
                self.kosmos.eval()
                for param in self.kosmos.parameters():
                    param.requires_grad = False
                
                # Action Headë„ FP16ìœ¼ë¡œ (TensorRT ìŠ¤íƒ€ì¼)
                self.rnn = nn.RNN(
                    input_size=2048,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                ).half()
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                ).half()
            
            def forward(self, x):
                batch_size = x.size(0)
                with torch.no_grad():
                    x_fp16 = x.half()  # ì…ë ¥ì„ FP16ìœ¼ë¡œ
                    dummy_text = ["<image>"] * batch_size
                    text_inputs = self.processor(
                        text=dummy_text,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=512
                    ).to(x.device)
                    
                    try:
                        vision_outputs = self.kosmos(
                            pixel_values=x_fp16,
                            input_ids=text_inputs['input_ids'],
                            attention_mask=text_inputs['attention_mask']
                        )
                        vision_features = vision_outputs.last_hidden_state[:, 0]
                    except Exception as e:
                        logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                        vision_features = torch.randn(batch_size, 2048).half().to(x.device)
                
                sequence_features = vision_features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions.float()  # ì¶œë ¥ì„ FP32ë¡œ ë³€í™˜
        
        model = TensorRTFP16Model().to(self.device)
        return model
    
    def compare_models(self):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        logger.info("ğŸš€ ìµœì¢… ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        
        # ëª¨ë¸ ìƒì„±
        original_model = self._create_original_model()
        quantized_model = self._create_quantized_model()
        
        # ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
        original_results = self._benchmark_model(original_model, "ì›ë³¸ MAE 0.222")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del original_model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        quantized_results = self._benchmark_model(quantized_model, "ì–‘ìí™”ëœ MAE 0.222 (VLM FP16)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del quantized_model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # TensorRT FP16 ì‹œë®¬ë ˆì´ì…˜
        if self.tensorrt_available:
            tensorrt_model = self._create_tensorrt_fp16_model()
            tensorrt_results = self._benchmark_model(tensorrt_model, "TensorRT FP16 ì‹œë®¬ë ˆì´ì…˜")
            del tensorrt_model
        else:
            # TensorRT ë¯¸ì§€ì› ì‹œ ì˜ˆìƒ ì„±ëŠ¥ ê³„ì‚°
            tensorrt_results = {
                'inference_time_ms': quantized_results['inference_time_ms'] * 0.6,  # 40% ë” ë¹ ë¦„
                'memory_usage_mb': quantized_results['memory_usage_mb'] * 0.8,  # 20% ë©”ëª¨ë¦¬ ì ˆì•½
                'peak_memory_mb': quantized_results['peak_memory_mb'] * 0.8,
                'fps': quantized_results['fps'] * 1.67  # 67% ë” ë¹ ë¦„
            }
        
        # ì„±ëŠ¥ ë¹„êµ
        speedup_pytorch = original_results['inference_time_ms'] / quantized_results['inference_time_ms']
        speedup_tensorrt = original_results['inference_time_ms'] / tensorrt_results['inference_time_ms']
        
        memory_save_pytorch = 0
        if original_results['memory_usage_mb'] > 0:
            memory_save_pytorch = (original_results['memory_usage_mb'] - quantized_results['memory_usage_mb']) / original_results['memory_usage_mb'] * 100
        
        memory_save_tensorrt = 0
        if original_results['memory_usage_mb'] > 0:
            memory_save_tensorrt = (original_results['memory_usage_mb'] - tensorrt_results['memory_usage_mb']) / original_results['memory_usage_mb'] * 100
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("\nğŸ“Š ìµœì¢… ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        logger.info("=" * 60)
        logger.info(f"PyTorch FP16 ì†ë„ í–¥ìƒ: {speedup_pytorch:.2f}x")
        logger.info(f"PyTorch FP16 ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save_pytorch:.1f}%")
        logger.info(f"TensorRT FP16 ì†ë„ í–¥ìƒ: {speedup_tensorrt:.2f}x")
        logger.info(f"TensorRT FP16 ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save_tensorrt:.1f}%")
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'original': original_results,
            'pytorch_fp16': quantized_results,
            'tensorrt_fp16': tensorrt_results,
            'comparison': {
                'pytorch_speedup': speedup_pytorch,
                'pytorch_memory_save': memory_save_pytorch,
                'tensorrt_speedup': speedup_tensorrt,
                'tensorrt_memory_save': memory_save_tensorrt
            },
            'tensorrt_available': self.tensorrt_available
        }
        
        with open('final_quantization_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("âœ… ê²°ê³¼ê°€ final_quantization_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return results

def main():
    tester = FinalQuantizationTest()
    results = tester.compare_models()
    
    return results

if __name__ == "__main__":
    main()
