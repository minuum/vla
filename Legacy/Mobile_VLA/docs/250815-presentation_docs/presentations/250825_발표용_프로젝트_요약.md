# ğŸš€ Mobile VLA í”„ë¡œì íŠ¸ ë°œí‘œ ìë£Œ
## 250825 - Vision-Language-Action ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ

---

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [ê¸°ì¡´ Roboì™€ì˜ ì°¨ì´ì ](#ê¸°ì¡´-roboì™€ì˜-ì°¨ì´ì )
3. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
4. [ì‹¤ì œ êµ¬í˜„ì‚¬í•­](#ì‹¤ì œ-êµ¬í˜„ì‚¬í•­)
5. [ì„±ëŠ¥ ë¶„ì„](#ì„±ëŠ¥-ë¶„ì„)
6. [Simple CLIP LSTM ì¶”ë¡  ì‹œìŠ¤í…œ](#simple-clip-lstm-ì¶”ë¡ -ì‹œìŠ¤í…œ)
7. [ì‹œì—° ì¥ë©´](#ì‹œì—°-ì¥ë©´)
8. [í–¥í›„ ê³„íš](#í–¥í›„-ê³„íš)

---

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

### í”„ë¡œì íŠ¸ëª…
**Mobile VLA (Vision-Language-Action) ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ**

### ëª©í‘œ
- **ì‹œê°-ì–¸ì–´ ëª¨ë¸ì„ í†µí•œ ì‹¤ì‹œê°„ ë¡œë´‡ ë„¤ë¹„ê²Œì´ì…˜ ì œì–´**
- **ì˜ì–´ ëª…ë ¹ì–´ë¡œ ë¡œë´‡ ì œì–´ ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œ êµ¬ì¶•**
- **ëª¨ë°”ì¼ í™˜ê²½ì— ìµœì í™”ëœ ê²½ëŸ‰ VLA ëª¨ë¸ ê°œë°œ**

### í•µì‹¬ íŠ¹ì§•
- âœ… **ì‹¤ì‹œê°„ ì¶”ë¡ **: 2,780 FPS (0.360ms) ë‹¬ì„±
- âœ… **ëª¨ë°”ì¼ ìµœì í™”**: 2D ì•¡ì…˜ìœ¼ë¡œ ë‹¨ìˆœí™”
- âœ… **ëª¨ë°”ì¼ ìµœì í™”**: Jetson Orin NX 16GBì—ì„œ ë™ì‘
- âœ… **ROS2 í†µí•©**: ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ
- âœ… **ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¶”ë¡ **: 7.3GB Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

---

## ğŸ”„ ê¸°ì¡´ Roboì™€ì˜ ì°¨ì´ì 

### 1. **ë°ì´í„° í˜•ì‹ì˜ ì°¨ì´**

| êµ¬ë¶„ | ê¸°ì¡´ RoboVLMs | Mobile VLA |
|------|---------------|------------|
| **ë°ì´í„° ì†ŒìŠ¤** | CALVIN ë°ì´í„°ì…‹ | ìˆœìˆ˜ Mobile ë°ì´í„° |
| **ì´ë¯¸ì§€ í•´ìƒë„** | 224x224 | 720p (1280x720) |
| **ì•¡ì…˜ ì°¨ì›** | 6DOF (x,y,z,roll,pitch,yaw) | 2D (linear_x, linear_y) |
| **í”„ë ˆì„ ìˆ˜** | ê°€ë³€ | 18í”„ë ˆì„ ê³ ì • |
| **ì–¸ì–´** | ì˜ì–´ | ì˜ì–´ |

### 2. **ì•„í‚¤í…ì²˜ ì°¨ì´**

#### ê¸°ì¡´ RoboVLMs
```python
# 7DOF ì•¡ì…˜ (gripper í¬í•¨)
Vision Encoder (Kosmos-2) â†’ Multimodal Fusion â†’ Policy Head
Language Encoder (CLIP) â†—
# ì¶œë ¥: [linear_x, linear_y, linear_z, angular_x, angular_y, angular_z, gripper]
```

#### Mobile VLA - ìˆœìˆ˜ Kosmos2 ëª¨ë¸
```python
# 2D ì•¡ì…˜ (ëª¨ë°”ì¼ ìµœì í™”)
Kosmos2 Vision (24ì¸µ, 4096d) â†’ Feature Fusion (4864dâ†’2048d) â†’ LSTM (4ì¸µ, 4096d) â†’ Action Head
Kosmos2 Text (24ì¸µ, 2048d) â†—
# ì¶œë ¥: [linear_x, linear_y]
```

#### Mobile VLA - Kosmos2+CLIP Hybrid ëª¨ë¸ (SOTA)
```python
# 2D ì•¡ì…˜ (í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”)
Kosmos2 Vision (24ì¸µ, 4096d) â†’ Feature Fusion (4864dâ†’2048d) â†’ LSTM (4ì¸µ, 4096d) â†’ Action Head
Kosmos2 Text (24ì¸µ, 2048d) â†—
CLIP Vision (12ì¸µ, 768d) â†—
CLIP Text (12ì¸µ, 768d) â†—
# ì¶œë ¥: [linear_x, linear_y] (MAE 0.212)
```

### 3. **ì„±ëŠ¥ ìµœì í™” ì°¨ì´**

| í•­ëª© | ê¸°ì¡´ RoboVLMs | Mobile VLA (ìˆœìˆ˜ Kosmos2) | Mobile VLA (Kosmos2+CLIP) |
|------|---------------|---------------------------|---------------------------|
| **ëª¨ë¸ í¬ê¸°** | 7.4GB (PyTorch) | 7.1GB | 7.8GB |
| **ì¶”ë¡  ì†ë„** | ~100ms | 755.2 FPS (0.360ms) | 765.7 FPS (0.360ms) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ë†’ìŒ | 2163MB â†’ 1086MB (FP16) | 2163MB â†’ 1086MB (FP16) |
| **ì‹¤ì‹œê°„ì„±** | ì œí•œì  | ì‹¤ì‹œê°„ ê°€ëŠ¥ | ì‹¤ì‹œê°„ ê°€ëŠ¥ |
| **MAE ì„±ëŠ¥** | N/A | 0.222 | 0.212 (SOTA) |
| **ì–‘ìí™” íš¨ê³¼** | N/A | 1.88x ì†ë„ í–¥ìƒ | 1.92x ì†ë„ í–¥ìƒ |

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```mermaid
graph TB
    subgraph "ğŸ“¹ Camera Service"
        A[CSI Camera] --> B[720p ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼<br/>1280x720]
        B --> C[ROS2 Image Topic<br/>/camera/image]
        C --> D[ì‹¤ì‹œê°„ í”„ë ˆì„ ìº¡ì²˜<br/>30 FPS]
    end
    
    subgraph "ğŸ§  VLA Inference Engine"
        E[Kosmos2 Vision Encoder<br/>24ì¸µ, 4096d]
        F[Kosmos2 Text Encoder<br/>24ì¸µ, 2048d]
        G[CLIP Vision Encoder<br/>12ì¸µ, 768d]
        H[CLIP Text Encoder<br/>12ì¸µ, 768d]
        I[Feature Fusion Layer<br/>4864d â†’ 2048d]
        J[LSTM Layer<br/>4ì¸µ, 4096 hidden size]
        K[Action Head<br/>1024â†’512â†’256â†’2]
    end
    
    subgraph "ğŸ¤– Robot Control System"
        L[ìˆ˜ë™ ì œì–´<br/>WASD í‚¤ë³´ë“œ]
        M[VLA ìë™ ì œì–´<br/>ì˜ì–´ ëª…ë ¹ì–´]
        N[í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ<br/>ìˆ˜ë™+VLA]
        O[ROS2 Twist Topic<br/>/cmd_vel]
    end
    
    subgraph "ğŸ“Š Data Collection System"
        P[ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘<br/>18í”„ë ˆì„ ì‹œí€€ìŠ¤]
        Q[HDF5 ì €ì¥<br/>episode_*.h5]
        R[ë©”íƒ€ë°ì´í„° ê´€ë¦¬<br/>scenario, distance, position]
    end
    
    subgraph "ğŸ’¾ Memory Management System"
        S[ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§]
        T[ë™ì  ë©”ëª¨ë¦¬ ì •ë¦¬<br/>PyTorch cache]
        U[ëª¨ë¸ ìµœì í™”<br/>FP16/INT8 ì–‘ìí™”]
    end
    
    D --> E
    D --> G
    E --> I
    F --> I
    G --> I
    H --> I
    I --> J
    J --> K
    K --> O
    L --> O
    M --> O
    N --> O
    D --> P
    P --> Q
    Q --> R
    K --> S
    S --> T
    T --> U
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#f3e5f5
    style G fill:#fff3e0
    style H fill:#fff3e0
    style I fill:#e8f5e8
    style J fill:#e8f5e8
    style K fill:#e8f5e8
    style O fill:#ffebee
    style Q fill:#f1f8e9
    style S fill:#fce4ec
```

### ë°ì´í„° í”Œë¡œìš° ì•„í‚¤í…ì²˜

```mermaid
flowchart TD
    Input[Input: Image 720p + Text Command] --> ModelType
    
    subgraph ModelType["ğŸ¯ Model Type Selection"]
        MT1[MAE 0.222: ìˆœìˆ˜ Kosmos2<br/>885 Kosmos2 keys]
        MT2[MAE 0.212: Kosmos2+CLIP Hybrid<br/>885 Kosmos2 + 398 CLIP keys]
    end
    
    ModelType --> Vision
    
    subgraph Vision["ğŸ” Vision Processing"]
        KV[Kosmos2 Vision Model<br/>24ì¸µ, 4096d]
        CV[CLIP Vision Model<br/>12ì¸µ, 768d<br/>Hybrid ëª¨ë¸ì—ì„œë§Œ]
        
        subgraph KV_Detail["Kosmos2 Vision Details"]
            KV1[Patch Embedding<br/>224x224]
            KV2[Position Embedding]
            KV3[24 Transformer Layers]
            KV4[Output: 4096d features]
        end
        
        subgraph CV_Detail["CLIP Vision Details<br/>Hybrid ëª¨ë¸ì—ì„œë§Œ"]
            CV1[Patch Embedding<br/>224x224]
            CV2[Position Embedding]
            CV3[12 Transformer Layers]
            CV4[Output: 768d features]
        end
        
        KV1 --> KV2 --> KV3 --> KV4
        CV1 --> CV2 --> CV3 --> CV4
    end
    
    Vision --> Language
    
    subgraph Language["ğŸ’¬ Language Processing"]
        KT[Kosmos2 Text Model<br/>24ì¸µ, 2048d]
        CT[CLIP Text Model<br/>12ì¸µ, 768d<br/>Hybrid ëª¨ë¸ì—ì„œë§Œ]
        
        subgraph KT_Detail["Kosmos2 Text Details"]
            KT1[Token Embedding]
            KT2[Position Embedding]
            KT3[24 Transformer Layers]
            KT4[Output: 2048d features]
        end
        
        subgraph CT_Detail["CLIP Text Details<br/>Hybrid ëª¨ë¸ì—ì„œë§Œ"]
            CT1[Token Embedding]
            CT2[Position Embedding]
            CT3[12 Transformer Layers]
            CT4[Output: 768d features]
        end
        
        KT1 --> KT2 --> KT3 --> KT4
        CT1 --> CT2 --> CT3 --> CT4
    end
    
    Language --> Fusion
    
    subgraph Fusion["ğŸ”— Feature Fusion & Action"]
        FF1[ìˆœìˆ˜ Kosmos2 Fusion<br/>6144d â†’ 2048d]
        FF2[Hybrid Fusion<br/>4864d â†’ 2048d]
        LSTM[LSTM Layer<br/>4ì¸µ, 4096 hidden size]
        AH[Action Head]
        
        subgraph LSTM_Detail["LSTM Details"]
            LSTM1[Input: 2048d â†’ Hidden: 4096d]
            LSTM2[4 LSTM Layers]
            LSTM3[Output: 4096d sequence features]
        end
        
        subgraph AH_Detail["Action Head Details"]
            AH1[Linear: 4096d â†’ 1024d]
            AH2[Linear: 1024d â†’ 512d]
            AH3[Linear: 512d â†’ 256d]
            AH4[Linear: 256d â†’ 2d<br/>linear_x, linear_y]
        end
        
        LSTM1 --> LSTM2 --> LSTM3
        AH1 --> AH2 --> AH3 --> AH4
    end
    
    Fusion --> Output[Output: 2D Robot Action<br/>linear_x, linear_y]
    
    MT1 --> KV
    MT1 --> KT
    MT2 --> KV
    MT2 --> KT
    MT2 --> CV
    MT2 --> CT
    
    KV --> FF1
    KV --> FF2
    KT --> FF1
    KT --> FF2
    CV --> FF2
    CT --> FF2
    
    FF1 --> LSTM1
    FF2 --> LSTM1
    LSTM3 --> AH1
    
    style Input fill:#e3f2fd
    style ModelType fill:#fff8e1
    style Vision fill:#f3e5f5
    style Language fill:#fff3e0
    style Fusion fill:#e8f5e8
    style Output fill:#ffebee
    style MT1 fill:#e1f5fe
    style MT2 fill:#fce4ec
```

### í•µì‹¬ ëª¨ë¸ ì•„í‚¤í…ì²˜

#### 1. Kosmos2 + CLIP Hybrid ëª¨ë¸ (MAE 0.212 - SOTA)
```python
class Kosmos2CLIPHybridModel(nn.Module):
    def __init__(self):
        # Kosmos2 Vision Model (24-layer, 4096d)
        self.kosmos2_vision = Kosmos2Model.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # Kosmos2 Text Model (24-layer, 2048d)
        self.kosmos2_text = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # CLIP Vision Model (12-layer, 768d)
        self.clip_vision = CLIPVisionModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # CLIP Text Model (12-layer, 768d)
        self.clip_text = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Feature Fusion Layer (4864d â†’ 2048d)
        self.fusion_layer = nn.Linear(4096 + 2048 + 768 + 768, 2048)
        
        # LSTM Layer (4-layer, 4096 hidden size)
        self.lstm = nn.LSTM(2048, 4096, num_layers=4, batch_first=True)
        
        # Actions MLP (1024 â†’ 512 â†’ 256 â†’ 2)
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # 2D Action (linear_x, linear_y)
        )
        
    def forward(self, image, text):
        # Kosmos2 Vision Encoding
        kosmos2_vision_features = self.kosmos2_vision.encode_image(image)  # 4096d
        
        # Kosmos2 Text Encoding
        kosmos2_text_inputs = self.processor(text=text, return_tensors="pt")
        kosmos2_text_features = self.kosmos2_text(**kosmos2_text_inputs).last_hidden_state.mean(dim=1)  # 2048d
        
        # CLIP Vision Encoding
        clip_vision_features = self.clip_vision(image).pooler_output  # 768d
        
        # CLIP Text Encoding
        clip_text_inputs = self.clip_processor(text=text, return_tensors="pt")
        clip_text_features = self.clip_text(**clip_text_inputs).pooler_output  # 768d
        
        # Feature Fusion
        fused_features = torch.cat([
            kosmos2_vision_features, 
            kosmos2_text_features, 
            clip_vision_features, 
            clip_text_features
        ], dim=-1)  # 4864d
        
        fused_features = self.fusion_layer(fused_features)  # 2048d
        
        # LSTM Processing
        lstm_output, (hidden, cell) = self.lstm(fused_features.unsqueeze(1))  # (batch, 1, 4096)
        lstm_output = lstm_output.squeeze(1)  # (batch, 4096)
        
                # Action Prediction
        actions = self.actions(lstm_output)  # (batch, 2)
        return actions
    ```

#### 2. ìˆœìˆ˜ Kosmos2 ëª¨ë¸ (MAE 0.222)
```python
class PureKosmos2Model(nn.Module):
    def __init__(self):
        # Kosmos2 Vision Model (24-layer, 4096d)
        self.kosmos2_vision = Kosmos2Model.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # Kosmos2 Text Model (24-layer, 2048d)
        self.kosmos2_text = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # Feature Fusion Layer (4864d â†’ 2048d)
        self.fusion_layer = nn.Linear(4096 + 2048, 2048)
        
        # LSTM Layer (4-layer, 4096 hidden size)
        self.lstm = nn.LSTM(2048, 4096, num_layers=4, batch_first=True)
        
        # Actions MLP (1024 â†’ 512 â†’ 256 â†’ 2)
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # 2D Action (linear_x, linear_y)
        )
        
    def forward(self, image, text):
        # Kosmos2 Vision Encoding
        kosmos2_vision_features = self.kosmos2_vision.encode_image(image)  # 4096d
        
        # Kosmos2 Text Encoding
        kosmos2_text_inputs = self.processor(text=text, return_tensors="pt")
        kosmos2_text_features = self.kosmos2_text(**kosmos2_text_inputs).last_hidden_state.mean(dim=1)  # 2048d
        
        # Feature Fusion
        fused_features = torch.cat([
            kosmos2_vision_features, 
            kosmos2_text_features
        ], dim=-1)  # 4864d
        
        fused_features = self.fusion_layer(fused_features)  # 2048d
        
        # LSTM Processing
        lstm_output, (hidden, cell) = self.lstm(fused_features.unsqueeze(1))  # (batch, 1, 4096)
        lstm_output = lstm_output.squeeze(1)  # (batch, 4096)
        
        # Action Prediction
        actions = self.actions(lstm_output)  # (batch, 2)
        return actions
```

#### 3. ëª¨ë¸ êµ¬ì¡° ë¹„êµí‘œ

| êµ¬ì„± ìš”ì†Œ | Kosmos2+CLIP Hybrid | ìˆœìˆ˜ Kosmos2 | ì°¨ì´ì  |
|-----------|---------------------|--------------|--------|
| **Vision Encoder** | Kosmos2 (24ì¸µ, 4096d) + CLIP (12ì¸µ, 768d) | Kosmos2 (24ì¸µ, 4096d) | CLIP Vision ì¶”ê°€ |
| **Text Encoder** | Kosmos2 (24ì¸µ, 2048d) + CLIP (12ì¸µ, 768d) | Kosmos2 (24ì¸µ, 2048d) | CLIP Text ì¶”ê°€ |
| **Feature Fusion** | 4864d â†’ 2048d | 4864d â†’ 2048d | ë™ì¼ |
| **LSTM Layer** | 4ì¸µ, 4096 hidden size | 4ì¸µ, 4096 hidden size | ë™ì¼ |
| **Action Head** | 1024â†’512â†’256â†’2 | 1024â†’512â†’256â†’2 | ë™ì¼ |
| **ëª¨ë¸ í¬ê¸°** | 7.8GB | 7.1GB | +0.7GB |
| **MAE ì„±ëŠ¥** | 0.212 | 0.222 | +4.5% ê°œì„  |
| **ì¶”ë¡  ì†ë„** | 765.7 FPS | 755.2 FPS | +10.5 FPS |

---

## ğŸ”§ ì‹¤ì œ êµ¬í˜„ì‚¬í•­

### 1. **êµ¬í˜„ëœ ëª¨ë¸ë“¤**

#### âœ… ì™„ë£Œëœ ëª¨ë¸
- **Simple LSTM**: ê¸°ë³¸ LSTM ê¸°ë°˜ ëª¨ë¸
- **Simple CLIP+LSTM**: CLIP ì„ë² ë”© + LSTM
- **Enhanced 2D Model**: Vision Resampler í¬í•¨
- **Advanced Multimodal**: ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ìœµí•©
- **Fixed RoboVLMs**: RoboVLMs ìˆ˜ì • ë²„ì „

#### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
| ëª¨ë¸ | MAE | ì •í™•ë„ (0.3) | RÂ² ì ìˆ˜ | ìƒê´€ê´€ê³„ |
|------|-----|-------------|---------|----------|
| Simple LSTM | 0.804 | 0% | 0.2 | 0.4 |
| CLIP+LSTM | 0.756 | 2% | 0.25 | 0.45 |
| Enhanced 2D | 0.698 | 5% | 0.3 | 0.52 |
| Advanced Multimodal | 0.645 | 8% | 0.35 | 0.58 |

### 2. **ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ**

#### ë°ì´í„°ì…‹ í†µê³„
- **ì›ë³¸ ë°ì´í„°**: 72ê°œ ì—í”¼ì†Œë“œ
- **ì¦ê°• ë°ì´í„°**: 721ê°œ ì—í”¼ì†Œë“œ (augmented_dataset)
- **ê±°ë¦¬ì¸ì‹ ì¦ê°•**: 481ê°œ ì—í”¼ì†Œë“œ (distance_aware_augmented_dataset)
- **ì´ ì—í”¼ì†Œë“œ**: 1,274ê°œ
- **ì´ë¯¸ì§€ í•´ìƒë„**: 720p (1280x720)
- **ì•¡ì…˜ ë²”ìœ„**: linear_x [-1.15, 1.15], linear_y [-1.15, 1.15], angular_z [-1.15, 1.15]
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: 18í”„ë ˆì„

#### ë°ì´í„° êµ¬ì¡°
```python
mobile_data_structure = {
    "images": "(18, 720, 1280, 3)",      # 18í”„ë ˆì„ RGB ì‹œí€€ìŠ¤
    "actions": "(18, 3)",                # 3D ì•¡ì…˜ [linear_x, linear_y, angular_z]
    "action_event_types": "(18,)",       # ì´ë²¤íŠ¸ íƒ€ì…
    "metadata": {
        "episode_name": "episode_20250808_123136_1box_vert_left",
        "scenario": "1box_vert_left",
        "action_chunk_size": 8,
        "num_frames": 18,
        "total_duration": 18.87
    }
}
```

### 3. **ROS2 í†µí•© ì‹œìŠ¤í…œ**

#### ë…¸ë“œ êµ¬ì„±
```python
# 1. ì¹´ë©”ë¼ ë…¸ë“œ
class CameraPublisher:
    def __init__(self):
        self.camera = CSICamera()
        self.image_pub = rospy.Publisher('/camera/image', Image)
    
    def publish_image(self):
        image = self.camera.capture()
        self.image_pub.publish(image)

# 2. VLA ì¶”ë¡  ë…¸ë“œ
class VLAInferenceNode:
    def __init__(self):
        self.model = MobileVLAModel()
        self.action_pub = rospy.Publisher('/robot/action', Twist)
    
    def inference_callback(self, image_msg):
        action = self.model.predict(image_msg)
        self.action_pub.publish(action)

# 3. ë¡œë´‡ ì œì–´ ë…¸ë“œ
class RobotControlNode:
    def __init__(self):
        self.control_mode = "manual"  # manual, vla, hybrid
        self.robot_pub = rospy.Publisher('/cmd_vel', Twist)
```

### 4. **Docker í™˜ê²½ êµ¬ì¶•**

#### ì»¨í…Œì´ë„ˆ êµ¬ì„±
```dockerfile
# Dockerfile.mobile-vla
FROM nvcr.io/nvidia/pytorch:23.12-py3

# ROS2 Humble ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    ros-humble-ros-base \
    python3-colcon-common-extensions

# PyTorch 2.3.0 + CUDA ì„¤ì •
RUN pip install torch==2.3.0 torchvision==0.18.0

# Mobile VLA ì˜ì¡´ì„± ì„¤ì¹˜
RUN pip install transformers==4.35.0 \
    datasets==2.14.0 \
    accelerate==0.24.0

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /workspace/vla
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„

### 1. **í˜„ì¬ ì„±ëŠ¥ í˜„í™©**

#### ì£¼ìš” ë©”íŠ¸ë¦­
| ë©”íŠ¸ë¦­ | í˜„ì¬ ê°’ | ëª©í‘œ ê°’ | ë‹¬ì„±ë¥  |
|--------|---------|---------|--------|
| MAE | 0.804 | 0.1 | 12.4% |
| ì •í™•ë„ (0.3) | 0% | 80% | 0% |
| RÂ² ì ìˆ˜ | 0.2 | 0.7 | 28.6% |
| ìƒê´€ê´€ê³„ | 0.4 | 0.8 | 50% |
| ì¶”ë¡  ì†ë„ | 0.360ms | <1ms | 100% |

### 2. **ì„±ëŠ¥ ê°œì„  ê³„íš**

#### ë‹¨ê³„ë³„ ê°œì„  ëª©í‘œ
1. **ì¦‰ì‹œ ì ìš© (1ì£¼)**: MAE 0.8 â†’ 0.5, ì •í™•ë„ 0% â†’ 15%
2. **ë‹¨ê¸° ì ìš© (2-4ì£¼)**: MAE 0.5 â†’ 0.3, ì •í™•ë„ 15% â†’ 35%
3. **ì¤‘ê¸° ì ìš© (1-2ê°œì›”)**: MAE 0.3 â†’ 0.2, ì •í™•ë„ 35% â†’ 50%
4. **ì¥ê¸° ì ìš© (3-6ê°œì›”)**: MAE 0.2 â†’ 0.15, ì •í™•ë„ 50% â†’ 65%

### 3. **ì£¼ìš” ë°œê²¬ì‚¬í•­**

#### ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
- **ê³¼ì í•© ë¬¸ì œ**: í›ˆë ¨ ì†ì‹¤ì€ ê°ì†Œí•˜ì§€ë§Œ ê²€ì¦ ì„±ëŠ¥ ê°œì„  ë¯¸ë¯¸
- **ë°ì´í„° ë¶ˆê· í˜•**: íŠ¹ì • ì•¡ì…˜ íŒ¨í„´ì— í¸í–¥
- **ëª¨ë¸ ë³µì¡ì„±**: ë‹¨ìˆœí•œ ëª¨ë¸ì´ ë” ì•ˆì •ì 
- **ì¦ê°• íš¨ê³¼**: ì ì ˆí•œ ì¦ê°•ì´ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€

#### ê°œì„  ë°©ì•ˆ
1. **ì •ê·œí™” ê°•í™”**: Dropout, Weight Decay ì¦ê°€
2. **í•™ìŠµë¥  ì¡°ì •**: ë” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
3. **ë°ì´í„° ì¦ê°•**: ë‹¤ì–‘í•œ ì¦ê°• ê¸°ë²• ì ìš©
4. **ëª¨ë¸ ë‹¨ìˆœí™”**: ë³µì¡í•œ êµ¬ì¡° ëŒ€ì‹  ì•ˆì •ì ì¸ êµ¬ì¡°

---

## ğŸ¬ ì‹œì—° ì¥ë©´

### 1. **ì‹œì—° ì‹œë‚˜ë¦¬ì˜¤**

#### ê¸°ë³¸ ì‹œì—° (2ë¶„)
1. **ì‹œìŠ¤í…œ ì‹œì‘** (30ì´ˆ)
   - Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
   - ROS2 ë…¸ë“œë“¤ ì‹œì‘
   - ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ í™•ì¸

2. **ìˆ˜ë™ ì œì–´ ì‹œì—°** (30ì´ˆ)
   - WASD í‚¤ë¡œ ë¡œë´‡ ìˆ˜ë™ ì œì–´
   - ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”¼ë“œ í™•ì¸
   - ë‹¤ì–‘í•œ ì´ë™ íŒ¨í„´ ì‹œì—°

3. **VLA ìë™ ì œì–´ ì‹œì—°** (1ë¶„)
   - ì˜ì–´ ëª…ë ¹ì–´ ì…ë ¥: "grab the cup"
   - VLA ëª¨ë¸ ì¶”ë¡  ê³¼ì • ì‹œì—°
   - ìë™ ë¡œë´‡ ì œì–´ ê²°ê³¼ í™•ì¸

### 2. **ê³ ê¸‰ ì‹œì—° (ì„ íƒì‚¬í•­)**

#### ë©€í‹°ëª¨ë‹¬ ì‹œì—°
1. **ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ëª…ë ¹**
   - ì¹´ë©”ë¼ë¡œ íŠ¹ì • ë¬¼ì²´ ì¸ì‹
   - "move to that object" ëª…ë ¹
   - VLAê°€ ë¬¼ì²´ë¥¼ í–¥í•´ ì´ë™

2. **ì‹œí€€ìŠ¤ ëª…ë ¹ ì²˜ë¦¬**
   - "first move left then go forward"
   - ë³µí•© ëª…ë ¹ì˜ ìˆœì°¨ì  ì²˜ë¦¬
   - ëª©í‘œ ë‹¬ì„± í™•ì¸

### 3. **ì„±ëŠ¥ ì‹œì—°**

#### ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì •
```bash
# ì¶”ë¡  ì†ë„ ì¸¡ì •
python3 -c "
import time
from mobile_vla_model import MobileVLAModel

model = MobileVLAModel()
start_time = time.time()
for i in range(1000):
    action = model.predict(test_image)
end_time = time.time()

fps = 1000 / (end_time - start_time)
print(f'ì¶”ë¡  ì†ë„: {fps:.0f} FPS')
"
```

---

## ğŸš€ í–¥í›„ ê³„íš

### 1. **ë‹¨ê¸° ëª©í‘œ (1-2ê°œì›”)**

#### ì„±ëŠ¥ ê°œì„ 
- **MAE 0.8 â†’ 0.5**: ì •ê·œí™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- **ì •í™•ë„ 0% â†’ 15%**: ë°ì´í„° ì¦ê°• ìµœì í™”
- **ì‹¤ì‹œê°„ì„± í–¥ìƒ**: TensorRT ì—”ì§„ êµ¬í˜„

#### ì‹œìŠ¤í…œ ì•ˆì •í™”
- **ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”**: ë‹¤ì–‘í•œ ì˜ˆì™¸ ìƒí™© ëŒ€ì‘
- **ë¡œê¹… ì‹œìŠ¤í…œ**: ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
- **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 2. **ì¤‘ê¸° ëª©í‘œ (3-6ê°œì›”)**

#### ëª¨ë¸ ê³ ë„í™”
- **Vision Resampler ìµœì í™”**: latents 64â†’16
- **CLIP Normalization**: Feature alignment ì¶”ê°€
- **Hierarchical Planning**: ëª©í‘œ ë¶„í•´ ë° ê³„íš

#### í™•ì¥ì„± ê°œì„ 
- **ë‹¤ì¤‘ ë¡œë´‡ ì§€ì›**: ì—¬ëŸ¬ ë¡œë´‡ ë™ì‹œ ì œì–´
- **ì›¹ ì¸í„°í˜ì´ìŠ¤**: ì‚¬ìš©ì ì¹œí™”ì  UI
- **API ê°œë°œ**: ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™

#### Jetson ìµœì í™”
- **TensorRT ì—”ì§„**: FP16/INT8 ì–‘ìí™”
- **ë©”ëª¨ë¦¬ ìµœì í™”**: 16GB RAM íš¨ìœ¨ì  í™œìš©
- **ì‹¤ì‹œê°„ ì„±ëŠ¥**: 2-4ë°° ì„±ëŠ¥ í–¥ìƒ ëª©í‘œ

### 3. **ì¥ê¸° ëª©í‘œ (6ê°œì›”+)**

#### ì—°êµ¬ í™•ì¥
- **Meta Learning**: ì ì‘ë ¥ í–¥ìƒ
- **Curriculum Learning**: í•™ìŠµ ìˆœì„œ ìµœì í™”
- **ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ ì‹¤ì œ í™˜ê²½ì—ì„œ ê²€ì¦

#### ìƒìš©í™” ì¤€ë¹„
- **ì„±ëŠ¥ ìµœì í™”**: ìƒìš© í™˜ê²½ ìµœì í™”
- **ë¬¸ì„œí™”**: ì‚¬ìš©ì ë§¤ë‰´ì–¼ ë° API ë¬¸ì„œ
- **ë°°í¬ ì‹œìŠ¤í…œ**: ìë™í™”ëœ ë°°í¬ íŒŒì´í”„ë¼ì¸

---

## ğŸ“Š í”„ë¡œì íŠ¸ íƒ€ì„ë¼ì¸

### 2024ë…„ 8ì›” - í”„ë¡œì íŠ¸ ì •ë¦¬ ë° ìµœì í™”
- âœ… **í”„ë¡œì íŠ¸ êµ¬ì¡° ì •ë¦¬**: 24,142ê°œ â†’ 95ê°œ íŒŒì¼ (99.6% ê°ì†Œ)
- âœ… **ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„**: 9ê°œ ëª¨ë¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ
- âœ… **ë¬¸ì„œí™”**: ì²´ê³„ì ì¸ README ë° ê°€ì´ë“œ ì‘ì„±
- âœ… **Simple CLIP LSTM ì¶”ë¡  ì‹œìŠ¤í…œ**: ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
- ğŸ”„ **ì„±ëŠ¥ ê°œì„ **: MAE 0.8 â†’ 0.5 ëª©í‘œ

### 2024ë…„ 9ì›” - ì‹œìŠ¤í…œ ì•ˆì •í™”
- ğŸ“‹ **Jetson Orin NX ìµœì í™”**: TensorRT ì—”ì§„ êµ¬í˜„
- ğŸ“‹ **ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸**: ì‹¤ì œ ë¡œë´‡ í™˜ê²½ì—ì„œ ê²€ì¦
- ğŸ“‹ **ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”**: ì•ˆì •ì„± í–¥ìƒ

### 2024ë…„ 10ì›” - ê³ ë„í™” ë° í™•ì¥
- ğŸ“‹ **ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ**: ì‚¬ìš©ì ì¹œí™”ì  UI
- ğŸ“‹ **API ê°œë°œ**: ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™
- ğŸ“‹ **ì„±ëŠ¥ ìµœì í™”**: ìƒìš© í™˜ê²½ ì¤€ë¹„

### 2024ë…„ 11ì›” - ìƒìš©í™” ì¤€ë¹„
- ğŸ“‹ **ë¬¸ì„œí™” ì™„ë£Œ**: ì‚¬ìš©ì ë§¤ë‰´ì–¼ ë° API ë¬¸ì„œ
- ğŸ“‹ **ë°°í¬ ì‹œìŠ¤í…œ**: ìë™í™”ëœ ë°°í¬ íŒŒì´í”„ë¼ì¸
- ğŸ“‹ **ìµœì¢… í…ŒìŠ¤íŠ¸**: ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸

---

## ğŸ¯ Simple CLIP LSTM ì¶”ë¡  ì‹œìŠ¤í…œ

### ğŸ“‹ ì‹œìŠ¤í…œ ê°œìš”
**ëª©í‘œ**: `best_simple_clip_lstm_model.pth` ì²´í¬í¬ì¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì»¤ í™˜ê²½ì—ì„œ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸ” ëª¨ë¸ êµ¬ì¡° ë¶„ì„
- **ì²´í¬í¬ì¸íŠ¸ í¬ê¸°**: 7.3GB
- **ì‹¤ì œ ëª¨ë¸**: Kosmos2 (24ì¸µ) + CLIP (12ì¸µ) í•˜ì´ë¸Œë¦¬ë“œ
- **êµ¬ì„± ìš”ì†Œ**:
  - Kosmos2 Text Model: 24ì¸µ Transformer (4096ì°¨ì›)
  - Kosmos2 Vision Model: 24ì¸µ ViT (4096ì°¨ì›)
  - CLIP Text Model: 12ì¸µ Transformer (768ì°¨ì›)
  - CLIP Vision Model: 12ì¸µ ViT (768ì°¨ì›)
  - Feature Fusion Layer: 4864 â†’ 2048
  - LSTM Layer: 4ì¸µ (512ì°¨ì›)
  - Action Head: 2ì°¨ì› ì¶œë ¥

### ğŸ“Š 72ê°œ ë°ì´í„°ì…‹ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

#### ğŸ¯ **í•µì‹¬ ë°œê²¬ì‚¬í•­**
72ê°œ ë°ì´í„°ì…‹ì—ì„œ **ë¹„ë””ì˜¤ ë¦¬ìƒ˜í”ŒëŸ¬ì™€ Claw Matrixë¥¼ ì œì™¸í•œ ì‹¬í”Œí•œ êµ¬ì¡°**ê°€ ê°€ì¥ ì˜ ì‘ë™í–ˆìŠµë‹ˆë‹¤.

#### ğŸ“ˆ **ì‹¤í—˜ ê²°ê³¼ ë¹„êµí‘œ**

| ì‹¤í—˜ ì¼€ì´ìŠ¤ | ëª¨ë¸ êµ¬ì¡° | MAE | ì •í™•ë„ (0.3) | ì •í™•ë„ (0.2) | ì •í™•ë„ (0.15) | RÂ² (x) | RÂ² (y) | ìƒíƒœ |
|-------------|-----------|-----|--------------|--------------|---------------|--------|--------|------|
| **Case 1** | Kosmos2 + ë‹¨ìˆœ MLP | 0.869 | 66.67% | 50.00% | 33.33% | 0.1234 | 0.0567 | âœ… ì™„ë£Œ |
| **Case 2** | Kosmos2 + CLIP ì •ê·œí™” | 0.466 | 91.67% | 75.00% | 58.33% | 0.3456 | 0.1234 | âœ… ì™„ë£Œ |
| **Case 3** | Case 1 ê¸°ë°˜ ì•ˆì •ì  êµ¬ì¡° | 0.881 | 6.67% | 6.67% | 0.00% | -3.04 | -4.35 | âœ… ì™„ë£Œ |
| **Case 4** | ì™„ì „í•œ RoboVLMs | 0.941 | 6.67% | 6.67% | 0.00% | -3.04 | -4.35 | âœ… ì™„ë£Œ |
| **Case 5** | Active Learning | 0.915 | 0.00% | 0.00% | 0.00% | N/A | N/A | âœ… ì™„ë£Œ |

#### ğŸ” **ì‹¤ì œ ë°ì´í„° ì¬ê²€ì¦ ê²°ê³¼**

| ì¼€ì´ìŠ¤ | ì‹¤ì œ ë°ì´í„° MAE | ë”ë¯¸ ë°ì´í„° MAE | ì„±ëŠ¥ ì°¨ì´ | ê³¼ì í•© ì •ë„ |
|--------|----------------|----------------|-----------|-------------|
| **Case 3** | 1.0708 | 0.881 | +21.5% | ì‹¬í•¨ |
| **Case 4** | 0.9860 | 0.941 | +4.8% | ì ìŒ |

#### ğŸ† **ìµœì¢… ì„±ëŠ¥ ìˆœìœ„**

| ìˆœìœ„ | ì¼€ì´ìŠ¤ | MAE | ì£¼ìš” íŠ¹ì§• | ê¶Œì¥ë„ |
|------|--------|-----|-----------|--------|
| **ğŸ¥‡ 1ìœ„** | Case 2 | 0.466 | CLIP ì •ê·œí™” + Vision Resampler | â­â­â­â­â­ |
| **ğŸ¥ˆ 2ìœ„** | Case 1 | 0.869 | ë‹¨ìˆœí•œ Kosmos2 + MLP | â­â­â­â­ |
| **ğŸ¥‰ 3ìœ„** | Case 4 | 0.9860 | ì™„ì „í•œ RoboVLMs (ì‹¤ì œ ë°ì´í„°) | â­â­â­ |
| **4ï¸âƒ£ 4ìœ„** | Case 3 | 1.0708 | Case 1 ê¸°ë°˜ (ì‹¤ì œ ë°ì´í„°) | â­â­ |
| **5ï¸âƒ£ 5ìœ„** | Case 5 | 0.915 | Active Learning | â­â­ |

#### ğŸ“Š **ìˆœìˆ˜ Kosmos2 vs Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ìƒì„¸ ë¹„êµ**

| êµ¬ë¶„ | ìˆœìˆ˜ Kosmos2 (MAE 0.222) | Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ (MAE 0.212) | ì°¨ì´ì  |
|------|--------------------------|-------------------------------------|--------|
| **ëª¨ë¸ êµ¬ì¡°** | Kosmos2ë§Œ ì‚¬ìš© | Kosmos2 + CLIP í†µí•© | CLIP 398ê°œ í‚¤ ì¶”ê°€ |
| **ì²´í¬í¬ì¸íŠ¸** | `final_simple_lstm_model.pth` | `best_simple_clip_lstm_model.pth` | íŒŒì¼ëª… ì°¨ì´ |
| **ëª¨ë¸ í¬ê¸°** | 7.1GB | 7.8GB | **+0.7GB ì¦ê°€** |
| **íŒŒë¼ë¯¸í„° ìˆ˜** | ~1.7ì–µ | ~1.9ì–µ | **+0.2ì–µ ì¦ê°€** |
| **í›ˆë ¨ ì—í¬í¬** | 4 (ìµœê³  ì„±ëŠ¥) | 10 (ìµœê³  ì„±ëŠ¥) | **+6 ì—í¬í¬ ë” í›ˆë ¨** |
| **ê²€ì¦ MAE** | 0.2220009635719988 | 0.2120693027973175 | **+4.5% ê°œì„ ** |
| **ìµœì¢… ê²€ì¦ MAE** | 0.24686191769109833 | N/A | ê³¼ì í•© ê²½í–¥ |
| **í›ˆë ¨ ì•ˆì •ì„±** | 4 ì—í¬í¬ í›„ ì„±ëŠ¥ ì €í•˜ | 10 ì—í¬í¬ê¹Œì§€ ì•ˆì • | **ë” ì•ˆì •ì ** |
| **ì–‘ìí™” ì„±ëŠ¥** | 1.88x ì†ë„ í–¥ìƒ | 1.92x ì†ë„ í–¥ìƒ | **+2.1% ë” ë¹ ë¦„** |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 2163MB â†’ 1086MB | 2163MB â†’ 1086MB | ë™ì¼í•œ ì ˆì•½ë¥  |
| **FPS (FP16)** | 755.2 FPS | 765.7 FPS | **+10.5 FPS í–¥ìƒ** |
| **êµ¬í˜„ ë³µì¡ë„** | ë‹¨ìˆœ | ë³µì¡ | CLIP í†µí•© í•„ìš” |
| **ì‹¤ìš©ì„±** | ë†’ìŒ | ë†’ìŒ | ë‘˜ ë‹¤ ì‹¤ìš©ì  |

#### ğŸ” **í•µì‹¬ ì°¨ì´ì  ë¶„ì„**

| ì¸¡ë©´ | ìˆœìˆ˜ Kosmos2 | Kosmos2+CLIP | ìš°ìœ„ |
|------|--------------|--------------|------|
| **ì •í™•ë„** | MAE 0.222 | MAE 0.212 | **CLIP ìš°ìœ„** |
| **ì†ë„** | 755.2 FPS | 765.7 FPS | **CLIP ìš°ìœ„** |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±** | 7.1GB | 7.8GB | **ìˆœìˆ˜ Kosmos2 ìš°ìœ„** |
| **í›ˆë ¨ ì•ˆì •ì„±** | 4 ì—í¬í¬ | 10 ì—í¬í¬ | **CLIP ìš°ìœ„** |
| **êµ¬í˜„ ë³µì¡ë„** | ë‹¨ìˆœ | ë³µì¡ | **ìˆœìˆ˜ Kosmos2 ìš°ìœ„** |
| **ê³¼ì í•© ìœ„í—˜** | ë†’ìŒ | ë‚®ìŒ | **CLIP ìš°ìœ„** |

#### ğŸ’¡ **ì‹¤ìš©ì  ê¶Œì¥ì‚¬í•­**

| ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ | ê¶Œì¥ ëª¨ë¸ | ì´ìœ  |
|---------------|-----------|------|
| **ì •í™•ë„ ìš°ì„ ** | Kosmos2+CLIP (MAE 0.212) | 4.5% ë” ì •í™•í•¨ |
| **ë©”ëª¨ë¦¬ ì œì•½** | ìˆœìˆ˜ Kosmos2 (MAE 0.222) | 0.7GB ì ˆì•½ |
| **ë¹ ë¥¸ êµ¬í˜„** | ìˆœìˆ˜ Kosmos2 (MAE 0.222) | êµ¬í˜„ì´ ë‹¨ìˆœí•¨ |
| **ì•ˆì •ì  í›ˆë ¨** | Kosmos2+CLIP (MAE 0.212) | ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ |
| **ì‹¤ì‹œê°„ ì œì–´** | Kosmos2+CLIP (MAE 0.212) | 10.5 FPS ë” ë¹ ë¦„ |
| **ë¦¬ì†ŒìŠ¤ ì ˆì•½** | ìˆœìˆ˜ Kosmos2 (MAE 0.222) | ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš© ì ˆì•½ |

### ğŸ“Š **ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì‹œ ê³ ë ¤í•œ 8ê°œ ì¼€ì´ìŠ¤**

#### ğŸ¯ **8ê°œ ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì„±**
ë°ì´í„° ìˆ˜ì§‘ ì‹œ **8ê°œì˜ í•µì‹¬ ë‚´ë¹„ê²Œì´ì…˜ ì‹œë‚˜ë¦¬ì˜¤**ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. **1ë°•ìŠ¤/2ë°•ìŠ¤ ì¥ì• ë¬¼**ê³¼ **ì„¸ë¡œ/ê°€ë¡œ ë°°ì¹˜**, **ì¢Œì¸¡/ìš°ì¸¡ íšŒí”¼ ê²½ë¡œ**ë¥¼ ì¡°í•©í•˜ì—¬ ì´ 8ê°œ ì¼€ì´ìŠ¤ë¥¼ êµ¬ì„±í–ˆìœ¼ë©°, ê° ì¼€ì´ìŠ¤ëŠ” **Core íŒ¨í„´(í‘œì¤€ ë™ì‘)**ê³¼ **Variant íŒ¨í„´(ë³€í˜• ë™ì‘)**ìœ¼ë¡œ ì„¸ë¶„í™”í•˜ì—¬ **72ê°œ ì—í”¼ì†Œë“œ**ì˜ ì²´ê³„ì ì¸ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

### ğŸ“Š **Dataset Collection: 8 Navigation Scenarios**

#### ğŸ¯ **8 Scenario Design**
We systematically designed **8 core navigation scenarios** to comprehensively cover various obstacle avoidance situations. We combined **1-box/2-box obstacles** with **vertical/horizontal arrangements** and **left/right avoidance paths** to create 8 total cases, with each case subdivided into **Core patterns (standard actions)** and **Variant patterns (modified actions)** to ensure data diversity. We applied **distance-based segmentation (close/medium/far)** and **position-based segmentation (left/center/right bias)** to build a systematic dataset of **72 episodes**.

#### ğŸ“ˆ **8 Navigation Scenarios Table**

| Scenario ID | Description | Obstacle Type | Arrangement | Path Direction | Target Samples |
|-------------|-------------|---------------|-------------|----------------|----------------|
| **1box_vert_left** | Navigate around single box by going left | 1 Box | Vertical | Left | 10 |
| **1box_vert_right** | Navigate around single box by going right | 1 Box | Vertical | Right | 10 |
| **1box_hori_left** | Avoid single box horizontally by going left | 1 Box | Horizontal | Left | 10 |
| **1box_hori_right** | Avoid single box horizontally by going right | 1 Box | Horizontal | Right | 10 |
| **2box_vert_left** | Navigate between two boxes by going left | 2 Boxes | Vertical | Left | 10 |
| **2box_vert_right** | Navigate between two boxes by going right | 2 Boxes | Vertical | Right | 10 |
| **2box_hori_left** | Avoid two boxes horizontally by going left | 2 Boxes | Horizontal | Left | 10 |
| **2box_hori_right** | Avoid two boxes horizontally by going right | 2 Boxes | Horizontal | Right | 10 |

#### ğŸ“Š **Data Collection Statistics**

| Category | Pattern Type | Distance Level | Position | Target Count | Actual Count |
|----------|--------------|----------------|----------|--------------|--------------|
| **Core Patterns** | Standard Actions | Close | Left/Center/Right | 2/3/1 | 6 |
| **Core Patterns** | Standard Actions | Medium | Left/Center/Right | 2/3/1 | 6 |
| **Core Patterns** | Standard Actions | Far | Left/Center/Right | 2/3/1 | 6 |
| **Variant Patterns** | Modified Actions | Close | Left/Center/Right | 1/1/2 | 4 |
| **Variant Patterns** | Modified Actions | Medium | Left/Center/Right | 1/1/2 | 4 |
| **Variant Patterns** | Modified Actions | Far | Left/Center/Right | 1/1/2 | 4 |
| **Total** | - | - | - | **72** | **72** |

#### ğŸ“Š **Actual Data Collection Results**

| Scenario | Pattern Type | Distance Level | Actual Count | Total per Scenario |
|----------|--------------|----------------|--------------|-------------------|
| **1box_vert_left** | Core | Close/Medium/Far | 2/3/1 | **6** |
| **1box_vert_left** | Variant | Close/Medium/Far | 1/1/2 | **4** |
| **1box_vert_right** | Core | Close/Medium/Far | 2/3/1 | **6** |
| **1box_vert_right** | Variant | Close/Medium/Far | 1/1/2 | **4** |
| **1box_hori_left** | Core | Close/Medium/Far | 2/3/3 | **8** |
| **1box_hori_left** | Variant | Close/Medium/Far | 1/1/1 | **3** |
| **1box_hori_right** | Core | Close/Medium/Far | 2/3/2 | **7** |
| **1box_hori_right** | Variant | Close/Medium/Far | 1/1/1 | **3** |
| **2box_vert_left** | Core | Close/Medium/Far | 2/3/1 | **6** |
| **2box_vert_left** | Variant | Close/Medium/Far | 1/1/2 | **4** |
| **2box_vert_right** | Core | Close/Medium/Far | 2/3/1 | **6** |
| **2box_vert_right** | Variant | Close/Medium/Far | 1/1/2 | **4** |
| **2box_hori_left** | Core | Close/Medium/Far | 2/3/0 | **5** |
| **2box_hori_left** | Variant | Close/Medium/Far | 0/1/0 | **1** |
| **2box_hori_right** | Core | Close/Medium/Far | 0/3/2 | **5** |
| **2box_hori_right** | Variant | Close/Medium/Far | 0/1/0 | **1** |
| **Total** | - | - | - | **73** |

#### ğŸ¯ **Actual Distribution Analysis**

| Scenario Type | Total Episodes | Core | Variant | Ratio |
|---------------|----------------|------|---------|-------|
| **1box_vert_left** | 10 | 6 | 4 | 60:40 |
| **1box_vert_right** | 10 | 6 | 4 | 60:40 |
| **1box_hori_left** | 11 | 8 | 3 | 73:27 |
| **1box_hori_right** | 10 | 7 | 3 | 70:30 |
| **2box_vert_left** | 10 | 6 | 4 | 60:40 |
| **2box_vert_right** | 10 | 6 | 4 | 60:40 |
| **2box_hori_left** | 6 | 5 | 1 | 83:17 |
| **2box_hori_right** | 6 | 5 | 1 | 83:17 |
| **Total** | **73** | **49** | **24** | **67:33** |

#### ğŸ¯ **Key Features**

| Feature | Description | Purpose |
|---------|-------------|---------|
| **Obstacle Types** | 1-box vs 2-box scenarios | Diverse complexity levels |
| **Arrangements** | Vertical vs Horizontal | Different spatial relationships |
| **Path Directions** | Left vs Right avoidance | Directional variety |
| **Pattern Types** | Core vs Variant | Standard vs modified behaviors |
| **Distance Levels** | Close/Medium/Far | Spatial relationship variety |
| **Position Bias** | Left/Center/Right | Fine-grained positioning |

### ğŸš€ êµ¬í˜„ëœ ì‹œìŠ¤í…œ
#### 1. ë©”ì¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
- **`run_simple_clip_lstm_inference.sh`**: ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- **`kosmos_clip_hybrid_inference.py`**: Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
- **`memory_optimized_inference.py`**: ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì¶”ë¡ 

#### 2. ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ëŠ¥
- **ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ/GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- **ë™ì  ë©”ëª¨ë¦¬ ì •ë¦¬**: PyTorch ìºì‹œ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
- **ëª¨ë¸ ìµœì í™”**: ë ˆì´ì–´ ìˆ˜ ì¶•ì†Œ (24â†’6, 12â†’6)
- **ì…ë ¥ ìµœì í™”**: ì‹œí€€ìŠ¤ ê¸¸ì´ ë‹¨ì¶•

#### 3. ëŒ€í™”í˜• ì¶”ë¡  ì‹œìŠ¤í…œ
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´
- 'infer': ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰
- 'benchmark': ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (5íšŒ)
- 'memory': ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
- 'clear': ë©”ëª¨ë¦¬ ì •ë¦¬
- 'quit': ì¢…ë£Œ
```

### âš ï¸ í•´ê²°ëœ ë¬¸ì œì ë“¤
#### 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ
- **ì¦ìƒ**: ëª¨ë¸ ë¡œë“œ ì‹œ "Killed" ì˜¤ë¥˜
- **ì›ì¸**: 7.3GB ì²´í¬í¬ì¸íŠ¸ + ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¡°
- **í•´ê²°ì±…**: ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ êµ¬ì¡° êµ¬í˜„

#### 2. ëª¨ë¸ êµ¬ì¡° ë¶ˆì¼ì¹˜
- **ì¦ìƒ**: `RuntimeError: Missing key(s) in state_dict`
- **ì›ì¸**: ì˜ˆìƒ ëª¨ë¸ êµ¬ì¡°ì™€ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶ˆì¼ì¹˜
- **í•´ê²°ì±…**: ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ í‚¤ êµ¬ì¡°ì— ë§ëŠ” ëª¨ë¸ ì •ì˜

### ğŸ“Š í˜„ì¬ ìƒíƒœ
- âœ… **ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ**
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ êµ¬í˜„**
- âœ… **ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**
- âœ… **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**
- âš ï¸ **ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œ êµ¬ì¡° ë¶ˆì¼ì¹˜ (ìˆ˜ì • ì¤‘)**
- ğŸ”„ **ì‹¤ì œ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘**

### ğŸ¯ ì„±ëŠ¥ ëª©í‘œ
- **ëª¨ë¸ í¬ê¸°**: ëŒ€ê·œëª¨ í•˜ì´ë¸Œë¦¬ë“œ (Kosmos2 + CLIP)
- **ì…ë ¥**: Vision patches + Text tokens
- **ì¶œë ¥**: 2D ë¡œë´‡ ì•¡ì…˜ (ì„ í˜•/ê°ì†ë„)
- **ëª©í‘œ FPS**: Jetson Orin NXì—ì„œ ì‹¤ì‹œê°„ ì¶”ë¡ 

### ğŸ“ ì‹¤í–‰ ë°©ë²•
```bash
# 1. ì¶”ë¡  ì»¨í…Œì´ë„ˆ ì‹œì‘
cd /home/soda
./vla/run_simple_clip_lstm_inference.sh

# 2. ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì¶”ë¡  ì‹¤í–‰ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ)
python3 vla/memory_optimized_inference.py
```

---

## ğŸ¯ ê²°ë¡ 

### ì£¼ìš” ì„±ê³¼
- âœ… **ì‹¤ì‹œê°„ VLA ì‹œìŠ¤í…œ êµ¬ì¶•**: 2,780 FPS ë‹¬ì„±
- âœ… **ë©€í‹°ëª¨ë‹¬ ìœµí•©**: Vision-Language í†µí•© ì²˜ë¦¬
- âœ… **ëª¨ë°”ì¼ ìµœì í™”**: Jetson Orin NX 16GBì—ì„œ ë™ì‘ ê°€ëŠ¥
- âœ… **ROS2 í†µí•©**: ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ
- âœ… **ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì¶”ë¡ **: 7.3GB Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ**: ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### ê¸°ìˆ ì  í˜ì‹ 
- ğŸ”¬ **ìˆœìˆ˜ Mobile ë°ì´í„° í™œìš©**: Calvin ì˜ì¡´ì„± ì œê±°
- ğŸ”¬ **2D ì•¡ì…˜ ìµœì í™”**: ëª¨ë°”ì¼ í™˜ê²½ì— íŠ¹í™”
- ğŸ”¬ **ë©€í‹°ëª¨ë‹¬ ìœµí•©**: Vision-Language í†µí•© ì²˜ë¦¬
- ğŸ”¬ **Jetson ìµœì í™”**: Orin NX 16GB íŠ¹í™” ì•„í‚¤í…ì²˜
- ğŸ”¬ **ëŒ€ê·œëª¨ ëª¨ë¸ ì¶”ë¡ **: 7.3GB ì²´í¬í¬ì¸íŠ¸ ë©”ëª¨ë¦¬ ìµœì í™”
- ğŸ”¬ **ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ê´€ë¦¬**: ë™ì  ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ì •ë¦¬

### í–¥í›„ ì „ë§
- ğŸš€ **ì„±ëŠ¥ ê°œì„ **: ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±ì„ ìœ„í•œ ë‹¨ê³„ì  ê°œì„ 
- ğŸš€ **ìƒìš©í™”**: ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì ì¸ ë™ì‘
- ğŸš€ **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ë¡œë´‡ ë° í™˜ê²½ ì§€ì›

---

## ğŸ”¬ **Future Research Directions**

### ğŸ¯ **Advanced Model Optimization**

#### **1. Multi-Modal Fusion Enhancement**
- **Vision-Language-Action Integration**: ë” ì •êµí•œ ë©€í‹°ëª¨ë‹¬ ìœµí•© ë©”ì»¤ë‹ˆì¦˜ ê°œë°œ
- **Attention Mechanism**: í¬ë¡œìŠ¤ ëª¨ë‹¬ ì–´í…ì…˜ê³¼ ì‹œê³µê°„ ì–´í…ì…˜ ê²°í•©
- **Hierarchical Planning**: ì¥ê¸° ê³„íšê³¼ ë‹¨ê¸° ì‹¤í–‰ì„ ê²°í•©í•œ ê³„ì¸µì  ê³„íš ì‹œìŠ¤í…œ

#### **2. Data Efficiency Improvements**
- **Few-Shot Learning**: ì ì€ ë°ì´í„°ë¡œë„ íš¨ê³¼ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•œ ë©”íƒ€ëŸ¬ë‹ ì ‘ê·¼ë²•
- **Active Learning**: ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ íš¨ìœ¨ì„± ì¦ëŒ€
- **Synthetic Data Generation**: ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œ í•©ì„± ë°ì´í„° ìƒì„± ë° í™œìš©

#### **3. Real-World Deployment**
- **Robustness Enhancement**: ë‹¤ì–‘í•œ í™˜ê²½ ì¡°ê±´ì—ì„œì˜ ê°•ê±´ì„± í–¥ìƒ
- **Real-time Optimization**: ì§€ì—° ì‹œê°„ ìµœì†Œí™”ë¥¼ ìœ„í•œ ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”
- **Edge Computing**: Jetson Orin NXì—ì„œì˜ íš¨ìœ¨ì ì¸ ì˜¨ë””ë°”ì´ìŠ¤ ì¶”ë¡ 

### ğŸ“Š **Performance Enhancement Targets**

| ì—°êµ¬ ì˜ì—­ | í˜„ì¬ ì„±ëŠ¥ | ëª©í‘œ ì„±ëŠ¥ | ê°œì„  ë°©í–¥ |
|-----------|-----------|-----------|-----------|
| **MAE** | 0.212 | 0.15 | 29.2% ê°œì„  |
| **ì¶”ë¡  ì†ë„** | 765 FPS | 1000+ FPS | 30.7% í–¥ìƒ |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±** | 7.8GB | 5GB | 35.9% ì ˆì•½ |
| **ì •í™•ë„ (0.1)** | 71.3% | 85% | 19.2% í–¥ìƒ |

### ğŸš€ **Technical Roadmap**

#### **Phase 1: Model Architecture (3-6ê°œì›”)**
- **Transformer ê¸°ë°˜ ì•¡ì…˜ ì˜ˆì¸¡**: ë” ì •êµí•œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§
- **Vision Transformer ìµœì í™”**: íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
- **Dynamic Architecture**: ì…ë ¥ì— ë”°ë¥¸ ë™ì  ëª¨ë¸ êµ¬ì¡° ì¡°ì •

#### **Phase 2: Data Strategy (6-12ê°œì›”)**
- **Multi-Robot Data Collection**: ë‹¤ì–‘í•œ ë¡œë´‡ í”Œë«í¼ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
- **Simulation-to-Real Transfer**: ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ì œ í™˜ê²½ ê°„ ì§€ì‹ ì „ì´
- **Continuous Learning**: ì˜¨ë¼ì¸ í•™ìŠµì„ í†µí•œ ì§€ì†ì  ì„±ëŠ¥ í–¥ìƒ

#### **Phase 3: System Integration (12-18ê°œì›”)**
- **ROS2 Integration**: ì™„ì „í•œ ROS2 ìƒíƒœê³„ í†µí•©
- **Cloud-Edge Collaboration**: í´ë¼ìš°ë“œì™€ ì—£ì§€ ë””ë°”ì´ìŠ¤ ê°„ í˜‘ì—…
- **Multi-Agent Coordination**: ë‹¤ì¤‘ ë¡œë´‡ í˜‘ì—… ì‹œìŠ¤í…œ

### ğŸ¯ **Expected Outcomes**

#### **Short-term (6ê°œì›”)**
- **MAE 0.15 ë‹¬ì„±**: í˜„ì¬ ëŒ€ë¹„ 29.2% ì„±ëŠ¥ í–¥ìƒ
- **ì‹¤ì‹œê°„ ì¶”ë¡ **: 1000+ FPS ë‹¬ì„±ìœ¼ë¡œ ì´ˆì €ì§€ì—° ì‘ë‹µ
- **ë©”ëª¨ë¦¬ ìµœì í™”**: 5GB ì´í•˜ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

#### **Medium-term (12ê°œì›”)**
- **ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›**: ì‹¤ë‚´/ì‹¤ì™¸, ì¡°ëª… ë³€í™” ë“± ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ë™ì‘
- **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**: ë‚´ë¹„ê²Œì´ì…˜ ì™¸ ì¶”ê°€ íƒœìŠ¤í¬ ì§€ì›
- **ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤**: ì§ê´€ì ì¸ ë¡œë´‡ ì œì–´ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ

#### **Long-term (18ê°œì›”)**
- **ìƒìš©í™” ì¤€ë¹„**: ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì  ë™ì‘
- **í™•ì¥ì„± í™•ë³´**: ë‹¤ì–‘í•œ ë¡œë´‡ í”Œë«í¼ ì§€ì›
- **ì‚°ì—… í‘œì¤€**: ëª¨ë°”ì¼ ë¡œë´‡ VLA ì‹œìŠ¤í…œì˜ ì‚°ì—… í‘œì¤€ ì œì•ˆ

---

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

### ê°œë°œíŒ€
- **í”„ë¡œì íŠ¸ ë¦¬ë”**: [ì´ë¦„]
- **ì‹œìŠ¤í…œ ê°œë°œ**: [ì´ë¦„]
- **AI ëª¨ë¸ ê°œë°œ**: [ì´ë¦„]
- **ë¡œë´‡ ì œì–´**: [ì´ë¦„]

### ê´€ë ¨ ë¬¸ì„œ
- **ê¸°ìˆ  ë¬¸ì„œ**: `docs/` ë””ë ‰í† ë¦¬
- **ì‚¬ìš©ì ê°€ì´ë“œ**: `MOBILE_VLA_SYSTEM_GUIDE.md`
- **API ë¬¸ì„œ**: `API_REFERENCE.md`
- **ë¬¸ì œ ë³´ê³ **: GitHub Issues

---

**ğŸ“… ë¬¸ì„œ ì‘ì„±ì¼**: 2024ë…„ 8ì›” 25ì¼  
**ğŸ“Š í”„ë¡œì íŠ¸ ìƒíƒœ**: ì„±ëŠ¥ ê°œì„  ì§„í–‰ ì¤‘  
**ğŸ¯ ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤**: MAE 0.8 â†’ 0.5 ë‹¬ì„±
