#!/usr/bin/env python3
"""
ğŸ¯ Enhanced Kosmos2+CLIP Hybrid with Discrete Action Head
VLM + Discrete Action Head êµ¬ì¡°ë¡œ ëª¨ë°”ì¼ ë¡œë´‡ VLA êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel
import logging
from typing import Optional, Tuple, Dict, Any
import math

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DiscreteActionHead(nn.Module):
    """Discrete Action Head - ì´ì‚° ì•¡ì…˜ ê³µê°„ ì‚¬ìš©"""
    
    def __init__(
        self,
        hidden_dim: int = 768,
        action_dim: int = 2,
        num_discrete_actions: int = 100,  # ê° ì•¡ì…˜ ì°¨ì›ë‹¹ ì´ì‚° ê°’ ê°œìˆ˜
        embedding_dim: int = 256,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.action_dim = action_dim
        self.num_discrete_actions = num_discrete_actions
        self.embedding_dim = embedding_dim
        
        # Action embedding layers
        self.action_embeddings = nn.ModuleList([
            nn.Embedding(num_discrete_actions, embedding_dim)
            for _ in range(action_dim)
        ])
        
        # Feature projection
        self.feature_projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, embedding_dim)
        )
        
        # Action classifiers for each dimension
        self.action_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embedding_dim * 2, embedding_dim),
                nn.LayerNorm(embedding_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(embedding_dim, num_discrete_actions)
            )
            for _ in range(action_dim)
        ])
        
        # Continuous action decoder (ì´ì‚° â†’ ì—°ì† ë³€í™˜)
        self.action_decoder = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embedding_dim, embedding_dim // 2),
                nn.GELU(),
                nn.Linear(embedding_dim // 2, 1)  # ì—°ì† ê°’ 1ê°œ
            )
            for _ in range(action_dim)
        ])
        
        logger.info(f"Discrete Action Head initialized:")
        logger.info(f"  - Hidden dim: {hidden_dim}")
        logger.info(f"  - Action dim: {action_dim}")
        logger.info(f"  - Num discrete actions: {num_discrete_actions}")
        logger.info(f"  - Embedding dim: {embedding_dim}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, hidden_dim] - VLM features
        Returns:
            actions: [batch_size, action_dim] - predicted continuous actions
        """
        batch_size, seq_len, hidden_dim = x.shape
        
        # Global average pooling
        x_pooled = x.mean(dim=1)  # [batch_size, hidden_dim]
        
        # Feature projection
        projected_features = self.feature_projection(x_pooled)  # [batch_size, embedding_dim]
        
        # Discrete action prediction for each dimension
        discrete_actions = []
        continuous_actions = []
        
        for i in range(self.action_dim):
            # Get action embedding
            action_emb = self.action_embeddings[i].weight  # [num_discrete_actions, embedding_dim]
            
            # Compute similarity between features and action embeddings
            similarity = torch.matmul(projected_features, action_emb.T)  # [batch_size, num_discrete_actions]
            
            # Get discrete action probabilities
            discrete_logits = self.action_classifiers[i](
                torch.cat([projected_features, action_emb.mean(dim=0).unsqueeze(0).expand(batch_size, -1)], dim=-1)
            )
            discrete_probs = F.softmax(discrete_logits, dim=-1)
            
            # Sample discrete action
            discrete_action = torch.multinomial(discrete_probs, 1).squeeze(-1)  # [batch_size]
            discrete_actions.append(discrete_action)
            
            # Convert discrete action to continuous
            discrete_emb = self.action_embeddings[i](discrete_action)  # [batch_size, embedding_dim]
            continuous_action = self.action_decoder[i](discrete_emb).squeeze(-1)  # [batch_size]
            continuous_actions.append(continuous_action)
        
        # Stack continuous actions
        continuous_actions = torch.stack(continuous_actions, dim=-1)  # [batch_size, action_dim]
        
        return continuous_actions
    
    def get_discrete_actions(self, x: torch.Tensor) -> torch.Tensor:
        """ì´ì‚° ì•¡ì…˜ë§Œ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        batch_size, seq_len, hidden_dim = x.shape
        x_pooled = x.mean(dim=1)
        projected_features = self.feature_projection(x_pooled)
        
        discrete_actions = []
        for i in range(self.action_dim):
            discrete_logits = self.action_classifiers[i](
                torch.cat([projected_features, self.action_embeddings[i].weight.mean(dim=0).unsqueeze(0).expand(batch_size, -1)], dim=-1)
            )
            discrete_probs = F.softmax(discrete_logits, dim=-1)
            discrete_action = torch.multinomial(discrete_probs, 1).squeeze(-1)
            discrete_actions.append(discrete_action)
        
        return torch.stack(discrete_actions, dim=-1)

class EnhancedKosmos2CLIPHybridWithDiscreteActionHead(nn.Module):
    """
    Enhanced Kosmos2+CLIP Hybrid Model with Discrete Action Head
    
    ì•„í‚¤í…ì²˜:
    - Kosmos2 Vision Encoder
    - CLIP Vision Encoder  
    - Vision Resampler
    - Discrete Action Head
    """
    
    def __init__(
        self,
        action_dim: int = 2,  # 2D ì•¡ì…˜ (linear_x, linear_y)
        vision_resampler_tokens: int = 64,
        hidden_dim: int = 768,
        num_discrete_actions: int = 100,
        embedding_dim: int = 256,
        dropout: float = 0.1,
        mobile_optimized: bool = True
    ):
        super().__init__()
        
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.mobile_optimized = mobile_optimized
        
        # Kosmos2 ëª¨ë¸ (Vision Encoder)
        logger.info("Loading Kosmos2 model...")
        self.kosmos_processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos_model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # CLIP ëª¨ë¸ (Vision Encoder)
        logger.info("Loading CLIP model...")
        self.clip_processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_model = AutoModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Vision Resampler (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ í† í° ìˆ˜ ê°ì†Œ)
        self.vision_resampler = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Resampler query tokens
        self.resampler_queries = nn.Parameter(
            torch.randn(vision_resampler_tokens, hidden_dim) * 0.02
        )
        
        # Feature fusion layer
        self.feature_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # Discrete Action Head
        self.discrete_action_head = DiscreteActionHead(
            hidden_dim=hidden_dim,
            action_dim=action_dim,
            num_discrete_actions=num_discrete_actions,
            embedding_dim=embedding_dim,
            dropout=dropout
        )
        
        # Initialize weights
        self._initialize_weights()
        
        logger.info(f"Enhanced Kosmos2+CLIP Hybrid Model with Discrete Action Head initialized:")
        logger.info(f"  - Action dim: {action_dim}")
        logger.info(f"  - Vision resampler tokens: {vision_resampler_tokens}")
        logger.info(f"  - Hidden dim: {hidden_dim}")
        logger.info(f"  - Num discrete actions: {num_discrete_actions}")
        logger.info(f"  - Embedding dim: {embedding_dim}")
        logger.info(f"  - Mobile optimized: {mobile_optimized}")
    
    def _initialize_weights(self):
        """Initialize model weights"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def extract_kosmos_features(self, images: torch.Tensor) -> torch.Tensor:
        """Kosmos2ì—ì„œ vision features ì¶”ì¶œ"""
        with torch.no_grad():
            # Kosmos2 vision encoder ì‚¬ìš©
            vision_outputs = self.kosmos_model.vision_model(images)
            # [batch_size, num_patches, hidden_dim]
            return vision_outputs.last_hidden_state
    
    def extract_clip_features(self, images: torch.Tensor) -> torch.Tensor:
        """CLIPì—ì„œ vision features ì¶”ì¶œ"""
        with torch.no_grad():
            # CLIP vision encoder ì‚¬ìš©
            vision_outputs = self.clip_model.vision_model(images)
            # [batch_size, num_patches, hidden_dim]
            return vision_outputs.last_hidden_state
    
    def resample_vision_features(self, features: torch.Tensor) -> torch.Tensor:
        """Vision featuresë¥¼ ê³ ì •ëœ í† í° ìˆ˜ë¡œ ë¦¬ìƒ˜í”Œë§"""
        batch_size = features.size(0)
        
        # Query tokensë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
        queries = self.resampler_queries.unsqueeze(0).expand(batch_size, -1, -1)
        
        # Multi-head attentionìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        resampled_features, _ = self.vision_resampler(
            query=queries,
            key=features,
            value=features
        )
        
        return resampled_features
    
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            images: [batch_size, channels, height, width] - ì…ë ¥ ì´ë¯¸ì§€
        Returns:
            actions: [batch_size, action_dim] - ì˜ˆì¸¡ëœ ì—°ì† ì•¡ì…˜
        """
        # 1. Vision features ì¶”ì¶œ
        kosmos_features = self.extract_kosmos_features(images)  # [B, N, 768]
        clip_features = self.extract_clip_features(images)      # [B, M, 768]
        
        # 2. Vision features ë¦¬ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        kosmos_resampled = self.resample_vision_features(kosmos_features)
        clip_resampled = self.resample_vision_features(clip_features)
        
        # 3. Feature fusion
        fused_features = torch.cat([kosmos_resampled, clip_resampled], dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # 4. Discrete Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.discrete_action_head(fused_features)
        
        return actions
    
    def get_discrete_actions(self, images: torch.Tensor) -> torch.Tensor:
        """ì´ì‚° ì•¡ì…˜ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        # 1-3. Vision features ì¶”ì¶œ ë° fusion (forwardì™€ ë™ì¼)
        kosmos_features = self.extract_kosmos_features(images)
        clip_features = self.extract_clip_features(images)
        kosmos_resampled = self.resample_vision_features(kosmos_features)
        clip_resampled = self.resample_vision_features(clip_features)
        fused_features = torch.cat([kosmos_resampled, clip_resampled], dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # 4. Discrete actionsë§Œ ë°˜í™˜
        return self.discrete_action_head.get_discrete_actions(fused_features)
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": "Enhanced Kosmos2+CLIP Hybrid with Discrete Action Head",
            "action_dim": self.action_dim,
            "hidden_dim": self.hidden_dim,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "mobile_optimized": self.mobile_optimized,
            "action_head_type": "Discrete"
        }

def create_model(
    action_dim: int = 2,
    vision_resampler_tokens: int = 64,
    hidden_dim: int = 768,
    num_discrete_actions: int = 100,
    embedding_dim: int = 256,
    dropout: float = 0.1,
    mobile_optimized: bool = True
) -> EnhancedKosmos2CLIPHybridWithDiscreteActionHead:
    """ëª¨ë¸ ìƒì„± í•¨ìˆ˜"""
    return EnhancedKosmos2CLIPHybridWithDiscreteActionHead(
        action_dim=action_dim,
        vision_resampler_tokens=vision_resampler_tokens,
        hidden_dim=hidden_dim,
        num_discrete_actions=num_discrete_actions,
        embedding_dim=embedding_dim,
        dropout=dropout,
        mobile_optimized=mobile_optimized
    )

if __name__ == "__main__":
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    logger.info("Testing Enhanced Kosmos2+CLIP Hybrid with Discrete Action Head...")
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(
        action_dim=2,
        vision_resampler_tokens=64,
        hidden_dim=768,
        num_discrete_actions=100,
        embedding_dim=256,
        dropout=0.1,
        mobile_optimized=True
    )
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 2
    test_images = torch.randn(batch_size, 3, 224, 224)
    
    # Forward pass
    with torch.no_grad():
        continuous_actions = model(test_images)
        discrete_actions = model.get_discrete_actions(test_images)
    
    logger.info(f"Model test successful!")
    logger.info(f"Input shape: {test_images.shape}")
    logger.info(f"Continuous actions shape: {continuous_actions.shape}")
    logger.info(f"Discrete actions shape: {discrete_actions.shape}")
    logger.info(f"Model info: {model.get_model_info()}")
