{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cell 0: ğŸ¯ Mobile VLA + Kosmos 2B ì•¡ì…˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ\n",
        "\n",
        "## ğŸ” **í•µì‹¬ ê°œì„ ì‚¬í•­**\n",
        "| ì´ì „ (ì´ë²¤íŠ¸ ë¶„ë¥˜) | í˜„ì¬ (ì•¡ì…˜ ì˜ˆì¸¡) |\n",
        "|------------------|------------------|\n",
        "| ğŸ”´ ì´ë²¤íŠ¸ 99.9% â†’ ë„ˆë¬´ ë‹¨ìˆœ | âœ… ì´ë¯¸ì§€â†’ì•¡ì…˜ ë§¤í•‘ í•™ìŠµ |\n",
        "| ğŸ”´ Frame-by-frame | âœ… Window/Chunk ë°©ì‹ |\n",
        "| ğŸ”´ Classification ìœ„ì£¼ | âœ… Regression ìœ„ì£¼ |\n",
        "| ğŸ”´ ë‹¨ìˆœ PIL ë³€í™˜ | âœ… RoboVLMs ìŠ¤íƒ€ì¼ ì²˜ë¦¬ |\n",
        "\n",
        "## ğŸ“Š **Window & Chunk ë©”ì»¤ë‹ˆì¦˜**\n",
        "- **Window Size**: 16 í”„ë ˆì„ (ê³¼ê±° ì»¨í…ìŠ¤íŠ¸)\n",
        "- **Chunk Size**: 2 í”„ë ˆì„ (ë¯¸ë˜ ì˜ˆì¸¡)\n",
        "- **Total Sequence**: 18 í”„ë ˆì„ (16+2)\n",
        "- **ì˜ˆì¸¡ ëª©í‘œ**: í˜„ì¬ ì´ë¯¸ì§€ë¡œ ë‹¤ìŒ 2 í”„ë ˆì„ ì•¡ì…˜ ì˜ˆì¸¡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: /home/billy/25-1kp/vla/Robo+/Mobile_VLA\n",
            "ğŸ“Š ì‹¤ì œ ë°ì´í„° ë””ë ‰í† ë¦¬: /home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset\n",
            "ğŸ¤– RoboVLMs ë””ë ‰í† ë¦¬: /home/billy/25-1kp/vla/RoboVLMs\n",
            "âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸ë¨: 72ê°œ H5 íŒŒì¼ ë°œê²¬\n",
            "ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: ğŸ”§ í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from transformers import AutoProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "ROOT_DIR = Path(\"/home/billy/25-1kp/vla/Robo+/Mobile_VLA\")\n",
        "DATA_DIR = Path(\"/home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset\")  # ì‹¤ì œ ë°ì´í„°ì…‹ ê²½ë¡œ\n",
        "ROBOVLMS_DIR = Path(\"/home/billy/25-1kp/vla/RoboVLMs\")\n",
        "\n",
        "# íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€\n",
        "sys.path.append(str(ROOT_DIR))\n",
        "sys.path.append(str(ROBOVLMS_DIR))\n",
        "\n",
        "print(f\"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: {ROOT_DIR}\")\n",
        "print(f\"ğŸ“Š ì‹¤ì œ ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}\")\n",
        "print(f\"ğŸ¤– RoboVLMs ë””ë ‰í† ë¦¬: {ROBOVLMS_DIR}\")\n",
        "\n",
        "# ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸\n",
        "if DATA_DIR.exists():\n",
        "    h5_files = list(DATA_DIR.glob(\"*.h5\"))\n",
        "    print(f\"âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸ë¨: {len(h5_files)}ê°œ H5 íŒŒì¼ ë°œê²¬\")\n",
        "else:\n",
        "    print(f\"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: ğŸ“¦ í•µì‹¬ ëª¨ë“ˆ ë¡œë“œ (Window/Chunk ì²˜ë¦¬ í¬í•¨)\n",
        "\n",
        "def load_class_from_file(class_name, file_path, target_class):\n",
        "    \"\"\"íŒŒì¼ì—ì„œ í´ë˜ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ\"\"\"\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(class_name, file_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "    return getattr(module, target_class)\n",
        "\n",
        "# ì—…ë°ì´íŠ¸ëœ ëª¨ë“ˆë“¤ ë¡œë“œ\n",
        "WindowChunkAdapter = load_class_from_file(\n",
        "    'window_chunk_adapter', \n",
        "    str(ROOT_DIR / 'data' / 'window_chunk_adapter.py'), \n",
        "    'WindowChunkAdapter'\n",
        ")\n",
        "\n",
        "ActionPredictionTrainer = load_class_from_file(\n",
        "    'action_trainer', \n",
        "    str(ROOT_DIR / 'training' / 'action_trainer.py'), \n",
        "    'ActionPredictionTrainer'\n",
        ")\n",
        "\n",
        "# Kosmos í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”\n",
        "kosmos_processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
        "print(\"âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ì—…ë°ì´íŠ¸ëœ ëª¨ë“ˆ ë‹¤ì‹œ ë¡œë“œ ì¤‘...\n",
            "ğŸ“Š ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘...\n",
            "\n",
            "ğŸ“Š ì•¡ì…˜ ì˜ˆì¸¡ ë°ì´í„°ì…‹ ì •ë³´:\n",
            "   ì´ ìƒ˜í”Œ: 72ê°œ\n",
            "   Window Size: 16 í”„ë ˆì„\n",
            "   Chunk Size: 2 í”„ë ˆì„\n",
            "   Total Sequence: 18 í”„ë ˆì„\n",
            "   ë°°ì¹˜ í¬ê¸°: 4\n",
            "   ì´ ë°°ì¹˜ ìˆ˜: 18\n",
            "\n",
            "ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„í¬:\n",
            "   2box_left_vertical: 10ê°œ\n",
            "   2box_right_horizontal: 6ê°œ\n",
            "   1box_left_vertical: 10ê°œ\n",
            "   1box_right_horizontal: 10ê°œ\n",
            "   2box_right_vertical: 10ê°œ\n",
            "   1box_left_horizontal: 10ê°œ\n",
            "   1box_right_vertical: 10ê°œ\n",
            "   2box_left_horizontal: 6ê°œ\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: ğŸ—‚ï¸ Window/Chunk ë°ì´í„°ì…‹ ì´ˆê¸°í™” (ì—…ë°ì´íŠ¸ëœ ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘)\n",
        "\n",
        "# ë¨¼ì € ì—…ë°ì´íŠ¸ëœ WindowChunkAdapter ë‹¤ì‹œ ë¡œë“œ\n",
        "print(\"ğŸ”„ ì—…ë°ì´íŠ¸ëœ ëª¨ë“ˆ ë‹¤ì‹œ ë¡œë“œ ì¤‘...\")\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# ëª¨ë“ˆ ë‹¤ì‹œ ë¡œë“œ\n",
        "if 'window_chunk_adapter' in sys.modules:\n",
        "    importlib.reload(sys.modules['window_chunk_adapter'])\n",
        "\n",
        "WindowChunkAdapter = load_class_from_file(\n",
        "    'window_chunk_adapter', \n",
        "    str(ROOT_DIR / 'data' / 'window_chunk_adapter.py'), \n",
        "    'WindowChunkAdapter'\n",
        ")\n",
        "\n",
        "# ì•¡ì…˜ ì˜ˆì¸¡ ì¤‘ì‹¬ ë°ì´í„°ì…‹ ì´ˆê¸°í™”\n",
        "print(\"ğŸ“Š ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘...\")\n",
        "dataset = WindowChunkAdapter(\n",
        "    data_dir=str(DATA_DIR),\n",
        "    window_size=16,  # ê³¼ê±° ì»¨í…ìŠ¤íŠ¸\n",
        "    chunk_size=2,    # ë¯¸ë˜ ì˜ˆì¸¡\n",
        "    image_processor=kosmos_processor.image_processor,\n",
        "    normalize_actions=True\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š ì•¡ì…˜ ì˜ˆì¸¡ ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "print(f\"   ì´ ìƒ˜í”Œ: {len(dataset)}ê°œ\")\n",
        "print(f\"   Window Size: {dataset.window_size} í”„ë ˆì„\")\n",
        "print(f\"   Chunk Size: {dataset.chunk_size} í”„ë ˆì„\")\n",
        "print(f\"   Total Sequence: {dataset.window_size + dataset.chunk_size} í”„ë ˆì„\")\n",
        "\n",
        "if len(dataset) > 0:\n",
        "    # ë°ì´í„° ë¡œë” ì„¤ì •\n",
        "    dataloader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size=4, \n",
        "        shuffle=True, \n",
        "        num_workers=2\n",
        "    )\n",
        "    \n",
        "    print(f\"   ë°°ì¹˜ í¬ê¸°: {dataloader.batch_size}\")\n",
        "    print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(dataloader)}\")\n",
        "    \n",
        "    # ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬ í™•ì¸\n",
        "    scenarios = [dataset.samples[i]['scenario'] for i in range(len(dataset))]\n",
        "    from collections import Counter\n",
        "    scenario_counts = Counter(scenarios)\n",
        "    \n",
        "    print(f\"\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„í¬:\")\n",
        "    for scenario, count in scenario_counts.items():\n",
        "        print(f\"   {scenario}: {count}ê°œ\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ìƒ˜í”Œ ë°°ì¹˜ ë°ì´í„° êµ¬ì¡°:\n",
            "==================================================\n",
            "ğŸ“Š pixel_values: torch.Size([4, 16, 3, 224, 224]) (torch.float32)\n",
            "ğŸ“Š target_actions: torch.Size([4, 2, 3]) (torch.float32)\n",
            "ğŸ“Š context_actions: torch.Size([4, 16, 3]) (torch.float32)\n",
            "ğŸ“ task_description: <class 'list'> - 4\n",
            "ğŸ“ scenario: <class 'list'> - 4\n",
            "ğŸ“ episode_id: <class 'list'> - 4\n",
            "\n",
            "ğŸ¯ ì•¡ì…˜ ì˜ˆì¸¡ íƒ€ê²Ÿ:\n",
            "   ì…ë ¥ ì´ë¯¸ì§€: torch.Size([4, 16, 3, 224, 224]) (Window)\n",
            "   ì˜ˆì¸¡ ì•¡ì…˜: torch.Size([4, 2, 3]) (Chunk)\n",
            "   ì•¡ì…˜ ë²”ìœ„: [-1.000, 1.000]\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: ğŸ” ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡° ë¶„ì„\n",
        "\n",
        "# ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
        "sample_batch = next(iter(dataloader))\n",
        "\n",
        "print(\"ğŸ” ìƒ˜í”Œ ë°°ì¹˜ ë°ì´í„° êµ¬ì¡°:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for key, value in sample_batch.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"ğŸ“Š {key}: {value.shape} ({value.dtype})\")\n",
        "    else:\n",
        "        print(f\"ğŸ“ {key}: {type(value)} - {len(value) if hasattr(value, '__len__') else 'scalar'}\")\n",
        "\n",
        "print(\"\\nğŸ¯ ì•¡ì…˜ ì˜ˆì¸¡ íƒ€ê²Ÿ:\")\n",
        "print(f\"   ì…ë ¥ ì´ë¯¸ì§€: {sample_batch['pixel_values'].shape} (Window)\")\n",
        "print(f\"   ì˜ˆì¸¡ ì•¡ì…˜: {sample_batch['target_actions'].shape} (Chunk)\")\n",
        "print(f\"   ì•¡ì…˜ ë²”ìœ„: [{sample_batch['target_actions'].min().item():.3f}, {sample_batch['target_actions'].max().item():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– ì•¡ì…˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì„±:\n",
            "========================================\n",
            "ğŸ“ VLM ë°±ë³¸: Kosmos 2B\n",
            "ğŸ¯ ì•¡ì…˜ ì°¨ì›: 3D (Mobile Navigation)\n",
            "ğŸªŸ Window Size: 16\n",
            "ğŸ§© Chunk Size: 2\n",
            "ğŸ“ˆ Learning Rate: 0.0001\n",
            "ğŸ’¾ ëª¨ë¸ íŒŒë¼ë¯¸í„°: 1,669,340,422\n",
            "âœ… ì•¡ì…˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: ğŸ¤– ì•¡ì…˜ ì˜ˆì¸¡ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
        "\n",
        "# ì•¡ì…˜ ì˜ˆì¸¡ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
        "trainer = ActionPredictionTrainer(\n",
        "    model_name=\"microsoft/kosmos-2-patch14-224\",\n",
        "    action_dim=3,  # [linear_x, linear_y, angular_z]\n",
        "    window_size=16,\n",
        "    chunk_size=2,\n",
        "    learning_rate=1e-4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"ğŸ¤– ì•¡ì…˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì„±:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"ğŸ“ VLM ë°±ë³¸: Kosmos 2B\")\n",
        "print(f\"ğŸ¯ ì•¡ì…˜ ì°¨ì›: 3D (Mobile Navigation)\")\n",
        "print(f\"ğŸªŸ Window Size: {trainer.window_size}\")\n",
        "print(f\"ğŸ§© Chunk Size: {trainer.chunk_size}\")\n",
        "print(f\"ğŸ“ˆ Learning Rate: {trainer.learning_rate}\")\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in trainer.model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "print(\"âœ… ì•¡ì…˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš¡ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: ğŸš€ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ë£¨í”„\n",
        "\n",
        "def train_action_prediction(\n",
        "    trainer, \n",
        "    dataloader, \n",
        "    num_epochs=10,\n",
        "    save_interval=5,\n",
        "    log_interval=10\n",
        "):\n",
        "    \"\"\"ì•¡ì…˜ ì˜ˆì¸¡ ì¤‘ì‹¬ í•™ìŠµ í•¨ìˆ˜\"\"\"\n",
        "    \n",
        "    # ì†ì‹¤ ì¶”ì ê¸°\n",
        "    class ActionLossTracker:\n",
        "        def __init__(self):\n",
        "            self.losses = {\n",
        "                'steps': [],\n",
        "                'action_loss': [],\n",
        "                'total_loss': [],\n",
        "                'lr': [],\n",
        "                'mae_linear_x': [],\n",
        "                'mae_linear_y': [], \n",
        "                'mae_angular_z': [],\n",
        "                'mae_avg': [],\n",
        "                'scenarios': []\n",
        "            }\n",
        "        \n",
        "        def update(self, step, loss_dict, scenario=None):\n",
        "            self.losses['steps'].append(step)\n",
        "            for key in ['action_loss', 'total_loss', 'lr', 'mae_linear_x', 'mae_linear_y', 'mae_angular_z', 'mae_avg']:\n",
        "                if key in loss_dict:\n",
        "                    self.losses[key].append(loss_dict[key])\n",
        "                else:\n",
        "                    self.losses[key].append(0.0)\n",
        "            self.losses['scenarios'].append(scenario or 'unknown')\n",
        "    \n",
        "    loss_tracker = ActionLossTracker()\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "    \n",
        "    print(\"ğŸš€ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ì‹œì‘!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = []\n",
        "        \n",
        "        # ì—í¬í¬ ì§„í–‰ ë°”\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            try:\n",
        "                # í•™ìŠµ ìŠ¤í…\n",
        "                loss_dict = trainer.train_step(batch)\n",
        "                epoch_losses.append(loss_dict['total_loss'])\n",
        "                global_step += 1\n",
        "                \n",
        "                # ì†ì‹¤ ì¶”ì \n",
        "                scenario = batch.get('scenario', ['unknown'])[0] if 'scenario' in batch else 'unknown'\n",
        "                loss_tracker.update(global_step, loss_dict, scenario)\n",
        "                \n",
        "                # ë¡œê·¸ ì¶œë ¥\n",
        "                if global_step % log_interval == 0:\n",
        "                    pbar.set_postfix({\n",
        "                        'Loss': f\"{loss_dict['total_loss']:.4f}\",\n",
        "                        'MAE_x': f\"{loss_dict.get('mae_linear_x', 0):.4f}\",\n",
        "                        'MAE_y': f\"{loss_dict.get('mae_linear_y', 0):.4f}\",\n",
        "                        'MAE_z': f\"{loss_dict.get('mae_angular_z', 0):.4f}\",\n",
        "                        'LR': f\"{loss_dict['lr']:.2e}\"\n",
        "                    })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\nâŒ í•™ìŠµ ìŠ¤í… ì‹¤íŒ¨ (Step {global_step}): {e}\")\n",
        "                continue\n",
        "        \n",
        "        # ì—í¬í¬ ìš”ì•½\n",
        "        avg_loss = np.mean(epoch_losses) if epoch_losses else float('inf')\n",
        "        print(f\"\\nğŸ“Š Epoch {epoch+1} ì™„ë£Œ:\")\n",
        "        print(f\"   í‰ê·  ì†ì‹¤: {avg_loss:.4f}\")\n",
        "        print(f\"   ì²˜ë¦¬ëœ ë°°ì¹˜: {len(epoch_losses)}/{len(dataloader)}\")\n",
        "        \n",
        "        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            save_path = ROOT_DIR / f\"checkpoints/action_prediction_best.pth\"\n",
        "            save_path.parent.mkdir(exist_ok=True)\n",
        "            trainer.save_checkpoint(str(save_path), epoch, best_loss)\n",
        "            print(f\"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥: {save_path}\")\n",
        "        \n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_path = ROOT_DIR / f\"checkpoints/action_prediction_epoch_{epoch+1}.pth\"\n",
        "            save_path.parent.mkdir(exist_ok=True)\n",
        "            trainer.save_checkpoint(str(save_path), epoch, best_loss)\n",
        "            print(f\"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}\")\n",
        "    \n",
        "    print(\"\\nğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(f\"ìµœì¢… ìµœê³  ì†ì‹¤: {best_loss:.4f}\")\n",
        "    \n",
        "    return loss_tracker\n",
        "\n",
        "print(\"âš¡ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Cell 7: ğŸ¯ í•™ìŠµ ì‹¤í–‰\n",
        "\n",
        "# print(\"âš¡ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "# final_loss_tracker = train_action_prediction(\n",
        "#     trainer=trainer,\n",
        "#     dataloader=dataloader, \n",
        "#     num_epochs=15,\n",
        "#     save_interval=5,\n",
        "#     log_interval=5\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: ğŸ“Š ì•¡ì…˜ ì˜ˆì¸¡ í‰ê°€ ë° ë¶„ì„\n",
        "\n",
        "def evaluate_action_prediction(trainer, dataloader, num_samples=20):\n",
        "    \"\"\"ì•¡ì…˜ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "    \n",
        "    print(\"ğŸ” ì•¡ì…˜ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\")\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_scenarios = []\n",
        "    \n",
        "    trainer.model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(dataloader, desc=\"í‰ê°€ ì¤‘\")):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "            \n",
        "            try:\n",
        "                # í‰ê°€ ìŠ¤í…\n",
        "                results = trainer.evaluate_step(batch)\n",
        "                \n",
        "                # ê²°ê³¼ ìˆ˜ì§‘\n",
        "                pred_actions = results['predictions']['predicted_actions']  # [B, T, chunk_size, action_dim]\n",
        "                target_actions = results['targets']['target_actions']  # [B, chunk_size, action_dim]\n",
        "                scenarios = results['scenarios']\n",
        "                \n",
        "                # Batch dimensionìœ¼ë¡œ í¼ì¹˜ê¸°\n",
        "                for b in range(pred_actions.shape[0]):\n",
        "                    # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í…ì˜ ì˜ˆì¸¡ë§Œ ì‚¬ìš© (ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)\n",
        "                    pred = pred_actions[b, -1, :, :].cpu()  # [chunk_size, action_dim]\n",
        "                    target = target_actions[b, :, :].cpu()  # [chunk_size, action_dim]\n",
        "                    \n",
        "                    all_predictions.append(pred)\n",
        "                    all_targets.append(target)\n",
        "                    all_scenarios.append(scenarios[b] if b < len(scenarios) else 'unknown')\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if not all_predictions:\n",
        "        print(\"âŒ í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return None\n",
        "    \n",
        "    # í…ì„œë¡œ ë³€í™˜\n",
        "    all_predictions = torch.stack(all_predictions)  # [N, chunk_size, action_dim]\n",
        "    all_targets = torch.stack(all_targets)  # [N, chunk_size, action_dim]\n",
        "    \n",
        "    print(f\"\\nğŸ“Š í‰ê°€ ì™„ë£Œ: {len(all_predictions)}ê°œ ìƒ˜í”Œ\")\n",
        "    print(f\"ì˜ˆì¸¡ shape: {all_predictions.shape}\")\n",
        "    print(f\"íƒ€ê²Ÿ shape: {all_targets.shape}\")\n",
        "    \n",
        "    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    metrics = {}\n",
        "    \n",
        "    # ì „ì²´ MAE\n",
        "    mae_total = torch.abs(all_predictions - all_targets).mean()\n",
        "    metrics['mae_total'] = mae_total.item()\n",
        "    \n",
        "    # ì°¨ì›ë³„ MAE\n",
        "    mae_per_dim = torch.abs(all_predictions - all_targets).mean(dim=(0, 1))  # [action_dim]\n",
        "    dim_names = ['linear_x', 'linear_y', 'angular_z']\n",
        "    for i, name in enumerate(dim_names):\n",
        "        metrics[f'mae_{name}'] = mae_per_dim[i].item()\n",
        "    \n",
        "    # MSE\n",
        "    mse_total = ((all_predictions - all_targets) ** 2).mean()\n",
        "    metrics['mse_total'] = mse_total.item()\n",
        "    \n",
        "    # RMSE\n",
        "    metrics['rmse_total'] = torch.sqrt(mse_total).item()\n",
        "    \n",
        "    # RÂ² Score (per dimension)\n",
        "    for i, name in enumerate(dim_names):\n",
        "        pred_dim = all_predictions[:, :, i].flatten()\n",
        "        target_dim = all_targets[:, :, i].flatten()\n",
        "        \n",
        "        ss_res = ((target_dim - pred_dim) ** 2).sum()\n",
        "        ss_tot = ((target_dim - target_dim.mean()) ** 2).sum()\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "        metrics[f'r2_{name}'] = r2.item()\n",
        "    \n",
        "    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥\n",
        "    scenario_metrics = {}\n",
        "    unique_scenarios = list(set(all_scenarios))\n",
        "    \n",
        "    for scenario in unique_scenarios:\n",
        "        scenario_mask = [i for i, s in enumerate(all_scenarios) if s == scenario]\n",
        "        if len(scenario_mask) > 0:\n",
        "            scenario_preds = all_predictions[scenario_mask]\n",
        "            scenario_targets = all_targets[scenario_mask]\n",
        "            scenario_mae = torch.abs(scenario_preds - scenario_targets).mean()\n",
        "            scenario_metrics[scenario] = scenario_mae.item()\n",
        "    \n",
        "    return {\n",
        "        'predictions': all_predictions,\n",
        "        'targets': all_targets,\n",
        "        'scenarios': all_scenarios,\n",
        "        'metrics': metrics,\n",
        "        'scenario_metrics': scenario_metrics\n",
        "    }\n",
        "\n",
        "# í‰ê°€ ì‹¤í–‰\n",
        "if 'final_loss_tracker' in locals():\n",
        "    print(\"ğŸ” í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì‹œì‘...\")\n",
        "    eval_results = evaluate_action_prediction(trainer, dataloader, num_samples=30)\n",
        "    \n",
        "    if eval_results:\n",
        "        print(\"\\nğŸ“ˆ ì•¡ì…˜ ì˜ˆì¸¡ ì„±ëŠ¥ ê²°ê³¼:\")\n",
        "        print(\"=\" * 50)\n",
        "        for key, value in eval_results['metrics'].items():\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "        \n",
        "        print(\"\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ MAE:\")\n",
        "        for scenario, mae in eval_results['scenario_metrics'].items():\n",
        "            print(f\"  {scenario}: {mae:.4f}\")\n",
        "else:\n",
        "    print(\"âš ï¸ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš” (Cell 7 ì‹¤í–‰)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: ğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„\n",
        "\n",
        "def plot_action_prediction_results(loss_tracker, eval_results=None):\n",
        "    \"\"\"ì•¡ì…˜ ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\"\"\"\n",
        "    \n",
        "    if not hasattr(loss_tracker, 'losses') or not loss_tracker.losses['steps']:\n",
        "        print(\"âŒ ì‹œê°í™”í•  í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "    \n",
        "    # í”Œë¡¯ ì„¤ì •\n",
        "    plt.style.use('default')\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('ğŸš€ Mobile VLA ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ê²°ê³¼', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. ì´ ì†ì‹¤ ê³¡ì„ \n",
        "    axes[0, 0].plot(loss_tracker.losses['steps'], loss_tracker.losses['total_loss'], 'b-', linewidth=2)\n",
        "    axes[0, 0].set_title('ğŸ“Š Total Loss', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Steps')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. ì°¨ì›ë³„ MAE\n",
        "    if 'mae_linear_x' in loss_tracker.losses:\n",
        "        axes[0, 1].plot(loss_tracker.losses['steps'], loss_tracker.losses['mae_linear_x'], 'r-', label='linear_x', linewidth=2)\n",
        "        axes[0, 1].plot(loss_tracker.losses['steps'], loss_tracker.losses['mae_linear_y'], 'g-', label='linear_y', linewidth=2)\n",
        "        axes[0, 1].plot(loss_tracker.losses['steps'], loss_tracker.losses['mae_angular_z'], 'b-', label='angular_z', linewidth=2)\n",
        "        axes[0, 1].set_title('ğŸ¯ MAE per Dimension', fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Steps')\n",
        "        axes[0, 1].set_ylabel('MAE')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. í•™ìŠµë¥ \n",
        "    axes[0, 2].plot(loss_tracker.losses['steps'], loss_tracker.losses['lr'], 'purple', linewidth=2)\n",
        "    axes[0, 2].set_title('ğŸ“ˆ Learning Rate', fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Steps')\n",
        "    axes[0, 2].set_ylabel('LR')\n",
        "    axes[0, 2].set_yscale('log')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. í‰ê°€ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)\n",
        "    if eval_results and 'metrics' in eval_results:\n",
        "        metrics = eval_results['metrics']\n",
        "        \n",
        "        # MAE ë¹„êµ\n",
        "        mae_names = ['mae_linear_x', 'mae_linear_y', 'mae_angular_z']\n",
        "        mae_values = [metrics.get(name, 0) for name in mae_names]\n",
        "        mae_labels = ['Linear X', 'Linear Y', 'Angular Z']\n",
        "        \n",
        "        bars = axes[1, 0].bar(mae_labels, mae_values, color=['red', 'green', 'blue'], alpha=0.7)\n",
        "        axes[1, 0].set_title('ğŸ¯ Final MAE by Dimension', fontweight='bold')\n",
        "        axes[1, 0].set_ylabel('MAE')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ\n",
        "        for bar, value in zip(bars, mae_values):\n",
        "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        # RÂ² Score\n",
        "        r2_names = ['r2_linear_x', 'r2_linear_y', 'r2_angular_z']\n",
        "        r2_values = [metrics.get(name, 0) for name in r2_names]\n",
        "        \n",
        "        bars_r2 = axes[1, 1].bar(mae_labels, r2_values, color=['red', 'green', 'blue'], alpha=0.7)\n",
        "        axes[1, 1].set_title('ğŸ“Š RÂ² Score by Dimension', fontweight='bold')\n",
        "        axes[1, 1].set_ylabel('RÂ² Score')\n",
        "        axes[1, 1].set_ylim(0, 1)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # RÂ² ê°’ í‘œì‹œ\n",
        "        for bar, value in zip(bars_r2, r2_values):\n",
        "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "        \n",
        "        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥\n",
        "        if 'scenario_metrics' in eval_results:\n",
        "            scenario_data = eval_results['scenario_metrics']\n",
        "            scenarios = list(scenario_data.keys())\n",
        "            scenario_maes = list(scenario_data.values())\n",
        "            \n",
        "            bars_scenario = axes[1, 2].bar(range(len(scenarios)), scenario_maes, \n",
        "                                         color='orange', alpha=0.7)\n",
        "            axes[1, 2].set_title('ğŸ® Performance by Scenario', fontweight='bold')\n",
        "            axes[1, 2].set_xlabel('Scenario')\n",
        "            axes[1, 2].set_ylabel('MAE')\n",
        "            axes[1, 2].set_xticks(range(len(scenarios)))\n",
        "            axes[1, 2].set_xticklabels([s.replace('_', '\\n') for s in scenarios], \n",
        "                                      rotation=45, ha='right', fontsize=8)\n",
        "            axes[1, 2].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        # í‰ê°€ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ í”Œë¡¯\n",
        "        for i in range(3):\n",
        "            axes[1, i].text(0.5, 0.5, 'í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤\\n(Cell 8 ì‹¤í–‰ í•„ìš”)', \n",
        "                           ha='center', va='center', transform=axes[1, i].transAxes,\n",
        "                           fontsize=12, style='italic')\n",
        "            axes[1, i].set_xticks([])\n",
        "            axes[1, i].set_yticks([])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # í…ìŠ¤íŠ¸ ìš”ì•½\n",
        "    print(\"\\nğŸ“‹ í•™ìŠµ ìš”ì•½:\")\n",
        "    print(\"=\" * 50)\n",
        "    if loss_tracker.losses['total_loss']:\n",
        "        print(f\"ğŸ”¹ ìµœì¢… ì†ì‹¤: {loss_tracker.losses['total_loss'][-1]:.4f}\")\n",
        "        print(f\"ğŸ”¹ ìµœì € ì†ì‹¤: {min(loss_tracker.losses['total_loss']):.4f}\")\n",
        "        print(f\"ğŸ”¹ ì´ ìŠ¤í…: {len(loss_tracker.losses['steps'])}\")\n",
        "    \n",
        "    if eval_results and 'metrics' in eval_results:\n",
        "        print(f\"ğŸ”¹ í‰ê°€ MAE: {eval_results['metrics']['mae_total']:.4f}\")\n",
        "        print(f\"ğŸ”¹ í‰ê°€ RMSE: {eval_results['metrics']['rmse_total']:.4f}\")\n",
        "\n",
        "# ì‹œê°í™” ì‹¤í–‰\n",
        "if 'final_loss_tracker' in locals():\n",
        "    plot_action_prediction_results(\n",
        "        final_loss_tracker, \n",
        "        eval_results if 'eval_results' in locals() else None\n",
        "    )\n",
        "else:\n",
        "    print(\"âš ï¸ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš” (Cell 7 ì‹¤í–‰)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš” (Cell 7 ì‹¤í–‰)\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: ğŸ’¾ ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ ìƒì„±\n",
        "\n",
        "def save_action_prediction_results(loss_tracker, eval_results, save_dir=None):\n",
        "    \"\"\"ì•¡ì…˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ê³¼ JSONìœ¼ë¡œ ì €ì¥\"\"\"\n",
        "    \n",
        "    if save_dir is None:\n",
        "        save_dir = ROOT_DIR / \"action_prediction_results\"\n",
        "    else:\n",
        "        save_dir = Path(save_dir)\n",
        "    \n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±\n",
        "    md_content = f\"\"\"# ğŸš€ Mobile VLA ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ë¦¬í¬íŠ¸\n",
        "\n",
        "**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "**ì‹œìŠ¤í…œ**: ì•¡ì…˜ ì˜ˆì¸¡ ì¤‘ì‹¬ Mobile VLA + Kosmos 2B\n",
        "\n",
        "## ğŸ“Š **í•µì‹¬ ê°œì„ ì‚¬í•­**\n",
        "\n",
        "| í•­ëª© | ì´ì „ (ì´ë²¤íŠ¸ ë¶„ë¥˜) | í˜„ì¬ (ì•¡ì…˜ ì˜ˆì¸¡) | ê°œì„  íš¨ê³¼ |\n",
        "|------|------------------|------------------|-----------|\n",
        "| **í•™ìŠµ ëª©í‘œ** | ğŸ”´ ì´ë²¤íŠ¸ 99.9% (ë„ˆë¬´ ë‹¨ìˆœ) | âœ… ì´ë¯¸ì§€â†’ì•¡ì…˜ ë§¤í•‘ | ì§„ì§œ VLA í•™ìŠµ |\n",
        "| **ë©”ì»¤ë‹ˆì¦˜** | ğŸ”´ Frame-by-frame | âœ… Window/Chunk | RoboVLMs ìŠ¤íƒ€ì¼ |\n",
        "| **ì†ì‹¤ í•¨ìˆ˜** | ğŸ”´ CrossEntropy + MSE | âœ… Huber Loss | íšŒê·€ ì¤‘ì‹¬ |\n",
        "| **ë©”íŠ¸ë¦­** | ğŸ”´ Accuracy, F1-Score | âœ… MAE, RÂ², RMSE | ì—°ì† ì•¡ì…˜ í‰ê°€ |\n",
        "\n",
        "## ğŸ¯ **í•™ìŠµ ì„¤ì •**\n",
        "\n",
        "- **Window Size**: 16 í”„ë ˆì„ (ê³¼ê±° ì»¨í…ìŠ¤íŠ¸)\n",
        "- **Chunk Size**: 2 í”„ë ˆì„ (ë¯¸ë˜ ì˜ˆì¸¡)\n",
        "- **ì•¡ì…˜ ì°¨ì›**: 3D [linear_x, linear_y, angular_z]\n",
        "- **ëª¨ë¸**: Kosmos 2B + ActionPredictionHead\n",
        "- **ì†ì‹¤**: Huber Loss (outlier robust)\n",
        "\n",
        "## ğŸ“ˆ **í•™ìŠµ ê²°ê³¼**\n",
        "\"\"\"\n",
        "    \n",
        "    if hasattr(loss_tracker, 'losses') and loss_tracker.losses['steps']:\n",
        "        final_loss = loss_tracker.losses['total_loss'][-1]\n",
        "        min_loss = min(loss_tracker.losses['total_loss'])\n",
        "        total_steps = len(loss_tracker.losses['steps'])\n",
        "        \n",
        "        md_content += f\"\"\"\n",
        "### ğŸ‹ï¸ **í•™ìŠµ ì„±ëŠ¥**\n",
        "\n",
        "- **ìµœì¢… ì†ì‹¤**: {final_loss:.4f}\n",
        "- **ìµœì € ì†ì‹¤**: {min_loss:.4f}  \n",
        "- **ì´ í•™ìŠµ ìŠ¤í…**: {total_steps:,}\n",
        "- **ìˆ˜ë ´ì„±**: {'âœ… ì•ˆì •ì ' if final_loss < min_loss * 1.1 else 'âš ï¸ ì¶”ê°€ í•™ìŠµ í•„ìš”'}\n",
        "\"\"\"\n",
        "    \n",
        "    if eval_results and 'metrics' in eval_results:\n",
        "        metrics = eval_results['metrics']\n",
        "        md_content += f\"\"\"\n",
        "### ğŸ¯ **í‰ê°€ ì„±ëŠ¥**\n",
        "\n",
        "#### ì „ì²´ ë©”íŠ¸ë¦­\n",
        "- **Total MAE**: {metrics['mae_total']:.4f}\n",
        "- **Total MSE**: {metrics['mse_total']:.4f}\n",
        "- **Total RMSE**: {metrics['rmse_total']:.4f}\n",
        "\n",
        "#### ì°¨ì›ë³„ ì„±ëŠ¥\n",
        "| ì•¡ì…˜ ì°¨ì› | MAE | RÂ² Score | í•´ì„ |\n",
        "|----------|-----|----------|------|\n",
        "| **Linear X** | {metrics.get('mae_linear_x', 0):.4f} | {metrics.get('r2_linear_x', 0):.3f} | {'âœ… ìš°ìˆ˜' if metrics.get('mae_linear_x', 1) < 0.1 else 'âš ï¸ ê°œì„  í•„ìš”'} |\n",
        "| **Linear Y** | {metrics.get('mae_linear_y', 0):.4f} | {metrics.get('r2_linear_y', 0):.3f} | {'âœ… ìš°ìˆ˜' if metrics.get('mae_linear_y', 1) < 0.1 else 'âš ï¸ ê°œì„  í•„ìš”'} |\n",
        "| **Angular Z** | {metrics.get('mae_angular_z', 0):.4f} | {metrics.get('r2_angular_z', 0):.3f} | {'âœ… ìš°ìˆ˜' if metrics.get('mae_angular_z', 1) < 0.1 else 'âš ï¸ ê°œì„  í•„ìš”'} |\n",
        "\"\"\"\n",
        "        \n",
        "        if 'scenario_metrics' in eval_results:\n",
        "            md_content += \"\"\"\n",
        "#### ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥\n",
        "| ì‹œë‚˜ë¦¬ì˜¤ | MAE | ìƒëŒ€ ì„±ëŠ¥ |\n",
        "|----------|-----|-----------|\n",
        "\"\"\"\n",
        "            scenario_data = eval_results['scenario_metrics']\n",
        "            avg_mae = metrics['mae_total']\n",
        "            for scenario, mae in scenario_data.items():\n",
        "                relative_perf = \"âœ… ìš°ìˆ˜\" if mae < avg_mae else \"âš ï¸ í‰ê·  ì´í•˜\"\n",
        "                md_content += f\"| {scenario} | {mae:.4f} | {relative_perf} |\\n\"\n",
        "    \n",
        "    md_content += f\"\"\"\n",
        "## ğŸ”„ **ì´ì „ vs í˜„ì¬ ë¹„êµ**\n",
        "\n",
        "### í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”\n",
        "1. **ë¶„ë¥˜ â†’ íšŒê·€**: ì´ë²¤íŠ¸ ë¶„ë¥˜ì—ì„œ ì•¡ì…˜ ì˜ˆì¸¡ìœ¼ë¡œ ì „í™˜\n",
        "2. **ë‹¨ìˆœ â†’ ë³µì¡**: 99.9% ì •í™•ë„ì—ì„œ ì§„ì§œ ì–´ë ¤ìš´ ë¬¸ì œë¡œ\n",
        "3. **í”„ë ˆì„ â†’ ì‹œí€€ìŠ¤**: Window/Chunk ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‹œê°„ì  ê´€ê³„ í•™ìŠµ\n",
        "4. **ìˆ˜ë™ â†’ ìë™**: ìˆ˜ë™ íŠ¹ì§•ì—ì„œ end-to-end í•™ìŠµìœ¼ë¡œ\n",
        "\n",
        "### ê¸°ëŒ€ íš¨ê³¼\n",
        "- **ì‹¤ì œ ë¡œë´‡ ì ìš©**: ì´ë¯¸ì§€ ë³´ê³  ì‹¤ì‹œê°„ ì•¡ì…˜ ìƒì„± ê°€ëŠ¥\n",
        "- **ì¼ë°˜í™” ì„±ëŠ¥**: ë‹¤ì–‘í•œ ì¥ì• ë¬¼ íŒ¨í„´ì—ì„œ robustí•œ ë„¤ë¹„ê²Œì´ì…˜\n",
        "- **í™•ì¥ì„±**: ë‹¤ë¥¸ ëª¨ë°”ì¼ ë¡œë´‡ íƒœìŠ¤í¬ë¡œ ì‰½ê²Œ í™•ì¥ ê°€ëŠ¥\n",
        "\n",
        "## ğŸš€ **ë‹¤ìŒ ë‹¨ê³„**\n",
        "\n",
        "1. **ì‹¤ì‹œê°„ í…ŒìŠ¤íŠ¸**: ì‹¤ì œ ë¡œë´‡ì—ì„œ inference ì„±ëŠ¥ ê²€ì¦\n",
        "2. **ë°ì´í„° ì¦ê°•**: ë” ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì™€ í™˜ê²½ ì¡°ê±´ ì¶”ê°€\n",
        "3. **ëª¨ë¸ ìµœì í™”**: TensorRT, ONNX ë“±ìœ¼ë¡œ ì¶”ë¡  ì†ë„ ê°œì„ \n",
        "4. **ë©€í‹°íƒœìŠ¤í¬**: ì»µ ì¶”ì  ì™¸ ë‹¤ë¥¸ ë„¤ë¹„ê²Œì´ì…˜ íƒœìŠ¤í¬ ì¶”ê°€\n",
        "\n",
        "---\n",
        "**ìƒì„± ì‹œìŠ¤í…œ**: Mobile VLA + Kosmos 2B Action Prediction\n",
        "**ë³´ê³ ì„œ ë²„ì „**: v2.0 (ì•¡ì…˜ ì˜ˆì¸¡ ì¤‘ì‹¬)\n",
        "\"\"\"\n",
        "    \n",
        "    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥\n",
        "    md_path = save_dir / f\"Action_Prediction_Report_{timestamp}.md\"\n",
        "    with open(md_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(md_content)\n",
        "    \n",
        "    # JSON ë°ì´í„° ì €ì¥\n",
        "    json_data = {\n",
        "        'timestamp': timestamp,\n",
        "        'system_info': {\n",
        "            'model': 'Kosmos 2B + ActionPredictionHead',\n",
        "            'window_size': 16,\n",
        "            'chunk_size': 2,\n",
        "            'action_dim': 3,\n",
        "            'loss_function': 'Huber Loss'\n",
        "        },\n",
        "        'training_results': {},\n",
        "        'evaluation_results': {}\n",
        "    }\n",
        "    \n",
        "    # í•™ìŠµ ê²°ê³¼ ì¶”ê°€\n",
        "    if hasattr(loss_tracker, 'losses'):\n",
        "        json_data['training_results'] = {\n",
        "            'final_loss': loss_tracker.losses['total_loss'][-1] if loss_tracker.losses['total_loss'] else None,\n",
        "            'min_loss': min(loss_tracker.losses['total_loss']) if loss_tracker.losses['total_loss'] else None,\n",
        "            'total_steps': len(loss_tracker.losses['steps']),\n",
        "            'loss_history': loss_tracker.losses['total_loss'][-100:],  # ë§ˆì§€ë§‰ 100ê°œë§Œ ì €ì¥\n",
        "            'mae_history': {\n",
        "                'linear_x': loss_tracker.losses['mae_linear_x'][-100:] if 'mae_linear_x' in loss_tracker.losses else [],\n",
        "                'linear_y': loss_tracker.losses['mae_linear_y'][-100:] if 'mae_linear_y' in loss_tracker.losses else [],\n",
        "                'angular_z': loss_tracker.losses['mae_angular_z'][-100:] if 'mae_angular_z' in loss_tracker.losses else []\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    # í‰ê°€ ê²°ê³¼ ì¶”ê°€\n",
        "    if eval_results:\n",
        "        json_data['evaluation_results'] = {\n",
        "            'metrics': eval_results.get('metrics', {}),\n",
        "            'scenario_metrics': eval_results.get('scenario_metrics', {}),\n",
        "            'num_samples': len(eval_results.get('predictions', []))\n",
        "        }\n",
        "    \n",
        "    # JSON íŒŒì¼ ì €ì¥\n",
        "    json_path = save_dir / f\"Action_Prediction_Data_{timestamp}.json\"\n",
        "    with open(json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:\")\n",
        "    print(f\"   ğŸ“„ ë§ˆí¬ë‹¤ìš´: {md_path}\")\n",
        "    print(f\"   ğŸ“Š JSON: {json_path}\")\n",
        "    \n",
        "    return md_path, json_path\n",
        "\n",
        "# ê²°ê³¼ ì €ì¥ ì‹¤í–‰\n",
        "if 'final_loss_tracker' in locals():\n",
        "    print(\"ğŸ’¾ ì•¡ì…˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "    md_path, json_path = save_action_prediction_results(\n",
        "        final_loss_tracker,\n",
        "        eval_results if 'eval_results' in locals() else None\n",
        "    )\n",
        "    print(\"ğŸ‰ ëª¨ë“  ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "else:\n",
        "    print(\"âš ï¸ ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•´ì£¼ì„¸ìš” (Cell 7 ì‹¤í–‰)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë° ë¬¸ì œ í•´ê²°\n",
        "\n",
        "# ë°ì´í„°ì…‹ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "if 'dataset' in locals() and len(dataset) > 0:\n",
        "    print(\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ!\")\n",
        "    \n",
        "    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "    try:\n",
        "        sample = dataset[0]\n",
        "        print(f\"\\nğŸ“Š ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸:\")\n",
        "        for key, value in sample.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value)}\")\n",
        "        \n",
        "        print(f\"\\nğŸ¯ íƒœìŠ¤í¬: {sample['task_description']}\")\n",
        "        print(f\"ğŸ® ì‹œë‚˜ë¦¬ì˜¤: {sample['scenario']}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ìƒ˜í”Œ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "elif 'dataset' in locals() and len(dataset) == 0:\n",
        "    print(\"âš ï¸ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
        "    print(\"\\nğŸ” ë¬¸ì œ í•´ê²° ë°©ë²•:\")\n",
        "    print(\"1. íŒŒì¼ ê²½ë¡œ í™•ì¸: /home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset/\")\n",
        "    print(\"2. HDF5 íŒŒì¼ ì¡´ì¬ í™•ì¸\")\n",
        "    print(\"3. ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘ ë¡œì§ í™•ì¸\")\n",
        "    \n",
        "    # ë°ì´í„° ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸\n",
        "    print(f\"\\nğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ ë‚´ìš©:\")\n",
        "    h5_files = list(DATA_DIR.glob(\"*.h5\"))\n",
        "    print(f\"   ì´ HDF5 íŒŒì¼: {len(h5_files)}ê°œ\")\n",
        "    \n",
        "    if h5_files:\n",
        "        print(f\"   ì˜ˆì‹œ íŒŒì¼: {h5_files[0].name}\")\n",
        "        \n",
        "        # ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
        "        test_filename = h5_files[0].name\n",
        "        print(f\"\\nğŸ§ª ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ í…ŒìŠ¤íŠ¸: {test_filename}\")\n",
        "        \n",
        "        # ìˆ˜ë™ìœ¼ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸\n",
        "        filename_lower = test_filename.lower()\n",
        "        print(f\"   ì†Œë¬¸ì ë³€í™˜: {filename_lower}\")\n",
        "        \n",
        "        if \"1box\" in filename_lower and \"vert\" in filename_lower and \"left\" in filename_lower:\n",
        "            result = \"1box_left_vertical\"\n",
        "        elif \"1box\" in filename_lower and \"hori\" in filename_lower and \"left\" in filename_lower:\n",
        "            result = \"1box_left_horizontal\"\n",
        "        else:\n",
        "            result = \"unknown\"\n",
        "        \n",
        "        print(f\"   ì¶”ì¶œëœ ì‹œë‚˜ë¦¬ì˜¤: {result}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ì…‹ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cell 3ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n",
            "   í• ë‹¹ëœ ë©”ëª¨ë¦¬: 6.24 GB\n",
            "   ìºì‹œëœ ë©”ëª¨ë¦¬: 6.53 GB\n",
            "\n",
            "âš™ï¸ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •ìœ¼ë¡œ ì¬ì´ˆê¸°í™”...\n",
            "   ë°°ì¹˜ í¬ê¸°: 1 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
            "   ì´ ë°°ì¹˜ ìˆ˜: 72\n",
            "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n",
            "   í• ë‹¹ëœ ë©”ëª¨ë¦¬: 0.01 GB\n",
            "   ìºì‹œëœ ë©”ëª¨ë¦¬: 0.03 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ìœˆë„ìš° í¬ê¸°: 8 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
            "   ì²­í¬ í¬ê¸°: 2\n",
            "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n",
            "   í• ë‹¹ëœ ë©”ëª¨ë¦¬: 6.24 GB\n",
            "   ìºì‹œëœ ë©”ëª¨ë¦¬: 6.26 GB\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ë° ì •ë¦¬\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"GPU ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
        "    print(\"ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\")\n",
        "    \n",
        "    # ìºì‹œ ì •ë¦¬\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
        "    gc.collect()\n",
        "    \n",
        "    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        cached = torch.cuda.memory_reserved() / 1024**3\n",
        "        print(f\"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f} GB\")\n",
        "        print(f\"   ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.2f} GB\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ ë° íŠ¸ë ˆì´ë„ˆ ì¬ì´ˆê¸°í™”\n",
        "print(\"\\nâš™ï¸ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì •ìœ¼ë¡œ ì¬ì´ˆê¸°í™”...\")\n",
        "\n",
        "# 1. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°\n",
        "if 'dataloader' in locals():\n",
        "    del dataloader\n",
        "\n",
        "# ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ë°ì´í„°ë¡œë” ì¬ìƒì„±\n",
        "dataloader = DataLoader(\n",
        "    dataset, \n",
        "    batch_size=1,  # 4 -> 1ë¡œ ì¤„ì„\n",
        "    shuffle=True, \n",
        "    num_workers=0  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •\n",
        ")\n",
        "\n",
        "print(f\"   ë°°ì¹˜ í¬ê¸°: {dataloader.batch_size} (ë©”ëª¨ë¦¬ ì ˆì•½)\")\n",
        "print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(dataloader)}\")\n",
        "\n",
        "# 2. íŠ¸ë ˆì´ë„ˆ ì¬ì´ˆê¸°í™” (ë” ì‘ì€ ìœˆë„ìš° ì‚¬ì´ì¦ˆ)\n",
        "if 'trainer' in locals():\n",
        "    del trainer\n",
        "\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŠ¸ë ˆì´ë„ˆ\n",
        "memory_efficient_trainer = ActionPredictionTrainer(\n",
        "    model_name=\"microsoft/kosmos-2-patch14-224\",\n",
        "    action_dim=3,\n",
        "    window_size=8,   # 16 -> 8ë¡œ ì¤„ì„\n",
        "    chunk_size=2,\n",
        "    learning_rate=1e-4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"   ìœˆë„ìš° í¬ê¸°: {memory_efficient_trainer.window_size} (ë©”ëª¨ë¦¬ ì ˆì•½)\")\n",
        "print(f\"   ì²­í¬ í¬ê¸°: {memory_efficient_trainer.chunk_size}\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬í™•ì¸\n",
        "clear_gpu_memory()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì¤‘...\n",
            "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n",
            "   í• ë‹¹ëœ ë©”ëª¨ë¦¬: 6.24 GB\n",
            "   ìºì‹œëœ ë©”ëª¨ë¦¬: 6.26 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. í•„í„°ë§ ì¡°ê±´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì •ë³´:\n",
            "   ì´ ìƒ˜í”Œ: 0ê°œ\n",
            "   Window Size: 8 í”„ë ˆì„\n",
            "   Chunk Size: 2 í”„ë ˆì„\n",
            "   Total Sequence: 10 í”„ë ˆì„\n",
            "âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# Cell 13: ğŸ”§ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„±\n",
        "\n",
        "# ìœˆë„ìš° í¬ê¸°ì— ë§ì¶° ë°ì´í„°ì…‹ ì¬ìƒì„±\n",
        "print(\"ğŸ”§ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë¦¬\n",
        "if 'dataset' in locals():\n",
        "    del dataset\n",
        "\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ (ìœˆë„ìš° 8 + ì²­í¬ 2 = 10í”„ë ˆì„)\n",
        "memory_efficient_dataset = WindowChunkAdapter(\n",
        "    data_dir=str(DATA_DIR),\n",
        "    window_size=8,   # 16 -> 8ë¡œ ì¤„ì„\n",
        "    chunk_size=2,\n",
        "    image_processor=kosmos_processor.image_processor,\n",
        "    normalize_actions=True\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "print(f\"   ì´ ìƒ˜í”Œ: {len(memory_efficient_dataset)}ê°œ\")\n",
        "print(f\"   Window Size: {memory_efficient_dataset.window_size} í”„ë ˆì„\")\n",
        "print(f\"   Chunk Size: {memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "print(f\"   Total Sequence: {memory_efficient_dataset.window_size + memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "\n",
        "if len(memory_efficient_dataset) > 0:\n",
        "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë¡œë”\n",
        "    memory_efficient_dataloader = DataLoader(\n",
        "        memory_efficient_dataset, \n",
        "        batch_size=1,  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°\n",
        "        shuffle=True, \n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    print(f\"   ë°°ì¹˜ í¬ê¸°: {memory_efficient_dataloader.batch_size}\")\n",
        "    print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(memory_efficient_dataloader)}\")\n",
        "    \n",
        "    # ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬ í™•ì¸\n",
        "    scenarios = [memory_efficient_dataset.samples[i]['scenario'] for i in range(len(memory_efficient_dataset))]\n",
        "    from collections import Counter\n",
        "    scenario_counts = Counter(scenarios)\n",
        "    \n",
        "    print(f\"\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„í¬:\")\n",
        "    for scenario, count in scenario_counts.items():\n",
        "        print(f\"   {scenario}: {count}ê°œ\")\n",
        "    \n",
        "    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "    print(f\"\\nğŸ§ª ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸:\")\n",
        "    sample = memory_efficient_dataset[0]\n",
        "    for key, value in sample.items():\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "        else:\n",
        "            print(f\"   {key}: {type(value)}\")\n",
        "    \n",
        "    clear_gpu_memory()\n",
        "    print(\"\\nâœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì • ì™„ë£Œ!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ ë¨¼ì € Cell 12-13ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì‹¤í–‰\n",
        "\n",
        "def train_memory_efficient(\n",
        "    trainer, \n",
        "    dataloader, \n",
        "    num_epochs=5,  # ì—í¬í¬ ìˆ˜ ì¤„ì„\n",
        "    save_interval=2,\n",
        "    log_interval=10,\n",
        "    memory_cleanup_interval=5  # 5 ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "):\n",
        "    \"\"\"ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ í•¨ìˆ˜\"\"\"\n",
        "    \n",
        "    # ì†ì‹¤ ì¶”ì ê¸°\n",
        "    class ActionLossTracker:\n",
        "        def __init__(self):\n",
        "            self.losses = {\n",
        "                'steps': [],\n",
        "                'action_loss': [],\n",
        "                'total_loss': [],\n",
        "                'lr': [],\n",
        "                'mae_linear_x': [],\n",
        "                'mae_linear_y': [], \n",
        "                'mae_angular_z': [],\n",
        "                'mae_avg': [],\n",
        "                'scenarios': []\n",
        "            }\n",
        "        \n",
        "        def update(self, step, loss_dict, scenario=None):\n",
        "            self.losses['steps'].append(step)\n",
        "            for key in ['action_loss', 'total_loss', 'lr', 'mae_linear_x', 'mae_linear_y', 'mae_angular_z', 'mae_avg']:\n",
        "                if key in loss_dict:\n",
        "                    self.losses[key].append(loss_dict[key])\n",
        "                else:\n",
        "                    self.losses[key].append(0.0)\n",
        "            self.losses['scenarios'].append(scenario or 'unknown')\n",
        "    \n",
        "    loss_tracker = ActionLossTracker()\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "    \n",
        "    print(\"ğŸš€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ ì‹œì‘!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ğŸ“Š ì„¤ì •: ë°°ì¹˜í¬ê¸°={dataloader.batch_size}, ìœˆë„ìš°={trainer.window_size}, ì—í¬í¬={num_epochs}\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = []\n",
        "        successful_batches = 0\n",
        "        \n",
        "        # ì—í¬í¬ ì§„í–‰ ë°”\n",
        "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            try:\n",
        "                # í•™ìŠµ ìŠ¤í…\n",
        "                loss_dict = trainer.train_step(batch)\n",
        "                epoch_losses.append(loss_dict['total_loss'])\n",
        "                global_step += 1\n",
        "                successful_batches += 1\n",
        "                \n",
        "                # ì†ì‹¤ ì¶”ì \n",
        "                scenario = batch.get('scenario', ['unknown'])[0] if 'scenario' in batch else 'unknown'\n",
        "                loss_tracker.update(global_step, loss_dict, scenario)\n",
        "                \n",
        "                # ë¡œê·¸ ì¶œë ¥\n",
        "                if global_step % log_interval == 0:\n",
        "                    pbar.set_postfix({\n",
        "                        'Loss': f\"{loss_dict['total_loss']:.4f}\",\n",
        "                        'MAE_x': f\"{loss_dict.get('mae_linear_x', 0):.4f}\",\n",
        "                        'MAE_y': f\"{loss_dict.get('mae_linear_y', 0):.4f}\",\n",
        "                        'MAE_z': f\"{loss_dict.get('mae_angular_z', 0):.4f}\",\n",
        "                        'LR': f\"{loss_dict['lr']:.2e}\",\n",
        "                        'Success': f\"{successful_batches}/{batch_idx+1}\"\n",
        "                    })\n",
        "                \n",
        "                # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "                if batch_idx % memory_cleanup_interval == 0:\n",
        "                    clear_gpu_memory()\n",
        "                \n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"\\\\nâš ï¸ OOM ì—ëŸ¬ (Step {global_step}), ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ê³„ì†...\")\n",
        "                    clear_gpu_memory()\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"\\\\nâŒ í•™ìŠµ ìŠ¤í… ì‹¤íŒ¨ (Step {global_step}): {e}\")\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"\\\\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (Step {global_step}): {e}\")\n",
        "                continue\n",
        "        \n",
        "        # ì—í¬í¬ ìš”ì•½\n",
        "        avg_loss = np.mean(epoch_losses) if epoch_losses else float('inf')\n",
        "        success_rate = successful_batches / len(dataloader) * 100\n",
        "        \n",
        "        print(f\"\\\\nğŸ“Š Epoch {epoch+1} ì™„ë£Œ:\")\n",
        "        print(f\"   í‰ê·  ì†ì‹¤: {avg_loss:.4f}\")\n",
        "        print(f\"   ì„±ê³µë¥ : {success_rate:.1f}% ({successful_batches}/{len(dataloader)})\")\n",
        "        \n",
        "        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "        if avg_loss < best_loss and epoch_losses:\n",
        "            best_loss = avg_loss\n",
        "            save_path = ROOT_DIR / f\"checkpoints/memory_efficient_best.pth\"\n",
        "            save_path.parent.mkdir(exist_ok=True)\n",
        "            trainer.save_checkpoint(str(save_path), epoch, best_loss)\n",
        "            print(f\"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥: {save_path}\")\n",
        "        \n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_path = ROOT_DIR / f\"checkpoints/memory_efficient_epoch_{epoch+1}.pth\"\n",
        "            save_path.parent.mkdir(exist_ok=True)\n",
        "            trainer.save_checkpoint(str(save_path), epoch, best_loss)\n",
        "            print(f\"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}\")\n",
        "        \n",
        "        # ì—í¬í¬ ëì— ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "        clear_gpu_memory()\n",
        "    \n",
        "    print(\"\\\\nğŸ‰ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(f\"ìµœì¢… ìµœê³  ì†ì‹¤: {best_loss:.4f}\")\n",
        "    \n",
        "    return loss_tracker\n",
        "\n",
        "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ ì‹¤í–‰\n",
        "if 'memory_efficient_trainer' in locals() and 'memory_efficient_dataloader' in locals():\n",
        "    print(\"âš¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "    print(f\"ğŸ”§ ë©”ëª¨ë¦¬ ì„¤ì •: Window={memory_efficient_trainer.window_size}, Batch={memory_efficient_dataloader.batch_size}\")\n",
        "    \n",
        "    memory_efficient_loss_tracker = train_memory_efficient(\n",
        "        trainer=memory_efficient_trainer,\n",
        "        dataloader=memory_efficient_dataloader, \n",
        "        num_epochs=5,  # ì ì€ ì—í¬í¬ë¡œ í…ŒìŠ¤íŠ¸\n",
        "        save_interval=2,\n",
        "        log_interval=5,\n",
        "        memory_cleanup_interval=3\n",
        "    )\n",
        "else:\n",
        "    print(\"âŒ ë¨¼ì € Cell 12-13ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: ğŸ› ï¸ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def create_dummy_mobile_vla_data(data_dir, num_episodes=10):\n",
        "    \"\"\"í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ Mobile VLA ë°ì´í„° ìƒì„±\"\"\"\n",
        "    \n",
        "    data_dir = Path(data_dir)\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    scenarios = [\n",
        "        \"1box_left_vertical\", \"1box_left_horizontal\", \n",
        "        \"1box_right_vertical\", \"1box_right_horizontal\",\n",
        "        \"2box_left_vertical\", \"2box_left_horizontal\",\n",
        "        \"2box_right_vertical\", \"2box_right_horizontal\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"ğŸ› ï¸ {data_dir}ì— ë”ë¯¸ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        # ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ\n",
        "        scenario = scenarios[i % len(scenarios)]\n",
        "        \n",
        "        # íŒŒì¼ëª… ìƒì„± (ì‹¤ì œ íŒ¨í„´ê³¼ ìœ ì‚¬í•˜ê²Œ)\n",
        "        filename = f\"episode_20250820_{i:06d}_{scenario}_core_medium.h5\"\n",
        "        filepath = data_dir / filename\n",
        "        \n",
        "        # ë”ë¯¸ ë°ì´í„° ìƒì„±\n",
        "        sequence_length = np.random.randint(50, 150)  # 50-150 í”„ë ˆì„\n",
        "        \n",
        "        # ë”ë¯¸ ì´ë¯¸ì§€ (720x1280x3 -> Mobile VLA ì›ë³¸ í•´ìƒë„)\n",
        "        dummy_images = np.random.randint(0, 255, (sequence_length, 720, 1280, 3), dtype=np.uint8)\n",
        "        \n",
        "        # ë”ë¯¸ ì•¡ì…˜ ([linear_x, linear_y, angular_z])\n",
        "        dummy_actions = np.random.randn(sequence_length, 3).astype(np.float32)\n",
        "        dummy_actions[:, 0] = np.clip(dummy_actions[:, 0], -1.0, 1.0)  # linear_x\n",
        "        dummy_actions[:, 1] = np.clip(dummy_actions[:, 1], -1.0, 1.0)  # linear_y  \n",
        "        dummy_actions[:, 2] = np.clip(dummy_actions[:, 2], -2.0, 2.0)  # angular_z\n",
        "        \n",
        "        # ë”ë¯¸ ì´ë²¤íŠ¸ (0: episode_start, 1: start_action)\n",
        "        dummy_events = np.ones(sequence_length, dtype=np.int32)\n",
        "        dummy_events[0] = 0  # ì²« í”„ë ˆì„ì€ episode_start\n",
        "        \n",
        "        # H5 íŒŒì¼ë¡œ ì €ì¥\n",
        "        with h5py.File(filepath, 'w') as f:\n",
        "            f.create_dataset('images', data=dummy_images, compression='lzf')\n",
        "            f.create_dataset('actions', data=dummy_actions)\n",
        "            f.create_dataset('events', data=dummy_events)\n",
        "            \n",
        "            # ë©”íƒ€ë°ì´í„°\n",
        "            f.attrs['scenario'] = scenario\n",
        "            f.attrs['sequence_length'] = sequence_length\n",
        "            f.attrs['created_by'] = 'mobile_vla_data_collector.py'\n",
        "    \n",
        "    print(f\"âœ… {num_episodes}ê°œì˜ ë”ë¯¸ ì—í”¼ì†Œë“œ ìƒì„± ì™„ë£Œ!\")\n",
        "    print(f\"   ì‹œë‚˜ë¦¬ì˜¤: {scenarios}\")\n",
        "    \n",
        "    # ìƒì„±ëœ íŒŒì¼ í™•ì¸\n",
        "    created_files = list(data_dir.glob(\"*.h5\"))\n",
        "    print(f\"   ìƒì„±ëœ íŒŒì¼: {len(created_files)}ê°œ\")\n",
        "    for f in created_files[:3]:\n",
        "        print(f\"   - {f.name}\")\n",
        "    if len(created_files) > 3:\n",
        "        print(f\"   - ... (ì´ {len(created_files)}ê°œ)\")\n",
        "\n",
        "# ë”ë¯¸ ë°ì´í„° ìƒì„±\n",
        "dummy_data_dir = ROOT_DIR / \"dummy_data\"\n",
        "create_dummy_mobile_vla_data(dummy_data_dir, num_episodes=16)\n",
        "\n",
        "# ë°ì´í„° ë””ë ‰í„°ë¦¬ ì—…ë°ì´íŠ¸\n",
        "DATA_DIR = dummy_data_dir\n",
        "print(f\"\\\\nğŸ“Š ë°ì´í„° ë””ë ‰í„°ë¦¬ ì—…ë°ì´íŠ¸: {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 16: ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„±\n",
        "\n",
        "# WindowChunkAdapter ëª¨ë“ˆ ë¦¬ë¡œë“œ\n",
        "import importlib\n",
        "import sys\n",
        "if 'window_chunk_adapter' in sys.modules:\n",
        "    importlib.reload(sys.modules['window_chunk_adapter'])\n",
        "\n",
        "# WindowChunkAdapter ë‹¤ì‹œ ë¡œë“œ\n",
        "WindowChunkAdapter = load_class_from_file(\n",
        "    'window_chunk_adapter',\n",
        "    str(ROOT_DIR / 'data' / 'window_chunk_adapter.py'),\n",
        "    'WindowChunkAdapter'\n",
        ")\n",
        "\n",
        "print(\"ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë¦¬\n",
        "if 'memory_efficient_dataset' in locals():\n",
        "    del memory_efficient_dataset\n",
        "\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ë”ë¯¸ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ìƒì„±\n",
        "memory_efficient_dataset = WindowChunkAdapter(\n",
        "    data_dir=str(DATA_DIR),\n",
        "    window_size=8,   # 16 -> 8ë¡œ ì¤„ì„\n",
        "    chunk_size=2,\n",
        "    image_processor=kosmos_processor.image_processor,\n",
        "    normalize_actions=True\n",
        ")\n",
        "\n",
        "print(f\"\\\\nğŸ“Š ë”ë¯¸ ë°ì´í„° ê¸°ë°˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "print(f\"   ì´ ìƒ˜í”Œ: {len(memory_efficient_dataset)}ê°œ\")\n",
        "print(f\"   Window Size: {memory_efficient_dataset.window_size} í”„ë ˆì„\")\n",
        "print(f\"   Chunk Size: {memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "print(f\"   Total Sequence: {memory_efficient_dataset.window_size + memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "\n",
        "if len(memory_efficient_dataset) > 0:\n",
        "    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë¡œë”\n",
        "    memory_efficient_dataloader = DataLoader(\n",
        "        memory_efficient_dataset, \n",
        "        batch_size=1,  # ìµœì†Œ ë°°ì¹˜ í¬ê¸°\n",
        "        shuffle=True, \n",
        "        num_workers=0\n",
        "    )\n",
        "    \n",
        "    print(f\"   ë°°ì¹˜ í¬ê¸°: {memory_efficient_dataloader.batch_size}\")\n",
        "    print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(memory_efficient_dataloader)}\")\n",
        "    \n",
        "    # ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬ í™•ì¸\n",
        "    scenarios = [memory_efficient_dataset.samples[i]['scenario'] for i in range(len(memory_efficient_dataset))]\n",
        "    from collections import Counter\n",
        "    scenario_counts = Counter(scenarios)\n",
        "    \n",
        "    print(f\"\\\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„í¬:\")\n",
        "    for scenario, count in scenario_counts.items():\n",
        "        print(f\"   {scenario}: {count}ê°œ\")\n",
        "    \n",
        "    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "    print(f\"\\\\nğŸ§ª ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸:\")\n",
        "    sample = memory_efficient_dataset[0]\n",
        "    for key, value in sample.items():\n",
        "        if isinstance(value, torch.Tensor):\n",
        "            print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "        else:\n",
        "            print(f\"   {key}: {type(value)}\")\n",
        "    \n",
        "    clear_gpu_memory()\n",
        "    print(\"\\\\nâœ… ë”ë¯¸ ë°ì´í„° ê¸°ë°˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì • ì™„ë£Œ!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ ë”ë¯¸ ë°ì´í„°ì…‹ë„ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. WindowChunkAdapter ë¡œì§ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: ğŸ”„ ì‹¤ì œ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„±\n",
        "\n",
        "# WindowChunkAdapter ëª¨ë“ˆ ë¦¬ë¡œë“œ\n",
        "import importlib\n",
        "import sys\n",
        "if 'window_chunk_adapter' in sys.modules:\n",
        "    importlib.reload(sys.modules['window_chunk_adapter'])\n",
        "\n",
        "# WindowChunkAdapter ë‹¤ì‹œ ë¡œë“œ\n",
        "WindowChunkAdapter = load_class_from_file(\n",
        "    'window_chunk_adapter',\n",
        "    str(ROOT_DIR / 'data' / 'window_chunk_adapter.py'),\n",
        "    'WindowChunkAdapter'\n",
        ")\n",
        "\n",
        "print(\"ğŸ”„ ì‹¤ì œ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì¬ìƒì„± ì¤‘...\")\n",
        "print(f\"ğŸ“Š ë°ì´í„° ê²½ë¡œ: {DATA_DIR}\")\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„°ì…‹ ì •ë¦¬\n",
        "for var_name in ['memory_efficient_dataset', 'dataset']:\n",
        "    if var_name in locals():\n",
        "        del locals()[var_name]\n",
        "\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ì‹¤ì œ ë°ì´í„°ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ìƒì„±\n",
        "try:\n",
        "    memory_efficient_dataset = WindowChunkAdapter(\n",
        "        data_dir=str(DATA_DIR),\n",
        "        window_size=8,   # 16 -> 8ë¡œ ì¤„ì„ (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "        chunk_size=2,\n",
        "        image_processor=kosmos_processor.image_processor,\n",
        "        normalize_actions=True\n",
        "    )\n",
        "    \n",
        "    print(f\"\\\\nğŸ“Š ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "    print(f\"   ì´ ìƒ˜í”Œ: {len(memory_efficient_dataset)}ê°œ\")\n",
        "    print(f\"   Window Size: {memory_efficient_dataset.window_size} í”„ë ˆì„\")\n",
        "    print(f\"   Chunk Size: {memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "    print(f\"   Total Sequence: {memory_efficient_dataset.window_size + memory_efficient_dataset.chunk_size} í”„ë ˆì„\")\n",
        "    \n",
        "    if len(memory_efficient_dataset) > 0:\n",
        "        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë¡œë”\n",
        "        memory_efficient_dataloader = DataLoader(\n",
        "            memory_efficient_dataset, \n",
        "            batch_size=1,  # ìµœì†Œ ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "            shuffle=True, \n",
        "            num_workers=0  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "        )\n",
        "        \n",
        "        print(f\"   ë°°ì¹˜ í¬ê¸°: {memory_efficient_dataloader.batch_size}\")\n",
        "        print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(memory_efficient_dataloader)}\")\n",
        "        \n",
        "        # ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬ í™•ì¸\n",
        "        scenarios = [memory_efficient_dataset.samples[i]['scenario'] for i in range(min(len(memory_efficient_dataset), 100))]\n",
        "        from collections import Counter\n",
        "        scenario_counts = Counter(scenarios)\n",
        "        \n",
        "        print(f\"\\\\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„í¬ (ì²˜ìŒ 100ê°œ):\")\n",
        "        for scenario, count in scenario_counts.items():\n",
        "            print(f\"   {scenario}: {count}ê°œ\")\n",
        "        \n",
        "        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "        print(f\"\\\\nğŸ§ª ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸:\")\n",
        "        sample = memory_efficient_dataset[0]\n",
        "        for key, value in sample.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value)} = {value}\")\n",
        "        \n",
        "        clear_gpu_memory()\n",
        "        print(\"\\\\nâœ… ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„¤ì • ì™„ë£Œ!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ ìœ íš¨í•œ ìƒ˜í”Œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        print(\"   WindowChunkAdapterì˜ í•„í„°ë§ ë¡œì§ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
        "        \n",
        "        # ë””ë²„ê¹…: ì›ë³¸ H5 íŒŒì¼ ëª‡ ê°œ í™•ì¸\n",
        "        h5_files = list(DATA_DIR.glob(\"*.h5\"))[:5]\n",
        "        print(f\"\\\\nğŸ” ë””ë²„ê¹…: H5 íŒŒì¼ ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 5ê°œ):\")\n",
        "        \n",
        "        import h5py\n",
        "        for h5_file in h5_files:\n",
        "            try:\n",
        "                with h5py.File(h5_file, 'r') as f:\n",
        "                    keys = list(f.keys())\n",
        "                    attrs = dict(f.attrs)\n",
        "                    print(f\"   {h5_file.name}:\")\n",
        "                    print(f\"     Keys: {keys}\")\n",
        "                    if 'images' in keys:\n",
        "                        print(f\"     Images: {f['images'].shape}\")\n",
        "                    if 'actions' in keys:\n",
        "                        print(f\"     Actions: {f['actions'].shape}\")\n",
        "                    print(f\"     Attrs: {attrs}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   {h5_file.name}: ì˜¤ë¥˜ - {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
        "    print(\"\\\\nğŸ” ì—ëŸ¬ ìƒì„¸:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 18: ğŸš€ RoboVLMs ìŠ¤íƒ€ì¼ Mobile VLA íŠ¸ë ˆì´ë‹\n",
        "\n",
        "# RoboVLMs ìŠ¤íƒ€ì¼ë¡œ ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ ì„í¬íŠ¸\n",
        "print(\"ğŸ”„ RoboVLMs ìŠ¤íƒ€ì¼ Mobile VLA ëª¨ë“ˆ ë¡œë”© ì¤‘...\")\n",
        "\n",
        "# ê²½ë¡œ ì¶”ê°€\n",
        "import sys\n",
        "sys.path.append(str(ROOT_DIR))\n",
        "\n",
        "# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ ì„í¬íŠ¸\n",
        "try:\n",
        "    from robovlms.data import MobileVLADataset\n",
        "    from robovlms.train import MobileVLATrainer, ActionLossTracker\n",
        "    print(\"âœ… Mobile VLA ëª¨ë“ˆ ë¡œë”© ì™„ë£Œ!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"   ê¸°ì¡´ RoboVLMs í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "# ê¸°ì¡´ ë³€ìˆ˜ ì •ë¦¬\n",
        "clear_gpu_memory()\n",
        "\n",
        "# MobileVLADataset ì´ˆê¸°í™” (RoboVLMs í˜¸í™˜)\n",
        "print(\"\\\\nğŸ“Š RoboVLMs ìŠ¤íƒ€ì¼ MobileVLADataset ì´ˆê¸°í™” ì¤‘...\")\n",
        "\n",
        "try:\n",
        "    # í† í¬ë‚˜ì´ì € ì„¤ì • (RoboVLMs ë°©ì‹)\n",
        "    tokenizer_config = {\n",
        "        \"tokenizer_type\": \"kosmos\",\n",
        "        \"max_text_len\": 512,\n",
        "        \"tokenizer_id\": \"microsoft/kosmos-2-patch14-224\"\n",
        "    }\n",
        "    \n",
        "    # ë°ì´í„°ì…‹ ì„¤ì •\n",
        "    dataset = MobileVLADataset(\n",
        "        data_dir=str(DATA_DIR),\n",
        "        model_name=\"kosmos\",\n",
        "        mode=\"train\",\n",
        "        organize_type=\"segment\",  # RoboVLMs í˜¸í™˜\n",
        "        window_size=8,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
        "        fwd_pred_next_n=2,\n",
        "        discrete=False,  # ì—°ì† ì•¡ì…˜\n",
        "        norm_action=True,\n",
        "        image_history=True,\n",
        "        action_history=True,\n",
        "        tokenizer=tokenizer_config,\n",
        "        rgb_pad=-1,  # ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”\n",
        "        gripper_pad=-1\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… MobileVLADataset ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "    print(f\"   ì´ ì—í”¼ì†Œë“œ: {len(dataset)}ê°œ\")\n",
        "    \n",
        "    if len(dataset) > 0:\n",
        "        # ë°ì´í„°ë¡œë” ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
        "        from torch.utils.data import DataLoader\n",
        "        \n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=1,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "            shuffle=True,\n",
        "            num_workers=0,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™” (ì•ˆì •ì„±)\n",
        "            collate_fn=dataset.collater  # RoboVLMs collater ì‚¬ìš©\n",
        "        )\n",
        "        \n",
        "        print(f\"   ë°°ì¹˜ ìˆ˜: {len(dataloader)}ê°œ\")\n",
        "        \n",
        "        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
        "        print(f\"\\\\nğŸ§ª ì²« ë²ˆì§¸ ë°°ì¹˜ ë°ì´í„° êµ¬ì¡° í™•ì¸:\")\n",
        "        sample_batch = next(iter(dataloader))\n",
        "        for key, value in sample_batch.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "            elif isinstance(value, list):\n",
        "                print(f\"   {key}: list[{len(value)}] - {type(value[0]) if value else 'empty'}\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value)}\")\n",
        "        \n",
        "        print(\"\\\\nâœ… RoboVLMs ìŠ¤íƒ€ì¼ ë°ì´í„° ë¡œë”© ì™„ë£Œ!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ê²½ë¡œì™€ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 19: ğŸ¯ RoboVLMs ìŠ¤íƒ€ì¼ Mobile VLA íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ\n",
        "\n",
        "if 'dataset' in locals() and len(dataset) > 0:\n",
        "    print(\"ğŸ¯ MobileVLATrainer ì´ˆê¸°í™” ì¤‘...\")\n",
        "    \n",
        "    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
        "    trainer = MobileVLATrainer(\n",
        "        model_name=\"microsoft/kosmos-2-patch14-224\",\n",
        "        action_dim=3,  # Mobile VLA: [linear_x, linear_y, angular_z]\n",
        "        window_size=8,\n",
        "        chunk_size=2,\n",
        "        learning_rate=1e-4,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        precision=\"fp16\"  # Mixed precision\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "    print(f\"   ë””ë°”ì´ìŠ¤: {trainer.device}\")\n",
        "    print(f\"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in trainer.model.parameters()):,}ê°œ\")\n",
        "    \n",
        "    # ì†ì‹¤ ì¶”ì ê¸°\n",
        "    loss_tracker = ActionLossTracker()\n",
        "    \n",
        "    print(\"\\\\nğŸš€ RoboVLMs ìŠ¤íƒ€ì¼ í•™ìŠµ ì‹œì‘!\")\n",
        "    \n",
        "    def train_mobile_vla_robovlms_style(\n",
        "        trainer, \n",
        "        dataloader, \n",
        "        num_epochs=3, \n",
        "        log_interval=5,\n",
        "        save_interval=50,\n",
        "        checkpoint_dir=\"checkpoints\"\n",
        "    ):\n",
        "        \\\"\\\"\\\"RoboVLMs ìŠ¤íƒ€ì¼ í•™ìŠµ í•¨ìˆ˜\\\"\\\"\\\"\n",
        "        \n",
        "        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "        checkpoint_path = ROOT_DIR / checkpoint_dir\n",
        "        checkpoint_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        print(f\"ğŸƒâ€â™‚ï¸ í•™ìŠµ ì‹œì‘: {num_epochs} ì—í¬í¬, {len(dataloader)} ë°°ì¹˜\")\n",
        "        print(f\"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ: {checkpoint_path}\")\n",
        "        \n",
        "        all_losses = []\n",
        "        best_loss = float('inf')\n",
        "        \n",
        "        try:\n",
        "            for epoch in range(num_epochs):\n",
        "                print(f\"\\\\nğŸ“… Epoch {epoch+1}/{num_epochs}\")\n",
        "                epoch_losses = []\n",
        "                \n",
        "                for step, batch in enumerate(dataloader):\n",
        "                    try:\n",
        "                        # í•™ìŠµ ìŠ¤í…\n",
        "                        loss_dict = trainer.train_step(batch)\n",
        "                        loss_tracker.update(loss_dict)\n",
        "                        epoch_losses.append(loss_dict[\"total_loss\"])\n",
        "                        \n",
        "                        # ë¡œê¹…\n",
        "                        if (step + 1) % log_interval == 0:\n",
        "                            avg_metrics = loss_tracker.get_averages(last_n=log_interval)\n",
        "                            print(f\"  Step {step+1:3d}: \"\n",
        "                                  f\"Loss={avg_metrics['avg_total_loss']:.4f}, \"\n",
        "                                  f\"MAE={avg_metrics['avg_mae']:.4f}, \"\n",
        "                                  f\"LR={avg_metrics['current_lr']:.2e}\")\n",
        "                        \n",
        "                        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "                        if (step + 1) % save_interval == 0:\n",
        "                            current_loss = loss_tracker.get_averages(last_n=10)['avg_total_loss']\n",
        "                            if current_loss < best_loss:\n",
        "                                best_loss = current_loss\n",
        "                                checkpoint_file = checkpoint_path / f\"mobile_vla_best_epoch{epoch+1}_step{step+1}.pt\"\n",
        "                                trainer.save_checkpoint(\n",
        "                                    str(checkpoint_file), \n",
        "                                    epoch=epoch+1,\n",
        "                                    metrics={\"best_loss\": best_loss, \"step\": step+1}\n",
        "                                )\n",
        "                                print(f\"ğŸ’¾ Best ëª¨ë¸ ì €ì¥: {checkpoint_file.name} (Loss: {best_loss:.4f})\")\n",
        "                        \n",
        "                        all_losses.append(loss_dict)\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Step {step+1} ì‹¤íŒ¨: {e}\")\n",
        "                        clear_gpu_memory()  # OOM ë³µêµ¬ ì‹œë„\n",
        "                        continue\n",
        "                \n",
        "                # ì—í¬í¬ ì¢…ë£Œ ìš”ì•½\n",
        "                if epoch_losses:\n",
        "                    epoch_avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "                    print(f\"ğŸ“Š Epoch {epoch+1} ì™„ë£Œ: í‰ê·  Loss = {epoch_avg_loss:.4f}\")\n",
        "                    \n",
        "                    # ì—í¬í¬ë³„ ì²´í¬í¬ì¸íŠ¸\n",
        "                    epoch_checkpoint = checkpoint_path / f\"mobile_vla_epoch{epoch+1}.pt\"\n",
        "                    trainer.save_checkpoint(\n",
        "                        str(epoch_checkpoint),\n",
        "                        epoch=epoch+1,\n",
        "                        metrics={\"epoch_avg_loss\": epoch_avg_loss}\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Epoch {epoch+1}: ìœ íš¨í•œ ë°°ì¹˜ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "                \n",
        "                clear_gpu_memory()\n",
        "        \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\\\nâ¹ï¸ í•™ìŠµì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\\\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "        \n",
        "        return all_losses, loss_tracker\n",
        "    \n",
        "    # ì‹¤ì œ í•™ìŠµ ì‹¤í–‰\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“ Mobile VLA RoboVLMs ìŠ¤íƒ€ì¼ í•™ìŠµ ì‹¤í–‰\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    losses, tracker = train_mobile_vla_robovlms_style(\n",
        "        trainer=trainer,\n",
        "        dataloader=dataloader,\n",
        "        num_epochs=2,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸\n",
        "        log_interval=3,\n",
        "        save_interval=20,\n",
        "        checkpoint_dir=\"mobile_vla_checkpoints\"\n",
        "    )\n",
        "    \n",
        "    print(\"\\\\nğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    \n",
        "    # ìµœì¢… í†µê³„\n",
        "    if tracker.losses:\n",
        "        final_metrics = tracker.get_averages()\n",
        "        print(f\"\\\\nğŸ“ˆ ìµœì¢… í•™ìŠµ í†µê³„:\")\n",
        "        print(f\"   í‰ê·  Loss: {final_metrics['avg_total_loss']:.4f}\")\n",
        "        print(f\"   í‰ê·  MAE: {final_metrics['avg_mae']:.4f}\")\n",
        "        print(f\"   ì´ ìŠ¤í…: {final_metrics['steps']}\")\n",
        "        print(f\"   ìµœì¢… í•™ìŠµë¥ : {final_metrics['current_lr']:.2e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ìœ íš¨í•œ ë°ì´í„°ì…‹ì´ ì—†ì–´ í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"   ì´ì „ ì…€ì—ì„œ ë°ì´í„°ì…‹ì„ ë¨¼ì € ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ì™„ì „ ë¦¬ì…‹ ë° í™˜ê²½ ì¬êµ¬ì„±...\n",
            "ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...\n",
            "   í• ë‹¹ëœ ë©”ëª¨ë¦¬: 6.24 GB\n",
            "   ìºì‹œëœ ë©”ëª¨ë¦¬: 6.26 GB\n",
            "âœ… ëª¨ë“ˆ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\n",
            "âœ… PyArrow: 14.0.2\n",
            "âœ… Datasets: 2.12.0\n",
            "âœ… Datasets/PyArrow í˜¸í™˜ì„± í™•ì¸\n",
            "\\nğŸ”„ Mobile VLA ëª¨ë“ˆ ì¬ë¡œë”©...\n",
            "âœ… ëª¨ë“  ëª¨ë“ˆ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œ!\n",
            "\\nğŸ“Š ì‹¤ì œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
            "   ë°ì´í„° ê²½ë¡œ: /home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset\n",
            "âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ!\n",
            "   ì´ ì—í”¼ì†Œë“œ: 72ê°œ\n",
            "\\nğŸ“‹ ì²« ë²ˆì§¸ ìƒ˜í”Œ:\n",
            "   ì‹œë‚˜ë¦¬ì˜¤: 2box_left_vertical\n",
            "   íƒœìŠ¤í¬: Navigate around the two box obstacles by going left to track the target cup\n",
            "   ì´ë¯¸ì§€ ìˆ˜: 18\n",
            "   ì•¡ì…˜ shape: (18, 3)\n",
            "âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 182, in collate\n",
            "    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n",
            "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
            "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in collate\n",
            "    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in <dictcomp>\n",
            "    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 189, in collate\n",
            "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 189, in <listcomp>\n",
            "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n",
            "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
            "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 182, in collate\n",
            "    clone[i] = collate(samples, collate_fn_map=collate_fn_map)\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n",
            "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
            "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_285130/993586381.py\", line 80, in <module>\n",
            "    first_batch = next(iter(dataloader))\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
            "    return self.collate_fn(data)\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n",
            "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate\n",
            "    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in <dictcomp>\n",
            "    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 189, in collate\n",
            "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 189, in <listcomp>\n",
            "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]\n",
            "  File \"/home/billy/.cache/pypoetry/virtualenvs/mobile-vla-FNblWQUj-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n",
            "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
            "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
          ]
        }
      ],
      "source": [
        "# Cell 20: ğŸ§ª PyArrow í˜¸í™˜ Mobile VLA ì™„ì „ ë¦¬ì…‹ ë° í…ŒìŠ¤íŠ¸\n",
        "\n",
        "print(\"ğŸ”„ ì™„ì „ ë¦¬ì…‹ ë° í™˜ê²½ ì¬êµ¬ì„±...\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "clear_gpu_memory()\n",
        "\n",
        "# ëª¨ë“  ê´€ë ¨ ëª¨ë“ˆ ì–¸ë¡œë“œ\n",
        "import sys\n",
        "modules_to_reload = [m for m in sys.modules.keys() if 'robovlms' in m or 'mobile_vla' in m]\n",
        "for module in modules_to_reload:\n",
        "    if module in sys.modules:\n",
        "        del sys.modules[module]\n",
        "\n",
        "print(\"âœ… ëª¨ë“ˆ ìºì‹œ ì •ë¦¬ ì™„ë£Œ\")\n",
        "\n",
        "# ì˜ì¡´ì„± ë²„ì „ ìµœì¢… í™•ì¸\n",
        "try:\n",
        "    import pyarrow\n",
        "    import datasets\n",
        "    print(f\"âœ… PyArrow: {pyarrow.__version__}\")\n",
        "    print(f\"âœ… Datasets: {datasets.__version__}\")\n",
        "    \n",
        "    # í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸\n",
        "    test_dataset = datasets.Dataset.from_dict({\"test\": [1, 2, 3]})\n",
        "    print(\"âœ… Datasets/PyArrow í˜¸í™˜ì„± í™•ì¸\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ì˜ì¡´ì„± ë¬¸ì œ: {e}\")\n",
        "    raise\n",
        "\n",
        "# ê²½ë¡œ ì¬ì„¤ì •\n",
        "sys.path.insert(0, str(ROOT_DIR))\n",
        "\n",
        "# ë¦¬íŒ©í† ë§ëœ ëª¨ë“ˆ ì¬ì„í¬íŠ¸\n",
        "print(\"\\\\nğŸ”„ Mobile VLA ëª¨ë“ˆ ì¬ë¡œë”©...\")\n",
        "try:\n",
        "    from robovlms.data import MobileVLADataset\n",
        "    from robovlms.train import MobileVLATrainer, ActionLossTracker\n",
        "    print(\"âœ… ëª¨ë“  ëª¨ë“ˆ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œ!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "# ì‹¤ì œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸\n",
        "print(f\"\\\\nğŸ“Š ì‹¤ì œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
        "print(f\"   ë°ì´í„° ê²½ë¡œ: {DATA_DIR}\")\n",
        "\n",
        "try:\n",
        "    # ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "    dataset = MobileVLADataset(\n",
        "        data_dir=str(DATA_DIR),\n",
        "        model_name=\"kosmos\",\n",
        "        mode=\"train\",\n",
        "        window_size=8,\n",
        "        fwd_pred_next_n=2,\n",
        "        discrete=False,\n",
        "        tokenizer=None  # ê°„ë‹¨íˆ Noneìœ¼ë¡œ ì„¤ì •\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ!\")\n",
        "    print(f\"   ì´ ì—í”¼ì†Œë“œ: {len(dataset)}ê°œ\")\n",
        "    \n",
        "    if len(dataset) > 0:\n",
        "        # ì²« ë²ˆì§¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸\n",
        "        sample = dataset[0]\n",
        "        print(f\"\\\\nğŸ“‹ ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
        "        print(f\"   ì‹œë‚˜ë¦¬ì˜¤: {sample['scenario']}\")\n",
        "        print(f\"   íƒœìŠ¤í¬: {sample['task_description']}\")\n",
        "        print(f\"   ì´ë¯¸ì§€ ìˆ˜: {len(sample['images'])}\")\n",
        "        print(f\"   ì•¡ì…˜ shape: {sample['actions'].shape}\")\n",
        "        \n",
        "        # ì»¤ìŠ¤í…€ collate functionìœ¼ë¡œ DataLoader ìƒì„± (PIL ì´ë¯¸ì§€ ì²˜ë¦¬)\n",
        "        def simple_collate_fn(batch):\n",
        "            \"\"\"ê°„ë‹¨í•œ PIL ì´ë¯¸ì§€ ì²˜ë¦¬ collate function\"\"\"\n",
        "            import torch\n",
        "            from torchvision import transforms\n",
        "            \n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            \n",
        "            collated = {}\n",
        "            for key in batch[0].keys():\n",
        "                if key == 'images':\n",
        "                    # PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
        "                    all_tensors = []\n",
        "                    for item in batch:\n",
        "                        seq_tensors = []\n",
        "                        for pil_img in item[key]:\n",
        "                            tensor_img = transform(pil_img)\n",
        "                            seq_tensors.append(tensor_img)\n",
        "                        all_tensors.append(torch.stack(seq_tensors))\n",
        "                    collated[key] = torch.stack(all_tensors)\n",
        "                elif key == 'actions':\n",
        "                    collated[key] = torch.stack([torch.tensor(item[key], dtype=torch.float32) for item in batch])\n",
        "                elif key == 'episode_mask':\n",
        "                    collated[key] = torch.stack([torch.tensor(item[key], dtype=torch.bool) for item in batch])\n",
        "                else:\n",
        "                    collated[key] = [item[key] for item in batch]\n",
        "            return collated\n",
        "        \n",
        "        from torch.utils.data import DataLoader\n",
        "        dataloader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=1, \n",
        "            shuffle=False, \n",
        "            num_workers=0,\n",
        "            collate_fn=simple_collate_fn  # ì»¤ìŠ¤í…€ collate function ì‚¬ìš©\n",
        "        )\n",
        "        \n",
        "        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
        "        print(\"   ë°°ì¹˜ ìƒì„± ì¤‘ (PIL â†’ Tensor ë³€í™˜)...\")\n",
        "        first_batch = next(iter(dataloader))\n",
        "        print(f\"\\\\nğŸ¯ ë°°ì¹˜ í…ŒìŠ¤íŠ¸:\")\n",
        "        for key, value in first_batch.items():\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"   {key}: {value.shape}\")\n",
        "            elif isinstance(value, (list, tuple)):\n",
        "                print(f\"   {key}: {type(value).__name__}[{len(value)}]\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value).__name__}\")\n",
        "        \n",
        "        print(\"\\\\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âŒ ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    dataset = None\n",
        "    dataloader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 81)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m<tokenize>:81\u001b[0;36m\u001b[0m\n\u001b[0;31m    if epoch_losses:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Cell 21: ğŸš€ ìµœì¢… Mobile VLA í•™ìŠµ ì‹¤í–‰ (PyArrow í˜¸í™˜)\n",
        "\n",
        "if 'dataset' in locals() and dataset is not None and len(dataset) > 0:\n",
        "    print(\"ğŸ¯ MobileVLATrainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘...\")\n",
        "    \n",
        "    try:\n",
        "        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
        "        trainer = MobileVLATrainer(\n",
        "            model_name=\"microsoft/kosmos-2-patch14-224\",\n",
        "            action_dim=3,  # Mobile VLA: [linear_x, linear_y, angular_z]\n",
        "            window_size=8,\n",
        "            chunk_size=2,\n",
        "            learning_rate=1e-4,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            precision=\"fp16\"  # Mixed precision\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ… íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "        print(f\"   ë””ë°”ì´ìŠ¤: {trainer.device}\")\n",
        "        print(f\"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in trainer.model.parameters()):,}ê°œ\")\n",
        "        \n",
        "        # ì†ì‹¤ ì¶”ì ê¸°\n",
        "        loss_tracker = ActionLossTracker()\n",
        "        \n",
        "        # í•™ìŠµ ì‹¤í–‰\n",
        "        print(\"\\\\nğŸš€ Mobile VLA í•™ìŠµ ì‹œì‘!\")\n",
        "        \n",
        "        num_epochs = 3  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì§§ê²Œ\n",
        "        save_interval = 1\n",
        "        log_interval = 5\n",
        "        \n",
        "                 # í•™ìŠµìš© DataLoader ì¬ìƒì„± (collate_fn í¬í•¨)\n",
        "         print(\"   í•™ìŠµìš© DataLoader ìƒì„± ì¤‘...\")\n",
        "         from torch.utils.data import DataLoader\n",
        "         train_dataloader = DataLoader(\n",
        "             dataset,\n",
        "             batch_size=1,\n",
        "             shuffle=True,\n",
        "             num_workers=0,\n",
        "             collate_fn=dataset.collater,  # ì¤‘ìš”: collate_fn ì‚¬ìš©\n",
        "             pin_memory=False,\n",
        "             drop_last=True\n",
        "         )\n",
        "         print(f\"   DataLoader ìƒì„± ì™„ë£Œ (ì´ {len(train_dataloader)}ê°œ ë°°ì¹˜)\")\n",
        "         \n",
        "         for epoch in range(num_epochs):\n",
        "             print(f\"\\\\nğŸ“Š Epoch {epoch+1}/{num_epochs}\")\n",
        "             epoch_losses = []\n",
        "             \n",
        "             for step, batch in enumerate(train_dataloader):\n",
        "                try:\n",
        "                    # í•™ìŠµ ìŠ¤í… ì‹¤í–‰\n",
        "                    loss_dict = trainer.train_step(batch)\n",
        "                    \n",
        "                    # ì†ì‹¤ ì¶”ì \n",
        "                    loss_tracker.update(loss_dict)\n",
        "                    epoch_losses.append(loss_dict['total_loss'])\n",
        "                    \n",
        "                    # ì£¼ê¸°ì  ë¡œê¹…\n",
        "                    if (step + 1) % log_interval == 0:\n",
        "                        current_metrics = loss_tracker.get_averages()\n",
        "                        print(f\"   Step {step+1:3d}: \"\n",
        "                              f\"Loss={loss_dict['total_loss']:.4f}, \"\n",
        "                              f\"MAE={loss_dict.get('mae_avg', 0):.4f}, \"\n",
        "                              f\"LR={current_metrics['current_lr']:.2e}\")\n",
        "                    \n",
        "                    # ë©”ëª¨ë¦¬ ê´€ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ ì •ë¦¬)\n",
        "                    if (step + 1) % 10 == 0:\n",
        "                        clear_gpu_memory()\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ ìŠ¤í… {step+1} ì‹¤íŒ¨: {e}\")\n",
        "                    if \"out of memory\" in str(e).lower():\n",
        "                        print(\"   GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ window_sizeë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”.\")\n",
        "                        clear_gpu_memory()\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "            \n",
        "            # ì—í¬í¬ ìš”ì•½\n",
        "            if epoch_losses:\n",
        "                avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "                print(f\"   ğŸ“ˆ Epoch {epoch+1} í‰ê·  Loss: {avg_epoch_loss:.4f}\")\n",
        "                \n",
        "                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
        "                if (epoch + 1) % save_interval == 0:\n",
        "                    checkpoint_path = f\"mobile_vla_epoch_{epoch+1}.pt\"\n",
        "                    trainer.save_checkpoint(checkpoint_path)\n",
        "                    print(f\"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}\")\n",
        "        \n",
        "        print(\"\\\\nğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
        "        \n",
        "        # ìµœì¢… í†µê³„\n",
        "        if loss_tracker.losses:\n",
        "            final_metrics = loss_tracker.get_averages()\n",
        "            print(f\"\\\\nğŸ“ˆ ìµœì¢… í•™ìŠµ í†µê³„:\")\n",
        "            print(f\"   í‰ê·  Total Loss: {final_metrics['avg_total_loss']:.4f}\")\n",
        "            print(f\"   í‰ê·  MAE: {final_metrics['avg_mae']:.4f}\")\n",
        "            print(f\"   ì´ ìŠ¤í…: {final_metrics['steps']}\")\n",
        "            print(f\"   ìµœì¢… í•™ìŠµë¥ : {final_metrics['current_lr']:.2e}\")\n",
        "            \n",
        "            # ê°„ë‹¨í•œ ì„±ëŠ¥ ë¶„ì„\n",
        "            print(f\"\\\\nğŸ” ì„±ëŠ¥ ë¶„ì„:\")\n",
        "            print(f\"   Loss ì¶”ì´: {loss_tracker.losses[-10:] if len(loss_tracker.losses) >= 10 else loss_tracker.losses}\")\n",
        "            \n",
        "        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "        clear_gpu_memory()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        clear_gpu_memory()\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ìœ íš¨í•œ ë°ì´í„°ì…‹ì´ ì—†ì–´ í•™ìŠµì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"   ì´ì „ ì…€(Cell 20)ì—ì„œ ë°ì´í„°ì…‹ì„ ë¨¼ì € ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.\")\n",
        "    print(f\"   í˜„ì¬ dataset ìƒíƒœ: {'ì •ì˜ë¨' if 'dataset' in locals() else 'ì •ì˜ë˜ì§€ ì•ŠìŒ'}\")\n",
        "    if 'dataset' in locals():\n",
        "        print(f\"   ë°ì´í„°ì…‹ ê¸¸ì´: {len(dataset) if dataset is not None else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Collate Function ë¬¸ì œ í•´ê²°...\n",
            "âŒ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "# Cell 22: ğŸ¯ Collate Function ë¬¸ì œ í•´ê²° ë° ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "\n",
        "print(\"ğŸ”§ Collate Function ë¬¸ì œ í•´ê²°...\")\n",
        "\n",
        "# ì»¤ìŠ¤í…€ collate í•¨ìˆ˜ ì •ì˜ (PIL ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë²½ ì§€ì›)\n",
        "def mobile_vla_collate_fixed(batch):\n",
        "    \"\"\"PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ collate í•¨ìˆ˜ (ì™„ì „ ìˆ˜ì •íŒ)\"\"\"\n",
        "    import torch\n",
        "    from torchvision import transforms\n",
        "    \n",
        "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (Kosmos í˜¸í™˜)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    collated = {}\n",
        "    \n",
        "    for key in batch[0].keys():\n",
        "        if key == 'images':\n",
        "            # PIL ì´ë¯¸ì§€ë“¤ì„ í…ì„œë¡œ ë³€í™˜\n",
        "            image_tensors = []\n",
        "            for item in batch:\n",
        "                seq_tensors = []\n",
        "                for pil_img in item[key]:\n",
        "                    tensor_img = transform(pil_img)\n",
        "                    seq_tensors.append(tensor_img)\n",
        "                # [T, C, H, W] ìŠ¤íƒ\n",
        "                if seq_tensors:\n",
        "                    stacked = torch.stack(seq_tensors)\n",
        "                    image_tensors.append(stacked)\n",
        "                else:\n",
        "                    # ë¹ˆ ì‹œí€€ìŠ¤ ì²˜ë¦¬\n",
        "                    image_tensors.append(torch.empty(0, 3, 224, 224))\n",
        "            # [B, T, C, H, W] ë°°ì¹˜\n",
        "            if image_tensors and image_tensors[0].numel() > 0:\n",
        "                collated[key] = torch.stack(image_tensors)\n",
        "            else:\n",
        "                collated[key] = torch.empty(len(batch), 0, 3, 224, 224)\n",
        "                \n",
        "        elif key == 'actions':\n",
        "            # ì•¡ì…˜ í…ì„œí™”\n",
        "            action_tensors = []\n",
        "            for item in batch:\n",
        "                if isinstance(item[key], torch.Tensor):\n",
        "                    action_tensors.append(item[key])\n",
        "                else:\n",
        "                    action_tensors.append(torch.tensor(item[key], dtype=torch.float32))\n",
        "            collated[key] = torch.stack(action_tensors)\n",
        "            \n",
        "        elif key == 'episode_mask':\n",
        "            # ë§ˆìŠ¤í¬ í…ì„œí™”  \n",
        "            mask_tensors = []\n",
        "            for item in batch:\n",
        "                if isinstance(item[key], torch.Tensor):\n",
        "                    mask_tensors.append(item[key])\n",
        "                else:\n",
        "                    mask_tensors.append(torch.tensor(item[key], dtype=torch.bool))\n",
        "            collated[key] = torch.stack(mask_tensors)\n",
        "            \n",
        "        else:\n",
        "            # ë¬¸ìì—´ ë“±ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€\n",
        "            collated[key] = [item[key] for item in batch]\n",
        "    \n",
        "    return collated\n",
        "\n",
        "# ë°ì´í„°ì…‹ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "if 'dataset' in locals() and dataset is not None and len(dataset) > 0:\n",
        "    print(f\"âœ… ê¸°ì¡´ ë°ì´í„°ì…‹ ì‚¬ìš©: {len(dataset)}ê°œ ì—í”¼ì†Œë“œ\")\n",
        "    \n",
        "    # ìˆ˜ì •ëœ collate í•¨ìˆ˜ë¡œ DataLoader ì¬ìƒì„±\n",
        "    from torch.utils.data import DataLoader\n",
        "    \n",
        "    dataloader = DataLoader(\n",
        "        dataset, \n",
        "        batch_size=1, \n",
        "        shuffle=False, \n",
        "        num_workers=0,\n",
        "        collate_fn=mobile_vla_collate_fixed  # ìˆ˜ì •ëœ ì»¤ìŠ¤í…€ collate í•¨ìˆ˜\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ”„ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...\")\n",
        "    \n",
        "    try:\n",
        "        # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
        "        first_batch = next(iter(dataloader))\n",
        "        \n",
        "        print(\"âœ… ë°°ì¹˜ ë¡œë“œ ì„±ê³µ!\")\n",
        "        print(\"\\\\nğŸ¯ ìµœì¢… ë°°ì¹˜ êµ¬ì¡°:\")\n",
        "        for key, value in first_batch.items():\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "            elif isinstance(value, (list, tuple)):\n",
        "                print(f\"   {key}: {type(value).__name__}[{len(value)}]\")\n",
        "                if len(value) > 0:\n",
        "                    print(f\"     â””â”€ ì²« ë²ˆì§¸ í•­ëª©: {value[0]}\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value).__name__}\")\n",
        "        \n",
        "        print(\"\\\\nğŸ‰ ëª¨ë“  Collate í…ŒìŠ¤íŠ¸ í†µê³¼! í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "        \n",
        "        # ê°„ë‹¨í•œ Window/Chunk í™•ì¸\n",
        "        images_shape = first_batch['images'].shape  # [B, T, C, H, W]\n",
        "        actions_shape = first_batch['actions'].shape  # [B, T, action_dim]\n",
        "        \n",
        "        window_size = 8\n",
        "        chunk_size = 2\n",
        "        \n",
        "        print(f\"\\\\nğŸ“Š Window/Chunk ë¶„ì„:\")\n",
        "        print(f\"   ì´ë¯¸ì§€ ì‹œí€€ìŠ¤: {images_shape} (ë°°ì¹˜={images_shape[0]}, ì‹œê°„={images_shape[1]})\")\n",
        "        print(f\"   ì•¡ì…˜ ì‹œí€€ìŠ¤: {actions_shape} (ë°°ì¹˜={actions_shape[0]}, ì‹œê°„={actions_shape[1]})\")\n",
        "        print(f\"   Window Size: {window_size} (ê³¼ê±° ê´€ì°°)\")\n",
        "        print(f\"   Chunk Size: {chunk_size} (ë¯¸ë˜ ì˜ˆì¸¡)\")\n",
        "        \n",
        "        if images_shape[1] >= window_size + chunk_size:\n",
        "            print(f\"   âœ… ì¶©ë¶„í•œ ì‹œí€€ìŠ¤ ê¸¸ì´: {images_shape[1]} >= {window_size + chunk_size}\")\n",
        "        else:\n",
        "            print(f\"   âš ï¸ ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶€ì¡±: {images_shape[1]} < {window_size + chunk_size}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        dataloader = None\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
        "    dataloader = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ PIL Image ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì»¤ìŠ¤í…€ DataLoader êµ¬ì„±...\n",
            "âŒ ìœ íš¨í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "# Cell 22: ğŸ”§ PIL Image ì²˜ë¦¬ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ DataLoader\n",
        "\n",
        "print(\"ğŸ”§ PIL Image ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì»¤ìŠ¤í…€ DataLoader êµ¬ì„±...\")\n",
        "\n",
        "if 'dataset' in locals() and dataset is not None and len(dataset) > 0:\n",
        "    \n",
        "    def mobile_vla_collate_fn(batch):\n",
        "        \"\"\"Mobile VLAìš© ì»¤ìŠ¤í…€ collate function - PIL Image ì•ˆì „ ì²˜ë¦¬\"\"\"\n",
        "        import torch\n",
        "        from PIL import Image\n",
        "        import numpy as np\n",
        "        \n",
        "        print(f\"ğŸ” ë°°ì¹˜ í¬ê¸°: {len(batch)}\")\n",
        "        \n",
        "        # ì²« ë²ˆì§¸ ìƒ˜í”Œ êµ¬ì¡° í™•ì¸\n",
        "        if batch:\n",
        "            sample = batch[0]\n",
        "            print(f\"ğŸ“‹ ìƒ˜í”Œ í‚¤: {list(sample.keys())}\")\n",
        "            for key, value in sample.items():\n",
        "                if key == 'images':\n",
        "                    print(f\"   {key}: {len(value)}ê°œ ì´ë¯¸ì§€ (íƒ€ì…: {type(value[0]) if value else 'empty'})\")\n",
        "                elif hasattr(value, 'shape'):\n",
        "                    print(f\"   {key}: {value.shape}\")\n",
        "                else:\n",
        "                    print(f\"   {key}: {type(value).__name__}\")\n",
        "        \n",
        "        collated = {}\n",
        "        \n",
        "        for key in batch[0].keys():\n",
        "            values = [item[key] for item in batch]\n",
        "            \n",
        "            if key == 'images':\n",
        "                # PIL ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬\n",
        "                all_image_tensors = []\n",
        "                for img_list in values:  # ê° ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸\n",
        "                    batch_images = []\n",
        "                    for img in img_list:\n",
        "                        if isinstance(img, Image.Image):\n",
        "                            # PIL -> numpy -> tensor (ì •ê·œí™” í¬í•¨)\n",
        "                            img_array = np.array(img)\n",
        "                            if img_array.ndim == 3:  # RGB\n",
        "                                img_array = img_array.transpose(2, 0, 1)  # HWC -> CHW\n",
        "                            img_tensor = torch.from_numpy(img_array).float() / 255.0\n",
        "                            batch_images.append(img_tensor)\n",
        "                        elif isinstance(img, torch.Tensor):\n",
        "                            batch_images.append(img)\n",
        "                        else:\n",
        "                            print(f\"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ íƒ€ì…: {type(img)}\")\n",
        "                            continue\n",
        "                    \n",
        "                    if batch_images:\n",
        "                        all_image_tensors.append(torch.stack(batch_images))\n",
        "                \n",
        "                # ìµœì¢… ë°°ì¹˜ í…ì„œ ìƒì„±\n",
        "                if all_image_tensors:\n",
        "                    collated[key] = torch.stack(all_image_tensors)  # [B, T, C, H, W]\n",
        "                    print(f\"   âœ… {key} ì²˜ë¦¬ ì™„ë£Œ: {collated[key].shape}\")\n",
        "                else:\n",
        "                    collated[key] = torch.empty(0)\n",
        "                    print(f\"   âš ï¸ {key} ë¹„ì–´ìˆìŒ\")\n",
        "                    \n",
        "            elif key == 'actions':\n",
        "                # ì•¡ì…˜ í…ì„œ ì²˜ë¦¬\n",
        "                if isinstance(values[0], torch.Tensor):\n",
        "                    collated[key] = torch.stack(values)\n",
        "                else:\n",
        "                    collated[key] = torch.stack([torch.tensor(v, dtype=torch.float32) for v in values])\n",
        "                print(f\"   âœ… {key} ì²˜ë¦¬ ì™„ë£Œ: {collated[key].shape}\")\n",
        "                \n",
        "            elif key == 'episode_mask':\n",
        "                # ë§ˆìŠ¤í¬ ì²˜ë¦¬\n",
        "                if isinstance(values[0], torch.Tensor):\n",
        "                    collated[key] = torch.stack(values)\n",
        "                else:\n",
        "                    collated[key] = torch.stack([torch.tensor(v, dtype=torch.bool) for v in values])\n",
        "                print(f\"   âœ… {key} ì²˜ë¦¬ ì™„ë£Œ: {collated[key].shape}\")\n",
        "                \n",
        "            elif isinstance(values[0], torch.Tensor):\n",
        "                # ê¸°íƒ€ í…ì„œë“¤\n",
        "                collated[key] = torch.stack(values)\n",
        "                print(f\"   âœ… {key} ì²˜ë¦¬ ì™„ë£Œ: {collated[key].shape}\")\n",
        "            else:\n",
        "                # ë¬¸ìì—´ì´ë‚˜ ê¸°íƒ€ ê°ì²´ë“¤\n",
        "                collated[key] = values\n",
        "                print(f\"   âœ… {key} ì²˜ë¦¬ ì™„ë£Œ: {type(values).__name__}[{len(values)}]\")\n",
        "        \n",
        "        return collated\n",
        "    \n",
        "    # ì»¤ìŠ¤í…€ collate functionì„ ì‚¬ìš©í•˜ëŠ” DataLoader ìƒì„±\n",
        "    try:\n",
        "        safe_dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=1,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
        "            shuffle=False,\n",
        "            num_workers=0,  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”\n",
        "            pin_memory=False,\n",
        "            drop_last=True,\n",
        "            collate_fn=mobile_vla_collate_fn  # ì»¤ìŠ¤í…€ collate function\n",
        "        )\n",
        "        \n",
        "        print(f\"\\\\nâœ… ì•ˆì „í•œ DataLoader ìƒì„± ì™„ë£Œ!\")\n",
        "        print(f\"   ë°°ì¹˜ í¬ê¸°: 1\")\n",
        "        print(f\"   ì´ ë°°ì¹˜ ìˆ˜: {len(safe_dataloader)}ê°œ\")\n",
        "        \n",
        "        # ì‹¤ì œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
        "        print(\"\\\\nğŸ§ª ì‹¤ì œ ë°°ì¹˜ ë¡œë”© í…ŒìŠ¤íŠ¸...\")\n",
        "        first_batch = next(iter(safe_dataloader))\n",
        "        \n",
        "        print(f\"\\\\nğŸ¯ ìµœì¢… ë°°ì¹˜ êµ¬ì¡°:\")\n",
        "        for key, value in first_batch.items():\n",
        "            if hasattr(value, 'shape'):\n",
        "                print(f\"   {key}: {value.shape} ({value.dtype})\")\n",
        "            elif isinstance(value, (list, tuple)):\n",
        "                print(f\"   {key}: {type(value).__name__}[{len(value)}]\")\n",
        "            else:\n",
        "                print(f\"   {key}: {type(value).__name__}\")\n",
        "        \n",
        "        print(\"\\\\nğŸ‰ DataLoader í…ŒìŠ¤íŠ¸ ì„±ê³µ! í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "        \n",
        "        # ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹\n",
        "        dataloader = safe_dataloader\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì•ˆì „í•œ DataLoader ìƒì„± ì‹¤íŒ¨: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        dataloader = None\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ìœ íš¨í•œ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mobile-vla-FNblWQUj-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
