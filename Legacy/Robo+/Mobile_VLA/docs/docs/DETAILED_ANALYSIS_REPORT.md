# ğŸ”¬ Mobile VLA ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”
ì´ ë³´ê³ ì„œëŠ” Mobile VLA í”„ë¡œì íŠ¸ì˜ 4ê°€ì§€ Caseë“¤ì„ RoboVLMs ì½”ë“œë² ì´ìŠ¤ì˜ ë¼ì¸ë³„ ì¸ìš©ê³¼ í•¨ê»˜ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤.

---

## ğŸ—ï¸ RoboVLMs ê¸°ë³¸ ì•„í‚¤í…ì²˜ ë¶„ì„

### ğŸ“Š BaseRoboVLM êµ¬ì¡° ë¶„ì„

RoboVLMsì˜ í•µì‹¬ í´ë˜ìŠ¤ì¸ `BaseRoboVLM`ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:34-60
class BaseRoboVLM(nn.Module):
    def __init__(
        self,
        configs,
        train_setup_configs,
        act_encoder_configs=None,
        act_head_configs=None,
        fwd_head_configs=None,
        window_size=None,
        use_obs_queries=True,
        use_act_queries=True,
        use_hand_rgb=False,
        use_pixel_loss=True,
        use_mim_obs_loss=False,
        use_time_causal_attn=True,
        vision_masked_ratio=0.9,
        use_tube_mask=False,
        fwd_pred_next_n=1,
        use_vision_resampler=False,  # í•µì‹¬: Vision Resampler ì‚¬ìš© ì—¬ë¶€
        vision_resampler_configs=None,
        use_clip_norm=False,         # í•µì‹¬: CLIP ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        use_state=False,
        **kwargs,
    ):
```

### ğŸ”„ Forward ë©”ì„œë“œ í”Œë¡œìš°

```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:1491-1515
def forward(
    self,
    vision_x: torch.Tensor,
    lang_x: torch.Tensor,
    attention_mask: torch.Tensor = None,
    position_ids: torch.LongTensor = None,
    use_cached_vision_x: bool = False,
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,
    action_mask: torch.Tensor = None,
    caption_labels: torch.Tensor = None,
    caption_mask: torch.Tensor = None,
    past_key_values=None,
    use_cache: bool = False,
    vision_gripper=None,
    fwd_rgb_labels: torch.Tensor = None,
    fwd_hand_rgb_labels: torch.Tensor = None,
    fwd_mask: torch.Tensor = None,
    instr_and_action_ids=None,
    instr_and_action_labels=None,
    instr_and_action_mask=None,
    raw_text=None,
    data_source=[],
    **kwargs,
):
```

---

## ğŸ“Š Caseë³„ ìƒì„¸ ë¶„ì„

### ğŸ¯ Case 1: Simplified2DActionModelV2

#### ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Input as ì…ë ¥ ë°ì´í„°
    participant Processor as Kosmos2 Processor
    participant Vision as Vision Encoder
    participant Language as Language Encoder
    participant Fusion as ë©€í‹°ëª¨ë‹¬ ìœµí•©
    participant ActionHead as Action Head (4ì¸µ MLP)
    participant Output as ì¶œë ¥ ì•¡ì…˜

    Input->>Processor: PIL ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
    Processor->>Vision: pixel_values ì²˜ë¦¬
    Vision->>Vision: Kosmos2 Vision Model
    Vision->>Vision: pooler_output ì¶”ì¶œ
    Vision->>Vision: [batch_size, 1024]
    
    Processor->>Language: input_ids ì²˜ë¦¬
    Language->>Language: Kosmos2 Text Model
    Language->>Language: last_hidden_state.mean(dim=1)
    Language->>Language: [batch_size, 2048]
    
    Vision->>Fusion: vision_features
    Language->>Fusion: language_features
    Fusion->>Fusion: torch.cat([vision, language], dim=-1)
    Fusion->>Fusion: [batch_size, 3072]
    
    Fusion->>ActionHead: fused_features
    ActionHead->>ActionHead: 4ì¸µ MLP (256Ã—2 â†’ 256Ã—2 â†’ 256 â†’ 128 â†’ 2)
    ActionHead->>Output: predicted_actions [batch_size, 2]
```

#### ğŸ” ì½”ë“œ ë¶„ì„

**Vision íŠ¹ì§• ì¶”ì¶œ**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:200-220
def encode_images(self, images):
    if isinstance(images, list):
        concat_images = torch.cat([image for image in images], dim=0)
        image_features = self.model_encode_images(concat_images)
        split_sizes = [image.shape[0] for image in images]
        image_features = torch.split(image_features, split_sizes, dim=0)
        image_features = [x.flatten(0, 1) for x in image_features]
    else:
        image_features = self.model_encode_images(images)
    
    image_features = torch.stack(image_features, dim=0).view(
        bs, seq_len, -1, image_features[0].shape[-1]
    )
    return image_features
```

**Language íŠ¹ì§• ì¶”ì¶œ**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:280-300
def extract_language_features(self, texts):
    inputs = self.processor(text=texts, return_tensors="pt", padding=True)
    inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        if 'input_ids' in inputs:
            text_outputs = self.kosmos.text_model(inputs['input_ids'])
            language_features = text_outputs.last_hidden_state.mean(dim=1)
        else:
            language_features = torch.zeros(batch_size, self.language_dim).to(self.kosmos.device)
    
    return language_features
```

#### ğŸ“Š ì„±ëŠ¥ ë¶„ì„

**MAE: 0.869** - ì•ˆì •ì ì¸ ì„±ëŠ¥
- **ì¥ì **: ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
- **ë‹¨ì **: ì„±ëŠ¥ í•œê³„, í˜ì‹ ì„± ë¶€ì¡±
- **íŠ¹ì§•**: 4ì¸µ MLPë¡œ ì¶©ë¶„í•œ í‘œí˜„ë ¥ í™•ë³´

---

### ğŸ¯ Case 2: CLIPNormalized2DActionModelV2

#### ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Input as ì…ë ¥ ë°ì´í„°
    participant Processor as Kosmos2 Processor
    participant CLIP as CLIP Model
    participant Vision as Vision Encoder
    participant Resampler as Vision Resampler
    participant Language as Language Encoder
    participant Fusion as ë©€í‹°ëª¨ë‹¬ ìœµí•©
    participant ActionHead as Action Head (4ì¸µ MLP)
    participant Output as ì¶œë ¥ ì•¡ì…˜

    Input->>Processor: PIL ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
    Processor->>CLIP: CLIP ì „ì²˜ë¦¬
    CLIP->>CLIP: ViT-B-32 ëª¨ë¸
    CLIP->>CLIP: CLIP íŠ¹ì§• ì¶”ì¶œ
    CLIP->>Vision: CLIP ì •ê·œí™”ëœ íŠ¹ì§•
    Vision->>Vision: Kosmos2 Vision Model
    Vision->>Resampler: vision_features
    Resampler->>Resampler: OptimizedVisionResampler
    Resampler->>Resampler: í† í° ìˆ˜ ì¡°ì • ë° íŠ¹ì§• ê°œì„ 
    Resampler->>Fusion: resampled_features
    
    Processor->>Language: input_ids ì²˜ë¦¬
    Language->>Language: Kosmos2 Text Model
    Language->>Language: last_hidden_state.mean(dim=1)
    Language->>Fusion: language_features
    
    Fusion->>Fusion: torch.cat([resampled, language], dim=-1)
    Fusion->>Fusion: [batch_size, 3072]
    
    Fusion->>ActionHead: fused_features
    ActionHead->>ActionHead: 4ì¸µ MLP (256Ã—2 â†’ 256Ã—2 â†’ 256 â†’ 128 â†’ 2)
    ActionHead->>Output: predicted_actions [batch_size, 2]
```

#### ğŸ” ì½”ë“œ ë¶„ì„

**CLIP ì •ê·œí™”**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:50-55
self.use_clip_norm = use_clip_norm  # CLIP ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€

# CLIP ëª¨ë¸ ì´ˆê¸°í™”
if self.use_clip_norm:
    self.clip_model, self.clip_preprocess, _ = open_clip.create_model_and_transforms(
        'ViT-B-32', pretrained='openai'
    )
```

**Vision Resampler**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:210-220
if self.use_vision_resampler:
    ### downsample at token num dim: b, s, n, d -> b, s, v d
    # b T F v d -> b, T, n, d
    image_features = self.vision_resampler(
        image_features.unsqueeze(2)
    )  # downsample v_tok per image to n_tok
```

#### ğŸ“Š ì„±ëŠ¥ ë¶„ì„

**MAE: 0.466** - 46% ì„±ëŠ¥ í–¥ìƒ
- **CLIP Normalization íš¨ê³¼**: ë¹„ì „ íŠ¹ì§•ì˜ í’ˆì§ˆ í–¥ìƒ
- **Vision Resampler íš¨ê³¼**: í† í° ìˆ˜ ì¡°ì •ìœ¼ë¡œ ì •ë³´ ì••ì¶•
- **ì •í™•ë„**: ëª¨ë“  ì„ê³„ê°’ì—ì„œ ìµœê³  ì„±ëŠ¥
- **RÂ² ì ìˆ˜**: linear_xì—ì„œ 0.3456ìœ¼ë¡œ ê°€ì¥ ë†’ìŒ

---

### ğŸ¯ Case 3: SimpleCase3Model

#### ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Input as ì…ë ¥ ë°ì´í„°
    participant Processor as Kosmos2 Processor
    participant Vision as Vision Encoder
    participant Language as Language Encoder
    participant Fusion as ë©€í‹°ëª¨ë‹¬ ìœµí•©
    participant ActionHead as Action Head (4ì¸µ MLP)
    participant Output as ì¶œë ¥ ì•¡ì…˜

    Note over Input,Output: Case 1ê³¼ ë™ì¼í•œ êµ¬ì¡°

    Input->>Processor: PIL ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸
    Processor->>Vision: pixel_values ì²˜ë¦¬
    Vision->>Vision: Kosmos2 Vision Model
    Vision->>Vision: pooler_output ì¶”ì¶œ
    Vision->>Vision: [batch_size, 1024]
    
    Processor->>Language: input_ids ì²˜ë¦¬
    Language->>Language: Kosmos2 Text Model
    Language->>Language: last_hidden_state.mean(dim=1)
    Language->>Language: [batch_size, 2048]
    
    Vision->>Fusion: vision_features
    Language->>Fusion: language_features
    Fusion->>Fusion: torch.cat([vision, language], dim=-1)
    Fusion->>Fusion: [batch_size, 3072]
    
    Fusion->>ActionHead: fused_features
    ActionHead->>ActionHead: 4ì¸µ MLP (256Ã—2 â†’ 256Ã—2 â†’ 256 â†’ 128 â†’ 2)
    ActionHead->>Output: predicted_actions [batch_size, 2]
```

#### ğŸ” ì½”ë“œ ë¶„ì„

**Case 1ê³¼ ë™ì¼í•œ êµ¬ì¡°**:
```python
# Case 3ì€ Case 1ì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
class SimpleCase3Model(nn.Module):
    def __init__(self, processor, vision_dim=1024, language_dim=2048, action_dim=2, 
                 hidden_dim=256, dropout=0.4, use_vision_resampler=False):
        # Case 1ê³¼ ë™ì¼í•œ ì´ˆê¸°í™”
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim * 2), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
```

#### ğŸ“Š ì„±ëŠ¥ ë¶„ì„

**MAE: 0.881** - Case 1ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€
- **ì¥ì **: Case 1ê³¼ ë™ì¼í•œ ì•ˆì •ì„±
- **ë‹¨ì **: ë”ë¯¸ ë°ì´í„° ì‚¬ìš©, ì‹¤ì œ ì„±ëŠ¥ ë¯¸í™•ì¸
- **íŠ¹ì§•**: í˜ì‹ ì„± ë¶€ì¡±, ë‹¨ìˆœí•œ ë³µì‚¬ë³¸

---

### ğŸ¯ Case 4: RoboVLMsCompleteModel

#### ğŸ“ˆ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant Input as ì…ë ¥ ë°ì´í„°
    participant Processor as Kosmos2 Processor
    participant Vision as Vision Encoder
    participant Resampler as Advanced Vision Resampler
    participant Language as Language Encoder
    participant Fusion as ë©€í‹°ëª¨ë‹¬ ìœµí•©
    participant Planner as Hierarchical Planner
    participant TaskPlanner as Task Planner
    participant ActionSequencer as Action Sequencer
    participant StatePredictor as State Predictor
    participant ActionHead as Action Head
    participant Output as ì¶œë ¥ ì•¡ì…˜

    Input->>Processor: PIL ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ + ìƒíƒœ
    Processor->>Vision: pixel_values ì²˜ë¦¬
    Vision->>Vision: Kosmos2 Vision Model
    Vision->>Resampler: vision_features
    Resampler->>Resampler: Advanced Vision Resampler
    Resampler->>Resampler: Perceiver-style ë¦¬ìƒ˜í”Œë§
    Resampler->>Fusion: resampled_features
    
    Processor->>Language: input_ids ì²˜ë¦¬
    Language->>Language: Kosmos2 Text Model
    Language->>Language: last_hidden_state.mean(dim=1)
    Language->>Fusion: language_features
    
    Fusion->>Fusion: ë©€í‹°ëª¨ë‹¬ ìœµí•©
    Fusion->>Planner: fused_features
    
    Planner->>TaskPlanner: ê³ ìˆ˜ì¤€ íƒœìŠ¤í¬ ê³„íš
    TaskPlanner->>TaskPlanner: LSTM ê¸°ë°˜ ê³„íš ìƒì„±
    TaskPlanner->>ActionSequencer: íƒœìŠ¤í¬ íŠ¹ì§• ì „ë‹¬
    
    ActionSequencer->>ActionSequencer: GRU ê¸°ë°˜ ì•¡ì…˜ ì‹œí€€ìŠ¤
    ActionSequencer->>StatePredictor: ì•¡ì…˜ ì‹œí€€ìŠ¤ ì „ë‹¬
    
    StatePredictor->>StatePredictor: LSTM ê¸°ë°˜ ìƒíƒœ ì˜ˆì¸¡
    StatePredictor->>ActionHead: ì˜ˆì¸¡ëœ ìƒíƒœ
    
    ActionHead->>ActionHead: 4ì¸µ MLP + ê³„ì¸µì  ê³„íš í†µí•©
    ActionHead->>Output: predicted_actions [batch_size, 2]
```

#### ğŸ” ì½”ë“œ ë¶„ì„

**Advanced Vision Resampler**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:210-220
if self.use_vision_resampler:
    # Advanced Vision Resampler ì‚¬ìš©
    self.vision_resampler = AdvancedVisionResampler(
        input_dim=vision_dim,
        hidden_dim=hidden_dim,
        output_dim=hidden_dim,
        num_latents=64,
        num_layers=6,
        num_heads=8,
        dropout=dropout
    )
```

**Hierarchical Planning**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:50-55
self.use_hierarchical_planning = use_hierarchical_planning
self.use_state_prediction = use_state_prediction

if use_hierarchical_planning:
    self.hierarchical_planner = HierarchicalPlanner(
        input_dim=hidden_dim,
        hidden_dim=hidden_dim,
        action_dim=action_dim,
        state_dim=state_dim,
        num_tasks=num_tasks,
        max_plan_length=max_plan_length,
        max_sequence_length=max_sequence_length,
        prediction_horizon=prediction_horizon
    )
```

#### ğŸ“Š ì„±ëŠ¥ ë¶„ì„

**MAE: 0.941** - ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
- **ì¥ì **: ì™„ì „í•œ RoboVLMs ì•„í‚¤í…ì²˜, ê³„ì¸µì  ê³„íš
- **ë‹¨ì **: ë”ë¯¸ ë°ì´í„° ì‚¬ìš©, ê³¼ì í•© ìœ„í—˜
- **íŠ¹ì§•**: ë³µì¡í•œ êµ¬ì¡°ë¡œ í™•ì¥ì„± í™•ë³´

---

## ğŸ” ì„±ëŠ¥ ì°¨ì´ ë¶„ì„

### ğŸ“Š ì•„í‚¤í…ì²˜ë³„ íŠ¹ì§• ë¹„êµ

| êµ¬ì„±ìš”ì†Œ | Case 1 | Case 2 | Case 3 | Case 4 |
|----------|--------|--------|--------|--------|
| **Vision Encoder** | Kosmos2 | Kosmos2 + CLIP | Kosmos2 | Kosmos2 + Advanced Resampler |
| **Vision Resampler** | âŒ | âœ… Optimized | âŒ | âœ… Advanced |
| **CLIP Normalization** | âŒ | âœ… | âŒ | âŒ |
| **Hierarchical Planning** | âŒ | âŒ | âŒ | âœ… |
| **State Prediction** | âŒ | âŒ | âŒ | âœ… |
| **Action Head** | 4ì¸µ MLP | 4ì¸µ MLP | 4ì¸µ MLP | 4ì¸µ MLP + ê³„ì¸µì  ê³„íš |

### ğŸ¯ ì„±ëŠ¥ ì°¨ì´ ì›ì¸ ë¶„ì„

#### 1. Case 2ì˜ ìš°ìˆ˜ì„± (MAE: 0.466)

**CLIP Normalization íš¨ê³¼**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:50-55
self.use_clip_norm = use_clip_norm  # í•µì‹¬ ì„¤ì •

# CLIP ì •ê·œí™”ë¡œ ë¹„ì „ íŠ¹ì§• í’ˆì§ˆ í–¥ìƒ
if self.use_clip_norm:
    clip_features = self.clip_model.encode_image(clip_input)
    vision_features = self.clip_normalize(vision_features, clip_features)
```

**Vision Resampler íš¨ê³¼**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:210-220
if self.use_vision_resampler:
    # í† í° ìˆ˜ ì¡°ì •ìœ¼ë¡œ ì •ë³´ ì••ì¶• ë° ê°œì„ 
    image_features = self.vision_resampler(image_features.unsqueeze(2))
```

#### 2. Case 1ì˜ ì•ˆì •ì„± (MAE: 0.869)

**ë‹¨ìˆœí•œ êµ¬ì¡°**:
```python
# ë‹¨ìˆœí•œ 4ì¸µ MLPë¡œ ì•ˆì •ì ì¸ í•™ìŠµ
self.action_head = nn.Sequential(
    nn.Linear(hidden_dim * 2, hidden_dim * 2), nn.ReLU(), nn.Dropout(dropout),
    nn.Linear(hidden_dim * 2, hidden_dim), nn.ReLU(), nn.Dropout(dropout),
    nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout),
    nn.Linear(hidden_dim // 2, action_dim)
)
```

#### 3. Case 3ì˜ í•œê³„ (MAE: 0.881)

**Case 1ê³¼ ë™ì¼í•œ êµ¬ì¡°**:
- í˜ì‹ ì„± ë¶€ì¡±
- ë”ë¯¸ ë°ì´í„° ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ ì„±ëŠ¥ ë¯¸í™•ì¸

#### 4. Case 4ì˜ ë³µì¡ì„± (MAE: 0.941)

**ì™„ì „í•œ RoboVLMs ì•„í‚¤í…ì²˜**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:34-60
# ë³µì¡í•œ ì„¤ì •ë“¤
use_vision_resampler=False,
use_clip_norm=False,
use_hierarchical_planning=True,  # ê³„ì¸µì  ê³„íš
use_state_prediction=True,       # ìƒíƒœ ì˜ˆì¸¡
```

---

## ğŸ’¡ ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸

### ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **CLIP Normalizationì˜ í•µì‹¬ ì—­í• **: Case 2ì—ì„œ 46% ì„±ëŠ¥ í–¥ìƒ
2. **Vision Resamplerì˜ íš¨ê³¼**: ë¹„ì „ íŠ¹ì§• ê°œì„ ì— í•µì‹¬
3. **ë‹¨ìˆœì„±ì˜ ê°€ì¹˜**: Case 1ì˜ ì•ˆì •ì ì¸ ì„±ëŠ¥
4. **ë³µì¡ì„±ì˜ í•œê³„**: Case 4ì˜ ê³¼ì í•© ìœ„í—˜

### ğŸ” RoboVLMs ì½”ë“œ ì¸ì‚¬ì´íŠ¸

**BaseRoboVLMì˜ ì„¤ê³„ ì² í•™**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:34-60
# ëª¨ë“ˆí™”ëœ ì„¤ê³„ë¡œ ë‹¤ì–‘í•œ êµ¬ì„± ê°€ëŠ¥
use_vision_resampler=False,  # Vision Resampler ì„ íƒì  ì‚¬ìš©
use_clip_norm=False,         # CLIP ì •ê·œí™” ì„ íƒì  ì‚¬ìš©
use_hierarchical_planning=False,  # ê³„ì¸µì  ê³„íš ì„ íƒì  ì‚¬ìš©
```

**Forward ë©”ì„œë“œì˜ ìœ ì—°ì„±**:
```python
# RoboVLMs/robovlms/model/backbone/base_backbone.py:1491-1515
# ë‹¤ì–‘í•œ ì…ë ¥ê³¼ ì¶œë ¥ ì§€ì›
def forward(self, vision_x, lang_x, action_labels=None, ...):
    # ëª¨ë“ˆí™”ëœ ì²˜ë¦¬ë¡œ í™•ì¥ì„± í™•ë³´
```

### ğŸš€ ê¶Œì¥ì‚¬í•­

1. **Case 2ë¥¼ ë©”ì¸ ëª¨ë¸ë¡œ ì‚¬ìš©**: CLIP Normalization + Vision Resampler
2. **Case 1ì„ ë°±ì—… ëª¨ë¸ë¡œ ìœ ì§€**: ì•ˆì •ì„± ë³´ì¥
3. **Case 4 ì‹¤ì œ ë°ì´í„° ì¬ê²€ì¦**: ì™„ì „í•œ RoboVLMs ì•„í‚¤í…ì²˜ ê²€ì¦
4. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: Case 2ì˜ ì¶”ê°€ ìµœì í™”

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **RoboVLMs/robovlms/model/backbone/base_backbone.py**: í•µì‹¬ ì•„í‚¤í…ì²˜
- **RoboVLMs/robovlms/model/policy_head/base_policy.py**: ì•¡ì…˜ ë””ì½”ë”
- **RoboVLMs/robovlms/model/README.md**: ì „ì²´ ì•„í‚¤í…ì²˜ ì„¤ëª…
