#!/usr/bin/env python3
"""
Mobile VLA Trainer - RoboVLMsì˜ BaseTrainerë¥¼ Mobile VLAì— ì ì‘
mobile_vla_data_collector.py ë°ì´í„°ë¡œ ì§ì ‘ í•™ìŠµ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning.pytorch as pl
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Any
import logging

# Mobile VLA ëª¨ë“ˆë“¤
try:
    from ..models.mobile_vla_model import MobileVLAModel
    from ..data.mobile_dataset import MobileVLADataset
except ImportError:
    # í…ŒìŠ¤íŠ¸ìš© ì ˆëŒ€ ì„í¬íŠ¸
    import sys
    import os
    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(parent_dir)
    from models.mobile_vla_model import MobileVLAModel
    from data.mobile_dataset import MobileVLADataset

logger = logging.getLogger(__name__)


class MobileVLATrainer(pl.LightningModule):
    """
    Mobile VLA ì „ìš© íŠ¸ë ˆì´ë„ˆ
    - RoboVLMs BaseTrainer êµ¬ì¡° ìœ ì§€
    - mobile_vla_data_collector.py ë°ì´í„° íŠ¹í™”
    - ì‹œë‚˜ë¦¬ì˜¤ë³„ í•™ìŠµ ìµœì í™”
    """
    
    def __init__(self, configs: Dict[str, Any]):
        super().__init__()
        
        self.configs = configs
        self.save_hyperparameters()
        
        # Mobile VLA ëª¨ë¸ ì´ˆê¸°í™”
        self.model = MobileVLAModel(
            hidden_size=configs.get("hidden_size", 768),
            image_backbone=configs.get("image_backbone", "efficientnet_v2_s"),
            text_model=configs.get("text_model", "klue/roberta-base"),
            use_lite_mode=configs.get("use_lite_mode", False),
            dropout=configs.get("dropout", 0.1)
        )
        
        # mobile_vla_data_collector.py ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜
        self.scenario_weights = configs.get("scenario_weights", {
            "1box_vert_left": 1.0,
            "1box_vert_right": 1.0,
            "1box_hori_left": 1.2,   # ë” ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
            "1box_hori_right": 1.1,
            "2box_vert_left": 1.5,   # ê°€ì¥ ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
            "2box_vert_right": 1.4,
            "2box_hori_left": 1.8,
            "2box_hori_right": 1.6
        })
        
        # ì†ì‹¤ í•¨ìˆ˜ ê°€ì¤‘ì¹˜
        self.action_loss_weight = configs.get("action_loss_weight", 1.0)
        self.event_loss_weight = configs.get("event_loss_weight", 0.5)
        self.scenario_loss_weight = configs.get("scenario_loss_weight", 0.1)
        
        # í•™ìŠµ ë©”íŠ¸ë¦­
        self.action_tolerance = configs.get("action_tolerance", 0.1)  # ì•¡ì…˜ ì •í™•ë„ í—ˆìš© ì˜¤ì°¨
        
        logger.info("ğŸ¤– Mobile VLA Trainer ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ëª¨ë¸ í¬ê¸°: {sum(p.numel() for p in self.model.parameters()):,}ê°œ")
        logger.info(f"   Lite ëª¨ë“œ: {configs.get('use_lite_mode', False)}")
        logger.info(f"   ì‹œë‚˜ë¦¬ì˜¤ ê°€ì¤‘ì¹˜: {self.scenario_weights}")
    
    def forward(self, batch: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        """ìˆœì „íŒŒ"""
        return self.model(
            images=batch["images"],
            scenarios=batch["scenario"],
            instructions=batch.get("instruction")
        )
    
    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        """í•™ìŠµ ìŠ¤í…"""
        # ëª¨ë¸ ì˜ˆì¸¡
        predictions = self.forward(batch)
        
        # íƒ€ê²Ÿ ë°ì´í„°
        target_actions = batch["actions"]          # [B, T, 3] ì •ê·œí™”ëœ ì•¡ì…˜
        target_events = batch["action_events"]     # [B, T] ì´ë²¤íŠ¸ ì¸ë±ìŠ¤
        scenarios = batch["scenario"]              # List[str]
        sequence_masks = batch.get("sequence_mask")  # [B, T] (ì˜µì…˜)
        
        # ì†ì‹¤ ê³„ì‚°
        losses = self._compute_losses(
            predictions, target_actions, target_events, scenarios, sequence_masks
        )
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._compute_metrics(
            predictions, target_actions, target_events, sequence_masks
        )
        
        # ë¡œê¹…
        self.log_dict({
            "train_total_loss": losses["total_loss"],
            "train_action_loss": losses["action_loss"],
            "train_event_loss": losses["event_loss"],
            "train_scenario_loss": losses["scenario_loss"],
            "train_action_accuracy": metrics["action_accuracy"],
            "train_event_accuracy": metrics["event_accuracy"],
            "train_scenario_weight_avg": losses["scenario_weight_avg"]
        }, prog_bar=True, on_step=True, on_epoch=True)
        
        return losses["total_loss"]
    
    def validation_step(self, batch: Dict[str, Any], batch_idx: int) -> torch.Tensor:
        """ê²€ì¦ ìŠ¤í…"""
        with torch.no_grad():
            predictions = self.forward(batch)
            
            target_actions = batch["actions"]
            target_events = batch["action_events"]
            scenarios = batch["scenario"]
            sequence_masks = batch.get("sequence_mask")
            
            losses = self._compute_losses(
                predictions, target_actions, target_events, scenarios, sequence_masks
            )
            
            metrics = self._compute_metrics(
                predictions, target_actions, target_events, sequence_masks
            )
            
            # ê²€ì¦ ë¡œê¹…
            self.log_dict({
                "val_total_loss": losses["total_loss"],
                "val_action_loss": losses["action_loss"],
                "val_event_loss": losses["event_loss"],
                "val_action_accuracy": metrics["action_accuracy"],
                "val_event_accuracy": metrics["event_accuracy"]
            }, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)
        
        return losses["total_loss"]
    
    def _compute_losses(
        self,
        predictions: Dict[str, torch.Tensor],
        target_actions: torch.Tensor,
        target_events: torch.Tensor,
        scenarios: List[str],
        sequence_masks: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """í†µí•© ì†ì‹¤ ê³„ì‚°"""
        
        # 1. ì•¡ì…˜ ì†ì‹¤ (MSE)
        pred_actions = predictions["actions"]  # [B, T, 3] ì •ê·œí™”ëœ ì•¡ì…˜
        action_loss = F.mse_loss(pred_actions, target_actions, reduction='none')  # [B, T, 3]
        
        if sequence_masks is not None:
            mask = sequence_masks.unsqueeze(-1).float()  # [B, T, 1]
            action_loss = (action_loss * mask).sum() / mask.sum()
        else:
            action_loss = action_loss.mean()
        
        # 2. ì´ë²¤íŠ¸ ì†ì‹¤ (Cross-entropy)
        pred_event_logits = predictions["event_logits"]  # [B, T, 3]
        B, T, num_classes = pred_event_logits.shape
        
        event_loss = F.cross_entropy(
            pred_event_logits.view(B * T, num_classes),
            target_events.view(B * T),
            reduction='none'
        ).view(B, T)
        
        if sequence_masks is not None:
            mask = sequence_masks.float()  # [B, T]
            event_loss = (event_loss * mask).sum() / mask.sum()
        else:
            event_loss = event_loss.mean()
        
        # 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        scenario_weights = torch.tensor([
            self.scenario_weights.get(scenario, 1.0) for scenario in scenarios
        ], device=self.device, dtype=torch.float32)
        
        scenario_weight_avg = scenario_weights.mean()
        
        # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì•¡ì…˜ ì†ì‹¤
        weighted_action_loss = action_loss * scenario_weight_avg
        weighted_event_loss = event_loss * scenario_weight_avg
        
        # 4. ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„± ì†ì‹¤ (ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¼ê´€ëœ í–‰ë™ ìœ ë„)
        scenario_consistency_loss = self._compute_scenario_consistency_loss(
            predictions, scenarios
        )
        
        # ì´ ì†ì‹¤
        total_loss = (
            self.action_loss_weight * weighted_action_loss +
            self.event_loss_weight * weighted_event_loss +
            self.scenario_loss_weight * scenario_consistency_loss
        )
        
        return {
            "total_loss": total_loss,
            "action_loss": action_loss,
            "event_loss": event_loss,
            "scenario_loss": scenario_consistency_loss,
            "scenario_weight_avg": scenario_weight_avg
        }
    
    def _compute_scenario_consistency_loss(
        self,
        predictions: Dict[str, torch.Tensor],
        scenarios: List[str]
    ) -> torch.Tensor:
        """ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„± ì†ì‹¤"""
        # ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ë¼ë¦¬ ê·¸ë£¹í™”
        scenario_groups = {}
        for i, scenario in enumerate(scenarios):
            if scenario not in scenario_groups:
                scenario_groups[scenario] = []
            scenario_groups[scenario].append(i)
        
        if len(scenario_groups) <= 1:
            # ëª¨ë“  ë°°ì¹˜ê°€ ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ë©´ ì¼ê´€ì„± ì†ì‹¤ 0
            return torch.tensor(0.0, device=self.device)
        
        consistency_loss = 0.0
        num_groups = 0
        
        for scenario, indices in scenario_groups.items():
            if len(indices) < 2:
                continue  # í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ê°€ 1ê°œë¿ì´ë©´ ìŠ¤í‚µ
            
            # ê°™ì€ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì•¡ì…˜ë“¤ ê°„ ë¶„ì‚° ìµœì†Œí™”
            scenario_actions = predictions["actions"][indices]  # [num_indices, T, 3]
            action_mean = scenario_actions.mean(dim=0, keepdim=True)  # [1, T, 3]
            action_variance = ((scenario_actions - action_mean) ** 2).mean()
            
            consistency_loss += action_variance
            num_groups += 1
        
        if num_groups > 0:
            consistency_loss = consistency_loss / num_groups
        else:
            consistency_loss = torch.tensor(0.0, device=self.device)
        
        return consistency_loss
    
    def _compute_metrics(
        self,
        predictions: Dict[str, torch.Tensor],
        target_actions: torch.Tensor,
        target_events: torch.Tensor,
        sequence_masks: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        # 1. ì•¡ì…˜ ì •í™•ë„ (í—ˆìš© ì˜¤ì°¨ ë‚´ ì˜ˆì¸¡)
        pred_actions = predictions["actions"]  # [B, T, 3]
        action_diff = torch.abs(pred_actions - target_actions)  # [B, T, 3]
        accurate_actions = (action_diff < self.action_tolerance).all(dim=-1)  # [B, T]
        
        if sequence_masks is not None:
            mask = sequence_masks.bool()
            action_accuracy = (accurate_actions & mask).float().sum() / mask.float().sum()
        else:
            action_accuracy = accurate_actions.float().mean()
        
        # 2. ì´ë²¤íŠ¸ ì •í™•ë„
        pred_events = predictions["predicted_events"]  # [B, T]
        correct_events = (pred_events == target_events)  # [B, T]
        
        if sequence_masks is not None:
            mask = sequence_masks.bool()
            event_accuracy = (correct_events & mask).float().sum() / mask.float().sum()
        else:
            event_accuracy = correct_events.float().mean()
        
        # 3. ì•¡ì…˜ë³„ MAE (Mean Absolute Error)
        action_mae = action_diff.mean(dim=(0, 1))  # [3] - ê° ì•¡ì…˜ ì¶•ë³„ MAE
        
        return {
            "action_accuracy": action_accuracy,
            "event_accuracy": event_accuracy,
            "action_mae_x": action_mae[0],
            "action_mae_y": action_mae[1],
            "action_mae_z": action_mae[2]
        }
    
    def configure_optimizers(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # RoboVLMs BaseTrainerì™€ ìœ ì‚¬í•œ ì„¤ì •
        lr = self.configs.get("learning_rate", 1e-4)
        weight_decay = self.configs.get("weight_decay", 0.01)
        
        # AdamW ì˜µí‹°ë§ˆì´ì €
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        
        # ì½”ì‚¬ì¸ ì–´ë‹ë§ ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler_type = self.configs.get("scheduler", "cosine")
        
        if scheduler_type == "cosine":
            max_epochs = self.configs.get("max_epochs", 100)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=max_epochs, eta_min=lr * 0.1
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1
                }
            }
        elif scheduler_type == "step":
            step_size = self.configs.get("step_size", 30)
            gamma = self.configs.get("gamma", 0.1)
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer, step_size=step_size, gamma=gamma
            )
            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "epoch",
                    "frequency": 1
                }
            }
        else:
            return optimizer
    
    def train_dataloader(self) -> DataLoader:
        """í•™ìŠµ ë°ì´í„°ë¡œë”"""
        dataset = MobileVLADataset(
            data_dir=self.configs.get("train_data_dir", "/home/soda/vla/ROS_action/mobile_vla_dataset/"),
            sequence_length=self.configs.get("sequence_length", 18),
            normalize_actions=True,
            scenario_filter=self.configs.get("train_scenarios")
        )
        
        return DataLoader(
            dataset,
            batch_size=self.configs.get("batch_size", 4),
            shuffle=True,
            num_workers=self.configs.get("num_workers", 4),
            pin_memory=True,
            drop_last=True
        )
    
    def val_dataloader(self) -> DataLoader:
        """ê²€ì¦ ë°ì´í„°ë¡œë”"""
        dataset = MobileVLADataset(
            data_dir=self.configs.get("val_data_dir", "/home/soda/vla/ROS_action/mobile_vla_dataset/"),
            sequence_length=self.configs.get("sequence_length", 18),
            normalize_actions=True,
            scenario_filter=self.configs.get("val_scenarios")
        )
        
        return DataLoader(
            dataset,
            batch_size=self.configs.get("val_batch_size", 2),
            shuffle=False,
            num_workers=self.configs.get("num_workers", 2),
            pin_memory=True
        )
    
    def on_validation_epoch_end(self):
        """ê²€ì¦ ì—í¬í¬ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¶„ì„ (ì¶”í›„ êµ¬í˜„)
        pass
    
    def predict_mobile_action(
        self,
        current_image: torch.Tensor,
        scenario: str
    ) -> Dict[str, float]:
        """
        mobile_vla_data_collector.py í˜¸í™˜ ì•¡ì…˜ ì˜ˆì¸¡
        (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)
        """
        self.eval()
        with torch.no_grad():
            mobile_action = self.model.get_mobile_vla_action(current_image, scenario)
        return mobile_action


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile VLA Trainer í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    test_configs = {
        "hidden_size": 512,
        "use_lite_mode": True,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ Lite ëª¨ë“œ
        "learning_rate": 1e-4,
        "batch_size": 2,
        "sequence_length": 18,
        "max_epochs": 5,
        "scheduler": "cosine"
    }
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = MobileVLATrainer(test_configs)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size, seq_len = 2, 18
    test_batch = {
        "images": torch.randn(batch_size, seq_len, 3, 224, 224),
        "actions": torch.randn(batch_size, seq_len, 3),  # ì •ê·œí™”ëœ ì•¡ì…˜
        "action_events": torch.randint(0, 3, (batch_size, seq_len)),  # ì´ë²¤íŠ¸ ì¸ë±ìŠ¤
        "scenario": ["1box_vert_left", "2box_hori_right"],
        "instruction": ["ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”", "ë‘ ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”"],
        "sequence_mask": torch.ones(batch_size, seq_len, dtype=torch.bool)
    }
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°°ì¹˜:")
    print(f"   ì´ë¯¸ì§€: {test_batch['images'].shape}")
    print(f"   ì•¡ì…˜: {test_batch['actions'].shape}")
    print(f"   ì´ë²¤íŠ¸: {test_batch['action_events'].shape}")
    print(f"   ì‹œë‚˜ë¦¬ì˜¤: {test_batch['scenario']}")
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    predictions = trainer.forward(test_batch)
    print(f"\nğŸ¤– ëª¨ë¸ ì˜ˆì¸¡:")
    print(f"   ì•¡ì…˜: {predictions['actions'].shape}")
    print(f"   ì´ë²¤íŠ¸: {predictions['predicted_events'].shape}")
    
    # í•™ìŠµ ìŠ¤í… í…ŒìŠ¤íŠ¸
    loss = trainer.training_step(test_batch, 0)
    print(f"\nğŸ“ˆ í•™ìŠµ ì†ì‹¤: {loss:.4f}")
    
    # ë‹¨ì¼ ì•¡ì…˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    single_image = torch.randn(1, 3, 224, 224)
    mobile_action = trainer.predict_mobile_action(single_image, "1box_vert_left")
    print(f"\nğŸ¯ Mobile ì•¡ì…˜ ì˜ˆì¸¡: {mobile_action}")
    
    print(f"\nâœ… Mobile VLA Trainer í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
