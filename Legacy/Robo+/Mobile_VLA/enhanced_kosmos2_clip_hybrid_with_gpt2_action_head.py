#!/usr/bin/env python3
"""
ğŸ¯ Enhanced Kosmos2+CLIP Hybrid with GPT2 Action Head
VLM + GPT2 Action Head êµ¬ì¡°ë¡œ ëª¨ë°”ì¼ ë¡œë´‡ VLA êµ¬í˜„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel, GPT2Model, GPT2Config
import logging
from typing import Optional, Tuple, Dict, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GPT2ActionHead(nn.Module):
    """GPT2 ê¸°ë°˜ Action Head"""
    
    def __init__(
        self,
        hidden_dim: int = 768,
        action_dim: int = 2,
        num_layers: int = 6,
        num_heads: int = 8,
        dropout: float = 0.1,
        max_length: int = 512
    ):
        super().__init__()
        
        # GPT2 ì„¤ì •
        config = GPT2Config(
            vocab_size=1,  # ë”ë¯¸ ê°’
            n_positions=max_length,
            n_ctx=max_length,
            n_embd=hidden_dim,
            n_layer=num_layers,
            n_head=num_heads,
            resid_pdrop=dropout,
            embd_pdrop=dropout,
            attn_pdrop=dropout,
            use_cache=False
        )
        
        # GPT2 ëª¨ë¸ (ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ë§Œ ì‚¬ìš©)
        self.gpt2 = GPT2Model(config)
        
        # Action projection layer
        self.action_projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Positional encoding for action sequence
        self.positional_encoding = nn.Parameter(
            torch.randn(max_length, hidden_dim) * 0.02
        )
        
        logger.info(f"GPT2 Action Head initialized:")
        logger.info(f"  - Hidden dim: {hidden_dim}")
        logger.info(f"  - Action dim: {action_dim}")
        logger.info(f"  - Num layers: {num_layers}")
        logger.info(f"  - Num heads: {num_heads}")
        logger.info(f"  - Max length: {max_length}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, hidden_dim] - VLM features
        Returns:
            actions: [batch_size, action_dim] - predicted actions
        """
        batch_size, seq_len, hidden_dim = x.shape
        
        # Add positional encoding
        if seq_len <= self.positional_encoding.size(0):
            pos_enc = self.positional_encoding[:seq_len].unsqueeze(0)
            x = x + pos_enc
        
        # GPT2 forward pass
        gpt2_output = self.gpt2(
            inputs_embeds=x,
            attention_mask=torch.ones(batch_size, seq_len, device=x.device)
        )
        
        # Use last hidden state for action prediction
        last_hidden = gpt2_output.last_hidden_state[:, -1, :]  # [batch_size, hidden_dim]
        
        # Project to action space
        actions = self.action_projection(last_hidden)
        
        return actions

class EnhancedKosmos2CLIPHybridWithGPT2ActionHead(nn.Module):
    """
    Enhanced Kosmos2+CLIP Hybrid Model with GPT2 Action Head
    
    ì•„í‚¤í…ì²˜:
    - Kosmos2 Vision Encoder
    - CLIP Vision Encoder  
    - Vision Resampler
    - GPT2 Action Head
    """
    
    def __init__(
        self,
        action_dim: int = 2,  # 2D ì•¡ì…˜ (linear_x, linear_y)
        vision_resampler_tokens: int = 64,
        hidden_dim: int = 768,
        gpt2_layers: int = 6,
        gpt2_heads: int = 8,
        dropout: float = 0.1,
        mobile_optimized: bool = True
    ):
        super().__init__()
        
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.mobile_optimized = mobile_optimized
        
        # Kosmos2 ëª¨ë¸ (Vision Encoder)
        logger.info("Loading Kosmos2 model...")
        self.kosmos_processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos_model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # CLIP ëª¨ë¸ (Vision Encoder)
        logger.info("Loading CLIP model...")
        self.clip_processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_model = AutoModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Vision Resampler (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ í† í° ìˆ˜ ê°ì†Œ)
        self.vision_resampler = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Resampler query tokens
        self.resampler_queries = nn.Parameter(
            torch.randn(vision_resampler_tokens, hidden_dim) * 0.02
        )
        
        # Feature fusion layer
        self.feature_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # GPT2 Action Head
        self.gpt2_action_head = GPT2ActionHead(
            hidden_dim=hidden_dim,
            action_dim=action_dim,
            num_layers=gpt2_layers,
            num_heads=gpt2_heads,
            dropout=dropout
        )
        
        # Initialize weights
        self._initialize_weights()
        
        logger.info(f"Enhanced Kosmos2+CLIP Hybrid Model with GPT2 Action Head initialized:")
        logger.info(f"  - Action dim: {action_dim}")
        logger.info(f"  - Vision resampler tokens: {vision_resampler_tokens}")
        logger.info(f"  - Hidden dim: {hidden_dim}")
        logger.info(f"  - GPT2 layers: {gpt2_layers}")
        logger.info(f"  - GPT2 heads: {gpt2_heads}")
        logger.info(f"  - Mobile optimized: {mobile_optimized}")
    
    def _initialize_weights(self):
        """Initialize model weights"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def extract_kosmos_features(self, images: torch.Tensor) -> torch.Tensor:
        """Kosmos2ì—ì„œ vision features ì¶”ì¶œ"""
        with torch.no_grad():
            # Kosmos2 vision encoder ì‚¬ìš©
            vision_outputs = self.kosmos_model.vision_model(images)
            # [batch_size, num_patches, hidden_dim]
            return vision_outputs.last_hidden_state
    
    def extract_clip_features(self, images: torch.Tensor) -> torch.Tensor:
        """CLIPì—ì„œ vision features ì¶”ì¶œ"""
        with torch.no_grad():
            # CLIP vision encoder ì‚¬ìš©
            vision_outputs = self.clip_model.vision_model(images)
            # [batch_size, num_patches, hidden_dim]
            return vision_outputs.last_hidden_state
    
    def resample_vision_features(self, features: torch.Tensor) -> torch.Tensor:
        """Vision featuresë¥¼ ê³ ì •ëœ í† í° ìˆ˜ë¡œ ë¦¬ìƒ˜í”Œë§"""
        batch_size = features.size(0)
        
        # Query tokensë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
        queries = self.resampler_queries.unsqueeze(0).expand(batch_size, -1, -1)
        
        # Multi-head attentionìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        resampled_features, _ = self.vision_resampler(
            query=queries,
            key=features,
            value=features
        )
        
        return resampled_features
    
    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            images: [batch_size, channels, height, width] - ì…ë ¥ ì´ë¯¸ì§€
        Returns:
            actions: [batch_size, action_dim] - ì˜ˆì¸¡ëœ ì•¡ì…˜
        """
        # 1. Vision features ì¶”ì¶œ
        kosmos_features = self.extract_kosmos_features(images)  # [B, N, 768]
        clip_features = self.extract_clip_features(images)      # [B, M, 768]
        
        # 2. Vision features ë¦¬ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        kosmos_resampled = self.resample_vision_features(kosmos_features)
        clip_resampled = self.resample_vision_features(clip_features)
        
        # 3. Feature fusion
        fused_features = torch.cat([kosmos_resampled, clip_resampled], dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # 4. GPT2 Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.gpt2_action_head(fused_features)
        
        return actions
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": "Enhanced Kosmos2+CLIP Hybrid with GPT2 Action Head",
            "action_dim": self.action_dim,
            "hidden_dim": self.hidden_dim,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "mobile_optimized": self.mobile_optimized,
            "action_head_type": "GPT2"
        }

def create_model(
    action_dim: int = 2,
    vision_resampler_tokens: int = 64,
    hidden_dim: int = 768,
    gpt2_layers: int = 6,
    gpt2_heads: int = 8,
    dropout: float = 0.1,
    mobile_optimized: bool = True
) -> EnhancedKosmos2CLIPHybridWithGPT2ActionHead:
    """ëª¨ë¸ ìƒì„± í•¨ìˆ˜"""
    return EnhancedKosmos2CLIPHybridWithGPT2ActionHead(
        action_dim=action_dim,
        vision_resampler_tokens=vision_resampler_tokens,
        hidden_dim=hidden_dim,
        gpt2_layers=gpt2_layers,
        gpt2_heads=gpt2_heads,
        dropout=dropout,
        mobile_optimized=mobile_optimized
    )

if __name__ == "__main__":
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    logger.info("Testing Enhanced Kosmos2+CLIP Hybrid with GPT2 Action Head...")
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(
        action_dim=2,
        vision_resampler_tokens=64,
        hidden_dim=768,
        gpt2_layers=6,
        gpt2_heads=8,
        dropout=0.1,
        mobile_optimized=True
    )
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 2
    test_images = torch.randn(batch_size, 3, 224, 224)
    
    # Forward pass
    with torch.no_grad():
        actions = model(test_images)
    
    logger.info(f"Model test successful!")
    logger.info(f"Input shape: {test_images.shape}")
    logger.info(f"Output shape: {actions.shape}")
    logger.info(f"Model info: {model.get_model_info()}")
