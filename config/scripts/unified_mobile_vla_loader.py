#!/usr/bin/env python3
"""
í†µí•© Mobile VLA ëª¨ë¸ ë¡œë”
MODEL_RANKING.md ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” í†µí•© ë¡œë”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import json
from typing import Optional, Dict, Any, Tuple, Union
import numpy as np
from enum import Enum

class ModelType(Enum):
    """ì§€ì›í•˜ëŠ” ëª¨ë¸ íƒ€ì…"""
    KOSMOS2_CLIP_HYBRID = "kosmos2_clip_hybrid"  # MAE 0.212 (1ìœ„)
    KOSMOS2_PURE = "kosmos2_pure"                # MAE 0.222 (2ìœ„)

class UnifiedMobileVLAModel(nn.Module):
    """í†µí•© Mobile VLA ëª¨ë¸ (ëª¨ë“  ëª¨ë¸ íƒ€ì… ì§€ì›)"""
    
    def __init__(self, 
                 model_type: ModelType = ModelType.KOSMOS2_CLIP_HYBRID,
                 vision_dim: int = 2048,
                 text_dim: int = 2048,
                 hidden_dim: int = 4096,
                 action_dim: int = 2,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.model_type = model_type
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.num_layers = num_layers
        
        # Vision Encoder (Kosmos2 features)
        self.vision_encoder = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Text Encoder (CLIP features) - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì—ì„œë§Œ ì‚¬ìš©
        if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
            self.text_encoder = nn.Sequential(
                nn.Linear(text_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
            
            # Feature Fusion (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸)
            self.feature_fusion = nn.Sequential(
                nn.Linear(hidden_dim * 2, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
        else:
            # ìˆœìˆ˜ Kosmos2 ëª¨ë¸ì€ text_encoder ì—†ìŒ
            self.text_encoder = None
            self.feature_fusion = None
        
        # LSTM for temporal modeling
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, action_dim)
        )
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, vision_features: torch.Tensor, text_features: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            vision_features: (batch_size, vision_dim) - Kosmos2 vision features
            text_features: (batch_size, text_dim) - CLIP text features (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì—ì„œë§Œ)
            
        Returns:
            actions: (batch_size, action_dim) - Predicted 2D actions
        """
        batch_size = vision_features.size(0)
        
        # Encode vision features
        vision_encoded = self.vision_encoder(vision_features)  # (batch_size, hidden_dim)
        
        if self.model_type == ModelType.KOSMOS2_CLIP_HYBRID:
            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸: Vision + Text fusion
            if text_features is None:
                raise ValueError("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì—ì„œëŠ” text_featuresê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            text_encoded = self.text_encoder(text_features)  # (batch_size, hidden_dim)
            combined = torch.cat([vision_encoded, text_encoded], dim=-1)  # (batch_size, hidden_dim*2)
            fused = self.feature_fusion(combined)  # (batch_size, hidden_dim)
        else:
            # ìˆœìˆ˜ Kosmos2 ëª¨ë¸: Visionë§Œ ì‚¬ìš©
            fused = vision_encoded  # (batch_size, hidden_dim)
        
        # LSTM processing
        fused = fused.unsqueeze(1)  # (batch_size, 1, hidden_dim)
        lstm_out, (hidden, cell) = self.lstm(fused)  # (batch_size, 1, hidden_dim)
        lstm_out = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
        
        # Predict actions
        actions = self.action_head(lstm_out)  # (batch_size, action_dim)
        
        return actions

class UnifiedMobileVLAModelLoader:
    """í†µí•© Mobile VLA ëª¨ë¸ ë¡œë” (ëª¨ë“  ëª¨ë¸ íƒ€ì… ì§€ì›)"""
    
    def __init__(self, model_dir: str = "./Robo+/Mobile_VLA"):
        self.model_dir = model_dir
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_type = None
        
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        if torch.cuda.is_available():
            print(f"ğŸ® CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
        # ëª¨ë¸ ì •ë³´ (MODEL_RANKING.md ê¸°ë°˜)
        self.model_info = {
            ModelType.KOSMOS2_CLIP_HYBRID: {
                'name': 'Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ',
                'mae': 0.212,
                'fps': 765.7,
                'params': 1859579651,
                'size_gb': 7.8,
                'rank': 1,
                'checkpoint_path': f"{model_dir}/results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth",
                'training_file': f"{model_dir}/models/core/train_simple_clip_lstm_core.py"
            },
            ModelType.KOSMOS2_PURE: {
                'name': 'ìˆœìˆ˜ Kosmos2',
                'mae': 0.222,
                'fps': 755.2,
                'params': 1703973122,
                'size_gb': 7.1,
                'rank': 2,
                'checkpoint_path': "./mobile-vla-omniwheel/best_simple_lstm_model.pth",
                'training_file': f"{model_dir}/models/core/train_simple_lstm_core.py"
            }
        }
    
    def list_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:")
        print("=" * 60)
        
        for model_type, info in self.model_info.items():
            status = "âœ…" if os.path.exists(info['checkpoint_path']) else "âŒ"
            print(f"{status} {info['rank']}ìœ„: {info['name']}")
            print(f"   - MAE: {info['mae']}")
            print(f"   - FPS: {info['fps']}")
            print(f"   - íŒŒë¼ë¯¸í„°: {info['params']:,}")
            print(f"   - í¬ê¸°: {info['size_gb']}GB")
            print(f"   - ì²´í¬í¬ì¸íŠ¸: {info['checkpoint_path']}")
            print(f"   - ì¡´ì¬: {os.path.exists(info['checkpoint_path'])}")
            print()
    
    def load_model(self, model_type: ModelType = ModelType.KOSMOS2_CLIP_HYBRID, 
                   checkpoint_path: Optional[str] = None) -> UnifiedMobileVLAModel:
        """
        ëª¨ë¸ ë¡œë“œ
        
        Args:
            model_type: ë¡œë“œí•  ëª¨ë¸ íƒ€ì…
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
            
        Returns:
            UnifiedMobileVLAModel: ë¡œë“œëœ ëª¨ë¸
        """
        self.model_type = model_type
        model_info = self.model_info[model_type]
        
        print(f"ğŸš€ {model_info['name']} ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.model_dir}")
        print(f"ğŸ¯ ëª©í‘œ ì„±ëŠ¥: MAE {model_info['mae']}, FPS {model_info['fps']}")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê²°ì •
        if checkpoint_path is None:
            checkpoint_path = model_info['checkpoint_path']
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {checkpoint_path}")
            print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
            self._list_available_checkpoints()
            return None
        
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = UnifiedMobileVLAModel(model_type=model_type)
            self.model = self.model.to(self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            print("ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            else:
                # ì²´í¬í¬ì¸íŠ¸ê°€ ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                self.model.load_state_dict(checkpoint)
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (ì§ì ‘ ë”•ì…”ë„ˆë¦¬)")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print_model_info(checkpoint, model_info)
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _list_available_checkpoints(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
        search_paths = [
            f"{self.model_dir}/results",
            "./mobile-vla-omniwheel",
            "./vla/mobile-vla-omniwheel",
            "./"
        ]
        
        for search_path in search_paths:
            if os.path.exists(search_path):
                for root, dirs, files in os.walk(search_path):
                    for file in files:
                        if file.endswith('.pth'):
                            full_path = os.path.join(root, file)
                            size_mb = os.path.getsize(full_path) / (1024 * 1024)
                            print(f"   - {full_path} ({size_mb:.1f} MB)")
    
    def _print_model_info(self, checkpoint: Dict[str, Any], model_info: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        if self.model is None:
            return
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   - ëª¨ë¸ íƒ€ì…: {model_info['name']}")
        print(f"   - ìˆœìœ„: {model_info['rank']}ìœ„")
        print(f"   - ëª©í‘œ MAE: {model_info['mae']}")
        print(f"   - ëª©í‘œ FPS: {model_info['fps']}")
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
        print(f"   - ëª¨ë¸ êµ¬ì¡°: {self.model_type.value}")
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë³´
        if 'epoch' in checkpoint:
            print(f"   - í›ˆë ¨ ì—í¬í¬: {checkpoint['epoch']}")
        if 'val_mae' in checkpoint:
            print(f"   - ê²€ì¦ MAE: {checkpoint['val_mae']:.4f}")
        if 'args' in checkpoint:
            print(f"   - í›ˆë ¨ ì„¤ì •: {checkpoint['args']}")
    
    def predict(self, vision_features: torch.Tensor, text_features: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ì¶”ë¡  ì‹¤í–‰
        
        Args:
            vision_features: (batch_size, 2048) - Kosmos2 vision features
            text_features: (batch_size, 2048) - CLIP text features (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì—ì„œë§Œ)
            
        Returns:
            actions: (batch_size, 2) - Predicted 2D actions
        """
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        with torch.no_grad():
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            vision_features = vision_features.to(self.device)
            if text_features is not None:
                text_features = text_features.to(self.device)
            
            # ì¶”ë¡ 
            actions = self.model(vision_features, text_features)
            
            return actions
    
    def get_model(self) -> Optional[UnifiedMobileVLAModel]:
        """ë¡œë“œëœ ëª¨ë¸ ë°˜í™˜"""
        return self.model
    
    def get_model_info(self) -> Optional[Dict[str, Any]]:
        """í˜„ì¬ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model_type is None:
            return None
        return self.model_info[self.model_type]

def test_unified_loader():
    """í†µí•© ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í†µí•© Mobile VLA ëª¨ë¸ ë¡œë” í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ë¡œë” ìƒì„±
    loader = UnifiedMobileVLAModelLoader()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
    loader.list_available_models()
    
    # ê° ëª¨ë¸ íƒ€ì…ë³„ë¡œ í…ŒìŠ¤íŠ¸
    for model_type in ModelType:
        print(f"\nğŸ¯ {model_type.value} ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        # ëª¨ë¸ ë¡œë“œ
        model = loader.load_model(model_type)
        
        if model is not None:
            print("âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            
            # í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
            batch_size = 2
            vision_features = torch.randn(batch_size, 2048)
            
            if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                text_features = torch.randn(batch_size, 2048)
                actions = loader.predict(vision_features, text_features)
                print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¶”ë¡  ì„±ê³µ: {actions.shape}")
            else:
                actions = loader.predict(vision_features)
                print(f"âœ… ìˆœìˆ˜ Kosmos2 ëª¨ë¸ ì¶”ë¡  ì„±ê³µ: {actions.shape}")
            
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            print("\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
            import time
            
            # CPUì—ì„œ í…ŒìŠ¤íŠ¸
            vision_features_cpu = torch.randn(1, 2048)
            if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                text_features_cpu = torch.randn(1, 2048)
            
            start_time = time.time()
            for _ in range(100):
                if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                    actions_cpu = loader.predict(vision_features_cpu, text_features_cpu)
                else:
                    actions_cpu = loader.predict(vision_features_cpu)
            cpu_time = time.time() - start_time
            
            print(f"   - CPU ì¶”ë¡  ì‹œê°„ (100íšŒ): {cpu_time:.4f}ì´ˆ")
            print(f"   - CPU í‰ê·  ì¶”ë¡  ì‹œê°„: {cpu_time/100*1000:.2f}ms")
            print(f"   - CPU FPS: {100/cpu_time:.1f}")
            
            # GPUì—ì„œ í…ŒìŠ¤íŠ¸ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                vision_features_gpu = torch.randn(1, 2048).cuda()
                if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                    text_features_gpu = torch.randn(1, 2048).cuda()
                
                # GPU ì›Œë°ì—…
                for _ in range(10):
                    if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                        actions_gpu = loader.predict(vision_features_gpu, text_features_gpu)
                    else:
                        actions_gpu = loader.predict(vision_features_gpu)
                
                torch.cuda.synchronize()
                start_time = time.time()
                for _ in range(100):
                    if model_type == ModelType.KOSMOS2_CLIP_HYBRID:
                        actions_gpu = loader.predict(vision_features_gpu, text_features_gpu)
                    else:
                        actions_gpu = loader.predict(vision_features_gpu)
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time
                
                print(f"   - GPU ì¶”ë¡  ì‹œê°„ (100íšŒ): {gpu_time:.4f}ì´ˆ")
                print(f"   - GPU í‰ê·  ì¶”ë¡  ì‹œê°„: {gpu_time/100*1000:.2f}ms")
                print(f"   - GPU FPS: {100/gpu_time:.1f}")
                print(f"   - GPU ê°€ì†: {cpu_time/gpu_time:.1f}x")
        else:
            print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
    
    print("\nâœ… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_unified_loader()

if __name__ == "__main__":
    main()
