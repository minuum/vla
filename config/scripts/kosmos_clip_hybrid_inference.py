#!/usr/bin/env python3
"""
ğŸ¯ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
ì²´í¬í¬ì¸íŠ¸: best_simple_clip_lstm_model.pth
ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°: Kosmos2 (24ì¸µ) + CLIP (12ì¸µ) + LSTM + Action Head
"""

import torch
import torch.nn as nn
import numpy as np
import time
import os
import sys
from typing import Optional, Tuple

class KosmosCLIPHybridModel(nn.Module):
    """Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸"""
    
    def __init__(self, 
                 kosmos_hidden_size=4096,
                 clip_hidden_size=768,
                 fusion_size=2048,
                 lstm_hidden_size=512,
                 lstm_layers=4,
                 action_size=2):
        super().__init__()
        
        # Kosmos2 ëª¨ë¸ (24ì¸µ Transformer)
        self.kosmos_text_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=kosmos_hidden_size,
                nhead=16,
                dim_feedforward=16384,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=24
        )
        
        self.kosmos_vision_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=kosmos_hidden_size,
                nhead=16,
                dim_feedforward=16384,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=24
        )
        
        # CLIP ëª¨ë¸ (12ì¸µ Transformer)
        self.clip_text_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=clip_hidden_size,
                nhead=12,
                dim_feedforward=3072,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=12
        )
        
        self.clip_vision_model = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=clip_hidden_size,
                nhead=12,
                dim_feedforward=3072,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=12
        )
        
        # Feature fusion layer
        self.feature_fusion = nn.Linear(
            kosmos_hidden_size + clip_hidden_size, 
            fusion_size
        )
        
        # LSTM layer (4ì¸µ)
        self.lstm = nn.LSTM(
            input_size=fusion_size,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_layers,
            batch_first=True,
            dropout=0.1
        )
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(lstm_hidden_size, lstm_hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(lstm_hidden_size // 2, lstm_hidden_size // 4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(lstm_hidden_size // 4, action_size)
        )
        
        # Embeddings
        self.kosmos_text_embedding = nn.Embedding(32000, kosmos_hidden_size)
        self.kosmos_vision_embedding = nn.Linear(768, kosmos_hidden_size)  # ViT patch size
        self.clip_text_embedding = nn.Embedding(49408, clip_hidden_size)
        self.clip_vision_embedding = nn.Linear(768, clip_hidden_size)
        
        # Position embeddings
        self.kosmos_text_pos_embedding = nn.Embedding(2048, kosmos_hidden_size)
        self.kosmos_vision_pos_embedding = nn.Embedding(257, kosmos_hidden_size)  # 16x16 + 1
        self.clip_text_pos_embedding = nn.Embedding(77, clip_hidden_size)
        self.clip_vision_pos_embedding = nn.Embedding(257, clip_hidden_size)
        
        # Layer norms
        self.kosmos_text_norm = nn.LayerNorm(kosmos_hidden_size)
        self.kosmos_vision_norm = nn.LayerNorm(kosmos_hidden_size)
        self.clip_text_norm = nn.LayerNorm(clip_hidden_size)
        self.clip_vision_norm = nn.LayerNorm(clip_hidden_size)
        
    def forward(self, 
                kosmos_text_input: Optional[torch.Tensor] = None,
                kosmos_vision_input: Optional[torch.Tensor] = None,
                clip_text_input: Optional[torch.Tensor] = None,
                clip_vision_input: Optional[torch.Tensor] = None,
                hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):
        """
        Forward pass
        
        Args:
            kosmos_text_input: Kosmos2 text input [batch, seq_len]
            kosmos_vision_input: Kosmos2 vision input [batch, seq_len, 768]
            clip_text_input: CLIP text input [batch, seq_len]
            clip_vision_input: CLIP vision input [batch, seq_len, 768]
            hidden: LSTM hidden state
        """
        
        # Kosmos2 text processing
        if kosmos_text_input is not None:
            kosmos_text_emb = self.kosmos_text_embedding(kosmos_text_input)
            kosmos_text_pos = self.kosmos_text_pos_embedding(
                torch.arange(kosmos_text_input.size(1), device=kosmos_text_input.device)
            ).unsqueeze(0)
            kosmos_text_emb = kosmos_text_emb + kosmos_text_pos
            kosmos_text_emb = self.kosmos_text_norm(kosmos_text_emb)
            kosmos_text_features = self.kosmos_text_model(kosmos_text_emb)
            kosmos_text_features = kosmos_text_features.mean(dim=1)  # Global pooling
        else:
            kosmos_text_features = torch.zeros(
                kosmos_vision_input.size(0), 4096, 
                device=kosmos_vision_input.device
            )
        
        # Kosmos2 vision processing
        if kosmos_vision_input is not None:
            kosmos_vision_emb = self.kosmos_vision_embedding(kosmos_vision_input)
            kosmos_vision_pos = self.kosmos_vision_pos_embedding(
                torch.arange(kosmos_vision_input.size(1), device=kosmos_vision_input.device)
            ).unsqueeze(0)
            kosmos_vision_emb = kosmos_vision_emb + kosmos_vision_pos
            kosmos_vision_emb = self.kosmos_vision_norm(kosmos_vision_emb)
            kosmos_vision_features = self.kosmos_vision_model(kosmos_vision_emb)
            kosmos_vision_features = kosmos_vision_features.mean(dim=1)  # Global pooling
        else:
            kosmos_vision_features = torch.zeros(
                kosmos_text_input.size(0), 4096,
                device=kosmos_text_input.device
            )
        
        # CLIP text processing
        if clip_text_input is not None:
            clip_text_emb = self.clip_text_embedding(clip_text_input)
            clip_text_pos = self.clip_text_pos_embedding(
                torch.arange(clip_text_input.size(1), device=clip_text_input.device)
            ).unsqueeze(0)
            clip_text_emb = clip_text_emb + clip_text_pos
            clip_text_emb = self.clip_text_norm(clip_text_emb)
            clip_text_features = self.clip_text_model(clip_text_emb)
            clip_text_features = clip_text_features.mean(dim=1)  # Global pooling
        else:
            clip_text_features = torch.zeros(
                kosmos_vision_input.size(0), 768,
                device=kosmos_vision_input.device
            )
        
        # CLIP vision processing
        if clip_vision_input is not None:
            clip_vision_emb = self.clip_vision_embedding(clip_vision_input)
            clip_vision_pos = self.clip_vision_pos_embedding(
                torch.arange(clip_vision_input.size(1), device=clip_vision_input.device)
            ).unsqueeze(0)
            clip_vision_emb = clip_vision_emb + clip_vision_pos
            clip_vision_emb = self.clip_vision_norm(clip_vision_emb)
            clip_vision_features = self.clip_vision_model(clip_vision_emb)
            clip_vision_features = clip_vision_features.mean(dim=1)  # Global pooling
        else:
            clip_vision_features = torch.zeros(
                kosmos_text_input.size(0), 768,
                device=kosmos_text_input.device
            )
        
        # Feature fusion
        kosmos_features = (kosmos_text_features + kosmos_vision_features) / 2
        clip_features = (clip_text_features + clip_vision_features) / 2
        
        combined_features = torch.cat([kosmos_features, clip_features], dim=-1)
        fused_features = self.feature_fusion(combined_features)
        
        # Add sequence dimension for LSTM
        if fused_features.dim() == 2:
            fused_features = fused_features.unsqueeze(1)  # [batch, 1, features]
        
        # LSTM processing
        lstm_out, hidden = self.lstm(fused_features, hidden)
        
        # Take the last output
        last_output = lstm_out[:, -1, :]
        
        # Action prediction
        actions = self.action_head(last_output)
        
        return actions, hidden

def load_model(checkpoint_path):
    """ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ë¡œë”© ì¤‘: {checkpoint_path}")
    
    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    model = KosmosCLIPHybridModel()
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
    print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤ ìˆ˜: {len(checkpoint.keys())}")
    print(f"ğŸ“‹ ì£¼ìš” í‚¤ë“¤: {list(checkpoint.keys())[:10]}...")
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì‹œë„)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
    elif 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        # ì§ì ‘ ë¡œë“œ ì‹œë„
        model.load_state_dict(checkpoint)
    
    # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, device

def generate_dummy_inputs(batch_size=1, device='cpu'):
    """ë”ë¯¸ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # Kosmos2 inputs
    kosmos_text = torch.randint(0, 32000, (batch_size, 128)).to(device)  # 128 tokens
    kosmos_vision = torch.randn(batch_size, 257, 768).to(device)  # 16x16 + 1 patches
    
    # CLIP inputs
    clip_text = torch.randint(0, 49408, (batch_size, 77)).to(device)  # 77 tokens
    clip_vision = torch.randn(batch_size, 257, 768).to(device)  # 16x16 + 1 patches
    
    return kosmos_text, kosmos_vision, clip_text, clip_vision

def run_inference(model, device, inputs=None):
    """ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if inputs is None:
        inputs = generate_dummy_inputs(device=device)
    
    kosmos_text, kosmos_vision, clip_text, clip_vision = inputs
    
    start_time = time.time()
    
    with torch.no_grad():
        actions, hidden = model(
            kosmos_text_input=kosmos_text,
            kosmos_vision_input=kosmos_vision,
            clip_text_input=clip_text,
            clip_vision_input=clip_vision
        )
    
    inference_time = (time.time() - start_time) * 1000  # ms
    fps = 1000 / inference_time
    
    return actions, hidden, inference_time, fps

def main():
    print("ğŸ¯ Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì¶”ë¡  ë°ëª¨")
    print("=" * 60)
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
    checkpoint_path = './vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth'
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, device = load_model(checkpoint_path)
        
        print("\nğŸš€ ëŒ€í™”í˜• ì¶”ë¡  ëª¨ë“œ ì‹œì‘")
        print("ëª…ë ¹ì–´:")
        print("  'infer': ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰")
        print("  'benchmark': ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (10íšŒ)")
        print("  'continuous': ì—°ì† ì¶”ë¡  ëª¨ë“œ")
        print("  'quit': ì¢…ë£Œ")
        print("-" * 60)
        
        while True:
            try:
                command = input("\nğŸ’¬ ëª…ë ¹ì–´ ì…ë ¥: ").strip().lower()
                
                if command == 'quit':
                    print("ğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                elif command == 'infer':
                    print("ğŸ”„ ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
                    actions, hidden, inference_time, fps = run_inference(model, device)
                    
                    print(f"âœ… ì¶”ë¡  ì™„ë£Œ:")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ms")
                    print(f"   FPS: {fps:.0f}")
                    print(f"   ì•¡ì…˜ ê²°ê³¼: {actions[0].cpu().numpy()}")
                
                elif command == 'benchmark':
                    print("ğŸ”„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ (10íšŒ)...")
                    
                    times = []
                    for i in range(10):
                        _, _, inference_time, _ = run_inference(model, device)
                        times.append(inference_time)
                        print(f"   {i+1}/10 ì™„ë£Œ: {inference_time:.3f}ms")
                    
                    avg_time = sum(times) / len(times)
                    fps = 1000 / avg_time
                    min_time = min(times)
                    max_time = max(times)
                    
                    print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ:")
                    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.3f}ms")
                    print(f"   ìµœì†Œ ì¶”ë¡  ì‹œê°„: {min_time:.3f}ms")
                    print(f"   ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {max_time:.3f}ms")
                    print(f"   í‰ê·  FPS: {fps:.0f}")
                
                elif command == 'continuous':
                    print("ğŸ”„ ì—°ì† ì¶”ë¡  ëª¨ë“œ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ë‹¨)")
                    print("ì‹¤ì‹œê°„ FPS ëª¨ë‹ˆí„°ë§...")
                    
                    try:
                        count = 0
                        start_time = time.time()
                        
                        while True:
                            _, _, inference_time, fps = run_inference(model, device)
                            count += 1
                            
                            if count % 5 == 0:
                                elapsed = time.time() - start_time
                                avg_fps = count / elapsed
                                print(f"   {count}íšŒ ì™„ë£Œ - í˜„ì¬ FPS: {fps:.0f}, í‰ê·  FPS: {avg_fps:.0f}")
                    
                    except KeyboardInterrupt:
                        elapsed = time.time() - start_time
                        avg_fps = count / elapsed
                        print(f"\nâ¹ï¸ ì—°ì† ì¶”ë¡  ì¤‘ë‹¨")
                        print(f"   ì´ ì¶”ë¡  íšŸìˆ˜: {count}")
                        print(f"   ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
                        print(f"   í‰ê·  FPS: {avg_fps:.0f}")
                
                else:
                    print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. 'infer', 'benchmark', 'continuous', 'quit' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
