# 📊 데이터셋 분석 및 과적합 문제 진단

## 🔍 **현재 데이터셋 현황**

### **데이터셋 크기**
- **총 파일 수**: 72개 에피소드
- **샘플 에피소드**: 18개 프레임
- **총 프레임 수**: 72 × 18 = **1,296개 프레임**
- **이미지 해상도**: 720×1280×3
- **액션 차원**: 3D (x, y, z)

### **데이터셋 구조**
```
episode_20250815_122923_2box_vert_left_core_medium.h5
├── images: (18, 720, 1280, 3) - 18개 프레임
├── actions: (18, 3) - 18개 액션
└── action_event_types: (18,) - 18개 텍스트 명령
```

## 🚨 **과적합 문제 진단**

### **1. 모델 복잡도 vs 데이터 크기**

| 모델 | 파라미터 수 | 모델 크기 | 데이터 크기 | 복잡도 비율 |
|------|-------------|-----------|-------------|-------------|
| **Kosmos2+CLIP Hybrid** | ~1.8B | 7.43GB | 1,296 프레임 | **매우 높음** |
| **Enhanced Kosmos2+CLIP** | ~1.8B | 7.17GB | 1,296 프레임 | **매우 높음** |
| **Simple LSTM** | ~1.7B | 6.80GB | 1,296 프레임 | **매우 높음** |
| **Simple CLIP** | ~0.4B | 1.73GB | 1,296 프레임 | **높음** |

### **2. 과적합 증상 분석**

#### **Enhanced Kosmos2+CLIP 모델 (최신)**
```
Epoch 1: Train MAE 0.4264, Val MAE 0.4664 (과적합 시작)
Epoch 2: Train MAE 0.3625, Val MAE 0.3381 (일시적 개선)
Epoch 3: Train MAE 0.3666, Val MAE 0.3044 (최고 성능)
Epoch 4: Train MAE 0.3822, Val MAE 0.3370 (성능 저하)
Epoch 5: Train MAE 0.4121, Val MAE 0.3468 (과적합 진행)
```

**과적합 패턴**:
- Epoch 3에서 최고 성능 달성
- 이후 훈련 손실 증가, 검증 손실 증가
- 명확한 과적합 증상

#### **기존 모델들 비교**
```
Kosmos2+CLIP Hybrid: MAE 0.2121 (10 에포크) - 안정적
Simple LSTM: MAE 0.2469 (15 에포크) - 안정적
Enhanced 모델: MAE 0.3044 (3 에포크) - 불안정
```

## 📊 **데이터 부족 문제 분석**

### **1. 데이터 다양성 부족**
- **시나리오**: 1box, 2box (제한적)
- **위치**: vert_left, vert_right, hori_left, hori_right
- **거리**: close, medium, far
- **총 조합**: 2 × 4 × 3 = **24가지 시나리오**

### **2. 프레임당 데이터 효율성**
- **현재**: 1,296 프레임
- **모델 복잡도**: 1.8B 파라미터
- **비율**: 1,296 / 1,800,000,000 = **0.00007%**
- **권장 비율**: 최소 1% (18,000,000 프레임 필요)

### **3. 데이터 로더 문제**
```python
# 현재 데이터 로더
def __getitem__(self, idx):
    episode = self.episodes[idx]
    frame_idx = np.random.randint(0, len(episode['images']))  # 랜덤 프레임
    # 문제: 같은 에피소드에서 다른 프레임 선택 시 일관성 부족
```

## 🎯 **과적합 해결 방안**

### **1. 즉시 적용 가능한 해결책**

#### **A. 정규화 강화**
```python
# 현재 설정
dropout = 0.1
weight_decay = 1e-5

# 개선된 설정
dropout = 0.3  # 3배 증가
weight_decay = 1e-4  # 10배 증가
gradient_clipping = 0.5  # 더 강한 클리핑
```

#### **B. Early Stopping 적용**
```python
# 검증 손실이 3 에포크 연속 증가하면 학습 중단
patience = 3
min_delta = 0.001
```

#### **C. 학습률 조정**
```python
# 현재: 1e-4 (고정)
# 개선: 5e-5 (더 안정적)
# Warmup + Cosine Annealing 적용
```

### **2. 데이터 증강 강화**

#### **A. 기하학적 변환**
```python
# 현재: 없음
# 추가: 회전, 크기 조정, 이동
rotation_range = 15  # ±15도 회전
scale_range = 0.1    # ±10% 크기 조정
translation_range = 0.05  # ±5% 이동
```

#### **B. 색상 변환**
```python
# 밝기, 대비, 채도 조정
brightness_range = 0.2
contrast_range = 0.2
saturation_range = 0.2
```

#### **C. 노이즈 추가**
```python
# 가우시안 노이즈, 블러 효과
noise_std = 0.01
blur_probability = 0.1
```

### **3. 모델 경량화**

#### **A. 파라미터 감소**
```python
# 현재: 1.8B 파라미터
# 목표: 0.5B 파라미터 (70% 감소)
- Vision Resampler 토큰: 64 → 32
- LSTM 히든 크기: 256 → 128
- 어텐션 헤드: 8 → 4
```

#### **B. 지식 증류**
```python
# 큰 모델 → 작은 모델 지식 전달
teacher_model = Kosmos2+CLIP Hybrid (MAE 0.2121)
student_model = 경량화된 모델
```

## 📈 **데이터셋 확장 계획**

### **1. 단기 목표 (1-2주)**
- **현재**: 72개 에피소드 (1,296 프레임)
- **목표**: 200개 에피소드 (3,600 프레임)
- **증가율**: 2.8배

### **2. 중기 목표 (1개월)**
- **목표**: 500개 에피소드 (9,000 프레임)
- **증가율**: 7배
- **다양성**: 새로운 시나리오 추가

### **3. 장기 목표 (2개월)**
- **목표**: 1000개+ 에피소드 (18,000+ 프레임)
- **증가율**: 14배+
- **품질**: 고품질 라벨링, 검증

## 🎯 **예상 성능 개선**

### **과적합 해결 후 예상 성능**
| 개선 방안 | 현재 MAE | 예상 MAE | 개선율 |
|-----------|----------|----------|--------|
| **정규화 강화** | 0.3044 | 0.25-0.28 | 8-18% |
| **Early Stopping** | 0.3044 | 0.22-0.25 | 18-28% |
| **데이터 증강** | 0.3044 | 0.20-0.23 | 24-34% |
| **모델 경량화** | 0.3044 | 0.18-0.22 | 28-41% |
| **종합 최적화** | 0.3044 | **0.15-0.18** | **41-51%** |

### **데이터셋 확장 후 예상 성능**
| 데이터 크기 | 예상 MAE | 개선율 |
|-------------|----------|--------|
| **현재 (1,296 프레임)** | 0.3044 | 기준 |
| **200 에피소드 (3,600 프레임)** | 0.20-0.25 | 18-34% |
| **500 에피소드 (9,000 프레임)** | 0.15-0.20 | 34-51% |
| **1000+ 에피소드 (18,000+ 프레임)** | **0.10-0.15** | **51-67%** |

## 🚀 **즉시 실행 가능한 개선 방안**

### **1. 정규화 강화 (즉시 적용)**
```python
# 학습 스크립트 수정
dropout = 0.3
weight_decay = 1e-4
gradient_clipping = 0.5
learning_rate = 5e-5
```

### **2. Early Stopping 적용 (즉시 적용)**
```python
# 검증 손실 모니터링
patience = 3
min_delta = 0.001
```

### **3. 데이터 증강 추가 (1-2일)**
```python
# 데이터 로더에 증강 추가
rotation_range = 15
brightness_range = 0.2
noise_std = 0.01
```

### **4. 모델 경량화 (3-5일)**
```python
# 아키텍처 수정
vision_tokens = 32  # 64 → 32
lstm_hidden = 128   # 256 → 128
attention_heads = 4 # 8 → 4
```

이 분석을 바탕으로 체계적인 과적합 해결과 성능 개선을 진행할 수 있습니다!
