"""
ğŸš€ Optimized Hybrid Mobile VLA Model
Final Fixedì™€ Advanced Mobile VLAì˜ ì¥ì ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel
from typing import Optional, Tuple, Dict, Any
import numpy as np

class OptimizedHybridMobileVLAModel(nn.Module):
    """
    ğŸ¯ ìµœì í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
    - Final Fixedì˜ Zì¶• ê°€ì¤‘ì¹˜ ì¡°ì • ì ìš©
    - Advanced Mobile VLAì˜ ê³ ê¸‰ ê¸°ëŠ¥ ì„ íƒì  ì‚¬ìš©
    - ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê°•í™”ëœ ì •ê·œí™”
    """
    
    def __init__(
        self,
        processor,
        vision_dim: int = 1024,
        language_dim: int = 1024,
        action_dim: int = 3,
        hidden_dim: int = 512,
        dropout: float = 0.3,  # ê°•í™”ëœ ë“œë¡­ì•„ì›ƒ
        use_claw_matrix: bool = True,
        use_hierarchical: bool = False,  # ì„ íƒì  ì‚¬ìš©
        use_advanced_attention: bool = True,
        z_axis_weight: float = 0.05,  # Final Fixedì˜ Zì¶• ê°€ì¤‘ì¹˜
        feature_fusion_type: str = "adaptive"  # adaptive, weighted, attention
    ):
        super().__init__()
        
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.z_axis_weight = z_axis_weight
        self.feature_fusion_type = feature_fusion_type
        
        # ê¸°ëŠ¥ ì‚¬ìš© í”Œë˜ê·¸
        self.use_claw_matrix = use_claw_matrix
        self.use_hierarchical = use_hierarchical
        self.use_advanced_attention = use_advanced_attention
        
        # Kosmos2 ëª¨ë¸ ì´ˆê¸°í™”
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # íŒŒë¼ë¯¸í„° requires_grad ì„¤ì •
        for param in self.kosmos.parameters():
            param.requires_grad = True
        
        # ë™ì  feature adapter
        self.feature_adapter = nn.Linear(1024, vision_dim)
        
        # ê°•í™”ëœ ì •ê·œí™” ë ˆì´ì–´
        self.layer_norm_vision = nn.LayerNorm(vision_dim)
        self.layer_norm_language = nn.LayerNorm(language_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë“¤
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        self.dropout_fusion = nn.Dropout(dropout)
        
        # ì„ íƒì  Claw Matrix
        if use_claw_matrix:
            self.claw_matrix = OptimizedClawMatrix(
                vision_dim, language_dim, action_dim, hidden_dim, dropout
            )
        
        # ì„ íƒì  Hierarchical Planning
        if use_hierarchical:
            self.hierarchical_planner = OptimizedHierarchicalPlanner(
                hidden_dim, action_dim, dropout
            )
        
        # ì„ íƒì  Advanced Attention
        if use_advanced_attention:
            self.advanced_attention = OptimizedAdvancedAttention(
                hidden_dim, dropout
            )
        
        # ì ì‘í˜• íŠ¹ì§• ìœµí•©
        if feature_fusion_type == "adaptive":
            self.adaptive_fusion = AdaptiveFeatureFusion(
                vision_dim, language_dim, hidden_dim, dropout
            )
        elif feature_fusion_type == "weighted":
            self.weighted_fusion = WeightedFeatureFusion(
                vision_dim, language_dim, hidden_dim
            )
        
        # ìµœì¢… ì•¡ì…˜ í—¤ë“œ (Final Fixed ìŠ¤íƒ€ì¼)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš©ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ë ˆì´ì–´
        self.z_axis_weight_layer = nn.Parameter(torch.tensor([1.0, 1.0, z_axis_weight]))
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (Final Fixedì™€ Advanced ëª¨ë¸ ì¡°í•©)
        self.ensemble_weight = nn.Parameter(torch.tensor(0.5))
        
    def extract_vision_features(self, images: torch.Tensor) -> torch.Tensor:
        """ì‹œê° íŠ¹ì§• ì¶”ì¶œ (ìµœì í™”ëœ ë²„ì „)"""
        batch_size, num_frames, channels, height, width = images.shape
        
        # ì´ë¯¸ì§€ë¥¼ 2Dë¡œ ë³€í™˜
        images_2d = images.view(-1, channels, height, width)
        
        # Kosmos2 ì²˜ë¦¬
        with torch.no_grad():
            inputs = self.processor(images=images_2d, return_tensors="pt")
            inputs = {k: v.to(images.device) for k, v in inputs.items()}
            
            # pixel_values ì‚¬ìš©
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output
            else:
                vision_outputs = self.kosmos.vision_model(inputs)
                vision_features = vision_outputs.pooler_output
        
        # íŠ¹ì§• ì°¨ì› ì¡°ì •
        vision_features = self.feature_adapter(vision_features)
        
        # ë°°ì¹˜ ì°¨ì› ë³µì›
        vision_features = vision_features.view(batch_size, num_frames, -1)
        
        # ê°•í™”ëœ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, text: str) -> torch.Tensor:
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ (ìµœì í™”ëœ ë²„ì „)"""
        inputs = self.processor(text=text, return_tensors="pt")
        inputs = {k: v.to(next(self.parameters()).device) for k, v in inputs.items()}
        
        with torch.no_grad():
            language_outputs = self.kosmos.text_model(**inputs)
            language_features = language_outputs.last_hidden_state.mean(dim=1)
        
        # ê°•í™”ëœ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(
        self,
        images: torch.Tensor,
        text: str,
        distance_labels: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """ìˆœì „íŒŒ (ìµœì í™”ëœ ë²„ì „)"""
        
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(images)
        language_features = self.extract_language_features(text)
        
        # ì ì‘í˜• íŠ¹ì§• ìœµí•©
        if hasattr(self, 'adaptive_fusion'):
            fused_features = self.adaptive_fusion(vision_features, language_features)
        elif hasattr(self, 'weighted_fusion'):
            fused_features = self.weighted_fusion(vision_features, language_features)
        else:
            # ê¸°ë³¸ ìœµí•©
            fused_features = torch.cat([vision_features.mean(dim=1), language_features], dim=-1)
            fused_features = self.layer_norm_fusion(fused_features)
            fused_features = self.dropout_fusion(fused_features)
        
        # ì„ íƒì  ê³ ê¸‰ ê¸°ëŠ¥ ì ìš©
        if self.use_claw_matrix and hasattr(self, 'claw_matrix'):
            fused_features = self.claw_matrix(fused_features)
        
        if self.use_advanced_attention and hasattr(self, 'advanced_attention'):
            fused_features = self.advanced_attention(fused_features)
        
        # ìµœì¢… ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.action_head(fused_features)
        
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš© (Final Fixed ìŠ¤íƒ€ì¼)
        actions = actions * self.z_axis_weight_layer.unsqueeze(0)
        
        return actions

class OptimizedClawMatrix(nn.Module):
    """ìµœì í™”ëœ Claw Matrix (ê³¼ì í•© ë°©ì§€)"""
    
    def __init__(self, vision_dim, language_dim, action_dim, hidden_dim, dropout):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # ê²½ëŸ‰í™”ëœ Claw Matrix
        self.claw_fusion = nn.Sequential(
            nn.Linear(vision_dim + language_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # ì •ê·œí™”
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, features):
        return self.layer_norm(self.claw_fusion(features))

class OptimizedHierarchicalPlanner(nn.Module):
    """ìµœì í™”ëœ Hierarchical Planning (ì„ íƒì  ì‚¬ìš©)"""
    
    def __init__(self, hidden_dim, action_dim, dropout):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.dropout = dropout
        
        # ê²½ëŸ‰í™”ëœ ê³„ì¸µì  ê³„íš
        self.goal_decomposition = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim)
        )
        
        self.action_generator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, action_dim)
        )
        
    def forward(self, features):
        goals = self.goal_decomposition(features)
        actions = self.action_generator(goals)
        return actions

class OptimizedAdvancedAttention(nn.Module):
    """ìµœì í™”ëœ Advanced Attention"""
    
    def __init__(self, hidden_dim, dropout):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # ê²½ëŸ‰í™”ëœ ì–´í…ì…˜
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, features):
        # ìê¸° ì–´í…ì…˜ ì ìš©
        attended_features, _ = self.attention(features, features, features)
        return self.layer_norm(features + attended_features)

class AdaptiveFeatureFusion(nn.Module):
    """ì ì‘í˜• íŠ¹ì§• ìœµí•©"""
    
    def __init__(self, vision_dim, language_dim, hidden_dim, dropout):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # ì ì‘í˜• ê°€ì¤‘ì¹˜
        self.adaptive_weights = nn.Sequential(
            nn.Linear(vision_dim + language_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2),  # vision, language ê°€ì¤‘ì¹˜
            nn.Softmax(dim=-1)
        )
        
        # ìœµí•© ë ˆì´ì–´
        self.fusion_layer = nn.Sequential(
            nn.Linear(vision_dim + language_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
    def forward(self, vision_features, language_features):
        # ì‹œê° íŠ¹ì§• í‰ê· 
        vision_avg = vision_features.mean(dim=1)
        
        # ì ì‘í˜• ê°€ì¤‘ì¹˜ ê³„ì‚°
        combined = torch.cat([vision_avg, language_features], dim=-1)
        weights = self.adaptive_weights(combined)
        
        # ê°€ì¤‘ ìœµí•©
        weighted_vision = vision_avg * weights[:, 0:1]
        weighted_language = language_features * weights[:, 1:2]
        
        # ìµœì¢… ìœµí•©
        fused = torch.cat([weighted_vision, weighted_language], dim=-1)
        return self.fusion_layer(fused)

class WeightedFeatureFusion(nn.Module):
    """ê°€ì¤‘ íŠ¹ì§• ìœµí•©"""
    
    def __init__(self, vision_dim, language_dim, hidden_dim):
        super().__init__()
        
        self.vision_weight = nn.Parameter(torch.tensor(0.6))
        self.language_weight = nn.Parameter(torch.tensor(0.4))
        
        self.fusion_layer = nn.Linear(vision_dim + language_dim, hidden_dim)
        
    def forward(self, vision_features, language_features):
        vision_avg = vision_features.mean(dim=1)
        
        # ê°€ì¤‘ ìœµí•©
        weighted_vision = vision_avg * self.vision_weight
        weighted_language = language_features * self.language_weight
        
        fused = torch.cat([weighted_vision, weighted_language], dim=-1)
        return self.fusion_layer(fused)

class HybridEnsembleModel(nn.Module):
    """Final Fixedì™€ Advanced ëª¨ë¸ì˜ ì•™ìƒë¸”"""
    
    def __init__(self, final_fixed_model, advanced_model, ensemble_weight=0.5):
        super().__init__()
        
        self.final_fixed_model = final_fixed_model
        self.advanced_model = advanced_model
        self.ensemble_weight = nn.Parameter(torch.tensor(ensemble_weight))
        
    def forward(self, images, text, distance_labels=None):
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡
        final_fixed_pred = self.final_fixed_model(images, text, distance_labels)
        advanced_pred = self.advanced_model(images, text, distance_labels)
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì ìš©
        ensemble_pred = (
            self.ensemble_weight * final_fixed_pred + 
            (1 - self.ensemble_weight) * advanced_pred
        )
        
        return ensemble_pred

# ğŸ¯ ìµœì í™”ëœ í›ˆë ¨ í•¨ìˆ˜
def train_optimized_hybrid_model(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,  # ë‚®ì€ í•™ìŠµë¥ 
    weight_decay=1e-4,   # ê°•í™”ëœ ì •ê·œí™”
    early_stopping_patience=5,
    device='cuda'
):
    """ìµœì í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í›ˆë ¨"""
    
    model = model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì € (AdamW with ê°•í™”ëœ ì •ê·œí™”)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Cosine Annealing)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # ì†ì‹¤ í•¨ìˆ˜ (Final Fixed ìŠ¤íƒ€ì¼)
    def compute_loss(predicted_actions, target_actions):
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš©
        z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
        weighted_target = target_actions * z_weight.unsqueeze(0).unsqueeze(0)
        weighted_pred = predicted_actions * z_weight.unsqueeze(0).unsqueeze(0)
        
        return F.mse_loss(weighted_pred, weighted_target)
    
    # ì¡°ê¸° ì¢…ë£Œ
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0.0
        
        for batch in train_loader:
            images, actions, distance_labels = batch
            images = images.float().to(device)
            actions = actions.long().to(device)
            
            optimizer.zero_grad()
            
            # ì˜ˆì¸¡
            predicted_actions = model(images, "Navigate to target", distance_labels)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = compute_loss(predicted_actions, actions.float())
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += loss.item()
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                images, actions, distance_labels = batch
                images = images.float().to(device)
                actions = actions.long().to(device)
                
                predicted_actions = model(images, "Navigate to target", distance_labels)
                loss = compute_loss(predicted_actions, actions.float())
                val_loss += loss.item()
        
        # í‰ê·  ì†ì‹¤
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step()
        
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        print(f"Learning Rate: {scheduler.get_last_lr()[0]:.6f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': {
                    'learning_rate': learning_rate,
                    'weight_decay': weight_decay,
                    'dropout': model.dropout,
                    'z_axis_weight': model.z_axis_weight
                }
            }, 'optimized_hybrid_model_best.pth')
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
    
    return model

# ğŸš€ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ì´ˆê¸°í™”
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    model = OptimizedHybridMobileVLAModel(
        processor=processor,
        dropout=0.3,  # ê°•í™”ëœ ë“œë¡­ì•„ì›ƒ
        use_claw_matrix=True,
        use_hierarchical=False,  # ì„ íƒì  ì‚¬ìš©
        use_advanced_attention=True,
        z_axis_weight=0.05,  # Final Fixed ìŠ¤íƒ€ì¼
        feature_fusion_type="adaptive"
    )
    
    print("ğŸš€ Optimized Hybrid Mobile VLA Model ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ¯ ì‚¬ìš© ê¸°ëŠ¥:")
    print(f"   - Claw Matrix: {model.use_claw_matrix}")
    print(f"   - Hierarchical Planning: {model.use_hierarchical}")
    print(f"   - Advanced Attention: {model.use_advanced_attention}")
    print(f"   - Zì¶• ê°€ì¤‘ì¹˜: {model.z_axis_weight}")
    print(f"   - ë“œë¡­ì•„ì›ƒ: {model.dropout}")
