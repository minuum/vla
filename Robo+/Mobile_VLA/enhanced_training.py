#!/usr/bin/env python3
"""
ğŸ¤– Enhanced Mobile VLA Training with Data Augmentation & Action Normalization
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import numpy as np
import json
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, r2_score
from transformers import AutoProcessor, AutoModel
import torchvision.transforms as transforms
from typing import Dict, List, Tuple, Optional
import gc

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path("/home/billy/25-1kp/vla/Robo+/Mobile_VLA")
DATA_DIR = Path("/home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset")
ROBOVLMS_DIR = Path("/home/billy/25-1kp/vla/RoboVLMs")

import sys
sys.path.append(str(ROOT_DIR))
sys.path.append(str(ROBOVLMS_DIR))

from robovlms.data.mobile_vla_dataset import MobileVLADataset
from robovlms.train.mobile_vla_trainer import MobileVLATrainer

class EnhancedMobileVLATrainer:
    """í–¥ìƒëœ Mobile VLA íŠ¸ë ˆì´ë„ˆ - ë°ì´í„° ì¦ê°•, ì•¡ì…˜ ì •ê·œí™”, ê°œì„ ëœ ê²€ì¦ í¬í•¨"""
    
    def __init__(
        self,
        model_name: str = "microsoft/kosmos-2-patch14-224",
        action_dim: int = 3,
        window_size: int = 8,
        chunk_size: int = 2,
        learning_rate: float = 1e-4,
        num_epochs: int = 20,
        batch_size: int = 1,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        augmentation_multiplier: float = 3.0
    ):
        self.model_name = model_name
        self.action_dim = action_dim
        self.window_size = window_size
        self.chunk_size = chunk_size
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.device = device
        self.augmentation_multiplier = augmentation_multiplier
        
        # ì•¡ì…˜ ì •ê·œí™” í†µê³„ (ë‚˜ì¤‘ì— ê³„ì‚°)
        self.action_mean = None
        self.action_std = None
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_model()
        
        # ë°ì´í„° ì¦ê°• ì„¤ì •
        self._setup_augmentation()
        
        print(f"âœ… EnhancedMobileVLATrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   ë°ì´í„° ì¦ê°• ë°°ìˆ˜: {self.augmentation_multiplier}x")
        print(f"   ì•¡ì…˜ ì°¨ì›: {self.action_dim}")
        
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        # Kosmos 2B ëª¨ë¸ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(self.model_name)
        self.vision_model = AutoModel.from_pretrained(self.model_name)
        
        # ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ (Kosmos2ëŠ” text_config.hidden_size ì‚¬ìš©)
        try:
            hidden_size = self.vision_model.config.hidden_size
        except AttributeError:
            try:
                hidden_size = self.vision_model.config.text_config.hidden_size
            except AttributeError:
                hidden_size = 2048  # ê¸°ë³¸ê°’
        
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, self.action_dim)
        )
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.vision_model.to(self.device)
        self.action_head.to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        self.optimizer = optim.AdamW([
            {'params': self.vision_model.parameters(), 'lr': self.learning_rate * 0.1},
            {'params': self.action_head.parameters(), 'lr': self.learning_rate}
        ], weight_decay=0.01)
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=self.num_epochs, eta_min=self.learning_rate * 0.01
        )
        
        # ì†ì‹¤ í•¨ìˆ˜ (ê°€ì¤‘ Huber Loss)
        self.criterion = nn.HuberLoss(delta=0.1)
        
    def _setup_augmentation(self):
        """ë°ì´í„° ì¦ê°• ì„¤ì •"""
        self.augmentation_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.3),
            transforms.RandomRotation(degrees=5),
            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),
            transforms.RandomResizedCrop(224, scale=(0.9, 1.0), ratio=(0.9, 1.1)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.test_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
    def compute_action_statistics(self, dataset: MobileVLADataset) -> Tuple[torch.Tensor, torch.Tensor]:
        """ì•¡ì…˜ ë°ì´í„°ì˜ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š ì•¡ì…˜ í†µê³„ ê³„ì‚° ì¤‘...")
        
        all_actions = []
        for i in range(len(dataset)):
            episode = dataset[i]
            actions = episode['actions']
            # numpy ë°°ì—´ì„ tensorë¡œ ë³€í™˜
            if isinstance(actions, np.ndarray):
                actions = torch.from_numpy(actions)
            all_actions.append(actions)
        
        all_actions = torch.cat(all_actions, dim=0)  # [N, 3]
        
        action_mean = all_actions.mean(dim=0)  # [3]
        action_std = all_actions.std(dim=0)    # [3]
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        action_std = torch.clamp(action_std, min=1e-6)
        
        print(f"   ì•¡ì…˜ ë²”ìœ„: {all_actions.min():.4f} ~ {all_actions.max():.4f}")
        print(f"   ì•¡ì…˜ í‰ê· : {action_mean}")
        print(f"   ì•¡ì…˜ í‘œì¤€í¸ì°¨: {action_std}")
        
        return action_mean, action_std
        
    def normalize_actions(self, actions: torch.Tensor) -> torch.Tensor:
        """ì•¡ì…˜ ì •ê·œí™”"""
        if self.action_mean is None or self.action_std is None:
            return actions
        return (actions - self.action_mean.to(actions.device)) / self.action_std.to(actions.device)
        
    def denormalize_actions(self, actions: torch.Tensor) -> torch.Tensor:
        """ì•¡ì…˜ ì—­ì •ê·œí™”"""
        if self.action_mean is None or self.action_std is None:
            return actions
        return actions * self.action_std.to(actions.device) + self.action_mean.to(actions.device)
        
    def augment_batch(self, batch: Dict) -> Dict:
        """ë°°ì¹˜ ë°ì´í„° ì¦ê°•"""
        images = batch['images']  # [T, C, H, W] ë˜ëŠ” [B, T, C, H, W]
        actions = batch['actions']  # [T, 3] ë˜ëŠ” [B, T, 3]
        
        # numpy ë°°ì—´ì„ tensorë¡œ ë³€í™˜
        if isinstance(images, np.ndarray):
            images = torch.from_numpy(images)
        if isinstance(actions, np.ndarray):
            actions = torch.from_numpy(actions)
        
        # ë°°ì¹˜ ì°¨ì›ì´ ì—†ëŠ” ê²½ìš° ì¶”ê°€
        if len(images.shape) == 4:  # [T, C, H, W]
            images = images.unsqueeze(0)  # [1, T, C, H, W]
            actions = actions.unsqueeze(0)  # [1, T, 3]
        
        batch_size, seq_len = images.shape[:2]
        
        # ì¦ê°•ëœ ì´ë¯¸ì§€ ìƒì„±
        augmented_images = []
        for b in range(batch_size):
            seq_images = []
            for t in range(seq_len):
                img = images[b, t]  # [C, H, W]
                
                # ì´ë¯¸ì§€ ì°¨ì› í™•ì¸ ë° ìˆ˜ì •
                if len(img.shape) == 2:  # [H, W] -> [C, H, W]
                    img = img.unsqueeze(0).repeat(3, 1, 1)
                elif len(img.shape) == 3 and img.shape[0] == 1:  # [1, H, W] -> [C, H, W]
                    img = img.repeat(3, 1, 1)
                
                # ì¦ê°• ì ìš©
                if torch.rand(1) < 0.7:  # 70% í™•ë¥ ë¡œ ì¦ê°•
                    img = self.augmentation_transforms(img)
                else:
                    img = self.test_transforms(img)
                seq_images.append(img)
            augmented_images.append(torch.stack(seq_images))
        
        augmented_images = torch.stack(augmented_images)
        
        return {
            'images': augmented_images,
            'actions': actions,
            'task_description': batch['task_description'],
            'scenario': batch['scenario']
        }
        
    def train_step(self, batch: Dict) -> Dict:
        """í•™ìŠµ ìŠ¤í…"""
        self.vision_model.train()
        self.action_head.train()
        
        # ë°ì´í„° ì¦ê°•
        batch = self.augment_batch(batch)
        
        # ë°ì´í„° ì¤€ë¹„
        images = batch['images'].to(self.device)  # [B, T, C, H, W]
        actions = batch['actions'].to(self.device)  # [B, T, 3]
        
        # ì•¡ì…˜ ì •ê·œí™”
        actions_normalized = self.normalize_actions(actions)
        
        batch_size, seq_len = images.shape[:2]
        
        # Window/Chunk ë¶„í• 
        if seq_len >= self.window_size + self.chunk_size:
            window_images = images[:, :self.window_size]  # [B, W, C, H, W]
            target_actions = actions_normalized[:, self.window_size:self.window_size + self.chunk_size]  # [B, C, 3]
        else:
            window_images = images[:, :min(seq_len, self.window_size)]
            target_actions = actions_normalized[:, -self.chunk_size:] if seq_len >= self.chunk_size else actions_normalized
        
        # ì˜ˆì¸¡
        predictions = []
        for t in range(window_images.shape[1]):
            img = window_images[:, t]  # [B, C, H, W]
            
            # Vision ëª¨ë¸ (ë”ë¯¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì¶”ê°€)
            with torch.no_grad():
                batch_size = img.shape[0]
                dummy_text = ["<image>"] * batch_size  # ë”ë¯¸ í…ìŠ¤íŠ¸
                
                # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
                text_inputs = self.processor(
                    text=dummy_text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(self.device)
                
                vision_outputs = self.vision_model(
                    pixel_values=img,
                    input_ids=text_inputs['input_ids'],
                    attention_mask=text_inputs['attention_mask']
                )
                vision_features = vision_outputs.last_hidden_state[:, 0]  # [B, hidden_size]
            
            # ì•¡ì…˜ ì˜ˆì¸¡
            action_pred = self.action_head(vision_features)  # [B, 3]
            predictions.append(action_pred)
        
        # ì‹œí€€ìŠ¤ í‰ê· 
        predictions = torch.stack(predictions, dim=1)  # [B, T, 3]
        predictions = predictions.mean(dim=1, keepdim=True).expand(-1, target_actions.shape[1], -1)  # [B, C, 3]
        
        # ì†ì‹¤ ê³„ì‚°
        loss = self.criterion(predictions, target_actions)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.vision_model.parameters(), max_norm=1.0)
        torch.nn.utils.clip_grad_norm_(self.action_head.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        with torch.no_grad():
            predictions_denorm = self.denormalize_actions(predictions)
            target_actions_denorm = self.denormalize_actions(target_actions)
            
            mae = torch.mean(torch.abs(predictions_denorm - target_actions_denorm))
            mae_linear_x = torch.mean(torch.abs(predictions_denorm[:, :, 0] - target_actions_denorm[:, :, 0]))
            mae_linear_y = torch.mean(torch.abs(predictions_denorm[:, :, 1] - target_actions_denorm[:, :, 1]))
            mae_angular_z = torch.mean(torch.abs(predictions_denorm[:, :, 2] - target_actions_denorm[:, :, 2]))
        
        return {
            'total_loss': loss.item(),
            'mae_avg': mae.item(),
            'mae_linear_x': mae_linear_x.item(),
            'mae_linear_y': mae_linear_y.item(),
            'mae_angular_z': mae_angular_z.item()
        }
        
    def evaluate_model(self, dataloader: DataLoader) -> Tuple[Dict, List, List]:
        """ëª¨ë¸ í‰ê°€"""
        self.vision_model.eval()
        self.action_head.eval()
        
        all_predictions = []
        all_targets = []
        total_loss = 0
        total_mae = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in dataloader:
                # ë°ì´í„° ì¤€ë¹„ (ì¦ê°• ì—†ìŒ)
                images = batch['images'].to(self.device)
                actions = batch['actions'].to(self.device)
                actions_normalized = self.normalize_actions(actions)
                
                batch_size, seq_len = images.shape[:2]
                
                # Window/Chunk ë¶„í• 
                if seq_len >= self.window_size + self.chunk_size:
                    window_images = images[:, :self.window_size]
                    target_actions = actions_normalized[:, self.window_size:self.window_size + self.chunk_size]
                else:
                    window_images = images[:, :min(seq_len, self.window_size)]
                    target_actions = actions_normalized[:, -self.chunk_size:] if seq_len >= self.chunk_size else actions_normalized
                
                # ì˜ˆì¸¡
                predictions = []
                for t in range(window_images.shape[1]):
                    img = window_images[:, t]
                    
                    # ë”ë¯¸ í…ìŠ¤íŠ¸ ì…ë ¥ ì¶”ê°€
                    batch_size = img.shape[0]
                    dummy_text = ["<image>"] * batch_size
                    
                    # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
                    text_inputs = self.processor(
                        text=dummy_text,
                        return_tensors="pt",
                        padding=True,
                        truncation=True
                    ).to(self.device)
                    
                    vision_outputs = self.vision_model(
                        pixel_values=img,
                        input_ids=text_inputs['input_ids'],
                        attention_mask=text_inputs['attention_mask']
                    )
                    vision_features = vision_outputs.last_hidden_state[:, 0]
                    action_pred = self.action_head(vision_features)
                    predictions.append(action_pred)
                
                predictions = torch.stack(predictions, dim=1)
                predictions = predictions.mean(dim=1, keepdim=True).expand(-1, target_actions.shape[1], -1)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = self.criterion(predictions, target_actions)
                
                # ì—­ì •ê·œí™”
                predictions_denorm = self.denormalize_actions(predictions)
                target_actions_denorm = self.denormalize_actions(target_actions)
                
                # ë©”íŠ¸ë¦­
                mae = torch.mean(torch.abs(predictions_denorm - target_actions_denorm))
                
                total_loss += loss.item()
                total_mae += mae.item()
                num_batches += 1
                
                # ì˜ˆì¸¡/íƒ€ê²Ÿ ì €ì¥
                all_predictions.extend(predictions_denorm.cpu().numpy().reshape(-1, 3))
                all_targets.extend(target_actions_denorm.cpu().numpy().reshape(-1, 3))
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / num_batches
        avg_mae = total_mae / num_batches
        
        # RÂ² ê³„ì‚°
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        r2_linear_x = r2_score(all_targets[:, 0], all_predictions[:, 0])
        r2_linear_y = r2_score(all_targets[:, 1], all_predictions[:, 1])
        r2_angular_z = r2_score(all_targets[:, 2], all_predictions[:, 2])
        
        # ì„ê³„ê°’ ì •í™•ë„
        threshold_0_1 = np.mean(np.all(np.abs(all_predictions - all_targets) < 0.1, axis=1))
        
        return {
            'loss': avg_loss,
            'mae_avg': avg_mae,
            'r2_linear_x': r2_linear_x,
            'r2_linear_y': r2_linear_y,
            'r2_angular_z': r2_angular_z,
            'threshold_0.1': threshold_0_1
        }, all_predictions, all_targets
        
    def save_checkpoint(self, path: str, epoch: int, metrics: Dict):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': {
                'vision_model': self.vision_model.state_dict(),
                'action_head': self.action_head.state_dict()
            },
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'action_mean': self.action_mean,
            'action_std': self.action_std,
            'metrics': metrics,
            'config': {
                'model_name': self.model_name,
                'action_dim': self.action_dim,
                'window_size': self.window_size,
                'chunk_size': self.chunk_size,
                'learning_rate': self.learning_rate
            }
        }, path)
        
    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        
        self.vision_model.load_state_dict(checkpoint['model_state_dict']['vision_model'])
        self.action_head.load_state_dict(checkpoint['model_state_dict']['action_head'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.action_mean = checkpoint['action_mean']
        self.action_std = checkpoint['action_std']
        
        return checkpoint['epoch'], checkpoint['metrics']

def custom_collate_fn(batch):
    """PIL ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ì»¤ìŠ¤í…€ collate í•¨ìˆ˜"""
    # ë°°ì¹˜ í¬ê¸°ê°€ 1ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì²˜ë¦¬
    sample = batch[0]
    
    # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
    if 'images' in sample:
        images = sample['images']
        if isinstance(images, list) and len(images) > 0 and hasattr(images[0], 'convert'):
            # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            import torchvision.transforms as transforms
            to_tensor = transforms.ToTensor()
            tensor_images = []
            for img in images:
                if hasattr(img, 'convert'):
                    img = img.convert('RGB')
                tensor_images.append(to_tensor(img))
            sample['images'] = torch.stack(tensor_images)
    
    return sample

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Enhanced Mobile VLA Training ì‹œì‘!")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = MobileVLADataset(DATA_DIR)
    print(f"   ì´ ì—í”¼ì†Œë“œ: {len(dataset)}ê°œ")
    
    # ì•¡ì…˜ í†µê³„ ê³„ì‚°
    trainer = EnhancedMobileVLATrainer()
    trainer.action_mean, trainer.action_std = trainer.compute_action_statistics(dataset)
    
    # ë°ì´í„° ë¶„í•  (ì‹œê°„ ê¸°ë°˜)
    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size
    
    # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í•  (ë‚˜ì¤‘ ì—í”¼ì†Œë“œë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ)
    train_dataset = torch.utils.data.Subset(dataset, range(train_size))
    val_dataset = torch.utils.data.Subset(dataset, range(train_size, total_size))
    
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    
    # DataLoader ìƒì„± (ì»¤ìŠ¤í…€ collate_fn ì‚¬ìš©)
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)
    
    # í•™ìŠµ ë£¨í”„
    print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
    best_val_loss = float('inf')
    train_history = []
    val_history = []
    
    for epoch in range(trainer.num_epochs):
        print(f"\nğŸ“ˆ ì—í¬í¬ {epoch+1}/{trainer.num_epochs}")
        print("-" * 40)
        
        # í•™ìŠµ
        epoch_losses = []
        epoch_maes = []
        
        for step, batch in enumerate(train_loader):
            metrics = trainer.train_step(batch)
            epoch_losses.append(metrics['total_loss'])
            epoch_maes.append(metrics['mae_avg'])
            
            if (step + 1) % 10 == 0:
                print(f"   ë°°ì¹˜ {step+1}/{len(train_loader)}: Loss={metrics['total_loss']:.4f}, MAE={metrics['mae_avg']:.4f}")
        
        train_metrics = {
            'total_loss': np.mean(epoch_losses),
            'mae_avg': np.mean(epoch_maes)
        }
        train_history.append(train_metrics)
        
        # ê²€ì¦
        val_metrics, _, _ = trainer.evaluate_model(val_loader)
        val_history.append(val_metrics)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… í•™ìŠµ ì™„ë£Œ:")
        print(f"   Loss: {train_metrics['total_loss']:.4f}")
        print(f"   MAE: {train_metrics['mae_avg']:.4f}")
        
        print(f"ğŸ” ê²€ì¦ ê²°ê³¼:")
        print(f"   Loss: {val_metrics['loss']:.4f}")
        print(f"   MAE: {val_metrics['mae_avg']:.4f}")
        print(f"   RÂ² Linear X: {val_metrics['r2_linear_x']:.4f}")
        print(f"   RÂ² Linear Y: {val_metrics['r2_linear_y']:.4f}")
        print(f"   RÂ² Angular Z: {val_metrics['r2_angular_z']:.4f}")
        print(f"   ì„ê³„ê°’ ì •í™•ë„ (0.1): {val_metrics['threshold_0.1']:.4f}")
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
        trainer.scheduler.step()
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            trainer.save_checkpoint('best_enhanced_model.pth', epoch + 1, val_metrics)
            print(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ë¨ (Loss: {best_val_loss:.4f})")
    
    # ìµœì¢… í‰ê°€
    print("\nğŸ¯ ìµœì¢… í‰ê°€...")
    try:
        trainer.load_checkpoint('best_enhanced_model.pth')
        final_metrics, final_preds, final_targets = trainer.evaluate_model(val_loader)
        
        print(f"\nğŸ† ìµœì¢… ì„±ëŠ¥:")
        print(f"   ì „ì²´ MAE: {final_metrics['mae_avg']:.4f}")
        print(f"   ì„ê³„ê°’ ì •í™•ë„ (0.1): {final_metrics['threshold_0.1']:.4f}")
        print(f"   Linear X RÂ²: {final_metrics['r2_linear_x']:.4f}")
        print(f"   Linear Y RÂ²: {final_metrics['r2_linear_y']:.4f}")
        print(f"   Angular Z RÂ²: {final_metrics['r2_angular_z']:.4f}")
        
    except FileNotFoundError:
        print("âŒ ìµœê³  ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        final_metrics = val_metrics
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'final_metrics': final_metrics,
        'train_history': train_history,
        'val_history': val_history,
        'action_statistics': {
            'mean': trainer.action_mean.tolist() if trainer.action_mean is not None else None,
            'std': trainer.action_std.tolist() if trainer.action_std is not None else None
        },
        'dataset_info': {
            'total_episodes': len(dataset),
            'train_episodes': len(train_dataset),
            'val_episodes': len(val_dataset),
            'augmentation_multiplier': trainer.augmentation_multiplier
        },
        'model_info': {
            'architecture': 'Enhanced Kosmos 2B + Action Head',
            'loss_function': 'Huber Loss',
            'optimizer': 'AdamW with Cosine Annealing',
            'epochs': trainer.num_epochs,
            'learning_rate': trainer.learning_rate,
            'action_normalization': True,
            'data_augmentation': True
        },
        'created_at': datetime.now().isoformat()
    }
    
    with open('enhanced_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: enhanced_training_results.json")
    print("ğŸ‰ í–¥ìƒëœ í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
