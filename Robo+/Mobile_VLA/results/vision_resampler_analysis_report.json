{
  "model_comparison": [
    {
      "구분": "Vision Resampler 적용 모델",
      "모델명": "Enhanced 2D with Vision Resampler",
      "Vision Resampler": "✅ SimpleVisionResampler 적용",
      "Vision 처리 방식": "1. Kosmos2 vision_model → pooler_output\n2. feature_adapter로 차원 조정\n3. SimpleVisionResampler로 토큰 압축\n4. LayerNorm + Dropout",
      "Vision Resampler 구조": "• Learnable latents (64개)\n• MultiheadAttention (8 heads)\n• Cross-attention + Self-attention\n• Feed-forward network\n• 최종: latents.mean(dim=1)",
      "액션 차원": "2D (linear_x, linear_y)",
      "MAE 성능": "0.804 (가장 높음)",
      "정확도 (10% 임계값)": "0.0% (가장 낮음)",
      "샘플 수": "15개 (검증셋)",
      "주요 차이점": "• Vision Resampler로 인한 정보 손실\n• 복잡한 attention 메커니즘\n• 추가적인 파라미터들"
    },
    {
      "구분": "기존 2D 최적화 모델",
      "모델명": "Optimized 2D Action Model",
      "Vision Resampler": "❌ Vision Resampler 없음",
      "Vision 처리 방식": "1. Kosmos2 vision_model → pooler_output\n2. feature_adapter로 차원 조정\n3. LayerNorm + Dropout (직접)",
      "Vision Resampler 구조": "• 없음 (직접 특징 사용)",
      "액션 차원": "2D (linear_x, linear_y)",
      "MAE 성능": "0.292 (가장 낮음)",
      "정확도 (10% 임계값)": "24.8% (중간)",
      "샘플 수": "1224개 (전체)",
      "주요 차이점": "• 직접적인 특징 사용\n• 단순한 구조\n• 효율적인 처리"
    },
    {
      "구분": "기존 3D 모델들",
      "모델명": "Realistic, No First Frame 등",
      "Vision Resampler": "❌ Vision Resampler 없음",
      "Vision 처리 방식": "1. Kosmos2 vision_model → pooler_output\n2. feature_adapter로 차원 조정\n3. LayerNorm + Dropout (직접)",
      "Vision Resampler 구조": "• 없음 (직접 특징 사용)",
      "액션 차원": "3D (linear_x, linear_y, angular_z)",
      "MAE 성능": "0.001~0.576 (중간)",
      "정확도 (10% 임계값)": "48.9%~100% (높음)",
      "샘플 수": "15개 (검증셋)",
      "주요 차이점": "• 3D 액션으로 더 많은 정보"
    }
  ],
  "mae_comparison": [
    {
      "모델": "Enhanced 2D with Vision Resampler",
      "MAE 계산 방식": "torch.mean(torch.abs(predictions - actions))",
      "계산 코드 위치": "evaluate_enhanced_model.py line 95",
      "정확도 계산 방식": "torch.all(torch.abs(predictions - actions) < threshold, dim=1)",
      "데이터 정규화": "이미지: [-1,1] → [0,1] 정규화\n액션: 원본 값 사용",
      "평가 데이터셋": "검증셋 15개 샘플",
      "계산 결과": "MAE: 0.804\n정확도: 0.0%"
    },
    {
      "모델": "Optimized 2D Action Model",
      "MAE 계산 방식": "nn.functional.l1_loss(predictions, actions)",
      "계산 코드 위치": "evaluate_optimized_2d_model.py line 118",
      "정확도 계산 방식": "torch.all(torch.abs(predictions - actions) < threshold, dim=1)",
      "데이터 정규화": "이미지: [-1,1] → [0,1] 정규화\n액션: 원본 값 사용",
      "평가 데이터셋": "전체 데이터셋 1224개 샘플",
      "계산 결과": "MAE: 0.292\n정확도: 24.8%"
    },
    {
      "모델": "Realistic Models",
      "MAE 계산 방식": "torch.mean(torch.abs(predictions - actions))",
      "계산 코드 위치": "fixed_evaluation_with_real_data.py",
      "정확도 계산 방식": "torch.all(torch.abs(predictions - actions) < threshold, dim=1)",
      "데이터 정규화": "이미지: [-1,1] → [0,1] 정규화\n액션: 원본 값 사용",
      "평가 데이터셋": "검증셋 15개 샘플",
      "계산 결과": "MAE: 0.001~0.576\n정확도: 48.9%~100%"
    }
  ],
  "impact_analysis": [
    {
      "Vision Resampler 단계": "1. 입력 이미지 처리",
      "기존 모델 처리": "동일",
      "Vision Resampler 모델 처리": "동일",
      "정보 손실 가능성": "없음",
      "계산 복잡도": "낮음",
      "파라미터 수": "기본"
    },
    {
      "Vision Resampler 단계": "2. Kosmos2 vision_model",
      "기존 모델 처리": "동일",
      "Vision Resampler 모델 처리": "동일",
      "정보 손실 가능성": "없음",
      "계산 복잡도": "낮음",
      "파라미터 수": "기본"
    },
    {
      "Vision Resampler 단계": "3. feature_adapter",
      "기존 모델 처리": "동일",
      "Vision Resampler 모델 처리": "동일",
      "정보 손실 가능성": "없음",
      "계산 복잡도": "낮음",
      "파라미터 수": "기본"
    },
    {
      "Vision Resampler 단계": "4. Vision Resampler 적용",
      "기존 모델 처리": "❌ 없음 (직접 사용)",
      "Vision Resampler 모델 처리": "✅ SimpleVisionResampler 적용",
      "정보 손실 가능성": "❌ 높음 (64개 latents로 압축)",
      "계산 복잡도": "❌ 높음 (attention + FFN)",
      "파라미터 수": "❌ 증가 (latents + attention + FFN)"
    },
    {
      "Vision Resampler 단계": "5. 최종 특징 출력",
      "기존 모델 처리": "vision_features 직접 사용",
      "Vision Resampler 모델 처리": "resampled_features 사용",
      "정보 손실 가능성": "❌ 평균화로 인한 정보 손실",
      "계산 복잡도": "❌ 추가 연산 오버헤드",
      "파라미터 수": "❌ 전체 모델 크기 증가"
    }
  ],
  "performance_analysis": [
    {
      "성능 지표": "MAE (Mean Absolute Error)",
      "Vision Resampler 모델": "0.804 (최악)",
      "기존 2D 모델": "0.292 (최고)",
      "3D 모델들": "0.001~0.576 (중간)",
      "성능 차이 원인": "Vision Resampler로 인한 정보 손실"
    },
    {
      "성능 지표": "RMSE (Root Mean Squared Error)",
      "Vision Resampler 모델": "0.886 (최악)",
      "기존 2D 모델": "0.485 (최고)",
      "3D 모델들": "0.002~0.807 (중간)",
      "성능 차이 원인": "복잡한 구조로 인한 오버피팅"
    },
    {
      "성능 지표": "정확도 (10% 임계값)",
      "Vision Resampler 모델": "0.0% (최악)",
      "기존 2D 모델": "24.8% (중간)",
      "3D 모델들": "48.9%~100% (최고)",
      "성능 차이 원인": "샘플 수 차이 (15 vs 1224)"
    },
    {
      "성능 지표": "정확도 (5% 임계값)",
      "Vision Resampler 모델": "0.0% (최악)",
      "기존 2D 모델": "10.4% (중간)",
      "3D 모델들": "46.7%~100% (최고)",
      "성능 차이 원인": "데이터셋 크기 차이"
    },
    {
      "성능 지표": "정확도 (1% 임계값)",
      "Vision Resampler 모델": "0.0% (최악)",
      "기존 2D 모델": "0.16% (중간)",
      "3D 모델들": "N/A",
      "성능 차이 원인": "평가 방식 차이"
    }
  ],
  "recommendations": [
    {
      "문제점": "Vision Resampler로 인한 정보 손실",
      "원인": "64개 latents로 과도한 압축",
      "개선 방안": "latents 수 증가 (64→128, 256)",
      "우선순위": "높음 (핵심 문제)"
    },
    {
      "문제점": "복잡한 attention 메커니즘",
      "원인": "MultiheadAttention + FFN 오버헤드",
      "개선 방안": "attention heads 수 감소 (8→4)",
      "우선순위": "중간 (성능 개선)"
    },
    {
      "문제점": "적은 샘플 수로 인한 평가 편향",
      "원인": "검증셋만 15개 샘플",
      "개선 방안": "전체 데이터셋으로 재평가",
      "우선순위": "높음 (평가 정확성)"
    },
    {
      "문제점": "하이퍼파라미터 미최적화",
      "원인": "Vision Resampler 파라미터 미튜닝",
      "개선 방안": "학습률, dropout 등 튜닝",
      "우선순위": "중간 (성능 최적화)"
    },
    {
      "문제점": "데이터셋 크기 부족",
      "원인": "72개 에피소드로 제한",
      "개선 방안": "데이터 증강 또는 추가 수집",
      "우선순위": "낮음 (데이터 제약)"
    }
  ],
  "findings": [
    "1. Vision Resampler가 MAE 0.804로 가장 나쁜 성능을 보임",
    "2. 기존 2D 모델이 MAE 0.292로 가장 좋은 성능을 보임",
    "3. MAE 계산 방식은 모든 모델에서 동일함 (torch.abs 차이)",
    "4. Vision Resampler의 64개 latents 압축이 정보 손실의 주요 원인",
    "5. 복잡한 attention 메커니즘이 오버피팅을 유발",
    "6. 샘플 수 차이 (15 vs 1224)가 평가 편향을 만듦"
  ],
  "conclusions": [
    "• Vision Resampler의 현재 구현이 성능 저하의 주요 원인",
    "• 64개 latents로의 과도한 압축이 정보 손실을 야기",
    "• 복잡한 attention 구조가 작은 데이터셋에서 오버피팅 유발",
    "• MAE 계산 방식은 정확하나 데이터셋 크기 차이가 평가 편향 생성",
    "• 기존 2D 최적화 모델이 가장 실용적인 접근법"
  ]
}