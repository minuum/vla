"""
ğŸ” ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„
ëª¨ë“  í›ˆë ¨ëœ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ë¶„ì„
"""

import json
import os
from pathlib import Path

def load_evaluation_results():
    """ëª¨ë“  í‰ê°€ ê²°ê³¼ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    results = {}
    
    # í‰ê°€ ê²°ê³¼ íŒŒì¼ë“¤
    eval_files = [
        'optimized_2d_action_evaluation_results.json',
        'realistic_evaluation_results.json',
        'no_first_frame_evaluation_results.json',
        'advanced_mobile_vla_evaluation_results.json',
        'fixed_robovlms_evaluation_results.json'
    ]
    
    for file_path in eval_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    results[file_path] = data
                    print(f"âœ… {file_path} ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return results

def extract_performance_metrics(results):
    """ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ"""
    metrics = {}
    
    for file_path, data in results.items():
        model_name = file_path.replace('_evaluation_results.json', '').replace('_', ' ').title()
        
        if 'optimized_2d_action' in file_path:
            # 2D ì•¡ì…˜ ëª¨ë¸
            metrics[model_name] = {
                'model_type': '2D_Optimized',
                'mae': data.get('avg_mae', 'N/A'),
                'rmse': data.get('avg_rmse', 'N/A'),
                'accuracy_10': data.get('success_rates', {}).get('accuracy_10', 'N/A'),
                'accuracy_5': data.get('success_rates', {}).get('accuracy_5', 'N/A'),
                'accuracy_1': data.get('success_rates', {}).get('accuracy_1', 'N/A'),
                'total_samples': data.get('total_samples', 'N/A')
            }
        
        elif 'realistic_evaluation' in file_path:
            # Realistic í‰ê°€ (ì²« í”„ë ˆì„ vs ì¤‘ê°„ í”„ë ˆì„)
            first_frame = data.get('first_frame_results', {})
            middle_frame = data.get('middle_frame_results', {})
            
            metrics[f"{model_name} (First Frame)"] = {
                'model_type': '3D_Realistic_First',
                'mae': first_frame.get('mae', 'N/A'),
                'rmse': first_frame.get('rmse', 'N/A'),
                'accuracy_10': first_frame.get('accuracy_0.1', 'N/A'),
                'accuracy_5': first_frame.get('accuracy_0.05', 'N/A'),
                'accuracy_1': 'N/A',
                'total_samples': len(first_frame.get('predictions', []))
            }
            
            metrics[f"{model_name} (Middle Frame)"] = {
                'model_type': '3D_Realistic_Middle',
                'mae': middle_frame.get('mae', 'N/A'),
                'rmse': middle_frame.get('rmse', 'N/A'),
                'accuracy_10': middle_frame.get('accuracy_0.1', 'N/A'),
                'accuracy_5': middle_frame.get('accuracy_0.05', 'N/A'),
                'accuracy_1': 'N/A',
                'total_samples': len(middle_frame.get('predictions', []))
            }
        
        elif 'no_first_frame' in file_path:
            # ì²« í”„ë ˆì„ ì œì™¸ ëª¨ë¸
            random_frame = data.get('random_frame_results', {})
            middle_frame = data.get('middle_frame_results', {})
            
            metrics[f"{model_name} (Random)"] = {
                'model_type': '3D_NoFirstFrame_Random',
                'mae': random_frame.get('mae', 'N/A'),
                'rmse': random_frame.get('rmse', 'N/A'),
                'accuracy_10': random_frame.get('accuracy_0.1', 'N/A'),
                'accuracy_5': random_frame.get('accuracy_0.05', 'N/A'),
                'accuracy_1': 'N/A',
                'total_samples': len(random_frame.get('predictions', []))
            }
            
            metrics[f"{model_name} (Middle)"] = {
                'model_type': '3D_NoFirstFrame_Middle',
                'mae': middle_frame.get('mae', 'N/A'),
                'rmse': middle_frame.get('rmse', 'N/A'),
                'accuracy_10': middle_frame.get('accuracy_0.1', 'N/A'),
                'accuracy_5': middle_frame.get('accuracy_0.05', 'N/A'),
                'accuracy_1': 'N/A',
                'total_samples': len(middle_frame.get('predictions', []))
            }
        
        elif 'advanced_mobile_vla' in file_path:
            # Advanced Mobile VLA
            metrics[model_name] = {
                'model_type': '3D_Advanced',
                'mae': data.get('avg_mae', 'N/A'),
                'rmse': data.get('avg_rmse', 'N/A'),
                'accuracy_10': data.get('success_rates', {}).get('accuracy_10', 'N/A'),
                'accuracy_5': data.get('success_rates', {}).get('accuracy_5', 'N/A'),
                'accuracy_1': data.get('success_rates', {}).get('accuracy_1', 'N/A'),
                'total_samples': data.get('total_samples', 'N/A')
            }
        
        elif 'fixed_robovlms' in file_path:
            # Fixed RoboVLMs
            metrics[model_name] = {
                'model_type': '3D_Fixed_RoboVLMs',
                'mae': data.get('avg_mae', 'N/A'),
                'rmse': data.get('avg_rmse', 'N/A'),
                'accuracy_10': data.get('success_rates', {}).get('accuracy_10', 'N/A'),
                'accuracy_5': data.get('success_rates', {}).get('accuracy_5', 'N/A'),
                'accuracy_1': data.get('success_rates', {}).get('accuracy_1', 'N/A'),
                'total_samples': data.get('total_samples', 'N/A')
            }
    
    return metrics

def print_comparison_table(metrics):
    """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "="*120)
    print("ğŸ” ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”")
    print("="*120)
    
    # í—¤ë”
    print(f"{'ëª¨ë¸ëª…':<35} {'íƒ€ì…':<20} {'MAE':<8} {'RMSE':<8} {'Acc@0.1':<8} {'Acc@0.05':<8} {'Acc@0.01':<8} {'ìƒ˜í”Œìˆ˜':<8}")
    print("-" * 120)
    
    # ëª¨ë¸ë“¤ì„ ì„±ëŠ¥ë³„ë¡œ ì •ë ¬ (MAE ê¸°ì¤€)
    sorted_models = []
    for name, metric in metrics.items():
        mae = metric['mae']
        if mae != 'N/A':
            sorted_models.append((name, metric, mae))
        else:
            sorted_models.append((name, metric, float('inf')))
    
    sorted_models.sort(key=lambda x: x[2])
    
    # ê²°ê³¼ ì¶œë ¥
    for i, (name, metric, _) in enumerate(sorted_models):
        rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
        
        mae_str = f"{metric['mae']:.4f}" if metric['mae'] != 'N/A' else 'N/A'
        rmse_str = f"{metric['rmse']:.4f}" if metric['rmse'] != 'N/A' else 'N/A'
        acc10_str = f"{metric['accuracy_10']:.1f}%" if metric['accuracy_10'] != 'N/A' else 'N/A'
        acc5_str = f"{metric['accuracy_5']:.1f}%" if metric['accuracy_5'] != 'N/A' else 'N/A'
        acc1_str = f"{metric['accuracy_1']:.1f}%" if metric['accuracy_1'] != 'N/A' else 'N/A'
        samples_str = f"{metric['total_samples']}" if metric['total_samples'] != 'N/A' else 'N/A'
        
        print(f"{rank} {name:<32} {metric['model_type']:<20} {mae_str:<8} {rmse_str:<8} {acc10_str:<8} {acc5_str:<8} {acc1_str:<8} {samples_str:<8}")

def analyze_model_types(metrics):
    """ëª¨ë¸ íƒ€ì…ë³„ ë¶„ì„"""
    print("\n" + "="*80)
    print("ğŸ“Š ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ ë¶„ì„")
    print("="*80)
    
    # ëª¨ë¸ íƒ€ì…ë³„ ê·¸ë£¹í™”
    type_groups = {}
    for name, metric in metrics.items():
        model_type = metric['model_type']
        if model_type not in type_groups:
            type_groups[model_type] = []
        type_groups[model_type].append((name, metric))
    
    # ê° íƒ€ì…ë³„ ë¶„ì„
    for model_type, models in type_groups.items():
        print(f"\nğŸ” {model_type} ëª¨ë¸ë“¤:")
        print("-" * 60)
        
        # MAE ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        valid_models = [(name, metric) for name, metric in models if metric['mae'] != 'N/A']
        if valid_models:
            valid_models.sort(key=lambda x: x[1]['mae'])
            
            for i, (name, metric) in enumerate(valid_models):
                rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
                print(f"{rank} {name}: MAE={metric['mae']:.4f}, RMSE={metric['rmse']:.4f}, Acc@0.1={metric['accuracy_10']:.1f}%")
        else:
            print("   ìœ íš¨í•œ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def generate_recommendations(metrics):
    """ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    print("\n" + "="*80)
    print("ğŸ’¡ ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­")
    print("="*80)
    
    # 2D ëª¨ë¸ê³¼ 3D ëª¨ë¸ ë¶„ë¦¬
    d2_models = [(name, metric) for name, metric in metrics.items() if '2D' in metric['model_type']]
    d3_models = [(name, metric) for name, metric in metrics.items() if '3D' in metric['model_type']]
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_2d = None
    best_3d = None
    
    if d2_models:
        best_2d = min(d2_models, key=lambda x: x[1]['mae'] if x[1]['mae'] != 'N/A' else float('inf'))
    
    if d3_models:
        best_3d = min(d3_models, key=lambda x: x[1]['mae'] if x[1]['mae'] != 'N/A' else float('inf'))
    
    print("ğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
    if best_2d:
        print(f"   - 2D ëª¨ë¸: {best_2d[0]} (MAE: {best_2d[1]['mae']:.4f})")
    if best_3d:
        print(f"   - 3D ëª¨ë¸: {best_3d[0]} (MAE: {best_3d[1]['mae']:.4f})")
    
    print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    
    if best_2d and best_3d:
        if best_2d[1]['mae'] < best_3d[1]['mae']:
            print("   âœ… 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            print("   ğŸ’¡ ì‹¤ì œ ë¡œë´‡ ì œì–´ì—ì„œëŠ” Zì¶• íšŒì „ì´ ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ 2D ëª¨ë¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        else:
            print("   âœ… 3D ëª¨ë¸ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            print("   ğŸ’¡ ëª¨ë“  ì•¡ì…˜ ì°¨ì›ì´ í•„ìš”í•œ ê²½ìš° 3D ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    print("   ğŸ”§ ì¶”ê°€ ê°œì„  ë°©í–¥:")
    print("      - ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©")
    print("      - ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©")
    print("      - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("      - ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘!")
    
    # í‰ê°€ ê²°ê³¼ ë¡œë“œ
    results = load_evaluation_results()
    
    if not results:
        print("âŒ í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
    metrics = extract_performance_metrics(results)
    
    # ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    print_comparison_table(metrics)
    
    # ëª¨ë¸ íƒ€ì…ë³„ ë¶„ì„
    analyze_model_types(metrics)
    
    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    generate_recommendations(metrics)
    
    # ê²°ê³¼ ì €ì¥
    comparison_results = {
        'metrics': metrics,
        'summary': {
            'total_models': len(metrics),
            'model_types': list(set(m['model_type'] for m in metrics.values())),
            'best_2d_mae': min([m['mae'] for m in metrics.values() if '2D' in m['model_type'] and m['mae'] != 'N/A'], default='N/A'),
            'best_3d_mae': min([m['mae'] for m in metrics.values() if '3D' in m['model_type'] and m['mae'] != 'N/A'], default='N/A')
        }
    }
    
    with open('comprehensive_model_comparison_results.json', 'w') as f:
        json.dump(comparison_results, f, indent=2)
    
    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: comprehensive_model_comparison_results.json")
    
    print("\n" + "="*80)
    print("âœ… ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
    print("="*80)

if __name__ == "__main__":
    main()
