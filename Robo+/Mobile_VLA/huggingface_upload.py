#!/usr/bin/env python3
"""
ğŸ¤— HuggingFace Mobile VLA ëª¨ë¸ ì—…ë¡œë“œ ë° ë‹¤ìš´ë¡œë“œ

Mobile VLA ëª¨ë¸ì„ HuggingFace Hubì— ì—…ë¡œë“œí•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import torch
import json
from datetime import datetime
from pathlib import Path
from huggingface_hub import HfApi, Repository, upload_file, snapshot_download
from transformers import AutoTokenizer, AutoProcessor
from transformers import Kosmos2Model

def create_model_card():
    """Mobile VLA ëª¨ë¸ ì¹´ë“œ ìƒì„±"""
    
    timestamp = datetime.now().strftime("%Y-%m-%d")
    
    model_card = f"""---
license: apache-2.0
tags:
- vision-language-action
- mobile-robot
- kosmos-2b
- robotics
- obstacle-avoidance
datasets:
- mobile-vla-dataset
language:
- en
- ko
metrics:
- mae
- r2_score
library_name: transformers
pipeline_tag: robotics
---

# ğŸš€ Mobile VLA: Vision-Language-Action Model for Mobile Robots

## ğŸ“‹ Model Description

Mobile VLAëŠ” Kosmos-2Bë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Mobile Robot ì „ìš© Vision-Language-Action ëª¨ë¸ì…ë‹ˆë‹¤. 
ì¥ì• ë¬¼ íšŒí”¼ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì—°ì†ì ì¸ 3D ì•¡ì…˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ê¸°ëŠ¥

- **Vision-Language-Action**: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì§€ì‹œì‚¬í•­ì„ ë°›ì•„ ë¡œë´‡ ì•¡ì…˜ ì˜ˆì¸¡
- **3D ì—°ì† ì œì–´**: `[linear_x, linear_y, angular_z]` í˜•íƒœì˜ ì—°ì† ì•¡ì…˜ ê³µê°„
- **ì¥ì• ë¬¼ íšŒí”¼**: 1-box, 2-box ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì¢Œìš° íšŒí”¼ ì „ëµ í•™ìŠµ
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ vision-only ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ì¶”ë¡ 

### ğŸ”§ ê¸°ìˆ  ì‚¬ì–‘

- **ë°±ë³¸ ëª¨ë¸**: microsoft/kosmos-2-patch14-224
- **ì…ë ¥**: RGB ì´ë¯¸ì§€ (224x224) + í…ìŠ¤íŠ¸ ì§€ì‹œì‚¬í•­
- **ì¶œë ¥**: 3D ì—°ì† ì•¡ì…˜ ë²¡í„°
- **í•™ìŠµ ë°©ì‹**: Huber Loss ê¸°ë°˜ íšŒê·€
- **ë°ì´í„°**: 72ê°œ ì‹¤ì œ ë¡œë´‡ ì—í”¼ì†Œë“œ

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

### ì „ì²´ ì„±ëŠ¥
- **ì „ì²´ MAE**: 0.285
- **ì„ê³„ê°’ ì •í™•ë„ (0.1)**: 37.5%

### ì•¡ì…˜ë³„ ì„±ëŠ¥
| ì•¡ì…˜ | MAE | RÂ² Score | ì„¤ëª… |
|------|-----|----------|------|
| linear_x | 0.243 | 0.354 | ì „ì§„/í›„ì§„ (ìš°ìˆ˜) |
| linear_y | 0.550 | 0.293 | ì¢Œìš° ì´ë™ (ë³´í†µ) |
| angular_z | 0.062 | 0.000 | íšŒì „ (ë‚®ìŒ) |

### ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥
| ì‹œë‚˜ë¦¬ì˜¤ | MAE | ë“±ê¸‰ | ì„¤ëª… |
|----------|-----|------|------|
| 1box_right_vertical | 0.217 | B+ | ìš°ìˆ˜ |
| 1box_left_horizontal | 0.303 | B | ì–‘í˜¸ |
| 2box_left_vertical | 0.322 | B | ì–‘í˜¸ |
| 1box_left_vertical | 0.337 | B- | ë³´í†µ |

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ì„¤ì¹˜
```bash
pip install transformers torch pillow numpy
```

### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
from mobile_vla import MobileVLAModel, MobileVLATrainer
from PIL import Image
import torch

# ëª¨ë¸ ë¡œë“œ
model = MobileVLAModel.from_pretrained("minuum/mobile-vla")

# ì´ë¯¸ì§€ì™€ íƒœìŠ¤í¬ ì¤€ë¹„
image = Image.open("robot_camera.jpg")
task = "Navigate around obstacles to track the target cup"

# ì˜ˆì¸¡
with torch.no_grad():
    actions = model.predict(image, task)
    
print(f"Predicted actions: {{actions}}")
# ì¶œë ¥: [linear_x, linear_y, angular_z]
```

### ê³ ê¸‰ ì‚¬ìš©ë²•
```python
# ë°°ì¹˜ ì²˜ë¦¬
images = [Image.open(f"frame_{{i}}.jpg") for i in range(8)]
actions = model.predict_sequence(images, task)

# ì‹¤ì‹œê°„ ì œì–´
for frame in camera_stream:
    action = model.predict(frame, task)
    robot.execute(action)
```

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

```
[RGB Images] â†’ [Kosmos-2B Vision] â†’ [Action Head] â†’ [3D Actions]
     â†“              â†“                    â†“             â†“
   224x224    Image Features         Regression    [x, y, Î¸]
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸
1. **Kosmos-2B Vision Model**: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
2. **Action Head**: 3D íšŒê·€ í—¤ë“œ (512 â†’ 3*chunk_size)
3. **Window/Chunk**: 8í”„ë ˆì„ ê´€ì°° â†’ 2í”„ë ˆì„ ì˜ˆì¸¡

## ğŸ“ˆ RoboVLMsì™€ì˜ ë¹„êµ

| í•­ëª© | RoboVLMs | Mobile VLA |
|------|----------|------------|
| **ë°ì´í„° ìš”êµ¬ëŸ‰** | ìˆ˜ë°±ë§Œ ë°ëª¨ | 72 ì—í”¼ì†Œë“œ |
| **ì•¡ì…˜ ê³µê°„** | 7-DOF Discrete | 3D Continuous |
| **ì¶”ë¡  ì†ë„** | ë³µí•©ì  | ë¹ ë¦„ |
| **íŠ¹í™” ë¶„ì•¼** | ë²”ìš© Manipulation | Mobile Robot |
| **í‰ê°€ ë°©ì‹** | ì„±ê³µë¥  | ë‹¤ì°¨ì› íšŒê·€ ì§€í‘œ |

## ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­

- **ë°ì´í„° íš¨ìœ¨ì„±**: 1000ë°° ì ì€ ë°ì´í„°ë¡œ ì‹¤ìš©ì  ì„±ëŠ¥
- **ì‹¤ì‹œê°„ ì„±ëŠ¥**: Vision-only ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
- **ì—°ì† ì œì–´**: ì •ë°€í•œ 3D ì•¡ì…˜ ì˜ˆì¸¡
- **ì‹œë‚˜ë¦¬ì˜¤ íŠ¹í™”**: ì¥ì• ë¬¼ íšŒí”¼ ì „ìš© ìµœì í™”

## ğŸ“š í•™ìŠµ ë°ì´í„°

- **ì—í”¼ì†Œë“œ ìˆ˜**: 72ê°œ
- **ì‹œë‚˜ë¦¬ì˜¤**: 1box/2box Ã— left/right Ã— vertical/horizontal
- **ì•¡ì…˜**: [linear_x, linear_y, angular_z] ì—°ì† ê°’
- **ì´ë¯¸ì§€**: ì‹¤ì œ ë¡œë´‡ ì¹´ë©”ë¼ RGB (224x224)

## ğŸ”¬ ì—°êµ¬ ë°°ê²½

ì´ ëª¨ë¸ì€ RoboVLMsì˜ Window/Chunk ë©”ì»¤ë‹ˆì¦˜ì„ ìœ ì§€í•˜ë©´ì„œ Mobile Robotì— íŠ¹í™”ëœ ê¸°ëŠ¥ì„ ì¶”ê°€í•œ ì—°êµ¬ì…ë‹ˆë‹¤:

1. **Window/Chunk ìœ ì§€**: 8í”„ë ˆì„ ê´€ì°° â†’ 2í”„ë ˆì„ ì˜ˆì¸¡ êµ¬ì¡°
2. **Kosmos-2B í†µí•©**: Vision-Language ë°±ë³¸ í™œìš©
3. **ì—°ì† ì œì–´**: Discrete â†’ Continuous ì•¡ì…˜ ê³µê°„ ì „í™˜
4. **ì‹¤ì œ ë¡œë´‡ ë°ì´í„°**: HDF5 í˜•íƒœì˜ ì‹¤ì œ ìˆ˜ì§‘ ë°ì´í„°

## ğŸ“„ ì¸ìš©

```bibtex
@misc{{mobile_vla_2024,
  title={{Mobile VLA: Vision-Language-Action Model for Mobile Robot Navigation}},
  author={{Mobile VLA Team}},
  year={{2024}},
  publisher={{HuggingFace}},
  url={{https://huggingface.co/minuum/mobile-vla}}
}}
```

## ğŸ¤ ê¸°ì—¬

ì´ ëª¨ë¸ì€ RoboVLMs í”„ë ˆì„ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, Mobile Robot ì»¤ë®¤ë‹ˆí‹°ì˜ ë°œì „ì„ ìœ„í•´ ê³µê°œë©ë‹ˆë‹¤.

## ğŸ“ ì—°ë½ì²˜

- **Issues**: [GitHub Issues](https://github.com/minuum/vla/issues)
- **Discussions**: [HuggingFace Discussions](https://huggingface.co/minuum/mobile-vla/discussions)

---
*Generated on {timestamp}*
"""

    return model_card

def create_config_json():
    """Mobile VLA ì„¤ì • íŒŒì¼ ìƒì„±"""
    
    config = {
        "model_type": "mobile_vla",
        "architecture": "kosmos2_mobile_vla",
        "backbone": "microsoft/kosmos-2-patch14-224",
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„°
        "hidden_size": 1536,
        "action_dim": 3,
        "window_size": 8,
        "chunk_size": 2,
        
        # í•™ìŠµ ì„¤ì •
        "learning_rate": 1e-4,
        "batch_size": 1,
        "num_epochs": 3,
        "loss_function": "huber_loss",
        
        # ë°ì´í„° ì„¤ì •
        "image_size": [224, 224],
        "normalize_actions": True,
        "scenarios": [
            "1box_left_vertical", "1box_left_horizontal",
            "1box_right_vertical", "1box_right_horizontal", 
            "2box_left_vertical", "2box_left_horizontal",
            "2box_right_vertical", "2box_right_horizontal"
        ],
        
        # ì„±ëŠ¥ ì§€í‘œ
        "performance": {
            "overall_mae": 0.285,
            "threshold_accuracy_0_1": 0.375,
            "per_action_mae": {
                "linear_x": 0.243,
                "linear_y": 0.550, 
                "angular_z": 0.062
            },
            "per_action_r2": {
                "linear_x": 0.354,
                "linear_y": 0.293,
                "angular_z": 0.000
            }
        },
        
        # ë©”íƒ€ë°ì´í„°
        "dataset_size": 72,
        "training_episodes": 52,
        "validation_episodes": 20,
        "model_parameters": 1665537542,
        "created_date": datetime.now().isoformat(),
        "framework": "pytorch",
        "transformers_version": "4.41.2"
    }
    
    return config

def prepare_huggingface_upload(model_name="mobile-vla", local_model_path="mobile_vla_epoch_3.pt"):
    """HuggingFace ì—…ë¡œë“œ ì¤€ë¹„"""
    
    print("ğŸ¤— HuggingFace ëª¨ë¸ ì—…ë¡œë“œ ì¤€ë¹„ ì‹œì‘!")
    print("=" * 50)
    
    # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
    upload_dir = Path("huggingface_upload")
    upload_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ ì—…ë¡œë“œ ë””ë ‰í† ë¦¬: {upload_dir}")
    
    # 1. ëª¨ë¸ ì¹´ë“œ ìƒì„±
    model_card = create_model_card()
    with open(upload_dir / "README.md", "w", encoding="utf-8") as f:
        f.write(model_card)
    print("âœ… README.md (ëª¨ë¸ ì¹´ë“œ) ìƒì„± ì™„ë£Œ")
    
    # 2. ì„¤ì • íŒŒì¼ ìƒì„±
    config = create_config_json()
    with open(upload_dir / "config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print("âœ… config.json ìƒì„± ì™„ë£Œ")
    
    # 3. ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬ (ë§Œì•½ ì¡´ì¬í•œë‹¤ë©´)
    if Path(local_model_path).exists():
        import shutil
        shutil.copy2(local_model_path, upload_dir / "pytorch_model.bin")
        print(f"âœ… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë³µì‚¬: {local_model_path}")
    else:
        print(f"âš ï¸  ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {local_model_path}")
    
    # 4. ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    example_script = '''#!/usr/bin/env python3
"""
Mobile VLA ì‚¬ìš© ì˜ˆì œ
"""

import torch
from transformers import AutoTokenizer, AutoProcessor
from PIL import Image
import numpy as np

def load_mobile_vla_model(model_name="minuum/mobile-vla"):
    """Mobile VLA ëª¨ë¸ ë¡œë“œ"""
    
    # ì—¬ê¸°ì„œ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ë¡œì§ êµ¬í˜„
    print(f"Loading Mobile VLA model: {model_name}")
    
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MobileVLATrainerë¥¼ ì‚¬ìš©
    # from robovlms.train.mobile_vla_trainer import MobileVLATrainer
    # model = MobileVLATrainer.from_pretrained(model_name)
    
    return None  # í”Œë ˆì´ìŠ¤í™€ë”

def predict_action(model, image_path, task_description):
    """ì•¡ì…˜ ì˜ˆì¸¡"""
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = Image.open(image_path).convert("RGB")
    
    # ì „ì²˜ë¦¬ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” mobile_vla_collate_fn ì‚¬ìš©)
    # processed = preprocess_image(image)
    
    # ì˜ˆì¸¡ (í”Œë ˆì´ìŠ¤í™€ë”)
    dummy_action = [0.5, 0.2, 0.1]  # [linear_x, linear_y, angular_z]
    
    return dummy_action

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ Mobile VLA ì˜ˆì œ ì‹¤í–‰")
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_mobile_vla_model()
    
    # ì˜ˆì œ ì˜ˆì¸¡
    task = "Navigate around obstacles to track the target cup"
    action = predict_action(model, "example_image.jpg", task)
    
    print(f"Task: {task}")
    print(f"Predicted Action: {action}")
    print(f"  - Linear X (forward/backward): {action[0]:.3f}")
    print(f"  - Linear Y (left/right): {action[1]:.3f}")
    print(f"  - Angular Z (rotation): {action[2]:.3f}")

if __name__ == "__main__":
    main()
'''
    
    with open(upload_dir / "example_usage.py", "w", encoding="utf-8") as f:
        f.write(example_script)
    print("âœ… example_usage.py ìƒì„± ì™„ë£Œ")
    
    # 5. ìš”êµ¬ì‚¬í•­ íŒŒì¼ ìƒì„±
    requirements = """torch>=2.3.0
transformers>=4.41.2
pillow>=8.0.0
numpy>=1.21.0
h5py>=3.0.0
scikit-learn>=1.0.0
matplotlib>=3.5.0
"""
    
    with open(upload_dir / "requirements.txt", "w") as f:
        f.write(requirements)
    print("âœ… requirements.txt ìƒì„± ì™„ë£Œ")
    
    print(f"\nğŸ‰ HuggingFace ì—…ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"ğŸ“‚ ì—…ë¡œë“œ íŒŒì¼ë“¤:")
    for file in upload_dir.iterdir():
        if file.is_file():
            size = file.stat().st_size / (1024*1024)  # MB
            print(f"   ğŸ“„ {file.name} ({size:.2f} MB)")
    
    return upload_dir

def upload_to_huggingface(upload_dir, repo_name="minuum/mobile-vla", token=None):
    """HuggingFace Hubì— ì—…ë¡œë“œ"""
    
    print(f"\nğŸ¤— HuggingFace Hub ì—…ë¡œë“œ ì‹œì‘: {repo_name}")
    
    if not token:
        print("âš ï¸  HuggingFace í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   1. https://huggingface.co/settings/tokens ì—ì„œ í† í° ìƒì„±")
        print("   2. í™˜ê²½ë³€ìˆ˜ HUGGINGFACE_TOKEN ì„¤ì • ë˜ëŠ” ì§ì ‘ ì „ë‹¬")
        return False
    
    try:
        api = HfApi()
        
        # ì €ì¥ì†Œ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        try:
            api.create_repo(repo_name, token=token, exist_ok=True)
            print(f"âœ… ì €ì¥ì†Œ ìƒì„±/í™•ì¸: {repo_name}")
        except Exception as e:
            print(f"âŒ ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
        
        # íŒŒì¼ë“¤ ì—…ë¡œë“œ
        for file_path in upload_dir.iterdir():
            if file_path.is_file():
                try:
                    upload_file(
                        path_or_fileobj=str(file_path),
                        path_in_repo=file_path.name,
                        repo_id=repo_name,
                        token=token
                    )
                    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {file_path.name}")
                except Exception as e:
                    print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ {file_path.name}: {e}")
        
        print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ”— ëª¨ë¸ í˜ì´ì§€: https://huggingface.co/{repo_name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def download_from_huggingface(repo_name="minuum/mobile-vla", local_dir="./downloaded_model"):
    """HuggingFace Hubì—ì„œ ë‹¤ìš´ë¡œë“œ"""
    
    print(f"ğŸ“¥ HuggingFaceì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: {repo_name}")
    
    try:
        snapshot_download(
            repo_id=repo_name,
            local_dir=local_dir,
            local_dir_use_symlinks=False
        )
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_dir}")
        return True
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¤— Mobile VLA HuggingFace ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ë„êµ¬")
    print("=" * 60)
    
    # 1. ì—…ë¡œë“œ ì¤€ë¹„
    upload_dir = prepare_huggingface_upload()
    
    # 2. ì—…ë¡œë“œ (í† í°ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
    token = os.getenv("HUGGINGFACE_TOKEN")
    if token:
        print(f"\nğŸ’¡ í† í° ë°œê²¬, ì—…ë¡œë“œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
        upload_to_huggingface(upload_dir, token=token)
    else:
        print(f"\nğŸ’¡ ì—…ë¡œë“œ ì¤€ë¹„ë§Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   ì—…ë¡œë“œí•˜ë ¤ë©´ HUGGINGFACE_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    
    # 3. ë‹¤ìš´ë¡œë“œ ì˜ˆì œ
    print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì˜ˆì œ:")
    print(f"   python -c 'from huggingface_upload import download_from_huggingface; download_from_huggingface()'")

if __name__ == "__main__":
    main()
