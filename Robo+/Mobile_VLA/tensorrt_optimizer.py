#!/usr/bin/env python3
"""
TensorRT ëª¨ë¸ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
- PyTorch ëª¨ë¸ì„ TensorRTë¡œ ë³€í™˜
- FP16/INT8 ì–‘ìí™” ì§€ì›
- ë¡œë´‡ íƒœìŠ¤í¬ ìµœì í™”
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from typing import Dict, Any, List
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

class Kosmos2CLIPHybridModel(nn.Module):
    """Kosmos2 + CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212)"""
    
    def __init__(self):
        super().__init__()
        
        # ëª¨ë¸ êµ¬ì¡° ì •ì˜
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten()
        )
        
        self.text_encoder = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        
        self.fusion_layer = nn.Sequential(
            nn.Linear(256 + 128, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 3)  # linear_x, linear_y, angular_z
        )
    
    def forward(self, images, text_embeddings):
        """ì „ë°© ì „íŒŒ"""
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        image_features = self.image_encoder(images)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features = self.text_encoder(text_embeddings)
        
        # íŠ¹ì§• ìœµí•©
        combined_features = torch.cat([image_features, text_features], dim=1)
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.fusion_layer(combined_features)
        
        return actions

class TensorRTOptimizer:
    """TensorRT ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.runtime = trt.Runtime(self.logger)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        self.test_images = torch.randn(1, 3, 224, 224, device=self.device)
        self.test_text = torch.randn(1, 512, device=self.device)
        
        print(f"ğŸ”§ Device: {self.device}")
        print(f"ğŸ“Š Test data shape: images {self.test_images.shape}, text {self.test_text.shape}")
        print(f"ğŸ¯ Target: Kosmos2 + CLIP Hybrid (MAE 0.212)")
        print(f"ğŸš€ TensorRT Version: {trt.__version__}")
    
    def create_onnx_model(self, onnx_path: str):
        """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜"""
        print(f"\nğŸ”¨ Creating ONNX model...")
        
        # ëª¨ë¸ ìƒì„±
        model = Kosmos2CLIPHybridModel().to(self.device)
        model.eval()
        
        # ONNX ë³€í™˜
        torch.onnx.export(
            model,
            (self.test_images, self.test_text),
            onnx_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['images', 'text_embeddings'],
            output_names=['actions'],
            dynamic_axes={
                'images': {0: 'batch_size'},
                'text_embeddings': {0: 'batch_size'},
                'actions': {0: 'batch_size'}
            }
        )
        
        print(f"âœ… ONNX model saved: {onnx_path}")
        print(f"ğŸ“Š ONNX size: {os.path.getsize(onnx_path) / (1024*1024):.1f} MB")
        
        return onnx_path
    
    def build_tensorrt_engine(self, onnx_path: str, engine_path: str, precision: str = 'fp16'):
        """ONNX ëª¨ë¸ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜"""
        print(f"\nğŸ”¨ Building TensorRT engine ({precision})...")
        
        # TensorRT ë¹Œë” ìƒì„±
        builder = trt.Builder(self.logger)
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 30  # 1GB
        
        # ì •ë°€ë„ ì„¤ì •
        if precision == 'fp16' and builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            print(f"   Using FP16 precision")
        elif precision == 'int8' and builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)
            config.set_flag(trt.BuilderFlag.STRICT_TYPES)
            print(f"   Using INT8 precision")
        else:
            print(f"   Using FP32 precision")
        
        # ë„¤íŠ¸ì›Œí¬ íŒŒì‹±
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, self.logger)
        
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                for error in range(parser.num_errors):
                    print(f"   ONNX parsing error: {parser.get_error(error)}")
                return None
        
        # ì—”ì§„ ë¹Œë“œ
        print(f"   Building TensorRT engine...")
        engine = builder.build_engine(network, config)
        
        if engine is None:
            print(f"   Failed to build TensorRT engine")
            return None
        
        # ì—”ì§„ ì €ì¥
        with open(engine_path, 'wb') as f:
            f.write(engine.serialize())
        
        print(f"âœ… TensorRT engine saved: {engine_path}")
        print(f"ğŸ“Š Engine size: {os.path.getsize(engine_path) / (1024*1024):.1f} MB")
        
        return engine_path
    
    def benchmark_tensorrt(self, engine_path: str, num_runs: int = 100):
        """TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ“ˆ Benchmarking TensorRT Engine ({num_runs} runs)")
        print("-" * 50)
        
        # ì—”ì§„ ë¡œë“œ
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        
        engine = self.runtime.deserialize_cuda_engine(engine_data)
        context = engine.create_execution_context()
        
        # ë©”ëª¨ë¦¬ í• ë‹¹
        input_images = cuda.mem_alloc(self.test_images.numpy().nbytes)
        input_text = cuda.mem_alloc(self.test_text.numpy().nbytes)
        output_actions = cuda.mem_alloc(3 * 4)  # 3 float32 values
        
        # ì›Œë°ì—…
        print("ğŸ”¥ Warming up TensorRT engine...")
        for i in range(50):
            cuda.memcpy_htod(input_images, self.test_images.cpu().numpy())
            cuda.memcpy_htod(input_text, self.test_text.cpu().numpy())
            
            context.execute_v2(bindings=[int(input_images), int(input_text), int(output_actions)])
            
            if (i + 1) % 10 == 0:
                print(f"   Warmup: {i + 1}/50")
        
        # ë²¤ì¹˜ë§ˆí¬
        print(f"âš¡ Running TensorRT benchmark...")
        times = []
        for i in range(num_runs):
            start_time = time.perf_counter()
            
            # GPUë¡œ ë°ì´í„° ë³µì‚¬
            cuda.memcpy_htod(input_images, self.test_images.cpu().numpy())
            cuda.memcpy_htod(input_text, self.test_text.cpu().numpy())
            
            # ì¶”ë¡  ì‹¤í–‰
            context.execute_v2(bindings=[int(input_images), int(input_text), int(output_actions)])
            
            # GPU ë™ê¸°í™”
            cuda.Context.synchronize()
            
            inference_time = time.perf_counter() - start_time
            times.append(inference_time)
            
            if (i + 1) % 20 == 0:
                print(f"   Progress: {i + 1}/{num_runs}")
        
        # ê²°ê³¼ ë¶„ì„
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        fps = 1.0 / avg_time
        
        result = {
            "model_name": "Kosmos2+CLIP_Hybrid",
            "framework": "TensorRT",
            "precision": "FP16",
            "average_time_ms": avg_time * 1000,
            "std_time_ms": std_time * 1000,
            "min_time_ms": min_time * 1000,
            "max_time_ms": max_time * 1000,
            "fps": fps,
            "num_runs": num_runs,
            "engine_size_mb": os.path.getsize(engine_path) / (1024 * 1024)
        }
        
        print(f"ğŸ“Š TensorRT Results:")
        print(f"   Average: {avg_time*1000:.3f} ms")
        print(f"   Std Dev: {std_time*1000:.3f} ms")
        print(f"   Min: {min_time*1000:.3f} ms")
        print(f"   Max: {max_time*1000:.3f} ms")
        print(f"   FPS: {fps:.1f}")
        print(f"   Engine Size: {result['engine_size_mb']:.1f} MB")
        
        return result
    
    def compare_all_frameworks(self):
        """ëª¨ë“  í”„ë ˆì„ì›Œí¬ ë¹„êµ"""
        print(f"\n" + "="*80)
        print("ğŸ† COMPREHENSIVE FRAMEWORK COMPARISON")
        print("="*80)
        
        results = []
        
        # 1. PyTorch ë²¤ì¹˜ë§ˆí¬
        print(f"\n1. PyTorch Benchmark")
        pytorch_result = self.benchmark_pytorch()
        results.append(pytorch_result)
        
        # 2. ONNX Runtime ë²¤ì¹˜ë§ˆí¬
        print(f"\n2. ONNX Runtime Benchmark")
        onnx_result = self.benchmark_onnx()
        if onnx_result:
            results.append(onnx_result)
        
        # 3. TensorRT ë²¤ì¹˜ë§ˆí¬
        print(f"\n3. TensorRT Benchmark")
        tensorrt_result = self.benchmark_tensorrt_optimized()
        if tensorrt_result:
            results.append(tensorrt_result)
        
        # 4. ê²°ê³¼ ë¹„êµ
        self.create_comparison_report(results)
        
        return results
    
    def benchmark_pytorch(self, num_runs: int = 100):
        """PyTorch ë²¤ì¹˜ë§ˆí¬"""
        print(f"ğŸ“ˆ Benchmarking PyTorch Model ({num_runs} runs)")
        
        model = Kosmos2CLIPHybridModel().to(self.device)
        model.eval()
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for i in range(50):
                _ = model(self.test_images, self.test_text)
        
        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for i in range(num_runs):
            start_time = time.perf_counter()
            
            with torch.no_grad():
                outputs = model(self.test_images, self.test_text)
            
            inference_time = time.perf_counter() - start_time
            times.append(inference_time)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1.0 / avg_time
        
        result = {
            "model_name": "Kosmos2+CLIP_Hybrid",
            "framework": "PyTorch",
            "precision": "FP32",
            "average_time_ms": avg_time * 1000,
            "std_time_ms": std_time * 1000,
            "fps": fps,
            "num_runs": num_runs
        }
        
        print(f"   Average: {avg_time*1000:.3f} ms, FPS: {fps:.1f}")
        return result
    
    def benchmark_onnx(self, num_runs: int = 100):
        """ONNX Runtime ë²¤ì¹˜ë§ˆí¬"""
        try:
            import onnxruntime as ort
            
            onnx_path = "Robo+/Mobile_VLA/tensorrt_best_model/best_model_kosmos2_clip.onnx"
            if not os.path.exists(onnx_path):
                print(f"   ONNX model not found, skipping...")
                return None
            
            print(f"ğŸ“ˆ Benchmarking ONNX Runtime Model ({num_runs} runs)")
            
            providers = ['CPUExecutionProvider']
            session = ort.InferenceSession(onnx_path, providers=providers)
            
            input_names = [input.name for input in session.get_inputs()]
            inputs = {
                input_names[0]: self.test_images.cpu().numpy(),
                input_names[1]: self.test_text.cpu().numpy()
            }
            
            # ì›Œë°ì—…
            for i in range(50):
                _ = session.run(None, inputs)
            
            # ë²¤ì¹˜ë§ˆí¬
            times = []
            for i in range(num_runs):
                start_time = time.perf_counter()
                outputs = session.run(None, inputs)
                inference_time = time.perf_counter() - start_time
                times.append(inference_time)
            
            avg_time = np.mean(times)
            std_time = np.std(times)
            fps = 1.0 / avg_time
            
            result = {
                "model_name": "Kosmos2+CLIP_Hybrid",
                "framework": "ONNX Runtime",
                "precision": "FP32",
                "average_time_ms": avg_time * 1000,
                "std_time_ms": std_time * 1000,
                "fps": fps,
                "num_runs": num_runs,
                "model_size_mb": os.path.getsize(onnx_path) / (1024 * 1024)
            }
            
            print(f"   Average: {avg_time*1000:.3f} ms, FPS: {fps:.1f}")
            return result
            
        except Exception as e:
            print(f"   ONNX benchmark failed: {e}")
            return None
    
    def benchmark_tensorrt_optimized(self, num_runs: int = 100):
        """TensorRT ìµœì í™” ë²¤ì¹˜ë§ˆí¬"""
        # ONNX ëª¨ë¸ ìƒì„±
        onnx_path = "Robo+/Mobile_VLA/tensorrt_optimized/model.onnx"
        os.makedirs("Robo+/Mobile_VLA/tensorrt_optimized", exist_ok=True)
        
        if not os.path.exists(onnx_path):
            self.create_onnx_model(onnx_path)
        
        # TensorRT ì—”ì§„ ìƒì„±
        engine_path = "Robo+/Mobile_VLA/tensorrt_optimized/model_fp16.engine"
        if not os.path.exists(engine_path):
            self.build_tensorrt_engine(onnx_path, engine_path, 'fp16')
        
        if os.path.exists(engine_path):
            return self.benchmark_tensorrt(engine_path, num_runs)
        else:
            print(f"   TensorRT engine creation failed")
            return None
    
    def create_comparison_report(self, results: List[Dict]):
        """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n" + "="*80)
        print("ğŸ† FINAL PERFORMANCE COMPARISON")
        print("="*80)
        
        if len(results) < 2:
            print("âŒ Need at least 2 results for comparison")
            return
        
        # ê²°ê³¼ ì •ë ¬ (FPS ê¸°ì¤€)
        sorted_results = sorted(results, key=lambda x: x['fps'], reverse=True)
        
        print(f"\nğŸ“Š Performance Ranking (by FPS):")
        print("-" * 80)
        
        for i, result in enumerate(sorted_results, 1):
            framework = result['framework']
            precision = result.get('precision', 'N/A')
            avg_time = result['average_time_ms']
            fps = result['fps']
            size = result.get('model_size_mb', result.get('engine_size_mb', 'N/A'))
            
            print(f"{i}. {framework} ({precision})")
            print(f"   â±ï¸  Time: {avg_time:.3f} ms")
            print(f"   ğŸš€ FPS: {fps:.1f}")
            print(f"   ğŸ“ Size: {size if isinstance(size, str) else f'{size:.1f} MB'}")
            print()
        
        # ë¡œë´‡ íƒœìŠ¤í¬ ë¶„ì„
        print(f"ğŸ¤– Robot Task Analysis:")
        print("-" * 80)
        
        fastest = sorted_results[0]
        control_cycle = 20  # 20ms ì œì–´ ì£¼ê¸°
        
        print(f"   Control Cycle: {control_cycle}ms")
        print(f"   Fastest Framework: {fastest['framework']} ({fastest['average_time_ms']:.3f}ms)")
        print(f"   Usage: {fastest['average_time_ms']/control_cycle*100:.1f}% of control cycle")
        
        if fastest['average_time_ms'] < 1.0:
            print(f"   âœ… Excellent for real-time robot control")
        elif fastest['average_time_ms'] < 5.0:
            print(f"   âš ï¸  Good for robot control")
        else:
            print(f"   âŒ May cause control delays")
        
        # ê²°ê³¼ ì €ì¥
        report_path = "Robo+/Mobile_VLA/tensorrt_comparison_results.json"
        with open(report_path, "w") as f:
            json.dump({
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "device": str(self.device),
                "results": results,
                "ranking": [r['framework'] for r in sorted_results]
            }, f, indent=2)
        
        print(f"\nâœ… Comparison report saved: {report_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting TensorRT Optimization")
    print("ğŸ¯ Optimizing for Robot Tasks")
    
    optimizer = TensorRTOptimizer()
    
    try:
        # ëª¨ë“  í”„ë ˆì„ì›Œí¬ ë¹„êµ
        results = optimizer.compare_all_frameworks()
        
        print(f"\nâœ… TensorRT optimization completed!")
        print(f"ğŸ“Š Tested {len(results)} frameworks")
        print(f"ğŸ”§ Device: {optimizer.device}")
        
    except Exception as e:
        print(f"âŒ Optimization failed: {e}")
        raise

if __name__ == "__main__":
    main()
