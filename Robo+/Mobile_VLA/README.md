# ğŸ¤– Mobile VLA - Pure Mobile Vision-Language-Action System

**Calvin ì—†ëŠ” ìˆœìˆ˜ Mobile VLA ì‹œìŠ¤í…œ** - mobile_vla_data_collector.py 100% í˜¸í™˜

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

RoboVLMsì˜ VLM ê¸°ìˆ ì„ mobile_vla_data_collector.pyì— ì™„ì „ ì ì‘ì‹œí‚¨ ìˆœìˆ˜ Mobile VLA ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
Calvin ì˜ì¡´ì„± ì—†ì´ mobile_vla_data_collector.pyê°€ ìƒì„±í•˜ëŠ” HDF5 ë°ì´í„°ë¥¼ ì§ì ‘ í•™ìŠµí•©ë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

### âœ… ì™„ì„±ëœ êµ¬í˜„ 
- **ğŸ“¦ MobileVLADataset**: 70ê°œ ì—í”¼ì†Œë“œ, 1,228ê°œ í”„ë ˆì„ ì§ì ‘ ë¡œë”©
- **ğŸ§  Mobile VLA Model**: 3.7M(Lite) ~ 155M(Full) íŒŒë¼ë¯¸í„° 
- **ğŸ‹ï¸ Simple Trainer**: í•™ìŠµ/ê²€ì¦/ì¶”ë¡  ì™„ì „ êµ¬í˜„
- **ğŸ¯ Mobile ì•¡ì…˜ ì˜ˆì¸¡**: mobile_vla_data_collector.py 100% í˜¸í™˜

### ğŸ”¥ í•µì‹¬ í˜ì‹ 
- **Calvin ì˜ì¡´ì„± ì œê±°**: ìˆœìˆ˜ Mobile ë°ì´í„° í˜•ì‹ ì‚¬ìš©
- **720p â†’ 224p ìë™ ë¦¬ì‚¬ì´ì¦ˆ**: VLM ìµœì í™” ì „ì²˜ë¦¬
- **í•œêµ­ì–´ ë„¤ë¹„ê²Œì´ì…˜ ëª…ë ¹**: ì‹œë‚˜ë¦¬ì˜¤ë³„ í•œêµ­ì–´ ì§€ì›
- **ì‹œë‚˜ë¦¬ì˜¤ ì¸ì§€ í•™ìŠµ**: 8ê°€ì§€ ì»µ ë„ë‹¬ ì‹œë‚˜ë¦¬ì˜¤ íŠ¹í™”
- **ì´ë²¤íŠ¸ ê¸°ë°˜ íƒ€ì„ìŠ¤íƒ¬í”„**: start_action, stop_action ì˜ˆì¸¡

## ğŸ“Š ë°ì´í„° í˜•ì‹ (mobile_vla_data_collector.py ê¸°ì¤€)

```python
# HDF5 íŒŒì¼ êµ¬ì¡°
{
    "images": [18, 720, 1280, 3],      # RGB ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
    "actions": [18, 3],                # [linear_x, linear_y, angular_z]
    "action_event_types": [18],        # ['episode_start', 'start_action', 'stop_action']
    "episode_name": "episode_20250808_123136_1box_vert_left",
    "num_frames": 18,
    "total_duration": 18.87
}

# ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘
scenarios = {
    "1box_vert_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "1box_vert_right": "ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "1box_hori_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ í”¼í•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "1box_hori_right": "ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í”¼í•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "2box_vert_left": "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì™¼ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "2box_vert_right": "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì˜¤ë¥¸ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "2box_hori_left": "ë‘ ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "2box_hori_right": "ë‘ ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”"
}
```

## ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜

### Full Model (155.1M íŒŒë¼ë¯¸í„°)
```python
MobileImageEncoder (EfficientNet V2-S)
    â†“ [B, T, 768]
KoreanTextEncoder (KLUE RoBERTa)  
    â†“ [B, 768]
MultiheadAttention Fusion
    â†“ [B, T, 768]
MobilePolicyHead (LSTM + MLP)
    â†“ actions: [B, T, 3], events: [B, T, 3]
```

### Lite Model (3.7M íŒŒë¼ë¯¸í„°, Jetson ìµœì í™”)
```python
MobileImageEncoderLite (MobileNet V3-Small)
    â†“ [B, T, 256]
KoreanTextEncoderLite (Scenario Embedding)
    â†“ [B, 256]  
Simple Concatenation + MLP
    â†“ [B, T, 512]
MobilePolicyHeadLite (MLP only)
    â†“ actions: [B, T, 3], events: [B, T, 3]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
```bash
cd /home/soda/vla/Robo+/Mobile_VLA/data
python3 mobile_dataset.py

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ“ Mobile VLA Dataset ë¡œë“œ ì™„ë£Œ!
# ğŸ“Š ì´ 70ê°œ ì—í”¼ì†Œë“œ, 1,228ê°œ í”„ë ˆì„
# ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬: {'1box_vert_left': 15, '1box_hori_right': 15, ...}
```

### 2. ëª¨ë¸ í…ŒìŠ¤íŠ¸
```bash
cd /home/soda/vla/Robo+/Mobile_VLA/models
python3 mobile_vla_model.py

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ’ª Full Model: 155,075,649ê°œ (155.1M)
# ğŸš€ Lite Model: 3,658,254ê°œ (3.7M)
# ê²½ëŸ‰í™”ìœ¨: 97.6%
```

### 3. í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
```bash
cd /home/soda/vla/Robo+/Mobile_VLA/training
python3 mobile_trainer_simple.py

# ì¶œë ¥ ì˜ˆì‹œ:
# ğŸ“ˆ í•™ìŠµ ê²°ê³¼: total_loss: 2.3396, action_accuracy: 0.0278
# ğŸ¯ Mobile ì•¡ì…˜ ì˜ˆì¸¡: {'linear_x': 0.45, 'linear_y': -0.45, 'angular_z': 0.59, 'event_type': 'start_action'}
```

## ğŸ“ˆ í•™ìŠµ ì„¤ì •

### ê¸°ë³¸ ì„¤ì •
```python
configs = {
    "hidden_size": 768,                    # Full: 768, Lite: 512
    "use_lite_mode": False,                # Jetsonìš© ê²½ëŸ‰í™” ëª¨ë“œ
    "learning_rate": 1e-4,
    "batch_size": 4,
    "sequence_length": 18,                 # mobile_vla_data_collector.py í‘œì¤€
    "max_epochs": 100,
    "scheduler": "cosine",
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ (ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤ ë†’ì€ ê°€ì¤‘ì¹˜)
    "scenario_weights": {
        "1box_vert_left": 1.0,
        "1box_vert_right": 1.0,
        "1box_hori_left": 1.2,
        "1box_hori_right": 1.1,
        "2box_vert_left": 1.5,
        "2box_vert_right": 1.4,
        "2box_hori_left": 1.8,             # ê°€ì¥ ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
        "2box_hori_right": 1.6
    }
}
```

### ì†ì‹¤ í•¨ìˆ˜
```python
total_loss = (
    action_loss_weight * action_mse_loss +        # ì•¡ì…˜ ì •í™•ë„
    event_loss_weight * event_cross_entropy +     # ì´ë²¤íŠ¸ ë¶„ë¥˜
    scenario_loss_weight * scenario_consistency   # ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„±
) * scenario_weight
```

## ğŸ¯ ì‹¤ì‹œê°„ ì¶”ë¡  (mobile_vla_data_collector.py ì—°ë™)

```python
# íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = SimpleMobileVLATrainer(configs)

# ì‹¤ì‹œê°„ ì•¡ì…˜ ì˜ˆì¸¡
current_image = torch.randn(1, 3, 224, 224)    # í˜„ì¬ ì¹´ë©”ë¼ ì´ë¯¸ì§€
scenario = "1box_vert_left"                    # í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤

mobile_action = trainer.predict_mobile_action(current_image, scenario)
# ê²°ê³¼: {'linear_x': 0.45, 'linear_y': -0.45, 'angular_z': 0.59, 'event_type': 'start_action'}

# mobile_vla_data_collector.pyì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥!
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Mobile_VLA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mobile_dataset.py              # 70ê°œ HDF5 ì—í”¼ì†Œë“œ ë¡œë”
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”œâ”€â”€ mobile_image_encoder.py    # 720pâ†’768D ì´ë¯¸ì§€ ì¸ì½”ë”©
â”‚   â”‚   â””â”€â”€ korean_text_encoder.py     # í•œêµ­ì–´ ëª…ë ¹ì–´ ì¸ì½”ë”©
â”‚   â”œâ”€â”€ policy_heads/
â”‚   â”‚   â””â”€â”€ mobile_policy_head.py      # 3D ì•¡ì…˜ + ì´ë²¤íŠ¸ ì˜ˆì¸¡
â”‚   â”œâ”€â”€ mobile_vla_model.py            # í†µí•© Mobile VLA ëª¨ë¸
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ mobile_trainer_simple.py       # í•™ìŠµ/ê²€ì¦/ì¶”ë¡  ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ README.md                          # ì´ íŒŒì¼
```

## ğŸ”¥ ì„±ëŠ¥ ê²°ê³¼

### ğŸ“Š ëª¨ë¸ í¬ê¸° ë¹„êµ
- **Full Model**: 155.1M íŒŒë¼ë¯¸í„° (ê³ ì„±ëŠ¥)
- **Lite Model**: 3.7M íŒŒë¼ë¯¸í„° (97.6% ê²½ëŸ‰í™”, Jetson ìµœì í™”)

### ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„
- **ì´ ì—í”¼ì†Œë“œ**: 70ê°œ
- **ì´ í”„ë ˆì„**: 1,228ê°œ  
- **ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬**: 4ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ ê· ë“± ë¶„ë°°
- **í‘œì¤€ ê¸¸ì´**: 18í”„ë ˆì„ (mobile_vla_data_collector.py ê¸°ì¤€)

### ğŸ¯ í•™ìŠµ ë©”íŠ¸ë¦­
- **ì•¡ì…˜ ì •í™•ë„**: í—ˆìš© ì˜¤ì°¨ 0.1 ì´ë‚´ ì˜ˆì¸¡ë¥ 
- **ì´ë²¤íŠ¸ ì •í™•ë„**: start/stop íƒ€ì´ë° ì˜ˆì¸¡ë¥   
- **ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„±**: ë™ì¼ ì‹œë‚˜ë¦¬ì˜¤ ë‚´ í–‰ë™ ì¼ê´€ì„±

## ğŸš€ í–¥í›„ ê³„íš

### Phase 1: ê³ ë„í™” (1-2ì£¼)
- [ ] PyTorch Lightning íŠ¸ë ˆì´ë„ˆ êµ¬í˜„
- [ ] TensorBoard/Wandb ë¡œê¹…
- [ ] ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### Phase 2: ì‹¤ì‹œê°„ í†µí•© (2-3ì£¼)  
- [ ] mobile_vla_data_collector.py ì§ì ‘ ì—°ë™
- [ ] ROS2 ì‹¤ì‹œê°„ ì¶”ë¡  ë…¸ë“œ
- [ ] Jetson ë°°í¬ ìµœì í™”

### Phase 3: ë…¼ë¬¸ (3-4ì£¼)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
- [ ] Ablation Studies
- [ ] Robo-Mobile VLA ë…¼ë¬¸ ì‘ì„±

## ğŸ† í•µì‹¬ ì„±ê³¼

âœ… **Calvin ì˜ì¡´ì„± ì™„ì „ ì œê±°** - ìˆœìˆ˜ Mobile ë°ì´í„° í˜•ì‹ ì‚¬ìš©  
âœ… **70ê°œ ì—í”¼ì†Œë“œ ì§ì ‘ í•™ìŠµ** - mobile_vla_data_collector.py 100% í˜¸í™˜  
âœ… **97.6% ëª¨ë¸ ê²½ëŸ‰í™”** - Jetson ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ëŠ¥  
âœ… **í•œêµ­ì–´ ë„¤ë¹„ê²Œì´ì…˜** - ì‹œë‚˜ë¦¬ì˜¤ë³„ í•œêµ­ì–´ ëª…ë ¹ì–´ ì§€ì›  
âœ… **ì´ë²¤íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡** - start/stop íƒ€ì´ë° í•™ìŠµ  
âœ… **ì‹¤ì‹œê°„ ì¶”ë¡  ì¤€ë¹„** - mobile_vla_data_collector.py ì—°ë™ ê°€ëŠ¥  

**RoboVLMsì˜ VLM ê¸°ìˆ **ê³¼ **mobile_vla_data_collector.pyì˜ ì‹¤ìš©ì„±**ì´ ì™„ë²½í•˜ê²Œ ê²°í•©ëœ **Mobile VLA ì‹œìŠ¤í…œ**ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰
