#!/usr/bin/env python3
"""
MAE 0.212 ëª¨ë¸ì˜ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from pathlib import Path
import h5py
from PIL import Image
import torchvision.transforms as transforms
from transformers import AutoProcessor, AutoModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Kosmos2CLIPHybridModel(nn.Module):
    """Kosmos2+CLIP Hybrid ëª¨ë¸"""
    
    def __init__(self, processor, hidden_dim=512, dropout=0.2):
        super().__init__()
        self.processor = processor
        
        # Kosmos2 í†µí•© ëª¨ë¸
        self.kosmos2_model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # CLIP ëª¨ë¸ë“¤
        self.clip_vision = AutoModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_text = AutoModel.from_pretrained("openai/clip-vit-base-patch32")
        
        # Feature Fusion (Kosmos2 + CLIP)
        self.fusion = nn.Sequential(
            nn.Linear(2048 + 512, hidden_dim),  # Kosmos2 + CLIP Text
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Action Head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 2)  # 2D action
        )
    
    def forward(self, images, text_inputs):
        # Kosmos2 í†µí•© ì²˜ë¦¬
        kosmos2_inputs = self.processor(
            images=images,
            text=text_inputs,
            return_tensors="pt"
        )
        kosmos2_outputs = self.kosmos2_model(**kosmos2_inputs)
        kosmos2_features = kosmos2_outputs.pooler_output  # [batch, 2048]
        
        # CLIP Text
        clip_text_outputs = self.clip_text(**text_inputs)
        clip_text_features = clip_text_outputs.pooler_output  # [batch, 512]
        
        # Feature Fusion
        combined = torch.cat([kosmos2_features, clip_text_features], dim=1)
        fused = self.fusion(combined)
        
        # Action Prediction
        actions = self.action_head(fused)
        
        return actions

class Original72EpisodesDataset:
    """ì›ë³¸ 72 ì—í”¼ì†Œë“œ ë°ì´í„°ì…‹"""
    def __init__(self, data_dir, processor):
        self.data_dir = Path(data_dir)
        self.processor = processor
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # HDF5 íŒŒì¼ë“¤ ë¡œë“œ
        self.all_data = []
        h5_files = list(self.data_dir.glob("*.h5"))
        logger.info(f"ğŸ“Š ë¡œë“œëœ HDF5 íŒŒì¼ ìˆ˜: {len(h5_files)}")
        
        for h5_file in h5_files:
            with h5py.File(h5_file, 'r') as f:
                # ì´ë¯¸ì§€ ë°ì´í„°
                images = f['images'][:]  # [num_frames, height, width, channels]
                actions = f['actions'][:]  # [num_frames, action_dim]
                
                # ê° í”„ë ˆì„ì„ ê°œë³„ ìƒ˜í”Œë¡œ ì²˜ë¦¬
                for i in range(len(images)):
                    image = Image.fromarray(images[i])
                    action = actions[i][:2]  # 2D ì•¡ì…˜ë§Œ ì‚¬ìš©
                    
                    self.all_data.append({
                        'image': image,
                        'action': action,
                        'text': "Navigate around obstacles to track the target cup"
                    })
        
        logger.info(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(self.all_data)}")
    
    def __len__(self):
        return len(self.all_data)
    
    def __getitem__(self, idx):
        data = self.all_data[idx]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = self.transform(data['image'])
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (CLIP ê¸°ë³¸ ê¸¸ì´ ì‚¬ìš©)
        text_inputs = self.processor(
            text=data['text'],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=77  # CLIP ê¸°ë³¸ ìµœëŒ€ ê¸¸ì´
        )
        
        # í…ì„œì—ì„œ ìŠ¤ì¹¼ë¼ ì¶”ì¶œ
        for key in text_inputs:
            text_inputs[key] = text_inputs[key].squeeze(0)
        
        # ì•¡ì…˜ì„ í…ì„œë¡œ ë³€í™˜
        action = torch.tensor(data['action'], dtype=torch.float32)
        
        return {
            'image': image,
            'text_inputs': text_inputs,
            'action': action
        }

def calculate_mae(model, dataset, device):
    """MAE ê³„ì‚°"""
    model.eval()
    total_mae = 0.0
    num_samples = 0
    
    with torch.no_grad():
        for i in range(min(100, len(dataset))):  # ì²˜ìŒ 100ê°œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
            sample = dataset[i]
            
            image = sample['image'].unsqueeze(0).to(device)
            text_inputs = {k: v.unsqueeze(0).to(device) for k, v in sample['text_inputs'].items()}
            target_action = sample['action'].unsqueeze(0).to(device)
            
            # ì˜ˆì¸¡
            predicted_action = model(image, text_inputs)
            
            # MAE ê³„ì‚°
            mae = torch.mean(torch.abs(predicted_action - target_action)).item()
            total_mae += mae
            num_samples += 1
            
            if i % 20 == 0:
                logger.info(f"ìƒ˜í”Œ {i}: ì˜ˆì¸¡={predicted_action[0].cpu().numpy()}, ì‹¤ì œ={target_action[0].cpu().numpy()}, MAE={mae:.4f}")
    
    avg_mae = total_mae / num_samples
    return avg_mae

def main():
    logger.info("ğŸ” MAE 0.212 ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ì‹œì‘")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    logger.info("ğŸ“¥ Kosmos2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ëª¨ë¸ ìƒì„±
    logger.info("ğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
    model = Kosmos2CLIPHybridModel(processor)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    logger.info("ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘...")
    checkpoint_path = "results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth"
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"   - ì—í¬í¬: {checkpoint['epoch']}")
    logger.info(f"   - ì €ì¥ëœ MAE: {checkpoint['val_mae']:.4f}")
    logger.info(f"   - ë°ì´í„° ê²½ë¡œ: {checkpoint['args'].data_path}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = Original72EpisodesDataset(checkpoint['args'].data_path, processor)
    
    # MAE ê³„ì‚°
    logger.info("ğŸ“ˆ MAE ê³„ì‚° ì¤‘...")
    actual_mae = calculate_mae(model, dataset, device)
    
    logger.info("ğŸ¯ ê²°ê³¼:")
    logger.info(f"   - ì €ì¥ëœ MAE: {checkpoint['val_mae']:.4f}")
    logger.info(f"   - ì‹¤ì œ ì¸¡ì • MAE: {actual_mae:.4f}")
    logger.info(f"   - ì°¨ì´: {abs(checkpoint['val_mae'] - actual_mae):.4f}")
    
    if abs(checkpoint['val_mae'] - actual_mae) < 0.01:
        logger.info("âœ… MAE ê°’ì´ ì¼ì¹˜í•©ë‹ˆë‹¤!")
    else:
        logger.warning("âš ï¸ MAE ê°’ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
