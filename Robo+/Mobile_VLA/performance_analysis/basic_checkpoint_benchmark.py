#!/usr/bin/env python3
"""
ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ë“¤ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµ
"""

import torch
import torch.nn as nn
import time
import json
import logging
import os
import gc
import numpy as np
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BasicCheckpointBenchmark:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_runs = 50
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def _load_checkpoint_info(self, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¡œë“œ"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            info = {
                'path': checkpoint_path,
                'mae': checkpoint.get('val_mae', 'N/A'),
                'epoch': checkpoint.get('epoch', 'N/A'),
                'model_keys': len(checkpoint.get('model_state_dict', {})),
                'checkpoint_size_mb': os.path.getsize(checkpoint_path) / (1024 * 1024)
            }
            
            # ëª¨ë¸ íƒ€ì… íŒë³„
            state_dict = checkpoint.get('model_state_dict', {})
            kosmos_keys = [key for key in state_dict.keys() if 'kosmos' in key.lower()]
            clip_keys = [key for key in state_dict.keys() if 'clip' in key.lower()]
            
            if len(clip_keys) > 0 and len(kosmos_keys) > 0:
                info['model_type'] = 'Kosmos2+CLIP Hybrid'
            elif len(kosmos_keys) > 0:
                info['model_type'] = 'Pure Kosmos2'
            elif len(clip_keys) > 0:
                info['model_type'] = 'CLIP Only'
            else:
                info['model_type'] = 'Unknown'
            
            return info, checkpoint
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ {checkpoint_path}: {e}")
            return None, None
    
    def _create_simple_benchmark_model(self, state_dict):
        """ë²¤ì¹˜ë§ˆí‚¹ìš© ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±"""
        class SimpleBenchmarkModel(nn.Module):
            def __init__(self, state_dict):
                super().__init__()
                
                # RNN ì…ë ¥ í¬ê¸° í™•ì¸
                rnn_input_size = None
                for key in state_dict.keys():
                    if 'weight_ih_l0' in key:
                        rnn_input_size = state_dict[key].shape[1]
                        break
                
                if rnn_input_size is None:
                    rnn_input_size = 2048  # ê¸°ë³¸ê°’
                
                # ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡°
                self.feature_extractor = nn.Linear(3*224*224, rnn_input_size)
                self.rnn = nn.RNN(
                    input_size=rnn_input_size,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                x_flat = x.view(batch_size, -1)
                features = self.feature_extractor(x_flat)
                sequence_features = features.unsqueeze(1)
                rnn_out, _ = self.rnn(sequence_features)
                actions = self.actions(rnn_out.squeeze(1))
                return actions
        
        return SimpleBenchmarkModel(state_dict)
    
    def _benchmark_model(self, model, name, input_data):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        model = model.to(self.device)
        model.eval()
        
        # ì›Œë°ì—…
        for _ in range(5):
            with torch.no_grad():
                _ = model(input_data)
        
        # ë©”ëª¨ë¦¬ ì¸¡ì •
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            start_memory = torch.cuda.memory_allocated()
        else:
            start_memory = 0
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        
        for i in range(self.num_runs):
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            
            if (i + 1) % 10 == 0:
                logger.info(f"   ì§„í–‰ë¥ : {i+1}/{self.num_runs}")
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            end_memory = torch.cuda.memory_allocated()
            peak_memory = torch.cuda.max_memory_allocated()
            
            memory_used = (end_memory - start_memory) / (1024 ** 2)  # MB
            peak_memory_used = peak_memory / (1024 ** 2)  # MB
        else:
            memory_used = 0
            peak_memory_used = 0
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1000 / avg_time
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {peak_memory_used:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': avg_time,
            'inference_time_std': std_time,
            'memory_usage_mb': memory_used,
            'peak_memory_mb': peak_memory_used,
            'fps': fps
        }
    
    def run_all_checkpoints_benchmark(self):
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        logger.info("ğŸš€ ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
        checkpoint_paths = [
            "results/simple_lstm_results_extended/final_simple_lstm_model.pth",
            "results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth",
            "results/mobile_vla_epoch_3.pt",
            "models/experimental/simplified_robovlms_best.pth"
        ]
        
        # ì…ë ¥ ë°ì´í„° ìƒì„±
        batch_size = 1
        input_data = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        results = {}
        
        for checkpoint_path in checkpoint_paths:
            if not os.path.exists(checkpoint_path):
                logger.warning(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
                continue
            
            logger.info("\n" + "="*60)
            logger.info(f"ğŸ¯ {checkpoint_path} ë¶„ì„")
            logger.info("="*60)
            
            # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë¡œë“œ
            info, checkpoint = self._load_checkpoint_info(checkpoint_path)
            
            if info is None:
                continue
            
            logger.info(f"   ëª¨ë¸ íƒ€ì…: {info['model_type']}")
            logger.info(f"   MAE: {info['mae']}")
            logger.info(f"   ì—í¬í¬: {info['epoch']}")
            logger.info(f"   ëª¨ë¸ í‚¤: {info['model_keys']}ê°œ")
            logger.info(f"   íŒŒì¼ í¬ê¸°: {info['checkpoint_size_mb']:.2f} MB")
            
            # ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ ìƒì„±
            state_dict = checkpoint.get('model_state_dict', {})
            benchmark_model = self._create_simple_benchmark_model(state_dict)
            
            # ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
            benchmark_results = self._benchmark_model(benchmark_model, info['model_type'], input_data)
            
            # ê²°ê³¼ ì €ì¥
            model_name = Path(checkpoint_path).stem
            results[model_name] = {
                'checkpoint_info': info,
                'benchmark_results': benchmark_results
            }
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del benchmark_model
            del checkpoint
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ê²°ê³¼ ì¶œë ¥
        if results:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ë²¤ì¹˜ë§ˆí‚¹ ê²°ê³¼")
            logger.info("="*80)
            
            # ì„±ëŠ¥ ìˆœìœ„
            performance_ranking = []
            for model_name, data in results.items():
                mae = data['checkpoint_info']['mae']
                fps = data['benchmark_results']['fps']
                inference_time = data['benchmark_results']['inference_time_ms']
                
                performance_ranking.append({
                    'model_name': model_name,
                    'model_type': data['checkpoint_info']['model_type'],
                    'mae': mae,
                    'fps': fps,
                    'inference_time_ms': inference_time,
                    'memory_mb': data['benchmark_results']['memory_usage_mb']
                })
            
            # FPS ê¸°ì¤€ ì •ë ¬
            performance_ranking.sort(key=lambda x: x['fps'], reverse=True)
            
            logger.info("\nğŸ† ì„±ëŠ¥ ìˆœìœ„ (FPS ê¸°ì¤€):")
            for i, model in enumerate(performance_ranking, 1):
                logger.info(f"{i}. {model['model_name']} ({model['model_type']})")
                logger.info(f"   MAE: {model['mae']}")
                logger.info(f"   FPS: {model['fps']:.2f}")
                logger.info(f"   ì¶”ë¡  ì‹œê°„: {model['inference_time_ms']:.2f} ms")
                logger.info(f"   ë©”ëª¨ë¦¬: {model['memory_mb']:.2f} MB")
                logger.info()
            
            # MAE ê¸°ì¤€ ì •ë ¬
            mae_ranking = [m for m in performance_ranking if m['mae'] != 'N/A']
            mae_ranking.sort(key=lambda x: x['mae'])
            
            if mae_ranking:
                logger.info("ğŸ¯ ì •í™•ë„ ìˆœìœ„ (MAE ê¸°ì¤€):")
                for i, model in enumerate(mae_ranking, 1):
                    logger.info(f"{i}. {model['model_name']} ({model['model_type']})")
                    logger.info(f"   MAE: {model['mae']}")
                    logger.info(f"   FPS: {model['fps']:.2f}")
                    logger.info()
            
            # ê²°ê³¼ ì €ì¥
            output_path = "results/all_checkpoints_benchmark_results.json"
            with open(output_path, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            logger.info(f"\nâœ… ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return results

def main():
    benchmark = BasicCheckpointBenchmark()
    results = benchmark.run_all_checkpoints_benchmark()
    return results

if __name__ == "__main__":
    main()
