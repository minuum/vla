#!/usr/bin/env python3
"""
ğŸ“ ê±°ë¦¬ë³„ íŠ¹í™” ì¦ê°• ë°ì´í„°ë¡œ í•™ìŠµ
"""
import sys
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import json
import logging
from datetime import datetime
from transformers import AutoProcessor
import torchvision.transforms as transforms
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DistanceAwareDataset(Dataset):
    def __init__(self, data_dir: Path, transform=None):
        self.data_dir = data_dir
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        self.episode_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir() and d.name.startswith('episode_')])
        logger.info(f"ğŸ“ ë°œê²¬ëœ ì—í”¼ì†Œë“œ: {len(self.episode_dirs)}ê°œ")
        
        # ê±°ë¦¬ë³„ í†µê³„
        self.distance_stats = self._analyze_distance_distribution()
        logger.info("ğŸ“Š ê±°ë¦¬ë³„ ë¶„í¬:")
        for distance, count in self.distance_stats.items():
            logger.info(f"   {distance}: {count}ê°œ")

    def _analyze_distance_distribution(self) -> Dict[str, int]:
        """ê±°ë¦¬ë³„ ë¶„í¬ ë¶„ì„"""
        distance_counts = {}
        for episode_dir in self.episode_dirs:
            metadata_path = episode_dir / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                    distance = metadata.get('distance', 'unknown')
                    distance_counts[distance] = distance_counts.get(distance, 0) + 1
        return distance_counts

    def __len__(self):
        return len(self.episode_dirs)

    def __getitem__(self, idx):
        episode_dir = self.episode_dirs[idx]
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = episode_dir / "metadata.json"
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_files = sorted(episode_dir.glob("frame_*.jpg"))
        images = []
        for img_path in image_files:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            images.append(image)
        
        # ì•¡ì…˜ ë¡œë“œ
        actions_path = episode_dir / "actions.npy"
        actions = np.load(actions_path)
        
        return {
            'images': torch.stack(images),  # [T, C, H, W]
            'actions': torch.from_numpy(actions).float(),  # [T, 3]
            'episode_id': metadata['episode_id'],
            'distance': metadata['distance'],
            'augmentation_type': metadata.get('augmentation_type', 'original')
        }

class DistanceAwareVLAModel(nn.Module):
    def __init__(self, processor, hidden_size=768, lstm_hidden_size=256):
        super().__init__()
        self.processor = processor
        self.hidden_size = hidden_size
        self.lstm_hidden_size = lstm_hidden_size
        
        # Kosmos2 ëª¨ë¸
        from transformers import AutoModel
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # ê±°ë¦¬ë³„ ê°€ì¤‘ì¹˜ (Close: ë†’ìŒ, Medium: ë³´í†µ, Far: ë‚®ìŒ)
        self.distance_weights = {
            'close': 1.5,    # ì •ë°€ë„ ì¤‘ìš”
            'medium': 1.0,   # í‘œì¤€
            'far': 0.8       # ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€
        }
        
        # LSTM + MLP ì•¡ì…˜ í—¤ë“œ
        self.lstm = nn.LSTM(hidden_size, lstm_hidden_size, batch_first=True)
        self.action_head = nn.Sequential(
            nn.Linear(lstm_hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 3)  # [linear_x, linear_y, angular_z]
        )
        
        # ê±°ë¦¬ë³„ íŠ¹í™” ë ˆì´ì–´
        self.distance_embedding = nn.Embedding(3, 32)  # close, medium, far
        self.distance_fusion = nn.Linear(lstm_hidden_size + 32, lstm_hidden_size)

    def forward(self, images, distance_labels=None):
        batch_size, seq_len, c, h, w = images.shape
        device = images.device
        
        # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        image_features = []
        for t in range(seq_len):
            try:
                # Kosmos2 í˜¸ì¶œ
                pixel_values = images[:, t, :, :, :]
                dummy_input_ids = torch.zeros(batch_size, 1, dtype=torch.long, device=device)
                dummy_attention_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=device)
                
                with torch.no_grad():
                    vision_outputs = self.kosmos.vision_model(pixel_values=pixel_values)
                    features = vision_outputs.last_hidden_state.mean(dim=1)
            except:
                # ëŒ€ì²´ ë°©ë²•
                features = torch.randn(batch_size, self.hidden_size, device=device)
            
            # í¬ê¸° í†µì¼
            if features.shape[-1] != self.hidden_size:
                if not hasattr(self, 'feature_adapter'):
                    self.feature_adapter = nn.Linear(features.shape[-1], self.hidden_size).to(features.device)
                features = self.feature_adapter(features)
            
            image_features.append(features)
        
        # ì‹œí€€ìŠ¤ íŠ¹ì§•
        sequence_features = torch.stack(image_features, dim=1)  # [B, T, H]
        
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(sequence_features)  # [B, T, LSTM_H]
        
        # ê±°ë¦¬ë³„ íŠ¹í™” ì²˜ë¦¬
        if distance_labels is not None:
            distance_embeds = self.distance_embedding(distance_labels)  # [B, 32]
            distance_embeds = distance_embeds.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, 32]
            
            # ê±°ë¦¬ ì •ë³´ì™€ LSTM ì¶œë ¥ ê²°í•©
            combined_features = torch.cat([lstm_out, distance_embeds], dim=-1)
            lstm_out = self.distance_fusion(combined_features)
        
        # ì•¡ì…˜ ì˜ˆì¸¡ (ë§ˆì§€ë§‰ ì‹œì )
        final_features = lstm_out[:, -1, :]  # [B, LSTM_H]
        predicted_actions = self.action_head(final_features)  # [B, 3]
        
        return predicted_actions

class DistanceAwareTrainer:
    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=3, factor=0.5)
        
        # ê±°ë¦¬ë³„ ê°€ì¤‘ì¹˜
        self.distance_weights = {
            'close': 1.5,
            'medium': 1.0,
            'far': 0.8
        }
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.HuberLoss()
        
        # ê±°ë¦¬ ë ˆì´ë¸” ë§¤í•‘
        self.distance_to_label = {'close': 0, 'medium': 1, 'far': 2}

    def _collate_fn(self, batch):
        """ë°°ì¹˜ ë°ì´í„° ê²°í•©"""
        images_list = [item['images'] for item in batch]
        actions_list = [item['actions'] for item in batch]
        episode_ids = [item['episode_id'] for item in batch]
        distances = [item['distance'] for item in batch]
        augmentation_types = [item['augmentation_type'] for item in batch]
        
        # ê±°ë¦¬ ë ˆì´ë¸” ë³€í™˜
        distance_to_label = {'close': 0, 'medium': 1, 'far': 2}
        distance_labels = torch.tensor([distance_to_label[d] for d in distances])
        
        # ë°°ì¹˜ë¡œ ìŠ¤íƒ
        images = torch.stack(images_list)
        actions = torch.stack(actions_list)
        
        return {
            'images': images,
            'actions': actions,
            'episode_id': episode_ids,
            'distance': distances,
            'distance_labels': distance_labels,
            'augmentation_type': augmentation_types
        }

    def train_epoch(self, dataloader):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        total_mae = 0
        num_batches = 0
        
        logger.info(f"  ğŸ“Š ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
        
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx % 10 == 0:  # ì§„í–‰ë¥  í‘œì‹œ
                logger.info(f"    ë°°ì¹˜ {batch_idx}/{len(dataloader)} ì²˜ë¦¬ ì¤‘...")
            images = batch['images'].to(self.device)
            actions = batch['actions'].to(self.device)
            distance_labels = batch['distance_labels'].to(self.device)
            distances = batch['distance']
            
            # ì•¡ì…˜ ì •ê·œí™” (ë§ˆì§€ë§‰ 2ê°œ í”„ë ˆì„ë§Œ ì‚¬ìš©)
            target_actions = actions[:, -2:, :].mean(dim=1)  # [B, 3]
            
            # ìˆœì „íŒŒ
            predicted_actions = self.model(images, distance_labels)
            
            # ê±°ë¦¬ë³„ ê°€ì¤‘ ì†ì‹¤ ê³„ì‚°
            batch_loss = 0
            batch_mae = 0
            for i, distance in enumerate(distances):
                weight = self.distance_weights.get(distance, 1.0)
                loss = self.criterion(predicted_actions[i:i+1], target_actions[i:i+1])
                batch_loss += loss * weight
                batch_mae += torch.mean(torch.abs(predicted_actions[i] - target_actions[i])).item()
            
            batch_loss /= len(distances)
            batch_mae /= len(distances)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += batch_loss.item()
            total_mae += batch_mae
            num_batches += 1
        
        return total_loss / num_batches, total_mae / num_batches

    def validate(self, dataloader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        total_mae = 0
        num_batches = 0
        
        distance_metrics = {'close': [], 'medium': [], 'far': []}
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                images = batch['images'].to(self.device)
                actions = batch['actions'].to(self.device)
                distance_labels = batch['distance_labels'].to(self.device)
                distances = batch['distance']
                
                # ì•¡ì…˜ ì •ê·œí™”
                target_actions = actions[:, -2:, :].mean(dim=1)
                
                # ì˜ˆì¸¡
                predicted_actions = self.model(images, distance_labels)
                
                # ê±°ë¦¬ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
                batch_loss = 0
                batch_mae = 0
                for i, distance in enumerate(distances):
                    weight = self.distance_weights.get(distance, 1.0)
                    loss = self.criterion(predicted_actions[i:i+1], target_actions[i:i+1])
                    mae = torch.mean(torch.abs(predicted_actions[i] - target_actions[i])).item()
                    
                    batch_loss += loss * weight
                    batch_mae += mae
                    
                    # ê±°ë¦¬ë³„ ë©”íŠ¸ë¦­ ì €ì¥
                    distance_metrics[distance].append(mae)
                
                batch_loss /= len(distances)
                batch_mae /= len(distances)
                
                total_loss += batch_loss.item()
                total_mae += batch_mae
                num_batches += 1
        
        # ê±°ë¦¬ë³„ í‰ê·  MAE ê³„ì‚°
        distance_avg_mae = {}
        for distance, mae_list in distance_metrics.items():
            if mae_list:
                distance_avg_mae[distance] = np.mean(mae_list)
            else:
                distance_avg_mae[distance] = 0.0
        
        return total_loss / num_batches, total_mae / num_batches, distance_avg_mae

    def train(self, train_dataloader, val_dataloader, num_epochs=20):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        logger.info("ğŸš€ ê±°ë¦¬ë³„ íŠ¹í™” í•™ìŠµ ì‹œì‘!")
        logger.info("=" * 60)
        
        best_val_loss = float('inf')
        training_history = []
        
        for epoch in range(num_epochs):
            logger.info(f"ğŸ”„ Epoch {epoch+1}/{num_epochs} ì‹œì‘...")
            
            # í•™ìŠµ
            train_loss, train_mae = self.train_epoch(train_dataloader)
            
            # ê²€ì¦
            val_loss, val_mae, distance_mae = self.validate(val_dataloader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step(val_loss)
            
            # ê¸°ë¡
            epoch_data = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_mae': train_mae,
                'val_loss': val_loss,
                'val_mae': val_mae,
                'distance_mae': distance_mae,
                'lr': self.optimizer.param_groups[0]['lr']
            }
            training_history.append(epoch_data)
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            logger.info(f"  Train - Loss: {train_loss:.4f}, MAE: {train_mae:.4f}")
            logger.info(f"  Val   - Loss: {val_loss:.4f}, MAE: {val_mae:.4f}")
            logger.info(f"  Distance MAE - Close: {distance_mae['close']:.4f}, Medium: {distance_mae['medium']:.4f}, Far: {distance_mae['far']:.4f}")
            logger.info(f"  LR: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'val_mae': val_mae
                }, 'best_distance_aware_model.pth')
                logger.info("  ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥!")
        
        # ê²°ê³¼ ì €ì¥
        results = {
            'training_history': training_history,
            'best_val_loss': best_val_loss,
            'final_train_mae': training_history[-1]['train_mae'],
            'final_val_mae': training_history[-1]['val_mae'],
            'final_distance_mae': training_history[-1]['distance_mae'],
            'total_episodes': len(train_dataloader.dataset) + len(val_dataloader.dataset),
            'distance_distribution': {'close': 160, 'medium': 160, 'far': 160},  # ê³ ì •ê°’ ì‚¬ìš©
            'num_epochs': num_epochs,
            'completion_date': datetime.now().isoformat()
        }
        
        with open('distance_aware_training_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ‰ ê±°ë¦¬ë³„ íŠ¹í™” í•™ìŠµ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥ - Val MAE: {results['final_val_mae']:.4f}")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥: distance_aware_training_results.json")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë°ì´í„° ë¡œë“œ
    data_dir = Path("distance_aware_augmented_dataset")
    if not data_dir.exists():
        logger.error("âŒ ê±°ë¦¬ë³„ íŠ¹í™” ì¦ê°• ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = DistanceAwareDataset(data_dir)
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í•  (8:2)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í•  - í•™ìŠµ: {len(train_dataset)}, ê²€ì¦: {len(val_dataset)}")
    
    # collate í•¨ìˆ˜ ì •ì˜
    def collate_fn(batch):
        """ë°°ì¹˜ ë°ì´í„° ê²°í•©"""
        images_list = [item['images'] for item in batch]
        actions_list = [item['actions'] for item in batch]
        episode_ids = [item['episode_id'] for item in batch]
        distances = [item['distance'] for item in batch]
        augmentation_types = [item['augmentation_type'] for item in batch]
        
        # ê±°ë¦¬ ë ˆì´ë¸” ë³€í™˜
        distance_to_label = {'close': 0, 'medium': 1, 'far': 2}
        distance_labels = torch.tensor([distance_to_label[d] for d in distances])
        
        # ë°°ì¹˜ë¡œ ìŠ¤íƒ
        images = torch.stack(images_list)
        actions = torch.stack(actions_list)
        
        return {
            'images': images,
            'actions': actions,
            'episode_id': episode_ids,
            'distance': distances,
            'distance_labels': distance_labels,
            'augmentation_type': augmentation_types
        }
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=8,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        shuffle=True, 
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
    )
    
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=8,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        shuffle=False, 
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
    )
    
    # ëª¨ë¸ ìƒì„±
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    model = DistanceAwareVLAModel(processor)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = DistanceAwareTrainer(model)
    results = trainer.train(train_dataloader, val_dataloader, num_epochs=15)
    
    logger.info("ğŸ¯ ê±°ë¦¬ë³„ íŠ¹í™” í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
