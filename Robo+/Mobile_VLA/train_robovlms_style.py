"""
ğŸš€ RoboVLMs Style Training Script
ì›ë³¸ 72ê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ RoboVLMs ìŠ¤íƒ€ì¼ ëª¨ë¸ í›ˆë ¨
ë‹¨ì¼ ì´ë¯¸ì§€ â†’ ë‹¨ì¼ ì•¡ì…˜ (ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ìš©)
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from tqdm import tqdm
import json

from robovlms_style_single_image_model import (
    RoboVLMStyleSingleImageModel,
    train_robovlms_style_model
)

class RoboVLMStyleDataset(Dataset):
    """RoboVLMs ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ (ë‹¨ì¼ ì´ë¯¸ì§€ â†’ ë‹¨ì¼ ì•¡ì…˜)"""
    
    def __init__(self, data_path, processor, split='train'):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ"""
        if os.path.isdir(self.data_path):
            # í´ë” ê¸°ë°˜ ë°ì´í„°
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            # ë‹¨ì¼ íŒŒì¼
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    # H5 íŒŒì¼ ë‚´ë¶€ êµ¬ì¡° í™•ì¸
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ì²« í”„ë ˆì„ë§Œ ì‚¬ìš© (ë‹¨ì¼ ì´ë¯¸ì§€)
                        single_image = images[0]  # [H, W, 3] - ì²« í”„ë ˆì„ë§Œ
                        single_action = actions[0]  # [3] - ì²« í”„ë ˆì„ ì•¡ì…˜ë§Œ
                        
                        self.episodes.append({
                            'image': single_image,  # [H, W, 3] - ë‹¨ì¼ ì´ë¯¸ì§€
                            'action': single_action,  # [3] - ë‹¨ì¼ ì•¡ì…˜
                            'episode_id': f"{h5_file.stem}"
                        })
                    else:
                        print(f"âš ï¸ {h5_file}ì— images/actions í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # ì•¡ì…˜: [3]
        action = episode['action']  # [3]
        
        return {
            'image': image,  # [3, H, W]
            'action': action,  # [3]
            'episode_id': episode['episode_id']
        }

def create_data_loaders(data_path, processor, batch_size=8, train_split=0.8):
    """ë°ì´í„° ë¡œë” ìƒì„±"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = RoboVLMStyleDataset(data_path, processor, 'full')
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    return train_loader, val_loader

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = {
        'data_path': '../../ROS_action/mobile_vla_dataset',  # ì›ë³¸ 72ê°œ ë°ì´í„°ì…‹
        'batch_size': 8,
        'num_epochs': 15,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'dropout': 0.2,
        'z_axis_weight': 0.05,
        'use_claw_matrix': False,  # ë¹„í™œì„±í™”
        'use_hierarchical': False,  # ë¹„í™œì„±í™”
        'use_advanced_attention': False,  # ë¹„í™œì„±í™”
        'early_stopping_patience': 5,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print("ğŸš€ RoboVLMs Style Training ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {json.dumps(config, indent=2)}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader = create_data_loaders(
        config['data_path'],
        processor,
        batch_size=config['batch_size']
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = RoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=config['dropout'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention'],
        z_axis_weight=config['z_axis_weight']
    )
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    trained_model = train_robovlms_style_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate'],
        weight_decay=config['weight_decay'],
        early_stopping_patience=config['early_stopping_patience'],
        device=config['device']
    )
    
    print("âœ… RoboVLMs Style Training ì™„ë£Œ!")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_type': 'RoboVLMs_Style_Single_Image',
        'input_type': 'single_image',
        'output_type': 'single_action',
        'data_size': len(train_loader.dataset) + len(val_loader.dataset),
        'config': config,
        'features': {
            'claw_matrix': config['use_claw_matrix'],
            'hierarchical_planning': config['use_hierarchical'],
            'advanced_attention': config['use_advanced_attention']
        }
    }
    
    with open('robovlms_style_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: robovlms_style_training_results.json")

if __name__ == "__main__":
    main()
