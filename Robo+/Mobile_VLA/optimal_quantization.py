#!/usr/bin/env python3
"""
ğŸ† ìµœì  ì–‘ìí™”: VLM(FP16) + Action Head(INT8) í•˜ì´ë¸Œë¦¬ë“œ
MAE 0.222 ëª¨ë¸ì— ìµœì í™”ëœ ì–‘ìí™” ì ìš©
"""

import torch
import torch.nn as nn
import torch.quantization as quantization
import time
import psutil
import os
import json
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OptimalQuantizedModel(nn.Module):
    """ìµœì  ì–‘ìí™” ëª¨ë¸: VLM(FP16) + Action Head(INT8)"""
    
    def __init__(self, model_path: str):
        super().__init__()
        
        # Kosmos2 ëª¨ë¸ ë¡œë“œ (FP16ìœ¼ë¡œ ë³€í™˜)
        from transformers import AutoProcessor, AutoModel
        
        self.processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # VLMì„ FP16ìœ¼ë¡œ ë³€í™˜
        self.kosmos = self.kosmos.half()
        self.kosmos.eval()
        for param in self.kosmos.parameters():
            param.requires_grad = False
        
        # Action Head (INT8 ì–‘ìí™” ëŒ€ìƒ)
        self.rnn = nn.RNN(
            input_size=2048,  # Kosmos2 hidden size
            hidden_size=4096,
            num_layers=4,
            batch_first=True,
            dropout=0.1
        )
        
        self.actions = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 2)  # linear_x, linear_y
        )
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ Action Head ë¡œë“œ
        self._load_action_head(model_path)
        
        logger.info("âœ… ìµœì  ì–‘ìí™” ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("   VLM: FP16 (ê³ ì •)")
        logger.info("   Action Head: INT8 (ì–‘ìí™” ëŒ€ìƒ)")
    
    def _load_action_head(self, model_path: str):
        """Action Headë§Œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dictì—ì„œ Action Head ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ì¶œ
            action_state_dict = {}
            for key, value in checkpoint['model_state_dict'].items():
                if 'rnn' in key or 'actions' in key:
                    action_state_dict[key] = value
            
            # Action Headë§Œ ë¡œë“œ
            self.load_state_dict(action_state_dict, strict=False)
            logger.info(f"âœ… Action Head ë¡œë“œ ì™„ë£Œ: {len(action_state_dict)} íŒŒë¼ë¯¸í„°")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Action Head ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ëœë¤ ì´ˆê¸°í™”ëœ Action Head ì‚¬ìš©")
    
    def forward(self, x):
        """ìˆœì „íŒŒ (VLM FP16 + Action Head INT8)"""
        batch_size = x.size(0)
        
        # 1. VLMìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (FP16)
        with torch.no_grad():
            # ì…ë ¥ì„ FP16ìœ¼ë¡œ ë³€í™˜
            x_fp16 = x.half()
            
            # ë”ë¯¸ í…ìŠ¤íŠ¸ ìƒì„±
            dummy_text = ["<image>"] * batch_size
            
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            text_inputs = self.processor(
                text=dummy_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(x.device)
            
            try:
                # Kosmos2ë¡œ íŠ¹ì§• ì¶”ì¶œ (FP16)
                vision_outputs = self.kosmos(
                    pixel_values=x_fp16,
                    input_ids=text_inputs['input_ids'],
                    attention_mask=text_inputs['attention_mask']
                )
                vision_features = vision_outputs.last_hidden_state[:, 0]  # [batch_size, 2048]
            except Exception as e:
                logger.warning(f"Kosmos2 ì¶”ë¡  ì˜¤ë¥˜: {e}")
                vision_features = torch.randn(batch_size, 2048).half().to(x.device)
        
        # 2. Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡ (INT8)
        # FP16ì—ì„œ FP32ë¡œ ë³€í™˜ (INT8 ì–‘ìí™”ë¥¼ ìœ„í•´)
        vision_features_fp32 = vision_features.float()
        
        sequence_features = vision_features_fp32.unsqueeze(1)  # [batch_size, 1, 2048]
        rnn_out, _ = self.rnn(sequence_features)  # [batch_size, 1, 4096]
        actions = self.actions(rnn_out.squeeze(1))  # [batch_size, 2]
        
        return actions

class OptimalQuantizer:
    """ìµœì  ì–‘ìí™”ê¸°: VLM(FP16) + Action Head(INT8)"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        self.original_model = OptimalQuantizedModel(model_path).to(self.device)
        self.original_model.eval()
        
        logger.info(f"ğŸš€ Optimal Quantizer ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        logger.info(f"   ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    def benchmark_model(self, model, name: str, num_runs: int = 100):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
        
        # ì›Œë°ì—…
        for _ in range(10):
            _ = model(dummy_input)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        process = psutil.Process(os.getpid())
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        for _ in range(num_runs):
            with torch.no_grad():
                _ = model(dummy_input)
        
        end_time = time.time()
        inference_time = (end_time - start_time) / num_runs * 1000  # ms
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = memory_after - memory_before
        
        fps = 1000 / inference_time if inference_time > 0 else 0
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': inference_time,
            'memory_usage_mb': memory_usage,
            'fps': fps
        }
    
    def quantize_action_head_to_int8(self):
        """Action Headë¥¼ INT8ë¡œ ì–‘ìí™”"""
        logger.info("ğŸ”§ Action Head INT8 ì–‘ìí™” ì‹œì‘...")
        
        try:
            # Action Headë§Œ INT8 ì–‘ìí™”
            int8_model = quantization.quantize_dynamic(
                self.original_model,
                {nn.Linear, nn.RNN},  # Action Headì˜ Linearì™€ RNNë§Œ ì–‘ìí™”
                dtype=torch.qint8
            )
            int8_model.eval()
            
            # ë²¤ì¹˜ë§ˆí¬
            benchmark = self.benchmark_model(int8_model, "VLM(FP16) + Action(INT8)")
            
            # ëª¨ë¸ ì €ì¥
            torch.save(int8_model.state_dict(), 'optimal_quantized_model.pth')
            logger.info("ğŸ’¾ ìµœì  ì–‘ìí™” ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
            return int8_model, benchmark
            
        except Exception as e:
            logger.error(f"âŒ INT8 ì–‘ìí™” ì‹¤íŒ¨: {e}")
            return None, None
    
    def quantize_model(self):
        """ì „ì²´ ìµœì  ì–‘ìí™” í”„ë¡œì„¸ìŠ¤"""
        logger.info("ğŸ¯ ìµœì  ì–‘ìí™” ì‹œì‘! (VLM FP16 + Action Head INT8)")
        
        results = {}
        
        # 1. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (VLM FP16 + Action Head FP32)
        logger.info("1. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (VLM FP16 + Action FP32)...")
        original_benchmark = self.benchmark_model(self.original_model, "VLM(FP16) + Action(FP32)")
        results['original'] = original_benchmark
        
        # 2. Action Head INT8 ì–‘ìí™”
        logger.info("2. Action Head INT8 ì–‘ìí™”...")
        int8_model, int8_benchmark = self.quantize_action_head_to_int8()
        if int8_benchmark:
            results['quantized'] = int8_benchmark
        
        # 3. ê²°ê³¼ ë¹„êµ
        logger.info("3. ê²°ê³¼ ë¹„êµ...")
        self._compare_optimal_results(results)
        
        # 4. ê²°ê³¼ ì €ì¥
        with open('optimal_quantization_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: optimal_quantization_results.json")
        
        return results
    
    def _compare_optimal_results(self, results):
        """ìµœì  ì–‘ìí™” ê²°ê³¼ ë¹„êµ"""
        logger.info("\nğŸ“Š ìµœì  ì–‘ìí™” ê²°ê³¼ ë¹„êµ:")
        logger.info("=" * 60)
        
        original = results.get('original', {})
        quantized = results.get('quantized', {})
        
        if original and quantized:
            speedup = original['inference_time_ms'] / quantized['inference_time_ms']
            memory_save = (original['memory_usage_mb'] - quantized['memory_usage_mb']) / original['memory_usage_mb'] * 100
            
            logger.info(f"ìµœì  ì–‘ìí™” vs ì›ë³¸ ì„±ëŠ¥ ë¹„êµ:")
            logger.info(f"   ì†ë„ í–¥ìƒ: {speedup:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_save:.1f}%")
            logger.info(f"   FPS í–¥ìƒ: {quantized['fps']:.1f} â†’ {original['fps']:.1f}")
            
            if speedup > 1.0:
                logger.info("   âœ… ì–‘ìí™”ë¡œ ì†ë„ í–¥ìƒ!")
            else:
                logger.info("   âš ï¸ ì–‘ìí™”ë¡œ ì†ë„ ì €í•˜")
            
            if memory_save > 0:
                logger.info("   âœ… ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½!")
            else:
                logger.info("   âš ï¸ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¦ê°€")
        
        # ìµœì¢… ê¶Œì¥ì‚¬í•­
        logger.info("\nğŸ¯ ìµœì  ì–‘ìí™” ê¶Œì¥ì‚¬í•­:")
        if original and quantized:
            if speedup > 1.1 and memory_save > 5:
                logger.info("   ğŸ† í•˜ì´ë¸Œë¦¬ë“œ ì–‘ìí™” ê°•ë ¥ ê¶Œì¥!")
                logger.info("   - VLM: FP16 (ì†ë„ + ë©”ëª¨ë¦¬ ì ˆì•½)")
                logger.info("   - Action Head: INT8 (ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½)")
            elif speedup > 1.0 or memory_save > 0:
                logger.info("   ğŸŸ¢ í•˜ì´ë¸Œë¦¬ë“œ ì–‘ìí™” ê¶Œì¥")
            else:
                logger.info("   ğŸŸ¡ ì›ë³¸ ëª¨ë¸ ìœ ì§€ ê¶Œì¥")
        else:
            logger.info("   âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ê¶Œì¥ì‚¬í•­ ì œê³µ ë¶ˆê°€")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ìµœì  ì–‘ìí™” ì‹œì‘ (VLM FP16 + Action Head INT8)")
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    # ìµœì  ì–‘ìí™” ì‹¤í–‰
    quantizer = OptimalQuantizer(model_path)
    results = quantizer.quantize_model()
    
    logger.info("ğŸ‰ ìµœì  ì–‘ìí™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
