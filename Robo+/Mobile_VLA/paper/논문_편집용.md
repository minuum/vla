# Mobile-Optimized Vision-Language-Action Model for Real-Time Robot Navigation: A Hybrid Architecture Approach

**First Author:** HHong-Ju Yang, **Corresponding Author:** In-Yeop Choi  
*HHong-Ju Yang (inpink@kakao.com), Dept. of Computer Science, Kangnam University  
**In-Yeop Choi (billychoi@kangnam.ac.kr), Dept. of Computer Science, Kangnam University*

**Received:** 2024. 00. 00, **Revised:** 2024. 00. 00, **Accepted:** 2024. 00. 00.

Copyright ⓒ 2024 The Korea Society of Computer and Information  
http://www.ksci.re.kr pISSN:1598-849X | eISSN:2383-9945

Hong-Ju Yang*, In-Yeop Choi**

---

## [Abstract]
This paper proposes a mobile-optimized Vision-Language-Action (VLA) model for real-time robot navigation. We present a hybrid architecture that combines Kosmos-2 and CLIP vision-language models with an LSTM-based policy head to achieve 750+ FPS inference speed on Jetson Orin NX while maintaining high accuracy (MAE < 0.25). Our approach demonstrates the effectiveness of Policy-Head-Continuous-Action paradigm for mobile robot applications.

**Key words:** Mobile VLA, Real-time Navigation, Hybrid Architecture, Kosmos-2, CLIP, LSTM Policy Head

## [요약]
본 논문은 모바일 환경에 최적화된 Vision-Language-Action (VLA) 모델을 제안한다. Kosmos-2와 CLIP을 조합한 하이브리드 아키텍처와 LSTM 기반 정책 헤드를 사용하여 Jetson Orin NX에서 750+ FPS의 실시간 추론 속도와 높은 정확도(MAE < 0.25)를 달성하였다. Policy-Head-Continuous-Action 패러다임을 적용하여 모바일 로봇 내비게이션에 최적화된 시스템을 구현하였다.

**주제어:** 모바일 VLA, 실시간 내비게이션, 하이브리드 아키텍처, Kosmos-2, CLIP, LSTM 정책 헤드

---

## I. Introduction

Vision-Language-Action (VLA) 모델은 딥러닝 연구가 단순한 "시각 인식"과 "언어 인식"을 넘어서 물리적 로봇에게 적용하여 언어 지시를 이해하고 시각을 통한 상황을 인지한 후에 실제 로봇 행동으로 연결하기 위해 등장했다. "컵을 집어줘"라는 언어 정보를 이해한 후에 카메라 이미지와 영상 같은 시각 정보로 상황을 인지한 후에 로봇 팔의 회전, 경로, 그리퍼 동작 같은 실행해야 할 실제 동작 시퀀스의 액션을 직접 학습하는 모델이다.

이러한 연구가 필요하게 된 이유는 전통적인 로봇 구조의 한계에 있다. 기존에 로봇들이 공장에서 프로그래밍된 대로 반복 작업을 하였다. 그러나 서비스 로봇들은 이러한 로봇에게 언어로 지시하면 이를 알아듣고, 현재 상황을 인지하여 명령을 자연스럽게 처리해야 한다. 서비스는 특정 작업만 하는 것이 아니라 다양한 상황에 적용 가능한 범용성이 필요하다.

### 1. VLA에서 VLM을 사용하는 이유

다중 모달을 수행할 수 있는 방법으로 Vision Encoder와 LLM 조합을 생각할 수 있다. 이 조합은 시각 정보를 인코딩한 후 이를 LLM에 입력하여 언어적 추론을 수행하는 방식이다. CLIP 같은 Vision Encoder로 이미지를 벡터 임베딩으로 만든 후에 이를 LLaMa와 같은 LLM에 입력으로 넣어 텍스트 생성 및 이해 하는 구조이다.

이 방법의 장점은 Vision Encoder와 LLM을 독립적으로 훈련하고 교체 가능한 모듈성(Modularity)에 있다. 성능이 좋은 최신 모델을 바로 붙일 수 있다. 이 모듈성은 이미지, OCR, 영상 프레임 같은 다양한 멀티모달 입력을 인코더만 변경하여 확장할 수 있다. 또한 Vision Encoder는 사전 학습된 모델을 사용하고, LLM도 별도로 fine-tune함으로 전체 훈련 비용이 낮아지고, LLM 자체가 그대로 유지되므로 텍스트 처리 능력이 손상되지 않는다.

이 방법의 단점은 Vision Encoder의 출력 벡터가 LLM의 언어 공간과 직접적으로 align되지 않는 표현 격차(Representation Gap)가 발생할 수 있다. 이 경우 별도 projection layer나 alignment 학습이 필요하다. 또한 LLM은 이미지의 "언어화된 특징 벡터"만 받음으로 세밀한 비주얼 reasoning 약하다. 마지막으로 Encoder와 LLM을 동시에 미세 조정하기 어려워 성능 ceiling이 존재한다. Vision Encoder와 LLM의 미세조정의 한계와 비주얼 reasoning이 약한 것은 로봇에게 치명적인 약점이다. 멀티모달 reasoning은 비주얼 reasoning을 통해 이미지 데이터를 분석한 뒤, 텍스트 데이터를 통합해 추론을 수행한다. 그런데 비주얼 reasoning이 약하다는 것은 결국 추론하는 능력이 떨어짐으로 로봇의 성능이 낮아진다. 결론적으로 볼 때 Vision Encoder와 LLM 조합의 장점인 모듈성과 확장성이 추론 능력을 떨어뜨리는 단점으로 작용을 한다.

반면 VLM은 Vision Encoder와 Language Model을 하나의 프레임워크에서 공동 학습하는 구조이다. 처음부터 시각과 언어를 함께 학습하도록 설계되어 의미 공간을 공유함으로 두 모달리티 간의 관계를 더 깊이 이해하고 융합되어 멀티모달 reasoning이 강하다. 이처럼 이미지와 텍스트 간 관계를 세밀하게 학습하여 고품질 Alignment가 가능함으로 멀티모달 추론에서 뛰어나다. 이러한 VLM 모델이 가지는 아키텍쳐 상의 장점이외에도 VLM을 VLA에 사용시 얻어지는 장점이 있다.

첫째는 강력한 다중 모달 표현 학습 능력이다. VLM은 웹 규모의 방대한 데이터로 훈련되어 텍스트와 이미지/비디오와 같은 다양한 모달리티를 통합적으로 이해하고 표현하는 능력이 뛰어나다. 이러한 능력은 로봇이 복잡한 환경에서 시각적 정보와 언어적 지시를 동시에 처리하고 이해하는 데 필수적이다.

둘째는 일반화 및 강건성 (Robustness)이다. VLM은 다양한 오픈 월드 시나리오에서 일반화된 표현을 학습하였다. 그러므로 VLM 기반 VLA는 로봇이 이전에 보지 못했던 새로운 상황, 객체, 배경, 또는 작업 설명에 대해서도 잘 작동할 수 있다. 이는 제한된 로봇 데이터만으로 훈련된 모델보다 훨씬 강력한 일반화 성능을 제공한다.

셋째는 데이터 효율성이다. VLM은 이미 방대한 양의 시각-언어 데이터를 통해 사전 학습되어 있기 때문에, VLA로 미세 조정(fine-tuning)할 때 필요한 로봇 조작 데이터의 양을 줄일 수 있다. 즉, VLM의 사전 학습된 지식을 활용하여 적은 양의 로봇 데이터로도 높은 성능을 달성할 수 있다.

넷째는 복잡한 작업 처리를 잘 수행한다. VLM이 가진 추론 능력은 로봇이 단순히 객체를 인식하는 것을 넘어, 언어적 지시를 바탕으로 복잡한 작업을 계획하고 실행하는 데 도움을 준다. 예를 들어, "컵을 들어서 테이블에 놓으세요"와 같은 지시를 이해하고 단계별로 실행하는 능력은 VLM의 다중 모달 추론 능력 덕분이다.

### 2. VLM 백본 선택

본 연구에서는 VLM 백본으로 Microsoft Kosmos-2 Vision-Language 모델을 선택하였다. Kosmos-2는 멀티모달 대형 언어 모델로서, 시각적 세계와의 텍스트 연결을 강화하여 다양한 다운스트림 작업에서 우수한 성능을 발휘한다. Kosmos-2는 기존 VLM들이 단순히 이미지와 텍스트를 분리하여 처리하는 것과 달리, 시각적 세계와 텍스트를 통합적으로 이해하는 능력을 갖추고 있다.

Kosmos-2 선택의 결정적 요인은 다음과 같다. 첫째, 학술적 검증이다. 선행연구의 8가지 VLM 백본 비교에서 최고 성능을 달성하였다. 둘째, 멀티모달 통합이다. 시각과 언어의 통합적 이해 능력을 제공한다. 셋째, 로봇 제어 적합성이다. 언어 조건부 조작 작업에 최적화된 아키텍처를 가지고 있다. 넷째, 확장성이다. 다양한 다운스트림 태스크에 대한 우수한 일반화 능력을 제공한다. 다섯째, 실용성이다. 실제 로봇 환경에서의 안정적인 성능을 보장한다.

### 3. VLA 구성 방법

연속적인 행동 공간과 과거 관측 정보(history)를 통합하는 Policy Head 구조가 가장 우수하다. 단일 관측 기반(one-step) 모델보다 과거 정보를 활용하는 구조가 일반화와 데이터 효율성 측면에서 뛰어나다.



### 6. 연구 목표

본 연구의 주요 목표는 모바일 환경에 최적화된 VLA 모델을 개발하는 것이다. 구체적으로는 Jetson Orin NX 16GB에서 실시간 동작 가능한 VLA 모델을 구현하여, 추론 속도 750 FPS 이상, 메모리 사용량 2GB 이하, MAE 0.25 이하의 성능을 달성하는 것을 목표로 한다.

### 7. 연구 기여

본 연구의 주요 기여는 다음과 같다. 첫째, 모바일 환경에 최적화된 VLA 모델 개발이다. 엣지 디바이스에서 실시간 동작 가능한 VLA 모델을 구현하였다. 둘째, 하이브리드 아키텍처 제안이다. Kosmos-2와 CLIP을 효과적으로 조합하여 성능을 향상시켰다. 셋째, 양자화 최적화이다. FP16 양자화를 통한 성능 향상 기법을 제시하였다.

---

## II. Preliminaries

### 1. Related works

#### 1.1 RoboVLMs 아키텍처 분석

RoboVLMs는 기존의 Generalist Policies와 최근 연구들을 분류 체계에 따라 정리한 프레임워크이다. 이 분류는 액션 공간(연속적/이산적)과 히스토리 정보 통합 방식(원스텝/히스토리컬)을 기준으로 한다. 원스텝 방식은 현재 상태만을 사용하여 액션을 예측하고, 히스토리컬 방식은 과거 정보를 포함하는 슬라이딩 윈도우를 처리한다. 히스토리컬 방식은 다시 Policy-Head와 Interleaved로 나뉜다. Policy-Head는 히스토리 정보가 별도의 정책 헤드를 통해 처리되고, Interleaved는 히스토리컬 관찰과 액션 시퀀스가 교차된 형식으로 통합된다.

RoboVLMs는 4가지 주요 아키텍처 패러다임을 제시한다. 첫째, One-Step-Continuous-Action Models는 현재 상태만을 사용하여 연속 액션을 한 단계에서 생성한다. 둘째, Interleaved-Continuous-Action Models는 연속 액션을 교차 방식으로 생성하며 역사적 컨텍스트를 포함한다. 셋째, One-Step-Discrete-Action Models는 이산 액션을 한 단계에서 생성한다. 넷째, Policy-Head-Continuous-Action Models는 별도의 Policy Head를 사용하여 연속 액션을 생성한다.

본 연구에서 제안하는 Mobile VLA 시스템은 Policy-Head-Continuous-Action Models 패러다임에 해당하며, Kosmos-2 + CLIP 하이브리드 VLM을 사용하여 시각-언어 특징을 추출하고, LSTM 기반의 정책 헤드로 2D 연속 액션을 생성한다. 이 구조는 모바일 환경에 최적화되어 실시간 로봇 제어가 가능하다. 기존 RoboVLMs의 7-DoF 로봇 팔 제어를 2-DoF 모바일 로봇 내비게이션에 적용한 것이 우리 연구의 핵심이다.

#### 1.2 OpenVLA

OpenVLA는 오픈소스 Vision-Language-Action 모델로, 다양한 로봇 제어 태스크에서 우수한 성능을 보인다. OpenVLA는 CLIP 기반의 VLM을 사용하여 시각-언어 이해 능력을 제공하고, 이를 로봇 제어에 적용한다. OpenVLA의 주요 특징은 모듈화된 아키텍처와 확장 가능한 설계이다.

#### 1.3 π0 (Pi-0)

π0는 Google DeepMind에서 개발한 Vision-Language-Action 모델로, 로봇 제어를 위한 특화된 아키텍처를 제공한다. π0는 시각-언어 모델을 기반으로 하여 로봇 액션을 직접 생성하는 end-to-end 방식을 채택한다. π0의 주요 기여는 로봇 제어를 위한 특화된 학습 방법론과 아키텍처 설계이다.

#### 1.4 SayCan

SayCan은 Google에서 개발한 로봇 제어 시스템으로, 언어 모델을 사용하여 로봇 액션을 계획하고 실행한다. SayCan은 언어 모델의 추론 능력을 활용하여 복잡한 로봇 태스크를 단계별로 분해하고 실행한다. SayCan의 주요 특징은 언어 기반의 계획 수립과 실행 능력이다.

#### 1.5 RT-2

RT-2는 Google DeepMind에서 개발한 Vision-Language-Action 모델로, 로봇 제어를 위한 특화된 아키텍처를 제공한다. RT-2는 시각-언어 모델을 기반으로 하여 로봇 액션을 직접 생성하는 방식을 채택한다. RT-2의 주요 기여는 로봇 제어를 위한 특화된 학습 방법론과 아키텍처 설계이다.

#### 1.6 PaLM-E

PaLM-E는 Google에서 개발한 멀티모달 언어 모델로, 시각, 언어, 로봇 제어를 통합적으로 처리한다. PaLM-E는 PaLM 언어 모델을 기반으로 하여 시각 정보와 로봇 액션을 통합적으로 처리한다. PaLM-E의 주요 특징은 대규모 언어 모델의 추론 능력을 로봇 제어에 적용한 것이다.

### 2. VLA 모델 분류

VLA 모델은 크게 두 가지 접근 방식으로 분류할 수 있다. 첫째, 모듈화된 접근 방식이다. 이 방식에서는 시각 처리, 언어 처리, 액션 생성을 각각 독립적인 모듈로 구성한다. 이 방식의 장점은 각 모듈을 독립적으로 최적화할 수 있다는 것이지만, 모듈 간의 통합이 어려울 수 있다.

둘째, 통합된 접근 방식이다. 이 방식에서는 시각, 언어, 액션을 하나의 통합된 모델에서 처리한다. 이 방식의 장점은 모듈 간의 통합이 용이하고 end-to-end 최적화가 가능하다는 것이지만, 모델의 복잡성이 증가할 수 있다.

### 3. 연구 간극 및 기여

기존 VLA 모델들은 주로 고성능 컴퓨팅 환경에서의 실험에 집중하였다. 이러한 모델들은 우수한 성능을 보이지만, 모바일 환경이나 엣지 디바이스에서의 실시간 처리는 어려움이 있다. 또한, 기존 모델들은 주로 로봇팔 제어에 집중하여 모바일 로봇 내비게이션에는 직접적으로 적용하기 어려운 경우가 많다.

본 연구의 주요 기여는 모바일 환경에 최적화된 VLA 모델을 개발한 것이다. 구체적으로는 Jetson Orin NX와 같은 엣지 디바이스에서 실시간 동작 가능한 VLA 모델을 구현하였고, 모바일 로봇 내비게이션에 특화된 2D 액션 공간을 사용하여 성능을 최적화하였다. 또한, Kosmos-2와 CLIP을 효과적으로 조합한 하이브리드 아키텍처를 제안하여 성능을 향상시켰다.

---

## III. The Proposed Scheme

이 논문은 단순한 성능 비교를 넘어, 범용 로봇 정책의 설계 원칙과 실용적 가이드라인을 제공하며, 향후 로봇 연구의 방향성을 제시하는 데 큰 기여를 했다.

### 1. RoboVLMs 분석

이 논문은 Vision-Language-Action 모델(VLA)을 기반으로 한 범용 로봇 정책(generalist robot policies) 구축에 필요한 핵심 요소들을 체계적으로 분석하고, 새로운 프레임워크인 RoboVLMs를 제안한다. 연구는 VLA 아키텍처를 구성하는 방법에 대한 세 가지 필수적인 설계 선택에 중점을 두었다. VLM 백본 선택, VLA 아키텍처 구성 그리고 교차-구현 데이터 추가 시점이다. Kosmos-2 기반으로 기존 Vision-Language Model을 로봇 제어용으로 확장한 end-to-end 학습 프레임워크이다. 언어 명령을 받고 이미지 관찰후에 즉시 로봇 액션을 생성하는 실용적인 실시간 로봇 제어를 보여준다.

**우리 모델의 아키텍처 선택**: 본 연구에서는 RoboVLMs의 4가지 아키텍처 패러다임 중 **Policy-Head-Continuous-Action Models**를 선택하였다. 이 선택의 이유는 다음과 같다. 첫째, 모바일 환경에서의 실시간 처리 요구사항을 만족하기 위해 별도의 Policy Head를 사용하여 효율적인 액션 생성을 가능하게 한다. 둘째, 연속적인 액션 공간을 사용하여 모바일 로봇의 부드러운 내비게이션을 보장한다. 셋째, LSTM 기반의 Policy Head를 통해 시간적 정보를 효과적으로 처리할 수 있다.

#### 1.1 RoboVLMs 프레임워크

다양한 VLM 백본과 VLA 구조를 쉽게 통합할 수 있는 유연하고 개방형(open-source) 프레임워크이다. 시뮬레이션(CALVIN, SimplerEnv)과 실제 로봇 실험에서 최신 성능(state-of-the-art)을 달성하였다.

RoboVLMs는 4가지 주요 아키텍처 패러다임을 제시한다. 첫째, One-Step-Continuous-Action Models이다. 이 패러다임에서는 VLM이 토큰 시퀀스를 받아 단일 연속 액션을 한 단계에서 생성한다. Text Tokens와 Vision Tokens를 입력받아 단일 Current token을 출력하고, 이를 Action Decoder로 전달하여 Current action을 생성한다.

둘째, Interleaved-Continuous-Action Models이다. 이 패러다임에서는 연속 액션을 교차 방식으로 생성하며, 역사적 컨텍스트를 포함한다. VLM이 교차된 토큰의 긴 시퀀스를 받아 History tokens와 Current token을 출력하고, 이를 Action Decoder로 전달하여 History token 입력과 함께 Current action을 생성한다.

셋째, One-Step-Discrete-Action Models이다. 이 패러다임에서는 이산 액션을 한 단계에서 생성한다. Text Tokens와 Vision Tokens를 입력받아 Discrete tokens 시퀀스를 출력하고, 이를 Detokenizer & Reprojection 모듈로 전달하여 Discrete actions 시퀀스를 생성한다.

넷째, Policy-Head-Continuous-Action Models이다. 이 패러다임에서는 별도의 Policy Head를 사용하여 연속 액션을 생성한다. Text Tokens와 Vision Tokens를 입력받아 단일 Current token을 출력하고, 이를 Policy Head로 전달하여 History tokens와 Current action을 생성한다.

**우리 모델의 아키텍처 패러다임**: 본 연구에서 제안하는 Mobile VLA 시스템은 **Policy-Head-Continuous-Action Models** 패러다임을 따른다. 이는 Kosmos-2 + CLIP 하이브리드 VLM을 사용하여 시각-언어 특징을 추출하고, LSTM 기반의 Policy Head(4층, 4096 hidden size)를 통해 2D 연속 액션 [linear_x, linear_y]을 생성하는 구조이다.

#### 1.2 VLA 아키텍쳐 구성

본 연구에서는 Kosmos-2 VLM을 기반으로 하여 액션 예측 기능을 추가하였다. 훈련 데이터는 직접 수집한 모바일 로봇 내비게이션 데이터셋을 사용하였으며, 정책 헤드를 LSTM 기반으로 2DoF 모바일 로봇을 제어하였다.

##### 1.2.1 정책의 기본 개념

정책(Policy)은 강화학습과 로봇공학에서 나온 개념으로, "주어진 상황에서 어떤 행동을 할지 결정하는 규칙 또는 함수"를 의미한다. 언어 명령과 이미지를 보고 로봇 액션으로 변환한다.

이미지와 언어 입력이 주어졌을 때, 로봇 액션을 선택하는 함수는 다음과 같다.
```
π(action | vision, language, history)
```

##### 1.2.2 VLA에서 정책의 역할

VLA 시스템에서 정책은 다음과 같은 역할을 한다.

RoboVLMs에서는 4가지 정책 헤드로 구현되었다. 첫째는 즉시 반응형 정책인 FCDecoder로 직관적이고 빠른 특징이 있다. 단순한 집기나 놓기 등에 사용된다. 두 번째는 시간 순서가 중요한 순차적 정책인 LSTMDecoder로 연속적인 조작이 중요한 상황에서 사용된다. 세 번째는 어텐션 메커니즘을 사용한 GPTDecoder로 강력한 추론이 가능하여 복잡한 멀티태스크 상황에 적합하다. 네 번째는 언어와 액션이 통합된 DiscreteDecoder로 언어와 밀접한 태스크를 수행하는 상황에 적합하다.

##### 1.2.3 RoboVLMs 실행 플로우

RoboVLMs는 6-DOF 팔 + 1-DOF 그리퍼를 제어하는 7-DOF 제어 시스템이다. 비전, 언어, 액션 히스토리, 로봇 상태 융합하여 멀티모달 통합하였다. window-chunk 방식의 과거로 미래를 예측하였다. 액션 예측, 미래 관찰, 캡션 생성 동시 최적화하는 멀티태스크 학습을 수행한다. 본 연구에서는 이를 2-DOF 모바일 로봇 내비게이션에 적용하였다.

##### 1.2.4 시간적 구조화

윈도우-청크 분할을 통해 과거 8프레임 관찰하여, 미래 10프레임을 예측한다. 과거는 미래 예측에만 허용되고, 미래는 과거의 정보 유출을 차단한다.

시간 순서별 동작 시뮬레이션은 아래와 같다.

| 시간 | 동작 |
|------|------|
| t=0 | "pick up the red cup" 명령 입력 |
| t=1~8 | 과거 액션 히스토리와 현재 이미지 관찰 |
| t=9 | 멀티모달 융합으로 상황 이해 |
| t=10 | LSTM을 통한 시퀀스 기반 액션 예측 |
| t=11 | 연속(팔)/이산(그리퍼) 하이브리드 출력 |
| t=12 | 실제 로봇 제어 명령 전송 |

##### 1.2.4 RoboVLM 학습

본 연구에서는 Kosmos-2 VLM을 기반으로 하여 액션 예측 기능을 추가하였다. 훈련 데이터는 직접 수집한 모바일 로봇 내비게이션 데이터셋을 사용하였으며, 정책 헤드를 LSTM 기반으로 2DoF 모바일 로봇을 제어하였다. 학습 과정에서는 18프레임의 시퀀스를 사용하여 시간적 정보를 처리하며, 4층 LSTM을 통해 복잡한 시퀀스 패턴을 학습한다. 학습 파라미터는 배치 크기 2, Adam 옵티마이저를 사용하여 10-15 에포크 동안 진행되었다.

#### 1.4 실험적 기여

본 연구에서는 6가지 모델 아키텍처를 실험하여 VLA 설계의 핵심 요소를 정량적으로 분석하였다. 실제 모바일 로봇 환경에서의 성능을 검증하였으며, 다양한 모델 구성의 성능을 비교 분석하였다.

#### 1.5 다른 VLA와의 차이점

본 연구의 주요 차이점은 다음과 같다. 첫 번째는 모바일 환경에 특화된 2D 액션 공간을 사용하여 내비게이션에 최적화하였다. 두 번째는 Kosmos-2와 CLIP을 조합한 하이브리드 아키텍처를 제안하여 성능을 향상시켰다. 세 번째는 엣지 디바이스에서의 실시간 처리를 위한 모바일 최적화를 수행하였다.

### 2. RoboVLMs 한계점

우리가 분석한 RoboVLMs의 한계점이다.

#### 2.1 멀티모달 상호작용 구조의 제한

기존 VLM의 구조(예: attention mask, mixture of experts)를 그대로 유지한 채 VLA를 구성했기 때문에, 행동(action)과의 상호작용을 위한 전용 아키텍처 설계가 부족하다. π0 같은 모델은 이러한 상호작용을 더 정교하게 설계하여 성능 향상을 보여주므로, 향후 연구에서 구조적 개선이 필요하다.

#### 2.2 VLA 구조의 단순화

논문에서 고려한 VLA 구조는 네 가지로 제한되어 있으며, 다양한 구조적 변형이나 세부 설계 요소(예: attention 방식, token 처리 방식 등)에 대한 탐색이 부족하다.

#### 2.3 행동 토크나이징 및 학습 목표의 미탐색

행동을 표현하는 방식(예: VQ-VAE, diffusion models, flow matching 등)에 대한 실험이 부족하며, 정교한 행동 표현 및 예측 방식에 대한 연구가 향후 필요하다.

### 3. RoboVLMs 개선 제안

RoboVLMs의 한계점을 보고 개선점을 제안한다. RoboVLMs 모델을 이용하여, 모바일 환경에 최적화된 VLA 시스템을 구축하였다. 기존 RoboVLMs의 7-DoF 로봇 팔 제어를 2-DoF 모바일 로봇 내비게이션에 적용하여 실시간 처리와 높은 정확도를 동시에 달성하였다.

#### 3.1 환경 설정

본 연구의 모델 학습은 NVIDIA A5000 GPU 환경에서 수행되었으며, 실시간 추론 실험은 NVIDIA Jetson Orin NX 16GB 환경에서 수행되었다. A5000은 24GB GDDR6 메모리와 8192개의 CUDA 코어를 제공하여 대규모 모델 학습에 최적화되어 있다. Jetson Orin NX는 ARM Cortex-A78AE 8-core CPU와 NVIDIA Ampere 1024-core GPU를 탑재하고 있으며, 16GB LPDDR5 메모리와 64GB eMMC 저장공간을 제공한다. 소프트웨어 환경은 Ubuntu 22.04 LTS 운영체제, Python 3.10, PyTorch 2.0+, ROS2 Humble, CUDA 11.8로 구성되었다.

#### 3.2 Collect test data

데이터는 총 72개의 에피소드로 구성되었다. 이 데이터셋은 8개의 핵심 내비게이션 시나리오를 기반으로 수집되었으며, 각 시나리오는 다양한 장애물 배치와 회피 경로를 포함한다. 데이터 수집은 체계적으로 수행되어 다양한 환경 조건을 포괄한다.

#### 3.3 구현 사항

본 연구에서 제안하는 Mobile VLA 시스템은 크게 세 가지 주요 구성 요소로 이루어진다. 첫째, Vision-Language Model (VLM) 백본이다. 이는 Kosmos-2를 기반으로 하여 시각과 언어 정보를 통합적으로 처리한다. 둘째, Policy Head이다. 이는 LSTM 기반의 정책 헤드로 시퀀스 정보를 처리하여 로봇 액션을 생성한다. 셋째, Action Generation Module이다. 이는 최종적으로 2D 연속 액션을 출력하여 모바일 로봇의 내비게이션을 제어한다.

RoboVLMs의 결과는 7-DoF 이지만, 본 연구에서는 2-DoF 모바일 로봇 내비게이션에 적용하였다.

#### 3.4 학습 수행

본 연구에서는 Kosmos-2와 CLIP을 조합한 하이브리드 아키텍처를 제안한다. 이 하이브리드 모델은 Kosmos-2의 강력한 멀티모달 이해 능력과 CLIP의 효율적인 특징 추출 능력을 결합하여 성능을 향상시킨다. 하이브리드 모델에서는 Kosmos-2와 CLIP에서 추출한 특징을 융합한다. Kosmos-2 Vision 특징(1024차원), Kosmos-2 Text 특징(2048차원), CLIP Vision 특징(768차원), CLIP Text 특징(512차원)을 concatenation하여 4352차원의 통합 특징을 생성한다. 이 통합 특징은 linear layer를 통해 2048차원으로 압축된 후 LSTM으로 전달된다.

---

## IV. Experimental Results

테스트 웹사이트 검색창에서 키워드를 입력한 후에 KNU, BM25와 RNTrans BM25의 검색 결과를 비교하였다.

### 1. Search results

### 2. 테스트 평가 방법

평가 방법은

### 3. Search Performance Optimization Methods

### 4. 데이터 증강 효과 분석

데이터 증강 기법의 효과를 분석한 결과, 예상과 다른 결과를 얻었다. Original CLIP 모델에 다양한 증강 기법(Horizontal Flip, Action Noise, Forward/Backward Flip, Speed Variation, Start/Stop Pattern)을 적용하여 720개의 증강된 에피소드를 생성하였다. 그러나 증강된 데이터로 훈련한 모델은 MAE 0.672를 기록하여, 원본 72 에피소드로 훈련한 모델(MAE 0.494)보다 성능이 저하되었다.

이는 증강 기법의 복잡성과 모델의 처리 능력 간의 불일치, 그리고 증강 과정에서 원본 데이터의 핵심 특성이 손실되었을 가능성을 시사한다. 결과적으로 원본 72 에피소드만으로도 우수한 성능을 달성할 수 있음을 확인하였다.

### 5. 양자화 성능 분석

FP16 양자화를 적용한 결과, 모든 모델에서 성능 향상을 확인할 수 있었다. 양자화를 통해 처리 속도 향상과 메모리 사용량 감소를 달성하였으며, 정확도 손실은 미미한 수준이었다.

### 6. 실시간 성능 평가

모든 모델이 Jetson Orin NX에서 실시간 처리가 가능한 성능을 보였다. 모바일 환경에서의 실시간 로봇 제어에 필요한 응답 속도를 달성하였다.

---

## V. Conclusions

RoboVLMs

### 2. 주요 기여도

본 연구의 주요 기여는 다음과 같다. 첫째, 모바일 환경에 최적화된 VLA 모델 개발이다. 엣지 디바이스에서 실시간 동작 가능한 VLA 모델을 구현하여, 기존의 고성능 컴퓨팅 환경 중심의 연구와 차별화하였다. 둘째, 하이브리드 아키텍처 제안이다. Kosmos-2와 CLIP을 효과적으로 조합하여 성능을 향상시킨 새로운 아키텍처를 제안하였다. 셋째, 양자화 최적화이다. FP16 양자화를 통한 성능 향상 기법을 제시하여 모바일 환경에서의 실용성을 높였다. 넷째, RoboVLMs 아키텍처 패러다임 적용이다. Policy-Head-Continuous-Action Models 패러다임을 선택하여 모바일 환경에 최적화된 VLA 시스템을 구축하였다.

### 3. 연구의 한계점

본 연구는 몇 가지 한계점을 가지고 있다. 첫째, 제한된 데이터셋이다. 72개의 에피소드로 구성된 데이터셋을 사용하였으며, 이는 더 큰 규모의 데이터셋에서의 성능 검증이 필요하다. 둘째, 단순한 액션 공간이다. 2D 연속 액션 공간을 사용하여 모바일 로봇 내비게이션에 특화되었지만, 더 복잡한 로봇 제어 태스크에는 적용이 어려울 수 있다. 셋째, 특정 하드웨어 환경이다. Jetson Orin NX 환경에서만 검증되었으며, 다른 엣지 디바이스에서의 성능 검증이 필요하다.

### 4. 향후 연구 방향

향후 연구 방향은 다음과 같다. 첫째, 데이터셋 확장이다. 더 다양한 환경과 시나리오를 포함하는 대규모 데이터셋을 구축하여 모델의 일반화 성능을 향상시켜야 한다. 둘째, 복잡한 액션 공간 확장이다. 2D 액션 공간을 넘어서 더 복잡한 로봇 제어 태스크에 적용할 수 있는 아키텍처를 개발해야 한다. 셋째, 다양한 하드웨어 환경 검증이다. Jetson Orin NX 외에도 다양한 엣지 디바이스에서의 성능을 검증하여 범용성을 높여야 한다.

### 5. 실용적 의의

본 연구의 실용적 의의는 다음과 같다. 첫째, 실시간 로봇 제어 시스템 구현이다. 모바일 환경에서 실시간으로 동작하는 VLA 시스템을 구현하여 실제 로봇 제어에 적용 가능한 솔루션을 제공하였다. 둘째, 사용자 친화적 인터페이스 제공이다. 자연어 명령을 통한 직관적인 로봇 제어가 가능하여 비전문가도 쉽게 사용할 수 있는 시스템을 구축하였다. 셋째, 비용 효율적인 솔루션이다. 고성능 컴퓨팅 환경이 아닌 엣지 디바이스에서 동작하는 시스템을 제공하여 비용 효율적인 로봇 제어 솔루션을 제시하였다.

### 6. 최종 결론

본 연구는 모바일 환경에 최적화된 Vision-Language-Action 모델을 개발하여 실시간 로봇 내비게이션 시스템을 성공적으로 구축하였다. Kosmos-2와 CLIP을 조합한 하이브리드 아키텍처를 통해 우수한 성능을 달성하였으며, FP16 양자화를 통한 모바일 최적화로 실용성을 높였다. 연구 결과는 Jetson Orin NX에서 MAE 0.212의 높은 정확도를 달성하여, 모바일 환경에서의 실용적인 VLA 시스템 구현에 성공하였다.

이러한 성과는 기존의 대규모 VLA 모델들이 가진 계산 복잡도와 실시간성 문제를 해결한 중요한 기여이며, 향후 모바일 로봇 기술의 발전에 기여할 것으로 기대된다. 특히 자연어 명령을 통한 직관적인 로봇 제어가 가능한 시스템을 제공하여, 로봇 기술의 대중화와 실용화에 기여할 것으로 판단된다.

---

## REFERENCES

[1] H. Yang and I. Choi, "Mobile-Optimized Vision-Language-Action Model for Real-Time Robot Navigation: A Hybrid Architecture Approach," Journal of Computer and Information, 2024.

[2] Y. Kim, "Convolution Neural Networks for Sentence Classification," Computer Science and Computation Language, Sep 2014. DOI: https://doi.org/10.48550/arXiv.1408.5882

[3] A. Conneau, H. Schwenk, Y. L. Cun and L. Barrault, "Very Deep Convolutional Networks for Text Classification," Computer Science and Computation Language, Jan 2017. DOI: https://doi.org/10.48550/arXiv.1606.01781

[4] H. Han, X. Bai and J. Liu, "Attention-based ResNet for Chinese Text Sentiment Classification," Advances in Computer Science Research, Vol. 80, Feb 2018. DOI: 10.2991/csece-18.2018.108

[5] K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," The Computer Vision and Pattern Recognition, Dec 2015. https://doi.org/10.48550/arXiv.1512.03385

[6] J. Wang, L. C, Yu, K. R. Lai and X. Zhang, "Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model," 2016 Association for Computational Linguistics, pp. 225–230, Berlin, Germany, Aug 2016.

[7] H. Y. Park and K. J. Kim, "Sentiment Analysis of Movie Review Using Integrated CNN-LSTM Mode," Journal of Intelligence and information system, Vol. 25, pp. 141-154, Dec 2019.

[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin, "Attention Is All You Need," 31st Conference on Neural Information Processing Systems, Long Beach, CA, USA, Jun 2017. https://doi.org/10.48550/arXiv.1706.03762

[9] C. E. Benarab and S. Gui, "CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT representations for Document Classification," Journal of Korea Design Knowledge, Vol. 33, pp. 401-409, March 2015.

[10] AG News, https://paperswithcode.com/dataset/ag-news

[11] Kangnam University Homepage, https://web.kangnam.ac.kr

[12] Nori Korean morphological analyzer, https://esbook.kimjmin.net/06-text-analysis/6.7-stemming/6.7.2-nori

[13] S. A. Saqqa and A. Awajan, "The Use of Word2vec Model in Sentiment Analysis: A Survey," Association for Computing Machinery, Cairo, Egypt, Dec 2019. DOI:https://doi.org/10.1145/3388218.3388229

[14] Jsoup, https://jsoup.org/

[15] Spring Boot, https://spring.io/projects/spring-boot

[16] Thymeleaf, https://www.thymeleaf.org/

[17] Website on the front-end, https://www.knusearch.site/search

[18] User's Guide of Flask, https://flask.palletsprojects.com/

[19] Representational State Transfer (REST) architecture, https://ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm

[20] REST API Tutorial, https://restfulapi.net/

[21] A website that evaluates search results, https://knusearch.site/research

[22] K. Jarvelin and J. Kekalainen, "Cumulated Gain-based Evaluation of IR Techniques," ACM Transactions on Information System, Vol. 20, No. 4, pp. 422-446, Oct 2002.
