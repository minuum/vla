# II. Preliminaries - Mobile-Optimized Vision-Language-Action Model for Real-Time Robot Navigation

## 실제 논문 내용

### 1. 관련 연구

#### 1.1 RoboVLMs 아키텍처 분석

RoboVLMs는 Vision-Language-Action 모델을 위한 통합 프레임워크로, 다양한 VLA 아키텍처 패러다임을 제공한다. RoboVLMs는 4가지 주요 아키텍처 패러다임을 제시한다.

첫째, Policy-Head-Continuous-Action 패러다임이다. 이 패러다임에서는 VLM이 시각-언어 특징을 추출한 후, 별도의 정책 헤드가 연속적인 액션을 생성한다. 이 방식은 VLM의 강력한 멀티모달 이해 능력을 활용하면서도 로봇 제어에 특화된 액션 생성을 가능하게 한다.

둘째, End-to-End-Continuous-Action 패러다임이다. 이 패러다임에서는 VLM이 직접 연속적인 액션을 생성한다. 이 방식은 전체 파이프라인을 end-to-end로 학습할 수 있어 최적화가 용이하지만, VLM의 원래 기능을 유지하면서 액션 생성을 통합하는 것이 도전적이다.

셋째, Policy-Head-Discrete-Action 패러다임이다. 이 패러다임에서는 VLM이 시각-언어 특징을 추출한 후, 정책 헤드가 이산적인 액션을 생성한다. 이 방식은 복잡한 로봇 제어 시나리오에서 유용할 수 있지만, 연속적인 액션 공간에서는 제한적이다.

넷째, End-to-End-Discrete-Action 패러다임이다. 이 패러다임에서는 VLM이 직접 이산적인 액션을 생성한다. 이 방식은 간단하지만, 복잡한 로봇 제어에는 적합하지 않을 수 있다.

#### 1.2 OpenVLA

OpenVLA는 오픈소스 Vision-Language-Action 모델로, 다양한 로봇 제어 태스크에서 우수한 성능을 보인다. OpenVLA는 CLIP 기반의 VLM을 사용하여 시각-언어 이해 능력을 제공하고, 이를 로봇 제어에 적용한다. OpenVLA의 주요 특징은 모듈화된 아키텍처와 확장 가능한 설계이다.

#### 1.3 π0 (Pi-0)

π0는 Google DeepMind에서 개발한 Vision-Language-Action 모델로, 로봇 제어를 위한 특화된 아키텍처를 제공한다. π0는 시각-언어 모델을 기반으로 하여 로봇 액션을 직접 생성하는 end-to-end 방식을 채택한다. π0의 주요 기여는 로봇 제어를 위한 특화된 학습 방법론과 아키텍처 설계이다.

#### 1.4 SayCan

SayCan은 Google에서 개발한 로봇 제어 시스템으로, 언어 모델을 사용하여 로봇 액션을 계획하고 실행한다. SayCan은 언어 모델의 추론 능력을 활용하여 복잡한 로봇 태스크를 단계별로 분해하고 실행한다. SayCan의 주요 특징은 언어 기반의 계획 수립과 실행 능력이다.

#### 1.5 RT-2

RT-2는 Google DeepMind에서 개발한 Vision-Language-Action 모델로, 로봇 제어를 위한 특화된 아키텍처를 제공한다. RT-2는 시각-언어 모델을 기반으로 하여 로봇 액션을 직접 생성하는 방식을 채택한다. RT-2의 주요 기여는 로봇 제어를 위한 특화된 학습 방법론과 아키텍처 설계이다.

#### 1.6 PaLM-E

PaLM-E는 Google에서 개발한 멀티모달 언어 모델로, 시각, 언어, 로봇 제어를 통합적으로 처리한다. PaLM-E는 PaLM 언어 모델을 기반으로 하여 시각 정보와 로봇 액션을 통합적으로 처리한다. PaLM-E의 주요 특징은 대규모 언어 모델의 추론 능력을 로봇 제어에 적용한 것이다.

### 2. VLA 모델 분류

VLA 모델은 크게 두 가지 접근 방식으로 분류할 수 있다. 첫째, 모듈화된 접근 방식이다. 이 방식에서는 시각 처리, 언어 처리, 액션 생성을 각각 독립적인 모듈로 구성한다. 이 방식의 장점은 각 모듈을 독립적으로 최적화할 수 있다는 것이지만, 모듈 간의 통합이 어려울 수 있다.

둘째, 통합된 접근 방식이다. 이 방식에서는 시각, 언어, 액션을 하나의 통합된 모델에서 처리한다. 이 방식의 장점은 모듈 간의 통합이 용이하고 end-to-end 최적화가 가능하다는 것이지만, 모델의 복잡성이 증가할 수 있다.

### 3. 연구 간극 및 기여

기존 VLA 모델들은 주로 고성능 컴퓨팅 환경에서의 실험에 집중하였다. 이러한 모델들은 우수한 성능을 보이지만, 모바일 환경이나 엣지 디바이스에서의 실시간 처리는 어려움이 있다. 또한, 기존 모델들은 주로 로봇팔 제어에 집중하여 모바일 로봇 내비게이션에는 직접적으로 적용하기 어려운 경우가 많다.

본 연구의 주요 기여는 모바일 환경에 최적화된 VLA 모델을 개발한 것이다. 구체적으로는 Jetson Orin NX와 같은 엣지 디바이스에서 실시간 동작 가능한 VLA 모델을 구현하였고, 모바일 로봇 내비게이션에 특화된 2D 액션 공간을 사용하여 성능을 최적화하였다. 또한, Kosmos-2와 CLIP을 효과적으로 조합한 하이브리드 아키텍처를 제안하여 성능을 향상시켰다.

---

## 📊 분석 및 검토 사항

### RoboVLMs 아키텍처 분석

**4가지 패러다임 비교:**

1. **Policy-Head-Continuous-Action**: VLM + 별도 정책 헤드로 연속 액션 생성
2. **End-to-End-Continuous-Action**: VLM이 직접 연속 액션 생성
3. **Policy-Head-Discrete-Action**: VLM + 별도 정책 헤드로 이산 액션 생성
4. **End-to-End-Discrete-Action**: VLM이 직접 이산 액션 생성

**우리 모델의 위치:**
우리 모델은 Policy-Head-Continuous-Action 패러다임에 해당하며, Kosmos-2 VLM을 사용하여 시각-언어 특징을 추출하고, LSTM 기반의 정책 헤드로 2D 연속 액션을 생성한다.

### 기존 연구와의 차별화

**기존 연구의 한계:**
1. 고성능 컴퓨팅 환경 중심
2. 로봇팔 제어 중심
3. 실시간성 고려 부족

**우리 연구의 기여:**
1. 모바일 환경 최적화
2. 모바일 로봇 내비게이션 특화
3. 실시간 처리 가능한 아키텍처

### 다음 단계

1. **더 상세한 관련 연구 분석**: 각 모델의 구체적인 아키텍처와 성능 비교
2. **실험적 검증**: 제안된 모델의 우수성 입증
3. **한계점 분석**: 현재 접근 방식의 한계와 개선 방향

---
*마지막 업데이트: 2024년 8월 29일*
