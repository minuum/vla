# I. Introduction - Mobile-Optimized Vision-Language-Action Model for Real-Time Robot Navigation

## 실제 논문 내용

### 1. 연구 배경 및 동기

Vision-Language-Action (VLA) 모델은 딥러닝 연구가 단순한 "시각 인식"과 "언어 인식"을 넘어서 물리적 로봇에게 적용하여 언어 지시를 이해하고 시각을 통한 상황을 인지한 후에 실제 로봇 행동으로 연결하기 위해 등장했다. "컵을 집어줘"라는 언어 정보를 이해한 후에 카메라 이미지와 영상 같은 시각 정보로 상황을 인지한 후에 로봇 팔의 회전, 경로, 그리퍼 동작 같은 실행해야 할 실제 동작 시퀀스의 액션을 직접 학습하는 모델이다.

이러한 연구가 필요하게 된 이유는 전통적인 로봇 구조의 한계에 있다. 기존에 로봇들이 공장에서 프로그래밍된 대로 반복 작업을 하였다. 그러나 서비스 로봇들은 이러한 로봇에게 언어로 지시하면 이를 알아듣고, 현재 상황을 인지하여 명령을 자연스럽게 처리해야 한다. 서비스는 특정 작업만 하는 것이 아니라 다양한 상황에 적용 가능한 범용성이 필요하다.

### 2. 전통 로봇 시스템의 한계

전통적인 로봇 시스템은 다음과 같은 구조적 한계를 가지고 있다. 첫째, 모듈화된 처리 방식의 한계이다. 기존의 컴퓨터 비전 모델은 객체 감지나 분류와 같은 특정 작업에 집중하였고, 언어 모델은 텍스트 기반의 이해와 생성에 주력하였다. 이러한 분리된 접근 방식은 복잡한 환경에서의 유연한 대응에 한계를 보였다.

둘째, 환경 적응성 부족이다. 고정된 제어 로직으로 인해 환경 변화에 대한 적응성이 부족하고, 새로운 태스크마다 코드 수정이 필요하다. 또한 확장성 부족으로 새로운 환경이나 태스크 적용 시 재설계가 필요하다.

셋째, 사용자 상호작용의 한계이다. 사용자와의 자연스러운 소통이 어렵고, 복잡한 자연어 명령 처리가 불가능하며, 현재 상황을 고려한 적응적 동작이 어렵다.

### 3. VLA 모델의 필요성

VLA 모델은 이러한 전통적인 로봇 시스템의 한계를 극복하기 위해 등장했다. VLA 모델은 시각과 언어 정보를 통합적으로 처리하여 상황을 정확히 이해하고, 자연어 명령어를 통한 직관적 제어가 가능하며, 실시간 적응성을 제공한다.

VLA 모델의 핵심 장점은 통합된 멀티모달 처리, 자연어 인터페이스, 실시간 적응성이다. 시각-언어 통합으로 이미지와 텍스트를 동시에 처리하여 상황을 정확히 이해하고, 복잡한 프로그래밍 없이 자연어로 로봇을 제어할 수 있으며, 변화하는 환경에 즉시 적응할 수 있다.

### 4. VLM을 VLA에 사용하는 이유

VLA 모델에서 Vision-Language Model (VLM)을 사용하는 이유는 VLM의 강력한 다중 모달 능력 때문이다. VLM은 웹 규모의 방대한 데이터로 훈련되어 텍스트와 이미지/비디오를 통합적으로 이해하고 표현하는 능력이 뛰어나다. 이러한 능력은 로봇이 복잡한 환경에서 시각적 정보와 언어적 지시를 동시에 처리하고 이해하는 데 필수적이다.

VLM을 VLA에 사용할 때 얻어지는 장점은 다음과 같다. 첫째, 강력한 다중 모달 표현 학습 능력이다. VLM은 웹 규모의 방대한 데이터로 훈련되어 텍스트와 이미지/비디오와 같은 다양한 모달리티를 통합적으로 이해하고 표현하는 능력이 뛰어나다. 둘째, 일반화 및 강건성이다. VLM은 다양한 오픈 월드 시나리오에서 일반화된 표현을 학습하였으므로, 로봇이 이전에 보지 못했던 새로운 상황, 객체, 배경, 또는 작업 설명에 대해서도 잘 작동할 수 있다. 셋째, 데이터 효율성이다. VLM은 이미 방대한 양의 시각-언어 데이터를 통해 사전 학습되어 있기 때문에, VLA로 미세 조정할 때 필요한 로봇 조작 데이터의 양을 줄일 수 있다. 넷째, 복잡한 작업 처리 능력이다. VLM이 가진 추론 능력은 로봇이 단순히 객체를 인식하는 것을 넘어, 언어적 지시를 바탕으로 복잡한 작업을 계획하고 실행하는 데 도움을 준다.

### 5. VLM 백본 선택

본 연구에서는 VLM 백본으로 Microsoft Kosmos-2 Vision-Language 모델을 선택하였다. Kosmos-2는 멀티모달 대형 언어 모델로서, 시각적 세계와의 텍스트 연결을 강화하여 다양한 다운스트림 작업에서 우수한 성능을 발휘한다. Kosmos-2는 기존 VLM들이 단순히 이미지와 텍스트를 분리하여 처리하는 것과 달리, 시각적 세계와 텍스트를 통합적으로 이해하는 능력을 갖추고 있다.

Kosmos-2 선택의 결정적 요인은 다음과 같다. 첫째, 학술적 검증이다. 선행연구의 8가지 VLM 백본 비교에서 최고 성능을 달성하였다. 둘째, 멀티모달 통합이다. 시각과 언어의 통합적 이해 능력을 제공한다. 셋째, 로봇 제어 적합성이다. 언어 조건부 조작 작업에 최적화된 아키텍처를 가지고 있다. 넷째, 확장성이다. 다양한 다운스트림 태스크에 대한 우수한 일반화 능력을 제공한다. 다섯째, 실용성이다. 실제 로봇 환경에서의 안정적인 성능을 보장한다.

### 6. 연구 목표

본 연구의 주요 목표는 모바일 환경에 최적화된 VLA 모델을 개발하는 것이다. 구체적으로는 Jetson Orin NX 16GB에서 실시간 동작 가능한 VLA 모델을 구현하여, 추론 속도 750 FPS 이상, 메모리 사용량 2GB 이하, MAE 0.25 이하의 성능을 달성하는 것을 목표로 한다.

### 7. 연구 기여

본 연구의 주요 기여는 다음과 같다. 첫째, 모바일 환경에 최적화된 VLA 모델 개발이다. 엣지 디바이스에서 실시간 동작 가능한 VLA 모델을 구현하였다. 둘째, 하이브리드 아키텍처 제안이다. Kosmos-2와 CLIP을 효과적으로 조합하여 성능을 향상시켰다. 셋째, 양자화 최적화이다. FP16 양자화를 통한 성능 향상 기법을 제시하였다.

---

## 📊 분석 및 검토 사항

### Simplified RoboVLMs MAE 분석

**의심스러운 MAE 값 발견:**
Simplified RoboVLMs 모델의 MAE 0.0017은 다른 모델들(MAE 0.212-0.247)에 비해 비정상적으로 낮다. 이는 다음과 같은 가능성을 시사한다:

1. **다른 데이터셋 사용**: Simplified RoboVLMs가 다른 데이터셋으로 훈련되었을 가능성
2. **다른 평가 방식**: MAE 계산 방식이나 평가 지표가 다를 가능성
3. **과적합**: 매우 작은 데이터셋에서 과적합이 발생했을 가능성
4. **정규화 문제**: 데이터 정규화나 스케일링 문제로 인한 왜곡된 결과

**검증 필요 사항:**
- Simplified RoboVLMs의 훈련 데이터셋 확인
- MAE 계산 방식 검증
- 동일한 조건에서 재평가 필요

### 논문 구조 개선 사항

**현재 문제점:**
1. 분석적 내용과 실제 논문 내용이 혼재
2. 문장형 논문 작성이 부족
3. 체계적 구조 부족

**개선 방향:**
1. 실제 논문 내용을 문장형으로 정리
2. 분석 부분을 별도 섹션으로 분리
3. 논리적 흐름 강화

### 다음 단계

1. **Simplified RoboVLMs 검증**: 훈련 데이터셋 및 평가 방식 확인
2. **논문 문장형 정리**: 모든 섹션을 실제 논문 형태로 작성
3. **분석 부분 분리**: 검토 사항을 별도 파일로 관리
4. **일관성 확보**: 용어 사용 및 논리적 흐름 통일

---
*마지막 업데이트: 2024년 8월 29일*
