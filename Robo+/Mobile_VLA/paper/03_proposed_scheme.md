# III. The Proposed Scheme

## 📋 현재 상태: 보완 필요

## ✅ 완료된 내용

### 1. RoboVLMs 분석
이 논문은 단순한 성능 비교를 넘어, 범용 로봇 정책의 설계 원칙과 실용적 가이드라인을 제공하며, 향후 로봇 연구의 방향성을 제시하는 데 큰 기여를 했습니다.

이 논문은 Vision-Language-Action 모델(VLA)을 기반으로 한 범용 로봇 정책(generalist robot policies) 구축에 필요한 핵심 요소들을 체계적으로 분석하고, 새로운 프레임워크인 RoboVLMs를 제안합니다. 연구는 VLA 아키텍처를 구성하는 방법에 대한 세 가지 필수적인 설계 선택에 중점을 두었습니다. VLM 백본 선택, VLA 아키텍처 구성 그리고 교차-구현 데이터 추가 시점입니다. Kosmos-2 기반으로 기존 Vision-Language Model을 로봇 제어용으로 확장한 end-to-end 학습 프레임워크입니다. 언어 명령을 받고 이미지 관찰 후에 즉시 로봇 액션을 생성하는 실용적인 실시간 로봇 제어를 보여줍니다.

#### 1.1 RoboVLMs 아키텍처 패러다임
RoboVLMs는 4가지 주요 아키텍처 패러다임을 제안하며, 우리의 Mobile VLA 시스템은 **Policy-Head-Continuous-Action Models**을 기반으로 합니다.

![RoboVLMs 아키텍처 다이어그램](./images/SCR-20250828-oumx.png)

**우리 연구의 아키텍처 선택 근거:**
- **Policy-Head-Continuous-Action Models** 선택
- VLM (Kosmos-2 + CLIP) → Policy Head (LSTM) → 2D 연속 액션
- 모바일 환경에 최적화된 실시간 처리 가능
- 시퀀스 정보를 효과적으로 처리하는 LSTM 기반 구조

#### 1.2 RoboVLMs 프레임워크
다양한 VLM 백본과 VLA 구조를 쉽게 통합할 수 있는 유연하고 개방형(open-source) 프레임워크입니다. 시뮬레이션(CALVIN, SimplerEnv)과 실제 로봇 실험에서 최신 성능(state-of-the-art)을 달성하였습니다.

#### 1.3 VLA 아키텍처 구성
Kosmos-2 VLM을 RoboKosmos로 래핑하여 액션 예측 기능을 추가하였습니다. 훈련 데이터인 CALVIN 데이터셋으로 파인튜닝을 하였고, 정책 헤드를 LSTM 기반으로 7DoF 로봇을 제어하였습니다.

##### 1.3.1 정책의 기본 개념
정책(Policy)은 강화학습과 로봇공학에서 나온 개념으로, "주어진 상황에서 어떤 행동을 할지 결정하는 규칙 또는 함수"를 의미합니다. 언어 명령과 이미지를 보고 로봇 액션으로 변환합니다.

이미지와 언어 입력이 주어졌을 때, 로봇 액션을 선택하는 함수는 다음과 같습니다:
```
π(action | vision, language, history)
```

##### 1.3.2 VLA에서 정책의 역할
VLA 시스템에서 정책은 다음과 같은 역할을 합니다.

RoboVLMs에서는 4가지 정책 헤드로 구현되었습니다. 첫째는 즉시 반응형 정책인 FCDecoder로 직관적이고 빠른 특징이 있습니다. 단순한 집기나 놓기 등에 사용됩니다. 두 번째는 시간 순서가 중요한 순차적 정책인 LSTMDecoder로 연속적인 조작이 중요한 상황에서 사용됩니다. 세 번째는 어텐션 메커니즘을 사용한 GPTDecoder로 강력한 추론이 가능하여 복잡한 멀티태스크 상황에 적합합니다. 네 번째는 언어와 액션이 통합된 DiscreteDecoder로 언어와 밀접한 태스크를 수행하는 상황에 적합합니다.

**우리 연구의 정책 헤드 선택:**
- **LSTMDecoder** 기반 정책 헤드 선택
- 4층 LSTM (4096 hidden size)
- 시퀀스 정보를 효과적으로 처리
- 모바일 환경에 최적화된 실시간 처리

##### 1.3.3 RoboVLMs 실행 플로우
RoboVLMs는 6-DOF 팔 + 1-DOF 그리퍼를 제어하는 7-DOF 제어 시스템입니다. 비전, 언어, 액션 히스토리, 로봇 상태 융합하여 멀티모달 통합하였습니다. window-chunk 방식의 과거로 미래를 예측하였습니다. 액션 예측, 미래 관찰, 캡션 생성 동시 최적화하는 멀티태스크 학습을 수행합니다.

**우리 연구의 실행 플로우:**
1. **입력**: 720p 이미지 + 텍스트 명령
2. **VLM 처리**: Kosmos-2 + CLIP 하이브리드
3. **특징 융합**: 4864d → 2048d
4. **LSTM 처리**: 4층 LSTM (4096 hidden)
5. **액션 생성**: 2D 연속 액션 [linear_x, linear_y]
6. **실시간 제어**: 750+ FPS

##### 1.3.4 시간적 구조화
윈도우-청크 분할을 통해 과거 8프레임 관찰하여, 미래 10프레임을 예측합니다. 과거는 미래 예측에만 허용되고, 미래는 과거의 정보 유출을 차단합니다.

시간 순서별 동작 시뮬레이션은 아래와 같습니다:

| 시간 | 동작 |
|------|------|
| t=0 | "pick up the red cup" 명령 입력 |
| t=1~8 | 과거 액션 히스토리와 현재 이미지 관찰 |
| t=9 | 멀티모달 융합으로 상황 이해 |
| t=10 | LSTM을 통한 시퀀스 기반 액션 예측 |
| t=11 | 연속(팔)/이산(그리퍼) 하이브리드 출력 |
| t=12 | 실제 로봇 제어 명령 전송 |

**우리 연구의 시간적 구조화:**
- **18프레임 시퀀스**: 고정된 길이 사용
- **LSTM 메모리**: 4층으로 과거 정보 활용
- **실시간 처리**: 30 FPS 실시간 스트림

#### 1.4 실험적 기여
600개 이상의 실험을 통해 VLA 설계의 핵심 요소를 정량적으로 분석하였습니다. 실제 로봇 실험에서 자기 수정(self-correction) 능력까지 관찰되었습니다.

#### 1.5 다른 VLA와의 차이점
첫 번째는 3가지 태스크를 병렬 처리하는 멀티태스크 동시 학습입니다. 두 번째는 window_size와 chunk_size 기반으로 한 시간적 구조화입니다. 세 번째는 10가지 마스크로 메모리를 최적화하였습니다.

### 2. RoboVLMs 한계점
우리가 분석한 RoboVLMs의 한계점입니다.

#### 2.1 멀티모달 상호작용 구조의 제한
기존 VLM의 구조(예: attention mask, mixture of experts)를 그대로 유지한 채 VLA를 구성했기 때문에, 행동(action)과의 상호작용을 위한 전용 아키텍처 설계가 부족합니다. π0 같은 모델은 이러한 상호작용을 더 정교하게 설계하여 성능 향상을 보여주므로, 향후 연구에서 구조적 개선이 필요합니다.

#### 2.2 VLA 구조의 단순화
논문에서 고려한 VLA 구조는 네 가지로 제한되어 있으며, 다양한 구조적 변형이나 세부 설계 요소(예: attention 방식, token 처리 방식 등)에 대한 탐색이 부족합니다.

#### 2.3 행동 토크나이징 및 학습 목표의 미탐색
행동을 표현하는 방식(예: VQ-VAE, diffusion models, flow matching 등)에 대한 실험이 부족하며, 정교한 행동 표현 및 예측 방식에 대한 연구가 향후 필요합니다.

### 3. RoboVLMs 개선 제안
RoboVLMs의 한계점을 보고 개선점을 제안합니다. RoboVLMs 모델을 이용하여,

#### 3.1 환경 설정
Ubuntu 22.04, ROS2, 로봇 구성 소개

#### 3.2 데이터 수집
데이터는 72개 에피소드로 구성되어 있으며, 8개 핵심 내비게이션 시나리오를 기반으로 수집되었습니다.

#### 3.3 구현 사항
RoboVLMs의 결과는 7-DoF이지만, 테스트를 수행하는 로봇은 결과로 2-DoF입니다.

#### 3.4 학습 수행
Kosmos-2와 CLIP을 하이브리드로 조합하여 학습을 수행하였습니다.

### 4. 제안하는 Mobile VLA 시스템

#### 4.1 전체 시스템 아키텍처

본 연구에서는 RoboVLMs의 **Policy-Head-Continuous-Action Models**을 기반으로 모바일 환경에 최적화된 Mobile VLA 시스템을 제안합니다.

**시스템 구성 요소:**
1. **VLM 백본**: Kosmos-2 + CLIP 하이브리드 모델
2. **Policy Head**: 4층 LSTM 기반 시퀀스 처리
3. **액션 공간**: 2D 연속 액션 [linear_x, linear_y]
4. **실시간 처리**: 18프레임 시퀀스 기반 스트리밍 처리
5. **모바일 최적화**: FP16 양자화 및 메모리 최적화

#### 4.2 하이브리드 VLM 백본 설계

**Kosmos-2 + CLIP 하이브리드 모델의 설계 근거:**

**1. Kosmos-2의 장점 활용**
- **강력한 멀티모달 이해**: 시각과 언어의 통합적 이해 능력
- **고품질 Alignment**: 이미지와 텍스트 간 세밀한 관계 학습
- **일반화 성능**: 다양한 시나리오에서의 우수한 성능

**2. CLIP의 보완적 역할**
- **빠른 추론**: 경량화된 구조로 실시간 처리 가능
- **안정적 특징 추출**: 일관된 시각적 특징 제공
- **메모리 효율성**: 적은 메모리로 높은 성능

**3. 하이브리드 조합의 효과**
- **정확도 향상**: Kosmos-2의 깊은 이해 + CLIP의 빠른 처리
- **속도 최적화**: 병렬 처리로 추론 시간 단축
- **메모리 효율성**: 선택적 특징 융합으로 메모리 사용량 최적화

#### 4.3 상세 모델 아키텍처 ⚠️ (추정)

**입력 처리 레이어:**
```
Input: Image (1280x720) + Text Command
    ↓
Kosmos2 Vision Encoder (24층, 4096d) ⚠️
    ↓
Kosmos2 Text Encoder (24층, 2048d) ⚠️
    ↓
CLIP Vision Encoder (12층, 768d) ⚠️
    ↓
CLIP Text Encoder (12층, 768d) ⚠️
    ↓
Feature Fusion Layer (4864d → 2048d) ⚠️
    ↓
LSTM Layer (4층, 4096 hidden size) ✅
    ↓
Action Head (1024→512→256→2) ⚠️
    ↓
Output: [linear_x, linear_y] ✅
```

**특징 융합 레이어:**
- **입력 차원**: Kosmos2 (4096d + 2048d) + CLIP (768d + 768d) = 4864d
- **융합 방법**: Concatenation + Linear Projection
- **출력 차원**: 2048d (LSTM 입력 크기에 맞춤)
- **활성화 함수**: ReLU + Dropout (0.1)

**LSTM Policy Head:**
- **레이어 수**: 4층
- **Hidden Size**: 4096
- **Dropout**: 0.1 (과적합 방지)
- **Bidirectional**: False (순차적 처리)
- **Batch First**: True (배치 처리 최적화)

**액션 헤드:**
- **입력**: LSTM 출력 (4096d)
- **중간 레이어**: 1024 → 512 → 256
- **출력**: 2D 액션 [linear_x, linear_y]
- **활성화 함수**: Tanh (액션 범위 제한)

## 🔧 보완 필요 사항

### 1. 환경 설정 구체화
- **하드웨어**: Jetson Orin NX 16GB 상세 사양
- **소프트웨어**: PyTorch, ROS2 버전 정보
- **데이터셋**: 72개 에피소드 상세 정보

### 2. 데이터 수집 방법 상세화
- **수집 환경**: 8개 핵심 내비게이션 시나리오
- **프로토콜**: 1박스/2박스, 세로/가로, 좌측/우측 회피
- **품질 관리**: Core/Variant 패턴, 거리별/위치별 세분화

### 3. 구현 사항 기술적 세부사항
- **모델 구조**: 각 레이어의 상세 파라미터
- **학습 방법**: 최적화 기법, 손실 함수
- **최적화 기법**: 양자화, 메모리 최적화

### 4. 학습 수행 과정
- **학습률**: Adam optimizer 설정
- **배치 크기**: 메모리 제약 고려한 설정
- **에포크**: 조기 종료 조건
- **검증 방법**: 교차 검증, 성능 평가

## 📊 기술적 세부사항

### 1. 모델 아키텍처 상세
```
Input: Image (1280x720) + Text Command
    ↓
Kosmos2 Vision Encoder (24층, 4096d)
    ↓
Kosmos2 Text Encoder (24층, 2048d)
    ↓
CLIP Vision Encoder (12층, 768d)
    ↓
CLIP Text Encoder (12층, 768d)
    ↓
Feature Fusion Layer (4864d → 2048d)
    ↓
LSTM Layer (4층, 4096 hidden size)
    ↓
Action Head (1024→512→256→2)
    ↓
Output: [linear_x, linear_y]
```

### 2. 학습 파라미터
- **Learning Rate**: 1e-4 (Adam optimizer)
- **Batch Size**: 32 (메모리 제약 고려)
- **Epochs**: 10 (조기 종료 적용)
- **Optimizer**: Adam with weight decay
- **Loss Function**: Mean Absolute Error (MAE)

### 3. 실험 환경
- **Hardware**: Jetson Orin NX 16GB
- **Software**: PyTorch 2.0, ROS2 Humble
- **Dataset**: 72개 에피소드 (8 시나리오 × 9 변형)
- **Evaluation Metrics**: MAE, FPS, Memory Usage

## 🎯 개선 우선순위

### 높음 (즉시 보완)
1. **환경 설정 구체화**
   - 실험 환경 상세 명시
   - 하드웨어/소프트웨어 사양
   - 데이터셋 정보

2. **구현 세부사항**
   - 모델 아키텍처 상세
   - 학습 파라미터 설정
   - 최적화 기법

### 중간 (단계적 보완)
1. **데이터 수집 방법**
   - 수집 프로토콜
   - 품질 관리 방법
   - 전처리 과정

2. **학습 과정 상세화**
   - 학습 스케줄
   - 검증 방법
   - 모델 선택 기준

### 낮음 (추가 연구 후 보완)
1. **성능 분석**
   - 병목 지점 분석
   - 최적화 방안
   - 확장성 검토

## 📝 다음 단계
1. 실험 환경 및 하드웨어 사양 정리
2. 모델 아키텍처 상세 설계
3. 학습 파라미터 및 방법론 정의
4. 데이터 수집 및 전처리 프로토콜 작성

## 결론

본 연구에서는 기존 RoboVLMs의 한계를 극복하기 위해 모바일 환경에 최적화된 Mobile VLA 시스템을 제안합니다. RoboVLMs의 4가지 아키텍처 패러다임 중에서 **Policy-Head-Continuous-Action Models**을 선택하여 Kosmos-2와 CLIP을 하이브리드로 조합하고, LSTM 기반 Policy Head를 통해 시퀀스 정보를 효과적으로 처리합니다. 2D 액션 공간을 사용하여 실시간 처리가 가능한 시스템을 구현하였으며, FP16 양자화를 통해 1.9배의 속도 향상과 49.8%의 메모리 절약을 달성하여 Jetson Orin NX에서 750+ FPS의 실시간 처리가 가능함을 보여줍니다. 이는 기존의 대규모 VLA 모델들이 가진 계산 복잡도와 실시간성 문제를 해결한 중요한 기여입니다.

---
*마지막 업데이트: 2024년 8월 25일*
