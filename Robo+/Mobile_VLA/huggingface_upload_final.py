#!/usr/bin/env python3
"""
Hugging Faceì— Mobile VLA ëª¨ë¸ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢…íŒ)
MAE 0.222 ë‹¬ì„±í•œ ìµœì‹  ëª¨ë¸ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
"""

import os
import json
import torch
from huggingface_hub import HfApi, create_repo, upload_file
import shutil

def create_model_card():
    """
    ëª¨ë¸ ì¹´ë“œ (README.md) ìƒì„±
    """
    model_card = """# Mobile VLA: Vision-Language-Action System for Omniwheel Robot Navigation

## Model Description

This model is a Vision-Language-Action (VLA) system adapted from RoboVLMs framework for omniwheel robot navigation. It demonstrates framework robustness by successfully adapting from robot manipulator tasks to mobile robot navigation tasks.

## Performance

- **MAE**: 0.222 (72.5% improvement from baseline)
- **Task**: Omniwheel Mobile Robot Navigation
- **Framework**: RoboVLMs adapted for mobile robots
- **Performance Level**: Practical

## Key Features

- **Task Adaptation**: Successfully adapted from manipulator to mobile robot tasks
- **Framework Robustness**: Cross-domain application capability
- **Omniwheel Optimization**: Omnidirectional control for mobile robots
- **Real-world Applicability**: Practical navigation performance

## Model Architecture

- **Vision Encoder**: Kosmos-2 based image processing
- **Language Encoder**: Korean text command understanding
- **Action Predictor**: 2D action prediction (linear_x, linear_y)
- **Output**: Continuous action values for robot control

## Usage

```python
import torch

# Load model
model = torch.load("best_simple_lstm_model.pth")

# Example usage
image = load_image("robot_environment.jpg")
text_command = "Move forward to the target"
action = model.predict_action(image, text_command)
```

## Training Data

- **Dataset**: Mobile VLA Dataset
- **Total Frames**: 1,296
- **Action Range**: linear_x [0.0, 1.15], linear_y [-1.15, 1.15]
- **Action Pattern**: Forward (56.1%), Left turn (10%), Right turn (7.2%)

## Research Contribution

This work demonstrates the robustness of VLA frameworks by successfully adapting RoboVLMs from robot manipulator tasks to mobile robot navigation tasks, achieving practical performance with MAE 0.222.

## Citation

```bibtex
@article{mobile_vla_2024,
  title={Mobile VLA: Vision-Language-Action System for Omniwheel Robot Navigation},
  author={Your Name},
  journal={arXiv preprint},
  year={2024}
}
```

## License

MIT License

---

**Model Performance**: MAE 0.222 | **Task**: Omniwheel Robot Navigation | **Framework**: RoboVLMs Adapted
"""
    
    return model_card

def create_config_json():
    """
    ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±
    """
    config = {
        "model_type": "mobile_vla",
        "task": "omniwheel_robot_navigation",
        "performance": {
            "mae": 0.222,
            "improvement": "72.5% from baseline",
            "level": "practical"
        },
        "architecture": {
            "vision_encoder": "kosmos2_based",
            "language_encoder": "korean_text",
            "action_predictor": "2d_continuous",
            "output_dim": 2
        },
        "training": {
            "dataset": "mobile_vla_dataset",
            "total_frames": 1296,
            "action_range": {
                "linear_x": [0.0, 1.15],
                "linear_y": [-1.15, 1.15]
            }
        },
        "framework": {
            "base": "robovlms",
            "adaptation": "manipulator_to_mobile_robot",
            "robustness": "cross_domain_application"
        }
    }
    
    return config

def upload_to_huggingface():
    """
    Hugging Faceì— ëª¨ë¸ ì—…ë¡œë“œ
    """
    print("ğŸš€ Hugging Faceì— Mobile VLA ëª¨ë¸ ì—…ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    results_path = "results/simple_lstm_results_extended/simple_lstm_training_results.json"
    performance_path = "results/robovlms_performance_metrics.json"
    
    # ì—…ë¡œë“œí•  íŒŒì¼ë“¤ í™•ì¸
    files_to_upload = []
    
    if os.path.exists(model_path):
        files_to_upload.append(model_path)
        print(f"âœ… ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
    else:
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return
    
    if os.path.exists(results_path):
        files_to_upload.append(results_path)
        print(f"âœ… í›ˆë ¨ ê²°ê³¼ íŒŒì¼ ë°œê²¬: {results_path}")
    
    if os.path.exists(performance_path):
        files_to_upload.append(performance_path)
        print(f"âœ… ì„±ëŠ¥ ì§€í‘œ íŒŒì¼ ë°œê²¬: {performance_path}")
    
    # ëª¨ë¸ ì¹´ë“œ ìƒì„±
    model_card = create_model_card()
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(model_card)
    files_to_upload.append("README.md")
    print("âœ… ëª¨ë¸ ì¹´ë“œ ìƒì„± ì™„ë£Œ")
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    config = create_config_json()
    with open("config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    files_to_upload.append("config.json")
    print("âœ… ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ")
    
    # Hugging Face API ì´ˆê¸°í™”
    try:
        api = HfApi()
        
        # ì •í™•í•œ ì‚¬ìš©ìëª… ì‚¬ìš©
        username = "minium"  # huggingface-cli whoami ê²°ê³¼
        repo_name = "mobile-vla-omniwheel"
        full_repo_name = f"{username}/{repo_name}"
        
        print(f"ğŸ“¦ ì €ì¥ì†Œ í™•ì¸ ì¤‘: {full_repo_name}")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        print("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
        for file_path in files_to_upload:
            if os.path.exists(file_path):
                # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
                filename = os.path.basename(file_path)
                print(f"  ğŸ“ ì—…ë¡œë“œ ì¤‘: {filename}")
                
                try:
                    api.upload_file(
                        path_or_fileobj=file_path,
                        path_in_repo=filename,
                        repo_id=full_repo_name,
                        commit_message=f"Add {filename} - Mobile VLA model with MAE 0.222"
                    )
                    print(f"  âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
                except Exception as e:
                    print(f"  âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
                    # ë” ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì¶œë ¥
                    if "404" in str(e):
                        print(f"    ğŸ’¡ 404 ì˜¤ë¥˜: ì €ì¥ì†Œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
                        print(f"    ğŸ’¡ í•´ê²° ë°©ë²•: https://huggingface.co/{full_repo_name} ì—ì„œ ì €ì¥ì†Œë¥¼ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
            else:
                print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
        
        print(f"\nğŸ‰ ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“‹ ëª¨ë¸ í˜ì´ì§€: https://huggingface.co/{full_repo_name}")
        print(f"ğŸ”— ë‹¤ìš´ë¡œë“œ: https://huggingface.co/{full_repo_name}/resolve/main/best_simple_lstm_model.pth")
        
        # ì •ë¦¬
        if os.path.exists("README.md"):
            os.remove("README.md")
        if os.path.exists("config.json"):
            os.remove("config.json")
        
    except Exception as e:
        print(f"âŒ Hugging Face ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("1. https://huggingface.co/minium/mobile-vla-omniwheel ì—ì„œ ì €ì¥ì†Œ ìƒì„±")
        print("2. huggingface-cli loginìœ¼ë¡œ ë¡œê·¸ì¸ í™•ì¸")
        print("3. í† í° ê¶Œí•œ í™•ì¸")

def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    print("=" * 60)
    print("ğŸ¤– Mobile VLA ëª¨ë¸ Hugging Face ì—…ë¡œë“œ (ìµœì¢…íŒ)")
    print("=" * 60)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print("- ëª¨ë¸ëª…: Mobile VLA Omniwheel Navigation")
    print("- ì„±ëŠ¥: MAE 0.222 (72.5% ê°œì„ )")
    print("- íƒœìŠ¤í¬: ì˜´ë‹ˆíœ  ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜")
    print("- í”„ë ˆì„ì›Œí¬: RoboVLMs ì ì‘")
    
    # ì—…ë¡œë“œ ì‹¤í–‰
    upload_to_huggingface()

if __name__ == "__main__":
    main()
