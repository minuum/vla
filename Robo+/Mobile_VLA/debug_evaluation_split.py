"""
ğŸ” í‰ê°€ ë¶„í•  ë° 100% ì •í™•ë„ ë¬¸ì œ ì§„ë‹¨
í•™ìŠµì…‹ê³¼ í‰ê°€ì…‹ì´ ì œëŒ€ë¡œ ë¶„ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
"""

import torch
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from torch.utils.data import DataLoader, Dataset, random_split

from fixed_robovlms_model import FixedRoboVLMStyleSingleImageModel
from train_fixed_robovlms import FixedRoboVLMStyleDataset

def debug_data_split():
    """ë°ì´í„° ë¶„í•  ë””ë²„ê¹…"""
    
    print("ğŸ” ë°ì´í„° ë¶„í•  ë””ë²„ê¹… ì‹œì‘")
    print("=" * 50)
    
    # ì›ë³¸ ë°ì´í„° í™•ì¸
    data_path = '../../ROS_action/mobile_vla_dataset'
    
    if os.path.isdir(data_path):
        h5_files = list(Path(data_path).glob("*.h5"))
    else:
        h5_files = [data_path]
    
    print(f"ğŸ“Š ì›ë³¸ H5 íŒŒì¼ ìˆ˜: {len(h5_files)}")
    
    # ê° H5 íŒŒì¼ì˜ ë‚´ìš© í™•ì¸
    all_episodes = []
    for h5_file in h5_files:
        try:
            with h5py.File(h5_file, 'r') as f:
                if 'images' in f and 'actions' in f:
                    images = f['images'][:]  # [18, H, W, 3]
                    actions = f['actions'][:]  # [18, 3]
                    
                    # ì²« í”„ë ˆì„ë§Œ ì‚¬ìš©
                    single_image = images[0]  # [H, W, 3]
                    single_action = actions[0]  # [3]
                    
                    all_episodes.append({
                        'file': h5_file.name,
                        'image': single_image,
                        'action': single_action,
                        'episode_id': f"{h5_file.stem}"
                    })
        except Exception as e:
            print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸ“Š ì´ ì—í”¼ì†Œë“œ ìˆ˜: {len(all_episodes)}")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    full_dataset = FixedRoboVLMStyleDataset(data_path, processor, 'full')
    
    print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(full_dataset)}")
    
    # ë¶„í•  í™•ì¸
    train_size = int(len(full_dataset) * 0.8)
    val_size = len(full_dataset) - train_size
    
    print(f"ğŸ“Š ë¶„í•  ê³„íš:")
    print(f"   - í›ˆë ¨: {train_size}ê°œ")
    print(f"   - ê²€ì¦: {val_size}ê°œ")
    print(f"   - ì´í•©: {train_size + val_size}ê°œ")
    
    # ì‹¤ì œ ë¶„í• 
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    print(f"ğŸ“Š ì‹¤ì œ ë¶„í• :")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ")
    
    # ê²€ì¦ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ í™•ì¸
    val_indices = val_dataset.indices
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° ì¸ë±ìŠ¤: {val_indices[:10]}... (ì´ {len(val_indices)}ê°œ)")
    
    return train_dataset, val_dataset, full_dataset

def debug_model_performance():
    """ëª¨ë¸ ì„±ëŠ¥ ë””ë²„ê¹…"""
    
    print("\nğŸ” ëª¨ë¸ ì„±ëŠ¥ ë””ë²„ê¹…")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model_path = 'fixed_robovlms_model_best.pth'
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
        return
    
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    model = FixedRoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=0.2,
        use_claw_matrix=True,
        use_hierarchical=True,
        use_advanced_attention=True,
        z_axis_weight=0.05
    ).to(device)
    
    # ëª¨ë¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location=device)
    
    # ë™ì  ì–´ëŒ‘í„° ì´ˆê¸°í™”
    dummy_image = torch.randn(1, 3, 224, 224).to(device)
    with torch.no_grad():
        try:
            _ = model(dummy_image, "Navigate to target")
        except:
            pass
    
    # í˜¸í™˜ë˜ëŠ” í‚¤ë§Œ ë¡œë“œ
    model_dict = model.state_dict()
    checkpoint_dict = checkpoint['model_state_dict']
    compatible_dict = {k: v for k, v in checkpoint_dict.items() 
                      if k in model_dict and model_dict[k].shape == v.shape}
    model_dict.update(compatible_dict)
    model.load_state_dict(model_dict)
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   - ì—í¬í¬: {checkpoint['epoch']}")
    print(f"   - í›ˆë ¨ ì†ì‹¤: {checkpoint['train_loss']:.6f}")
    print(f"   - ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.6f}")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_path = '../../ROS_action/mobile_vla_dataset'
    train_dataset, val_dataset, full_dataset = debug_data_split()
    
    # ê²€ì¦ ë°ì´í„°ë§Œ ì‚¬ìš©
    val_loader = DataLoader(
        val_dataset,
        batch_size=4,
        shuffle=False,
        num_workers=1,
        pin_memory=True
    )
    
    # ìƒì„¸ ì„±ëŠ¥ í‰ê°€
    model.eval()
    total_loss = 0.0
    total_mae = 0.0
    predictions = []
    targets = []
    
    print(f"\nğŸ¯ ê²€ì¦ ë°ì´í„° ìƒì„¸ í‰ê°€:")
    print(f"   - ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader)}")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            try:
                images = batch['image'].float().to(device)
                actions = batch['action'].float().to(device)
                
                # ì˜ˆì¸¡
                predicted_actions = model(images, "Navigate to target")
                
                # ì†ì‹¤ ê³„ì‚°
                z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
                weighted_target = actions * z_weight.unsqueeze(0)
                weighted_pred = predicted_actions * z_weight.unsqueeze(0)
                
                loss = torch.nn.functional.mse_loss(weighted_pred, weighted_target)
                total_loss += loss.item()
                
                # MAE ê³„ì‚°
                mae = torch.mean(torch.abs(predicted_actions - actions))
                total_mae += mae.item()
                
                # ì˜ˆì¸¡ê³¼ íƒ€ê²Ÿ ì €ì¥
                predictions.append(predicted_actions.cpu().numpy())
                targets.append(actions.cpu().numpy())
                
                print(f"   - ë°°ì¹˜ {batch_idx}: Loss={loss.item():.6f}, MAE={mae.item():.6f}")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    # í‰ê·  ê³„ì‚°
    avg_loss = total_loss / len(val_loader)
    avg_mae = total_mae / len(val_loader)
    
    # ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
    predictions = np.concatenate(predictions, axis=0)
    targets = np.concatenate(targets, axis=0)
    
    print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    print(f"   - í‰ê·  ì†ì‹¤: {avg_loss:.6f}")
    print(f"   - í‰ê·  MAE: {avg_mae:.6f}")
    
    # ë‹¤ì–‘í•œ ì„ê³„ê°’ìœ¼ë¡œ ì •í™•ë„ ê³„ì‚°
    thresholds = [0.01, 0.05, 0.1, 0.2, 0.5]
    for threshold in thresholds:
        within_threshold = np.abs(predictions - targets) < threshold
        accuracy = np.mean(within_threshold) * 100
        print(f"   - ì„ê³„ê°’ {threshold}: {accuracy:.2f}%")
    
    # ì¶•ë³„ ë¶„ì„
    axis_names = ['Xì¶•', 'Yì¶•', 'Zì¶•']
    for i, axis_name in enumerate(axis_names):
        axis_mae = np.mean(np.abs(predictions[:, i] - targets[:, i]))
        axis_rmse = np.sqrt(np.mean((predictions[:, i] - targets[:, i]) ** 2))
        print(f"   - {axis_name} MAE: {axis_mae:.6f}, RMSE: {axis_rmse:.6f}")
    
    # ì˜ˆì¸¡ê°’ê³¼ íƒ€ê²Ÿê°’ ë¹„êµ
    print(f"\nğŸ” ì˜ˆì¸¡ê°’ vs íƒ€ê²Ÿê°’ ìƒ˜í”Œ:")
    for i in range(min(5, len(predictions))):
        print(f"   ìƒ˜í”Œ {i}:")
        print(f"     ì˜ˆì¸¡: {predictions[i]}")
        print(f"     íƒ€ê²Ÿ: {targets[i]}")
        print(f"     ì°¨ì´: {np.abs(predictions[i] - targets[i])}")

def check_overfitting():
    """ê³¼ì í•© í™•ì¸"""
    
    print("\nğŸ” ê³¼ì í•© í™•ì¸")
    print("=" * 50)
    
    # í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ ë¹„êµ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ëª¨ë¸ ë¡œë“œ
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    model = FixedRoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=0.2,
        use_claw_matrix=True,
        use_hierarchical=True,
        use_advanced_attention=True,
        z_axis_weight=0.05
    ).to(device)
    
    model_path = 'fixed_robovlms_model_best.pth'
    checkpoint = torch.load(model_path, map_location=device)
    
    # ë™ì  ì–´ëŒ‘í„° ì´ˆê¸°í™”
    dummy_image = torch.randn(1, 3, 224, 224).to(device)
    with torch.no_grad():
        try:
            _ = model(dummy_image, "Navigate to target")
        except:
            pass
    
    # í˜¸í™˜ë˜ëŠ” í‚¤ë§Œ ë¡œë“œ
    model_dict = model.state_dict()
    checkpoint_dict = checkpoint['model_state_dict']
    compatible_dict = {k: v for k, v in checkpoint_dict.items() 
                      if k in model_dict and model_dict[k].shape == v.shape}
    model_dict.update(compatible_dict)
    model.load_state_dict(model_dict)
    
    # ë°ì´í„° ë¶„í• 
    data_path = '../../ROS_action/mobile_vla_dataset'
    train_dataset, val_dataset, full_dataset = debug_data_split()
    
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
    
    # í›ˆë ¨ ë°ì´í„° ì„±ëŠ¥
    model.eval()
    train_loss = 0.0
    train_mae = 0.0
    
    with torch.no_grad():
        for batch in train_loader:
            images = batch['image'].float().to(device)
            actions = batch['action'].float().to(device)
            
            predicted_actions = model(images, "Navigate to target")
            
            z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
            weighted_target = actions * z_weight.unsqueeze(0)
            weighted_pred = predicted_actions * z_weight.unsqueeze(0)
            
            loss = torch.nn.functional.mse_loss(weighted_pred, weighted_target)
            mae = torch.mean(torch.abs(predicted_actions - actions))
            
            train_loss += loss.item()
            train_mae += mae.item()
    
    # ê²€ì¦ ë°ì´í„° ì„±ëŠ¥
    val_loss = 0.0
    val_mae = 0.0
    
    with torch.no_grad():
        for batch in val_loader:
            images = batch['image'].float().to(device)
            actions = batch['action'].float().to(device)
            
            predicted_actions = model(images, "Navigate to target")
            
            z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
            weighted_target = actions * z_weight.unsqueeze(0)
            weighted_pred = predicted_actions * z_weight.unsqueeze(0)
            
            loss = torch.nn.functional.mse_loss(weighted_pred, weighted_target)
            mae = torch.mean(torch.abs(predicted_actions - actions))
            
            val_loss += loss.item()
            val_mae += mae.item()
    
    avg_train_loss = train_loss / len(train_loader)
    avg_train_mae = train_mae / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)
    avg_val_mae = val_mae / len(val_loader)
    
    print(f"ğŸ“Š í›ˆë ¨ vs ê²€ì¦ ì„±ëŠ¥:")
    print(f"   - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.6f}")
    print(f"   - ê²€ì¦ ì†ì‹¤: {avg_val_loss:.6f}")
    print(f"   - í›ˆë ¨ MAE: {avg_train_mae:.6f}")
    print(f"   - ê²€ì¦ MAE: {avg_val_mae:.6f}")
    
    # ê³¼ì í•© íŒë‹¨
    loss_gap = abs(avg_train_loss - avg_val_loss)
    mae_gap = abs(avg_train_mae - avg_val_mae)
    
    print(f"\nğŸ” ê³¼ì í•© ë¶„ì„:")
    print(f"   - ì†ì‹¤ ì°¨ì´: {loss_gap:.6f}")
    print(f"   - MAE ì°¨ì´: {mae_gap:.6f}")
    
    if loss_gap > 0.01 or mae_gap > 0.01:
        print(f"   âš ï¸ ê³¼ì í•© ì˜ì‹¬: í›ˆë ¨ê³¼ ê²€ì¦ ì„±ëŠ¥ ì°¨ì´ê°€ í¼")
    else:
        print(f"   âœ… ê³¼ì í•© ì—†ìŒ: í›ˆë ¨ê³¼ ê²€ì¦ ì„±ëŠ¥ì´ ë¹„ìŠ·í•¨")

if __name__ == "__main__":
    debug_data_split()
    debug_model_performance()
    check_overfitting()
