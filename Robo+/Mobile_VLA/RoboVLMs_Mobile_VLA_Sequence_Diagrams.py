#!/usr/bin/env python3
"""
ğŸ”„ RoboVLMs vs Mobile VLA ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±

ë°ì´í„° í”Œë¡œìš°, ëª¨ë¸ ì¶”ë¡ , í•™ìŠµ ê³¼ì •ì„ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
"""

from datetime import datetime

def generate_robovlms_sequence_diagram():
    """RoboVLMs ì›ë³¸ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨"""
    
    return """
RoboVLMs ë°ì´í„° í”Œë¡œìš° ë° ì¶”ë¡  ê³¼ì •
```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ User
    participant Dataset as ğŸ“Š RLDS Dataset
    participant DataLoader as ğŸ”„ DataLoader
    participant Backbone as ğŸ§  Backbone Model<br/>(RT-1/OpenVLA)
    participant ActionHead as ğŸ¯ Action Head<br/>(Discrete)
    participant Trainer as ğŸ‹ï¸ BaseTrainer
    
    Note over User, Trainer: ğŸ“š RoboVLMs ì›ë³¸ ì•„í‚¤í…ì²˜
    
    User->>Dataset: Load RLDS/TFRecord data
    Dataset->>DataLoader: RLDS format data
    
    loop Training Loop
        DataLoader->>DataLoader: Generate window/chunk<br/>(8 frames -> 2 actions)
        DataLoader->>Backbone: Batched images + text
        
        Backbone->>Backbone: Vision encoding
        Backbone->>Backbone: Text encoding  
        Backbone->>Backbone: Multi-modal fusion
        
        Backbone->>ActionHead: Hidden features
        ActionHead->>ActionHead: Discrete classification<br/>(7-DOF robot arm)
        ActionHead->>Trainer: Predicted actions
        
        Trainer->>Trainer: CrossEntropy loss
        Trainer->>Backbone: Backpropagation
    end
    
    Note over User, Trainer: âœ… ì¥ì : ë²”ìš©ì„±, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
    Note over User, Trainer: âŒ ë‹¨ì : ë³µì¡ì„±, ë§ì€ ë°ì´í„° í•„ìš”
```
"""

def generate_mobile_vla_sequence_diagram():
    """Mobile VLA ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨"""
    
    return """
Mobile VLA ë°ì´í„° í”Œë¡œìš° ë° ì¶”ë¡  ê³¼ì •
```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ User
    participant HDF5 as ğŸ“ HDF5 Files<br/>(Real Robot Data)
    participant Dataset as ğŸ“Š MobileVLADataset
    participant Collate as ğŸ”„ Custom Collate<br/>(PIL->Tensor)
    participant Kosmos as ğŸŒŒ Kosmos-2B<br/>(Vision Only)
    participant ActionHead as ğŸ¯ Action Head<br/>(3D Continuous)
    participant Trainer as ğŸ‹ï¸ MobileVLATrainer
    
    Note over User, Trainer: ğŸš€ Mobile VLA íŠ¹í™” ì•„í‚¤í…ì²˜
    
    User->>HDF5: Load .h5 files<br/>(72 episodes)
    HDF5->>Dataset: Real robot data<br/>RGB + 3D actions
    Dataset->>Dataset: Extract scenarios<br/>(1box/2box + left/right)
    
    loop Training Loop
        Dataset->>Collate: PIL images + actions
        Collate->>Collate: PIL -> normalized tensors<br/>(224x224, ImageNet norm)
        
        Collate->>Trainer: Batched data
        Trainer->>Trainer: Window/chunk processing<br/>(8 frames -> 2 actions)
        
        Trainer->>Kosmos: 5D -> 4D conversion<br/>Last frame extraction
        Kosmos->>Kosmos: Vision model only<br/>(Skip text processing)
        Kosmos->>ActionHead: Pooled vision features
        
        ActionHead->>ActionHead: 3D regression<br/>[linear_x, linear_y, angular_z]
        ActionHead->>Trainer: Continuous predictions
        
        Trainer->>Trainer: Huber loss (regression)
        Trainer->>Kosmos: Backpropagation
    end
    
    Note over User, Trainer: âœ… ì¥ì : ë°ì´í„° íš¨ìœ¨ì , ì‹¤ì‹œê°„, ì •ë°€ ì œì–´
    Note over User, Trainer: ğŸ¯ íŠ¹í™”: Mobile Robot, ì—°ì† ì œì–´, ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
```
"""

def generate_comparison_sequence_diagram():
    """RoboVLMs vs Mobile VLA ë¹„êµ ë‹¤ì´ì–´ê·¸ë¨"""
    
    return """
RoboVLMs vs Mobile VLA ì•„í‚¤í…ì²˜ ë¹„êµ
```mermaid
graph TB
    subgraph "ğŸ”µ RoboVLMs (Original)"
        A1[ğŸ“Š RLDS/TFRecord<br/>ìˆ˜ë°±ë§Œ ë°ëª¨] --> B1[ğŸ”„ Standard DataLoader]
        B1 --> C1[ğŸ§  Multi-modal Backbone<br/>RT-1/OpenVLA]
        C1 --> D1[ğŸ¯ Discrete Action Head<br/>7-DOF Classification]
        D1 --> E1[ğŸ“ˆ CrossEntropy Loss]
        
        F1[ğŸ“ Text Instructions] --> C1
        G1[ğŸ–¼ï¸ RGB Images] --> C1
    end
    
    subgraph "ğŸŸ¢ Mobile VLA (Specialized)"
        A2[ğŸ“ HDF5 Files<br/>72 episodes] --> B2[ğŸ”„ Custom Collate<br/>PIL->Tensor]
        B2 --> C2[ğŸŒŒ Kosmos-2B Vision<br/>Image-only Processing]
        C2 --> D2[ğŸ¯ Continuous Action Head<br/>3D Regression]
        D2 --> E2[ğŸ“ˆ Huber Loss]
        
        F2[ğŸ“ Scenario Descriptions<br/>Obstacle Avoidance] --> C2
        G2[ğŸ–¼ï¸ Real Robot RGB] --> C2
        
        H2[ğŸ“Š Scenario Analysis<br/>1box/2box, left/right] --> I2[ğŸ“ˆ Detailed Metrics<br/>MAE, RÂ², Per-dimension]
    end
    
    subgraph "ğŸ”„ ê³µí†µ ìš”ì†Œ (Inherited)"
        J[ğŸ“¦ Window/Chunk Mechanism<br/>8 frames -> 2 actions]
        K[ğŸ‹ï¸ BaseTrainer Pattern<br/>Training Loop Structure]
        L[ğŸ“Š Dataset Interface<br/>__getitem__ format]
    end
    
    C1 -.-> J
    C2 -.-> J
    E1 -.-> K
    E2 -.-> K
    B1 -.-> L
    B2 -.-> L
    
    style A1 fill:#e1f5fe
    style A2 fill:#e8f5e8
    style C1 fill:#e1f5fe
    style C2 fill:#e8f5e8
    style J fill:#fff3e0
    style K fill:#fff3e0
    style L fill:#fff3e0
```
"""

def generate_data_processing_comparison():
    """ë°ì´í„° ì²˜ë¦¬ ê³¼ì • ë¹„êµ"""
    
    return """
ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¹„êµ
```mermaid
sequenceDiagram
    participant Raw as ğŸ“ Raw Data
    participant Process as ğŸ”„ Processing
    participant Model as ğŸ§  Model Input
    participant Output as ğŸ“Š Output
    
    Note over Raw, Output: ğŸ”µ RoboVLMs Data Pipeline
    
    Raw->>Process: RLDS/TFRecord<br/>Millions of demos
    Process->>Process: Standard preprocessing<br/>Resize, normalize
    Process->>Model: Text + Images<br/>Multi-modal input
    Model->>Output: Discrete actions<br/>Classification logits
    
    Note over Raw, Output: ğŸŸ¢ Mobile VLA Data Pipeline
    
    Raw->>Process: HDF5 files<br/>72 real robot episodes
    Process->>Process: Custom PIL handling<br/>Scenario extraction
    Process->>Model: Images only<br/>Last frame selection
    Model->>Output: Continuous actions<br/>3D regression values
    
    Note over Raw, Output: ğŸ“Š Key Differences
    Note right of Raw: Data Volume:<br/>Millions vs 72 episodes
    Note right of Process: Processing:<br/>Standard vs Custom
    Note right of Model: Input:<br/>Multi-modal vs Vision-only
    Note right of Output: Output:<br/>Discrete vs Continuous
```
"""

def generate_evaluation_comparison():
    """í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ"""
    
    return """
í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ
```mermaid
flowchart TD
    subgraph "ğŸ”µ RoboVLMs Evaluation"
        A1[ğŸ¯ Task Success Rate] --> B1[ğŸ“Š Binary Success/Fail]
        B1 --> C1[ğŸ“ˆ Overall Accuracy]
        C1 --> D1[ğŸ“‹ Simple Report]
    end
    
    subgraph "ğŸŸ¢ Mobile VLA Evaluation"
        A2[ğŸ¯ Action Prediction] --> B2[ğŸ“Š Continuous Metrics]
        B2 --> C2[ğŸ“ˆ Multi-dimensional Analysis]
        
        B2 --> E2[ğŸ“ MAE/RMSE/RÂ²]
        B2 --> F2[ğŸ² Threshold Accuracy]
        B2 --> G2[ğŸ“‹ Per-action Analysis]
        B2 --> H2[ğŸ—ºï¸ Scenario-wise Performance]
        
        C2 --> I2[ğŸ“Š Comprehensive Report]
        E2 --> I2
        F2 --> I2  
        G2 --> I2
        H2 --> I2
        
        I2 --> J2[ğŸ“ Professor Evaluation]
        I2 --> K2[ğŸ“ˆ Performance Grading]
        I2 --> L2[ğŸ”® Improvement Roadmap]
    end
    
    subgraph "ğŸ“Š Evaluation Depth"
        M[ğŸ”µ Surface Level<br/>Success/Fail only]
        N[ğŸŸ¢ Deep Analysis<br/>Multi-metric, Multi-scenario]
    end
    
    D1 -.-> M
    L2 -.-> N
    
    style A1 fill:#e1f5fe
    style A2 fill:#e8f5e8
    style M fill:#ffebee
    style N fill:#e8f5e8
```
"""

def generate_all_sequence_diagrams():
    """ëª¨ë“  ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    content = f"""# ğŸ”„ RoboVLMs vs Mobile VLA ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ë¹„êµ

**ìƒì„± ì¼ì‹œ:** {timestamp}

{generate_robovlms_sequence_diagram()}

{generate_mobile_vla_sequence_diagram()}

{generate_comparison_sequence_diagram()}

{generate_data_processing_comparison()}

{generate_evaluation_comparison()}

## ğŸ“Š í•µì‹¬ ì°¨ì´ì  ìš”ì•½

### ğŸ”µ RoboVLMs íŠ¹ì§•
- **ë°ì´í„°**: RLDS/TFRecord í˜•ì‹, ìˆ˜ë°±ë§Œ ë°ëª¨
- **ëª¨ë¸**: Multi-modal backbone (vision + text)
- **ì•¡ì…˜**: Discrete classification (7-DOF)
- **í‰ê°€**: ë‹¨ìˆœ ì„±ê³µë¥ 
- **ì¥ì **: ë²”ìš©ì„±, í™•ì¥ì„±
- **ë‹¨ì **: ë³µì¡ì„±, ë§ì€ ë°ì´í„° í•„ìš”

### ğŸŸ¢ Mobile VLA íŠ¹ì§•  
- **ë°ì´í„°**: HDF5 í˜•ì‹, 72ê°œ ì‹¤ì œ ë¡œë´‡ ì—í”¼ì†Œë“œ
- **ëª¨ë¸**: Kosmos-2B vision-only processing
- **ì•¡ì…˜**: Continuous regression (3D mobile)
- **í‰ê°€**: ë‹¤ì°¨ì› ìƒì„¸ ë¶„ì„
- **ì¥ì **: ë°ì´í„° íš¨ìœ¨ì , ì‹¤ì‹œê°„, ì •ë°€ ì œì–´
- **íŠ¹í™”**: Mobile robot, ì¥ì• ë¬¼ íšŒí”¼ ì‹œë‚˜ë¦¬ì˜¤

### ğŸ”„ ê³µí†µ ìœ ì§€ ìš”ì†Œ
- **Window/Chunk**: 8í”„ë ˆì„ ê´€ì°° â†’ 2í”„ë ˆì„ ì˜ˆì¸¡
- **BaseTrainer**: í•™ìŠµ ë£¨í”„ êµ¬ì¡°
- **Dataset Interface**: ë°ì´í„° ë¡œë”© ì¸í„°í˜ì´ìŠ¤

### ğŸš€ Mobile VLA í˜ì‹ ì 
- **ì»¤ìŠ¤í…€ Collate**: PIL ì´ë¯¸ì§€ ì²˜ë¦¬
- **5Dâ†’4D ë³€í™˜**: Kosmos íŠ¹í™” ì²˜ë¦¬  
- **Huber Loss**: ì—°ì† ì•¡ì…˜ íšŒê·€
- **ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„**: ì¥ì• ë¬¼ íšŒí”¼ ì„±ëŠ¥ ë¶„ì„
- **ì¢…í•© í‰ê°€**: MAE, RÂ², ì„ê³„ê°’ ì •í™•ë„

---
*Sequence Diagrams Analysis - {timestamp}*
"""

    filename = f'RoboVLMs_Mobile_VLA_Sequence_Diagrams_{timestamp}.md'
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return filename

def create_mermaid_diagrams():
    """Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ë° í‘œì‹œ"""
    
    print("ğŸ”„ RoboVLMs vs Mobile VLA ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±")
    print("=" * 60)
    
    diagrams = {
        "RoboVLMs ì›ë³¸": generate_robovlms_sequence_diagram(),
        "Mobile VLA íŠ¹í™”": generate_mobile_vla_sequence_diagram(), 
        "ì•„í‚¤í…ì²˜ ë¹„êµ": generate_comparison_sequence_diagram(),
        "ë°ì´í„° ì²˜ë¦¬ ë¹„êµ": generate_data_processing_comparison(),
        "í‰ê°€ ì‹œìŠ¤í…œ ë¹„êµ": generate_evaluation_comparison()
    }
    
    for title, diagram in diagrams.items():
        print(f"\nğŸ“Š {title}")
        print("-" * 50)
        print(diagram)
    
    # í†µí•© íŒŒì¼ ìƒì„±
    filename = generate_all_sequence_diagrams()
    print(f"\nğŸ“„ í†µí•© ë‹¤ì´ì–´ê·¸ë¨ íŒŒì¼ ìƒì„±: {filename}")
    
    return filename

def main():
    """ë©”ì¸ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤í–‰"""
    
    print("ğŸ”„ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ê¸°ë°˜ ë¹„êµ ë¶„ì„ ì‹œì‘!")
    
    # Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„±
    diagram_file = create_mermaid_diagrams()
    
    print(f"\nğŸ‰ ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“‹ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
    print(f"   ğŸ”µ RoboVLMs: ë²”ìš©ì , ë³µì¡í•œ multi-modal ì²˜ë¦¬")
    print(f"   ğŸŸ¢ Mobile VLA: íŠ¹í™”ëœ, íš¨ìœ¨ì ì¸ vision-only ì²˜ë¦¬")
    print(f"   ğŸ”„ ê³µí†µì : Window/Chunk ë©”ì»¤ë‹ˆì¦˜ ì™„ì „ ìœ ì§€")
    print(f"   ğŸš€ í˜ì‹ ì : ë°ì´í„° íš¨ìœ¨ì„± + ì‹¤ì‹œê°„ ì„±ëŠ¥")
    
    return diagram_file

if __name__ == "__main__":
    main()
