"""
ğŸš€ Fixed RoboVLMs Style Training Script
ì™„ì „íˆ ìˆ˜ì •ëœ RoboVLMs ìŠ¤íƒ€ì¼ ëª¨ë¸ë¡œ í›ˆë ¨
ëª¨ë“  Claw Matrix, Hierarchical Planning, Advanced Attention ê¸°ëŠ¥ í¬í•¨
ì›ë³¸ 72ê°œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from tqdm import tqdm
import json

from fixed_robovlms_model import (
    FixedRoboVLMStyleSingleImageModel,
    train_fixed_robovlms_model
)

class FixedRoboVLMStyleDataset(Dataset):
    """ìˆ˜ì •ëœ RoboVLMs ìŠ¤íƒ€ì¼ ë°ì´í„°ì…‹ (ë‹¨ì¼ ì´ë¯¸ì§€ â†’ ë‹¨ì¼ ì•¡ì…˜)"""
    
    def __init__(self, data_path, processor, split='train'):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ"""
        if os.path.isdir(self.data_path):
            # í´ë” ê¸°ë°˜ ë°ì´í„°
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            # ë‹¨ì¼ íŒŒì¼
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    # H5 íŒŒì¼ ë‚´ë¶€ êµ¬ì¡° í™•ì¸
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ì²« í”„ë ˆì„ë§Œ ì‚¬ìš© (ë‹¨ì¼ ì´ë¯¸ì§€)
                        single_image = images[0]  # [H, W, 3] - ì²« í”„ë ˆì„ë§Œ
                        single_action = actions[0]  # [3] - ì²« í”„ë ˆì„ ì•¡ì…˜ë§Œ
                        
                        self.episodes.append({
                            'image': single_image,  # [H, W, 3] - ë‹¨ì¼ ì´ë¯¸ì§€
                            'action': single_action,  # [3] - ë‹¨ì¼ ì•¡ì…˜
                            'episode_id': f"{h5_file.stem}"
                        })
                    else:
                        print(f"âš ï¸ {h5_file}ì— images/actions í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if image.max() > 1:
            image = image.astype(np.float32) / 255.0
        
        # ì•¡ì…˜: [3]
        action = episode['action']  # [3]
        
        return {
            'image': image,  # [3, H, W]
            'action': action,  # [3]
            'episode_id': episode['episode_id']
        }

def create_data_loaders(data_path, processor, batch_size=4, train_split=0.8):
    """ë°ì´í„° ë¡œë” ìƒì„± (ë°°ì¹˜ í¬ê¸° ì¤„ì„)"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = FixedRoboVLMStyleDataset(data_path, processor, 'full')
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„± (ë°°ì¹˜ í¬ê¸° ì¤„ì„)
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,  # ì¤„ì„
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=1,  # ì¤„ì„
        pin_memory=True
    )
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    
    return train_loader, val_loader

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = {
        'data_path': '../../ROS_action/mobile_vla_dataset',  # ì›ë³¸ 72ê°œ ë°ì´í„°ì…‹
        'batch_size': 4,  # ë°°ì¹˜ í¬ê¸° ì¤„ì„ (ë©”ëª¨ë¦¬ ì ˆì•½)
        'num_epochs': 15,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'dropout': 0.2,
        'z_axis_weight': 0.05,
        'use_claw_matrix': True,  # í™œì„±í™”
        'use_hierarchical': True,  # í™œì„±í™”
        'use_advanced_attention': True,  # í™œì„±í™”
        'early_stopping_patience': 5,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print("ğŸš€ Fixed RoboVLMs Style Training ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {json.dumps(config, indent=2)}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader = create_data_loaders(
        config['data_path'],
        processor,
        batch_size=config['batch_size']
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = FixedRoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=config['dropout'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention'],
        z_axis_weight=config['z_axis_weight']
    )
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ¯ í™œì„±í™”ëœ ê³ ê¸‰ ê¸°ëŠ¥:")
    print(f"   - Fixed Claw Matrix: {model.use_claw_matrix}")
    print(f"   - Fixed Hierarchical Planning: {model.use_hierarchical}")
    print(f"   - Fixed Advanced Attention: {model.use_advanced_attention}")
    
    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    try:
        trained_model = train_fixed_robovlms_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=config['num_epochs'],
            learning_rate=config['learning_rate'],
            weight_decay=config['weight_decay'],
            early_stopping_patience=config['early_stopping_patience'],
            device=config['device']
        )
        
        print("âœ… Fixed RoboVLMs Style Training ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_type': 'Fixed_RoboVLMs_Style_Single_Image',
        'input_type': 'single_image',
        'output_type': 'single_action',
        'data_size': len(train_loader.dataset) + len(val_loader.dataset),
        'config': config,
        'advanced_features': {
            'fixed_claw_matrix': config['use_claw_matrix'],
            'fixed_hierarchical_planning': config['use_hierarchical'],
            'fixed_advanced_attention': config['use_advanced_attention']
        },
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'training_status': 'completed'
    }
    
    with open('fixed_robovlms_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: fixed_robovlms_training_results.json")
    
    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if os.path.exists('fixed_robovlms_model_best.pth'):
        checkpoint = torch.load('fixed_robovlms_model_best.pth', map_location='cpu')
        print(f"ğŸ“Š ìµœê³  ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - ì—í¬í¬: {checkpoint['epoch']}")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {checkpoint['train_loss']:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.4f}")

if __name__ == "__main__":
    main()
