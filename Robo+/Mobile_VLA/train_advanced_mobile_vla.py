#!/usr/bin/env python3
"""
ğŸš€ Advanced Mobile VLA Model Training Script
Claw Matrix + Hierarchical Planning + Advanced Attention í›ˆë ¨
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import h5py
import numpy as np
import json
import os
from pathlib import Path
import sys
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

# RoboVLMs ëª¨ë“ˆ ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "robovlms" / "models"))
from advanced_mobile_vla_model import AdvancedMobileVLAModel, test_advanced_model

class MobileVLADataset(Dataset):
    """Mobile VLA ë°ì´í„°ì…‹ ë¡œë”"""
    def __init__(self, data_path, max_episodes=None):
        self.data_path = data_path
        self.episodes = []
        
        # H5 íŒŒì¼ë“¤ ì°¾ê¸°
        h5_files = list(Path(data_path).glob("*.h5"))
        if max_episodes:
            h5_files = h5_files[:max_episodes]
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ë¡œë”©: {len(h5_files)}ê°œ ì—í”¼ì†Œë“œ")
        
        for h5_file in tqdm(h5_files, desc="ë°ì´í„° ë¡œë”©"):
            try:
                with h5py.File(h5_file, 'r') as f:
                    # ë°ì´í„° êµ¬ì¡° í™•ì¸ - imagesì™€ actions ì‚¬ìš©
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]
                        actions = f['actions'][:]
                        
                        # ë°ì´í„° ê²€ì¦
                        if len(images) > 0 and len(actions) > 0:
                            self.episodes.append({
                                'images': images,
                                'actions': actions,
                                'file': str(h5_file)
                            })
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {h5_file}: {e}")
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ: {len(self.episodes)}ê°œ ìœ íš¨í•œ ì—í”¼ì†Œë“œ")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€ì™€ ì•¡ì…˜ ë°ì´í„° ì¶”ì¶œ
        images = episode['images']
        actions = episode['actions']
        
        # ë°ì´í„° í˜•íƒœ ì •ê·œí™”
        if len(images.shape) == 3:
            images = images.reshape(1, *images.shape)
        
        # ì•¡ì…˜ ë°ì´í„° í˜•íƒœ í™•ì¸ ë° ì¡°ì •
        if len(actions.shape) == 1:
            actions = actions.reshape(-1, 1)
        
        return {
            'images': torch.FloatTensor(images),
            'actions': torch.FloatTensor(actions),
            'episode_id': idx
        }

def collate_fn(batch):
    """ë°°ì¹˜ ë°ì´í„° ì •ë¦¬"""
    images = [item['images'] for item in batch]
    actions = [item['actions'] for item in batch]
    episode_ids = [item['episode_id'] for item in batch]
    
    # íŒ¨ë”©ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ë§ì¶”ê¸°
    max_images = max(f.shape[0] for f in images)
    max_actions = max(a.shape[0] for a in actions)
    
    padded_images = []
    padded_actions = []
    
    for image, action in zip(images, actions):
        # ì´ë¯¸ì§€ íŒ¨ë”©
        if image.shape[0] < max_images:
            pad_size = max_images - image.shape[0]
            padded_image = torch.cat([image, torch.zeros(pad_size, *image.shape[1:])], dim=0)
        else:
            padded_image = image[:max_images]
        padded_images.append(padded_image)
        
        # ì•¡ì…˜ íŒ¨ë”©
        if action.shape[0] < max_actions:
            pad_size = max_actions - action.shape[0]
            padded_action = torch.cat([action, torch.zeros(pad_size, *action.shape[1:])], dim=0)
        else:
            padded_action = action[:max_actions]
        padded_actions.append(padded_action)
    
    return {
        'images': torch.stack(padded_images),
        'actions': torch.stack(padded_actions),
        'episode_ids': episode_ids
    }

def train_advanced_mobile_vla():
    """ê³ ê¸‰ Mobile VLA ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ Advanced Mobile VLA Model í›ˆë ¨ ì‹œì‘")
    
    # ì„¤ì •
    config = {
        'data_path': '../../ROS_action/mobile_vla_dataset',
        'batch_size': 1,  # ì´ì „ ì½”ë“œì™€ ë™ì¼
        'learning_rate': 1e-4,
        'num_epochs': 10,
        'save_interval': 2,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'max_episodes': None,  # ì œí•œ ì—†ìŒ - ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
        'vision_dim': 768,
        'language_dim': 768,
        'action_dim': 3,
        'fusion_dim': 512,
        'plan_dim': 256,
        'num_claw_layers': 3,
        'num_subgoals': 6,
        'frames_per_subgoal': 3,
        'use_claw_matrix': True,
        'use_hierarchical': True,
        'use_advanced_attention': True
    }
    
    print(f"ğŸ”§ ì„¤ì •: {config}")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {config['device']}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
    dataset = MobileVLADataset(config['data_path'], config['max_episodes'])
    
    if len(dataset) == 0:
        print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    dataloader = DataLoader(
        dataset, 
        batch_size=config['batch_size'],
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ—ï¸ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    from transformers import AutoProcessor, AutoModel
    
    # Kosmos-2 ëª¨ë¸ ë¡œë“œ
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    vision_model = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ê³ ê¸‰ ëª¨ë¸ ìƒì„±
    model = AdvancedMobileVLAModel(
        processor=processor,
        vision_dim=config['vision_dim'],
        language_dim=config['language_dim'],
        action_dim=config['action_dim'],
        fusion_dim=config['fusion_dim'],
        plan_dim=config['plan_dim'],
        num_claw_layers=config['num_claw_layers'],
        num_subgoals=config['num_subgoals'],
        frames_per_subgoal=config['frames_per_subgoal'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention']
    )
    
    model = model.to(config['device'])
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses = []
    val_losses = []
    
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    
    for epoch in range(config['num_epochs']):
        model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config['num_epochs']}")
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                images = batch['images'].to(config['device'])
                actions = batch['actions'].to(config['device'])
                
                # ëª¨ë¸ ìˆœì „íŒŒ
                optimizer.zero_grad()
                
                # ê±°ë¦¬ ë¼ë²¨ ìƒì„± (ë”ë¯¸ ë°ì´í„°)
                batch_size = images.shape[0]
                distance_labels = torch.randint(0, 3, (batch_size,), device=config['device'])
                
                # ëª¨ë¸ í˜¸ì¶œ (distance_labels í¬í•¨)
                predicted_actions = model(
                    images=images,
                    distance_labels=distance_labels
                )
                
                # ì†ì‹¤ ê³„ì‚° (ì•¡ì…˜ í˜•íƒœ ë§ì¶”ê¸°)
                target_actions = actions[:, :predicted_actions.shape[1], :predicted_actions.shape[2]]
                
                loss = criterion(predicted_actions, target_actions)
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Avg Loss': f'{epoch_loss/num_batches:.4f}'
                })
                
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì—í¬í¬ ì™„ë£Œ
        avg_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')
        train_losses.append(avg_loss)
        
        print(f"ğŸ“Š Epoch {epoch+1} ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ëª¨ë¸ ì €ì¥
        if (epoch + 1) % config['save_interval'] == 0:
            save_path = f"advanced_mobile_vla_epoch_{epoch+1}.pth"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'config': config
            }, save_path)
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {save_path}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = "advanced_mobile_vla_final.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': config,
        'train_losses': train_losses
    }, final_save_path)
    
    # í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    plt.plot(train_losses, label='Training Loss', color='blue')
    plt.title('Advanced Mobile VLA Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('advanced_mobile_vla_training_loss.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'final_loss': train_losses[-1] if train_losses else float('inf'),
        'best_loss': min(train_losses) if train_losses else float('inf'),
        'epochs_trained': len(train_losses),
        'config': config,
        'timestamp': datetime.now().isoformat()
    }
    
    with open('advanced_mobile_vla_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("âœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ìµœì¢… ì†ì‹¤: {results['final_loss']:.4f}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {results['best_loss']:.4f}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {final_save_path}")
    print(f"ğŸ“Š ê²°ê³¼ ì €ì¥: advanced_mobile_vla_training_results.json")

if __name__ == "__main__":
    train_advanced_mobile_vla()
