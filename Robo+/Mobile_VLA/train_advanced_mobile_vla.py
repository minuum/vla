#!/usr/bin/env python3
"""
ğŸš€ Advanced Mobile VLA Model Training Script
Claw Matrix + Hierarchical Planning + Advanced Attention í›ˆë ¨
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import h5py
import numpy as np
import json
import os
import random
from pathlib import Path
import sys
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

# RoboVLMs ëª¨ë“ˆ ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "robovlms" / "models"))
from advanced_mobile_vla_model import AdvancedMobileVLAModel, test_advanced_model

class RoboticsDataAugmentation:
    """ë¡œë´‡ ë°ì´í„° ì¦ê°• í´ë˜ìŠ¤"""
    def __init__(self):
        self.action_noise_std = 0.005
        self.image_noise_std = 0.01
        
    def augment_episode(self, episode, augment_prob=0.8):
        """ì—í”¼ì†Œë“œ ì¦ê°•"""
        images = episode['images']
        actions = episode['actions'].clone() if hasattr(episode['actions'], 'clone') else episode['actions'].copy()
        
        # ì•¡ì…˜ ë…¸ì´ì¦ˆ ì¶”ê°€
        if random.random() < augment_prob:
            noise = np.random.normal(0, self.action_noise_std, actions.shape)
            actions = actions + noise
            actions = np.clip(actions, -1.15, 1.15)
        
        return {
            'images': images,
            'actions': actions,
            'episode_id': episode['episode_id']
        }

class MobileVLADataset(Dataset):
    """Mobile VLA ë°ì´í„°ì…‹ ë¡œë” - ì—¬ëŸ¬ ë°ì´í„°ì…‹ í†µí•©"""
    def __init__(self, data_paths, max_episodes=None):
        self.data_paths = data_paths if isinstance(data_paths, list) else [data_paths]
        self.episodes = []
        
        print(f"ğŸ“ ë°ì´í„°ì…‹ ë¡œë”©: {len(self.data_paths)}ê°œ ê²½ë¡œ")
        
        for data_path in self.data_paths:
            print(f"   ğŸ“‚ {data_path} ì²˜ë¦¬ ì¤‘...")
            
            # H5 íŒŒì¼ë“¤ ì°¾ê¸°
            h5_files = list(Path(data_path).glob("*.h5"))
            
            # í´ë” êµ¬ì¡°ì˜ ì—í”¼ì†Œë“œë“¤ ì°¾ê¸°
            episode_dirs = [d for d in Path(data_path).iterdir() if d.is_dir() and d.name.startswith('episode_')]
            
            print(f"      ë°œê²¬ëœ H5 íŒŒì¼: {len(h5_files)}ê°œ")
            print(f"      ë°œê²¬ëœ ì—í”¼ì†Œë“œ í´ë”: {len(episode_dirs)}ê°œ")
            
            # H5 íŒŒì¼ ì²˜ë¦¬
            for h5_file in tqdm(h5_files, desc=f"ë¡œë”© H5 {Path(data_path).name}"):
                try:
                    with h5py.File(h5_file, 'r') as f:
                        # ë°ì´í„° êµ¬ì¡° í™•ì¸ - imagesì™€ actions ì‚¬ìš©
                        if 'images' in f and 'actions' in f:
                            images = f['images'][:]
                            actions = f['actions'][:]
                            
                            # ë°ì´í„° ê²€ì¦
                            if images.shape[0] > 0 and actions.shape[0] > 0:
                                self.episodes.append({
                                    'images': images,
                                    'actions': actions,
                                    'episode_id': len(self.episodes)
                                })
                        else:
                            print(f"âš ï¸ {h5_file.name}: images ë˜ëŠ” actions í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                            
                except Exception as e:
                    print(f"âŒ {h5_file.name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            # í´ë” êµ¬ì¡° ì²˜ë¦¬
            for episode_dir in episode_dirs:
                try:
                    # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
                    image_files = sorted([f for f in episode_dir.glob("*.jpg")])
                    if not image_files:
                        continue
                    
                    # ì•¡ì…˜ íŒŒì¼ ë¡œë“œ
                    actions_file = episode_dir / "actions.npy"
                    if not actions_file.exists():
                        continue
                    
                    actions = np.load(actions_file)
                    
                    # ì´ë¯¸ì§€ë“¤ì„ numpy ë°°ì—´ë¡œ ë¡œë“œ
                    images = []
                    for img_file in image_files:
                        try:
                            from PIL import Image
                            img = Image.open(img_file).convert('RGB')
                            img_array = np.array(img)
                            images.append(img_array)
                        except Exception as e:
                            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜ {img_file}: {e}")
                            continue
                    
                    if len(images) == 0:
                        continue
                    
                    images = np.array(images)  # [num_frames, height, width, 3]
                    
                    # ë°ì´í„° ê²€ì¦
                    if images.shape[0] > 0 and actions.shape[0] > 0:
                        self.episodes.append({
                            'images': images,
                            'actions': actions,
                            'episode_id': episode_dir.name
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ ì—í”¼ì†Œë“œ í´ë” ì²˜ë¦¬ ì˜¤ë¥˜ {episode_dir}: {e}")
                    continue
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ: {len(self.episodes)}ê°œ ìœ íš¨í•œ ì—í”¼ì†Œë“œ")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        return {
            'images': torch.tensor(episode['images'], dtype=torch.float32),
            'actions': torch.tensor(episode['actions'], dtype=torch.float32),
            'episode_id': idx  # episode_id ëŒ€ì‹  idx ì‚¬ìš©
        }

def collate_fn(batch):
    """ë°°ì¹˜ ë°ì´í„° ì •ë¦¬"""
    images = [item['images'] for item in batch]
    actions = [item['actions'] for item in batch]
    episode_ids = [item['episode_id'] for item in batch]
    
    # íŒ¨ë”©ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ë§ì¶”ê¸°
    max_images = max(f.shape[0] for f in images)
    max_actions = max(a.shape[0] for a in actions)
    
    padded_images = []
    padded_actions = []
    
    for image, action in zip(images, actions):
        # ì´ë¯¸ì§€ íŒ¨ë”©
        if image.shape[0] < max_images:
            pad_size = max_images - image.shape[0]
            padded_image = torch.cat([image, torch.zeros(pad_size, *image.shape[1:])], dim=0)
        else:
            padded_image = image[:max_images]
        padded_images.append(padded_image)
        
        # ì•¡ì…˜ íŒ¨ë”©
        if action.shape[0] < max_actions:
            pad_size = max_actions - action.shape[0]
            padded_action = torch.cat([action, torch.zeros(pad_size, *action.shape[1:])], dim=0)
        else:
            padded_action = action[:max_actions]
        padded_actions.append(padded_action)
    
    return {
        'images': torch.stack(padded_images),
        'actions': torch.stack(padded_actions),
        'episode_ids': episode_ids
    }

def train_advanced_mobile_vla():
    """ê³ ê¸‰ Mobile VLA ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ Advanced Mobile VLA Model í›ˆë ¨ ì‹œì‘")
    
    # ì„¤ì •
    config = {
        'data_paths': [
            '../../ROS_action/mobile_vla_dataset',  # ì›ë³¸ ë°ì´í„°
            'augmented_dataset',  # ì¦ê°•ëœ ë°ì´í„° (721ê°œ)
            'distance_aware_augmented_dataset'  # ê±°ë¦¬ ì¸ì‹ ì¦ê°• ë°ì´í„° (481ê°œ)
        ],
        'batch_size': 1,  # ì´ì „ ì½”ë“œì™€ ë™ì¼
        'learning_rate': 1e-4,
        'num_epochs': 20,  # kosmos2_optimizedì™€ ë™ì¼
        'save_interval': 2,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'max_episodes': None,  # ì œí•œ ì—†ìŒ - ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
        'vision_dim': 768,
        'language_dim': 768,
        'action_dim': 3,
        'fusion_dim': 512,
        'plan_dim': 256,
        'num_claw_layers': 3,
        'num_subgoals': 6,
        'frames_per_subgoal': 3,
        'use_claw_matrix': True,
        'use_hierarchical': True,
        'use_advanced_attention': True
    }
    
    print(f"ï¿½ï¿½ ì„¤ì •: {config}")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {config['device']}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
    dataset = MobileVLADataset(config['data_paths'], config['max_episodes'])
    
    if len(dataset) == 0:
        print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    print(f"ğŸ“ˆ ì´ {len(dataset)}ê°œ ì—í”¼ì†Œë“œ ë¡œë“œ ì™„ë£Œ")
    print(f"   - ì›ë³¸: 72ê°œ")
    print(f"   - ì¦ê°•: 721ê°œ") 
    print(f"   - ê±°ë¦¬ì¸ì‹ ì¦ê°•: 481ê°œ")
    print(f"   - ì´í•©: {len(dataset)}ê°œ")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)
    
    # ê²€ì¦ ë°ì´í„°ì…‹ (ì „ì²´ì˜ 10%)
    val_size = max(1, int(len(dataset) * 0.1))
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– Advanced Mobile VLA ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    
    # Kosmos2 processor ë¡œë“œ
    from transformers import AutoProcessor
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    model = AdvancedMobileVLAModel(
        processor=processor,
        vision_dim=config['vision_dim'],
        language_dim=config['language_dim'],
        action_dim=config['action_dim'],
        fusion_dim=config['fusion_dim'],
        plan_dim=config['plan_dim'],
        num_claw_layers=config['num_claw_layers'],
        num_subgoals=config['num_subgoals'],
        frames_per_subgoal=config['frames_per_subgoal'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention']
    ).to(config['device']).float()  # float32ë¡œ ëª…ì‹œì  ì„¤ì •
    
    # ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    
    for epoch in range(config['num_epochs']):
        model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['num_epochs']}")
        
        for batch_idx, batch in enumerate(progress_bar):
            try:
                images = batch['images'].to(config['device']).float()
                actions = batch['actions'].to(config['device']).float()
                
                # ëª¨ë¸ ìˆœì „íŒŒ
                optimizer.zero_grad()
                
                # ê±°ë¦¬ ë¼ë²¨ ìƒì„± (Long íƒ€ì…ìœ¼ë¡œ ì„¤ì •)
                batch_size = images.shape[0]
                distance_labels = torch.randint(0, 3, (batch_size,), device=config['device']).long()
                
                # ëª¨ë¸ í˜¸ì¶œ (distance_labels í¬í•¨)
                predicted_actions = model(
                    images=images,
                    distance_labels=distance_labels
                )
                
                # ì†ì‹¤ ê³„ì‚° (ì•¡ì…˜ í˜•íƒœ ë§ì¶”ê¸°)
                target_actions = actions[:, :predicted_actions.shape[1], :predicted_actions.shape[2]]
                loss = criterion(predicted_actions, target_actions)
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress_bar.set_postfix({'Loss': f'{loss.item():.3f}'})
                
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue
        
        # ì—í¬í¬ ì™„ë£Œ
        avg_train_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')
        train_losses.append(avg_train_loss)
        
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    images = batch['images'].to(config['device']).float()
                    actions = batch['actions'].to(config['device']).float()
                    
                    # ê±°ë¦¬ ë¼ë²¨ ìƒì„± (Long íƒ€ì…ìœ¼ë¡œ ì„¤ì •)
                    batch_size = images.shape[0]
                    distance_labels = torch.randint(0, 3, (batch_size,), device=config['device']).long()
                    
                    # ëª¨ë¸ í˜¸ì¶œ
                    predicted_actions = model(
                        images=images,
                        distance_labels=distance_labels
                    )
                    
                    # ì†ì‹¤ ê³„ì‚°
                    target_actions = actions[:, :predicted_actions.shape[1], :predicted_actions.shape[2]]
                    loss = criterion(predicted_actions, target_actions)
                    
                    val_loss += loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    print(f"âš ï¸ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        val_losses.append(avg_val_loss)
        
        print(f"ğŸ“Š Epoch {epoch+1} ì™„ë£Œ - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}, ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        
        # ìµœê³  ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_path = f"advanced_mobile_vla_best.pth"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': config
            }, best_model_path)
            print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥: {best_model_path}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ëª¨ë¸ ì €ì¥
        if (epoch + 1) % config['save_interval'] == 0:
            save_path = f"advanced_mobile_vla_epoch_{epoch+1}.pth"
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': config
            }, save_path)
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {save_path}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = "advanced_mobile_vla_final.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': config,
        'train_losses': train_losses
    }, final_save_path)
    
    # í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    plt.plot(train_losses, label='Training Loss', color='blue')
    plt.plot(val_losses, label='Validation Loss', color='red')
    plt.title('Advanced Mobile VLA Training & Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('advanced_mobile_vla_training_loss.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'final_train_loss': train_losses[-1] if train_losses else float('inf'),
        'final_val_loss': val_losses[-1] if val_losses else float('inf'),
        'best_val_loss': best_val_loss,
        'epochs_trained': len(train_losses),
        'config': config,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'timestamp': datetime.now().isoformat()
    }
    
    with open('advanced_mobile_vla_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("âœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ìµœì¢… í›ˆë ¨ ì†ì‹¤: {results['final_train_loss']:.4f}")
    print(f"ğŸ¯ ìµœì¢… ê²€ì¦ ì†ì‹¤: {results['final_val_loss']:.4f}")
    print(f"ğŸ† ìµœê³  ê²€ì¦ ì†ì‹¤: {results['best_val_loss']:.4f}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {final_save_path}")
    print(f"ğŸ† ìµœê³  ëª¨ë¸ ì €ì¥: advanced_mobile_vla_best.pth")
    print(f"ğŸ“Š ê²°ê³¼ ì €ì¥: advanced_mobile_vla_training_results.json")

if __name__ == "__main__":
    train_advanced_mobile_vla()
