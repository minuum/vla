#!/usr/bin/env python3
"""
RoboVLMs Adapter for Mobile VLA - Mobile VLAë¥¼ RoboVLMsì™€ í˜¸í™˜ì‹œí‚¤ëŠ” ì–´ëŒ‘í„°
"""

import torch
import numpy as np
from typing import Dict, List, Any, Optional
from pathlib import Path

try:
    from .mobile_dataset import MobileVLADataset
except ImportError:
    # í…ŒìŠ¤íŠ¸ìš© ì ˆëŒ€ ì„í¬íŠ¸
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from mobile_dataset import MobileVLADataset


class MobileVLAToRoboVLMsAdapter:
    """
    Mobile VLA ë°ì´í„°ë¥¼ RoboVLMs ActionPredictionDataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì–´ëŒ‘í„°
    """
    
    def __init__(
        self,
        data_dir: str,
        sequence_length: int = 18,
        scenario_filter: Optional[List[str]] = None,
        image_processor=None  # Kosmos processor
    ):
        # Mobile VLA ë°ì´í„°ì…‹ ë¡œë“œ
        self.mobile_dataset = MobileVLADataset(
            data_dir=data_dir,
            sequence_length=sequence_length,
            scenario_filter=scenario_filter
        )
        
        self.image_processor = image_processor
        self.sequence_length = sequence_length
        
        # RoboVLMs ì‹œë‚˜ë¦¬ì˜¤ ëª…ë ¹ì–´ ë§¤í•‘ (ì˜ì–´ - ì¥ì• ë¬¼ íšŒí”¼ ì»µ ì¶”ì )
        self.scenario_instructions = {
            "1box_vert_left": "Navigate around the single box obstacle by going left to track the target cup",
            "1box_vert_right": "Navigate around the single box obstacle by going right to track the target cup",
            "1box_hori_left": "Navigate around the single box obstacle by going left to track the target cup",
            "1box_hori_right": "Navigate around the single box obstacle by going right to track the target cup",
            "2box_vert_left": "Navigate around the two box obstacles by going left to track the target cup",
            "2box_vert_right": "Navigate around the two box obstacles by going right to track the target cup",
            "2box_hori_left": "Navigate around the two box obstacles by going left to track the target cup",
            "2box_hori_right": "Navigate around the two box obstacles by going right to track the target cup"
        }
    
    def __len__(self) -> int:
        return len(self.mobile_dataset)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """RoboVLMs ActionPredictionDataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # Mobile VLA ë°ì´í„° ë¡œë“œ
        mobile_data = self.mobile_dataset[idx]
        
        # 1. ì´ë¯¸ì§€ ì²˜ë¦¬ (Kosmos processor ì‚¬ìš©)
        images = mobile_data["images"]  # [T, 3, 224, 224]
        
        if self.image_processor is not None:
            # Kosmos processor ì‚¬ìš©
            # PIL Imagesë¡œ ë³€í™˜
            from PIL import Image
            pil_images = []
            for t in range(images.shape[0]):
                # tensor [3, 224, 224] â†’ numpy [224, 224, 3] â†’ PIL
                img_np = images[t].permute(1, 2, 0).numpy()
                img_np = (img_np * 255).astype(np.uint8)  # denormalize
                pil_img = Image.fromarray(img_np)
                pil_images.append(pil_img)
            
            # Kosmos processorë¡œ ì²˜ë¦¬
            processed_images = []
            for pil_img in pil_images:
                processed = self.image_processor(pil_img, return_tensors="pt")['pixel_values']
                processed_images.append(processed.squeeze(0))  # [3, 224, 224]
            
            vision_x = torch.stack(processed_images)  # [T, 3, 224, 224]
        else:
            # ê¸°ë³¸ ì²˜ë¦¬
            vision_x = images
        
        # 2. ì•¡ì…˜ ê·¸ëŒ€ë¡œ ì‚¬ìš© (Mobile VLA 3D ìœ ì§€)
        mobile_actions = mobile_data["actions"]  # [T, 3] normalized - ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # 3. ì´ë²¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (Mobile VLA ì´ë²¤íŠ¸ ìœ ì§€)
        event_indices = mobile_data["action_events"]  # [T]
        
        # 4. ì‹œë‚˜ë¦¬ì˜¤ ëª…ë ¹ì–´
        scenario = mobile_data["scenario"]
        task_description = self.scenario_instructions.get(scenario, "Navigate to track the target cup")
        
        # Mobile VLA í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (RoboVLMs ë°©ì‹ë§Œ ì°¨ìš©)
        return {
            # ì´ë¯¸ì§€ ë°ì´í„°
            "vision_x": vision_x.unsqueeze(0),  # [1, T, 3, 224, 224] - batch ì°¨ì› ì¶”ê°€
            
            # íƒœìŠ¤í¬ ì„¤ëª…
            "task_description": task_description,
            
            # ë©”íƒ€ë°ì´í„°
            "scenario": scenario,
            "episode_name": mobile_data["episode_name"],
            "num_frames": mobile_data["num_frames"],
            
            # Mobile VLA ì›ë³¸ ë°ì´í„° (í•™ìŠµìš©)
            "mobile_actions": mobile_data["actions"].unsqueeze(0),       # [1, T, 3] - Mobile VLA ì•¡ì…˜
            "mobile_events": mobile_data["action_events"].unsqueeze(0)   # [1, T] - Mobile VLA ì´ë²¤íŠ¸
        }
    
    # ë³€í™˜ í•¨ìˆ˜ë“¤ì€ í•„ìš”ì—†ìŒ - Mobile VLA ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    
    def get_scenario_statistics(self) -> Dict[str, int]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í†µê³„ ë°˜í™˜"""
        return self.mobile_dataset.get_scenario_statistics()


def create_robovlms_compatible_dataloader(
    data_dir: str,
    batch_size: int = 1,
    sequence_length: int = 18,
    scenario_filter: Optional[List[str]] = None,
    image_processor=None,
    num_workers: int = 0
):
    """RoboVLMs í˜¸í™˜ DataLoader ìƒì„±"""
    from torch.utils.data import DataLoader
    
    adapter = MobileVLAToRoboVLMsAdapter(
        data_dir=data_dir,
        sequence_length=sequence_length,
        scenario_filter=scenario_filter,
        image_processor=image_processor
    )
    
    def collate_fn(batch):
        """ë°°ì¹˜ ì²˜ë¦¬"""
        # ë‹¨ì¼ ë°°ì¹˜ë¼ê³  ê°€ì • (RoboVLMsëŠ” ë³´í†µ batch_size=1)
        if len(batch) == 1:
            return batch[0]
        
        # ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ì²« ë²ˆì§¸ë§Œ ë°˜í™˜ (ê°„ë‹¨íˆ)
        return batch[0]
    
    return DataLoader(
        adapter,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn
    )


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª RoboVLMs Adapter í…ŒìŠ¤íŠ¸")
    
    # ì–´ëŒ‘í„° ì´ˆê¸°í™”
    adapter = MobileVLAToRoboVLMsAdapter(
        data_dir="/home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset/"
    )
    
    if len(adapter) > 0:
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        sample = adapter[0]
        
        print(f"ğŸ“Š Mobile VLA + Kosmos ë°©ì‹ ë°ì´í„°:")
        print(f"   Vision X: {sample['vision_x'].shape}")
        print(f"   Mobile Actions: {sample['mobile_actions'].shape}")
        print(f"   Mobile Events: {sample['mobile_events'].shape}")
        print(f"   Task: {sample['task_description']}")
        print(f"   Scenario: {sample['scenario']}")
        
        # Mobile VLA ì›ë³¸ ë°ì´í„° í™•ì¸
        print(f"\nğŸ¤– Mobile VLA ë°ì´í„°:")
        print(f"   Actions (3D): {sample['mobile_actions'][0, :3]}")  # ì²˜ìŒ 3í”„ë ˆì„
        print(f"   Events: {sample['mobile_events'][0, :10]}")        # ì²˜ìŒ 10í”„ë ˆì„
    
    print(f"\nâœ… RoboVLMs Adapter í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
