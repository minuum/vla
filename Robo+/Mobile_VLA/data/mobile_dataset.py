#!/usr/bin/env python3
"""
Mobile VLA Dataset - mobile_vla_data_collector.py ë°ì´í„° ì§ì ‘ ë¡œë”©
Calvin ì—†ì´ ìˆœìˆ˜ Mobile HDF5 í˜•ì‹ ì‚¬ìš©
"""

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MobileVLADataset(Dataset):
    """
    mobile_vla_data_collector.pyê°€ ìƒì„±í•œ HDF5 íŒŒì¼ì„ ì§ì ‘ ë¡œë”©í•˜ëŠ” ë°ì´í„°ì…‹
    
    ë°ì´í„° í˜•ì‹:
    - images: [T, 720, 1280, 3] - RGB ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
    - actions: [T, 3] - [linear_x, linear_y, angular_z] 
    - action_event_types: [T] - [episode_start, start_action, stop_action]
    - episode_name: str - "episode_20250808_123136_1box_vert_left"
    """
    
    def __init__(
        self, 
        data_dir: str = "/home/soda/vla/ROS_action/mobile_vla_dataset/",
        sequence_length: int = 18,
        image_size: Tuple[int, int] = (224, 224),  # VLM ì…ë ¥ìš© ë¦¬ì‚¬ì´ì¦ˆ
        normalize_actions: bool = True,
        scenario_filter: Optional[List[str]] = None
    ):
        self.data_dir = Path(data_dir)
        self.sequence_length = sequence_length
        self.image_size = image_size
        self.normalize_actions = normalize_actions
        
        # mobile_vla_data_collector.pyì˜ ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘
        self.scenario_instructions = {
            "1box_vert_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_vert_right": "ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "1box_hori_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ í”¼í•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "1box_hori_right": "ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í”¼í•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_left": "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì™¼ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_vert_right": "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì˜¤ë¥¸ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
            "2box_hori_left": "ë‘ ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”", 
            "2box_hori_right": "ë‘ ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”"
        }
        
        # mobile_vla_data_collector.pyì˜ ì•¡ì…˜ ë²”ìœ„ (WASD_TO_CONTINUOUS ê¸°ì¤€)
        self.action_bounds = {
            "linear_x": 2.0,   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ
            "linear_y": 2.0,   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ  
            "angular_z": 2.0   # ì‹¤ì œë¡œëŠ” Â±1.15 ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ìœ ìˆê²Œ
        }
        
        # ì´ë²¤íŠ¸ íƒ€ì… ë§¤í•‘
        self.event_mapping = {
            b'episode_start': 0,
            b'start_action': 1, 
            b'stop_action': 2
        }
        
        # HDF5 íŒŒì¼ ë¡œë“œ ë° í•„í„°ë§
        self.h5_files = self._load_h5_files(scenario_filter)
        self.scenarios = self._extract_scenarios()
        
        # ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥
        self._print_dataset_stats()
        
    def _load_h5_files(self, scenario_filter: Optional[List[str]]) -> List[Path]:
        """HDF5 íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  í•„í„°ë§"""
        all_h5_files = list(self.data_dir.glob("*.h5"))
        
        if scenario_filter:
            filtered_files = []
            for h5_file in all_h5_files:
                scenario = self._extract_scenario_from_filename(h5_file.name)
                if scenario in scenario_filter:
                    filtered_files.append(h5_file)
            return filtered_files
        
        return all_h5_files
    
    def _extract_scenario_from_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ (mobile_vla_data_collector.py ë°©ì‹)"""
        for scenario in self.scenario_instructions.keys():
            if scenario in filename:
                return scenario
        return "unknown"
    
    def _extract_scenarios(self) -> List[str]:
        """ëª¨ë“  íŒŒì¼ì˜ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ"""
        scenarios = []
        for h5_file in self.h5_files:
            scenario = self._extract_scenario_from_filename(h5_file.name)
            scenarios.append(scenario)
        return scenarios
    
    def _print_dataset_stats(self):
        """ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥"""
        scenario_counts = defaultdict(int)
        total_frames = 0
        
        for i, h5_file in enumerate(self.h5_files):
            scenario = self.scenarios[i]
            scenario_counts[scenario] += 1
            
            # í”„ë ˆì„ ìˆ˜ í™•ì¸
            try:
                with h5py.File(h5_file, 'r') as f:
                    num_frames = f.attrs.get('num_frames', 0)
                    total_frames += num_frames
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {h5_file.name}: {e}")
        
        logger.info(f"ğŸ“ Mobile VLA Dataset ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ {len(self.h5_files)}ê°œ ì—í”¼ì†Œë“œ, {total_frames}ê°œ í”„ë ˆì„")
        logger.info(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ ë¶„í¬: {dict(scenario_counts)}")
        
        # 18í”„ë ˆì„ ì—í”¼ì†Œë“œ íŠ¹ë³„ í‘œì‹œ
        frame_18_count = sum(1 for scenario in scenario_counts.keys() if scenario != "unknown")
        logger.info(f"ğŸ¯ 18í”„ë ˆì„ ì—í”¼ì†Œë“œ: {frame_18_count}ê°œ (í‘œì¤€ ê¸¸ì´)")
    
    def __len__(self) -> int:
        return len(self.h5_files)
    
    def __getitem__(self, idx: int) -> Dict:
        """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë°ì´í„° ë¡œë“œ"""
        h5_file = self.h5_files[idx]
        scenario = self.scenarios[idx]
        
        try:
            with h5py.File(h5_file, 'r') as f:
                # mobile_vla_data_collector.py ë°ì´í„° ì§ì ‘ ë¡œë“œ
                images = f['images'][:]                    # [T, 720, 1280, 3]
                actions = f['actions'][:]                  # [T, 3] 
                action_events = f['action_event_types'][:]  # [T]
                
                # ë©”íƒ€ë°ì´í„°
                episode_name = f.attrs['episode_name']
                num_frames = f.attrs['num_frames']
                duration = f.attrs['total_duration']
                
        except Exception as e:
            logger.error(f"HDF5 íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {h5_file.name}: {e}")
            # ë¹ˆ ë°ì´í„° ë°˜í™˜
            return self._get_empty_sample(scenario)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self._preprocess_episode(
            images, actions, action_events, scenario, episode_name, num_frames, duration
        )
        
        return processed_data
    
    def _preprocess_episode(
        self, 
        images: np.ndarray, 
        actions: np.ndarray, 
        action_events: np.ndarray,
        scenario: str,
        episode_name: str,
        num_frames: int,
        duration: float
    ) -> Dict:
        """ì—í”¼ì†Œë“œ ë°ì´í„° ì „ì²˜ë¦¬"""
        
        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (720p â†’ 224x224 ë¦¬ì‚¬ì´ì¦ˆ + ì •ê·œí™”)
        processed_images = self._preprocess_images(images)  # [T, 3, 224, 224]
        
        # 2. ì•¡ì…˜ ì •ê·œí™” (mobile_vla_data_collector.py ê¸°ì¤€)
        if self.normalize_actions:
            processed_actions = self._normalize_actions(actions)  # [T, 3] normalized
        else:
            processed_actions = torch.FloatTensor(actions)
        
        # 3. ì´ë²¤íŠ¸ íƒ€ì… ë³€í™˜
        event_indices = np.array([
            self.event_mapping.get(event, 1) for event in action_events
        ])
        processed_events = torch.LongTensor(event_indices)  # [T]
        
        # 4. ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (18í”„ë ˆì„ í‘œì¤€)
        if len(processed_images) != self.sequence_length:
            processed_images, processed_actions, processed_events = self._pad_or_truncate_sequence(
                processed_images, processed_actions, processed_events
            )
        
        # 5. í•œêµ­ì–´ ëª…ë ¹ì–´ ì¶”ê°€
        instruction = self.scenario_instructions.get(scenario, "ì»µê¹Œì§€ ê°€ì„¸ìš”")
        
        return {
            "images": processed_images,              # [18, 3, 224, 224]
            "actions": processed_actions,            # [18, 3]
            "action_events": processed_events,       # [18]
            "scenario": scenario,                    # str
            "instruction": instruction,              # str (í•œêµ­ì–´)
            "episode_name": episode_name,            # str
            "num_frames": num_frames,                # int
            "duration": duration,                    # float
            "sequence_mask": torch.ones(self.sequence_length, dtype=torch.bool)  # [18] - ëª¨ë“  í”„ë ˆì„ ìœ íš¨
        }
    
    def _preprocess_images(self, images: np.ndarray) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬: 720p â†’ 224x224 ë¦¬ì‚¬ì´ì¦ˆ + ì •ê·œí™”"""
        import torchvision.transforms as transforms
        
        # [T, 720, 1280, 3] â†’ [T, 3, 224, 224]
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize(self.image_size),
            transforms.ToTensor(),  # [0, 1] ì •ê·œí™” + HWCâ†’CHW
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
        ])
        
        processed_images = []
        for i in range(len(images)):
            # uint8 [720, 1280, 3] â†’ normalized [3, 224, 224]
            img_tensor = transform(images[i])
            processed_images.append(img_tensor)
        
        return torch.stack(processed_images)  # [T, 3, 224, 224]
    
    def _normalize_actions(self, actions: np.ndarray) -> torch.Tensor:
        """ì•¡ì…˜ ì •ê·œí™” (mobile_vla_data_collector.py ê¸°ì¤€)"""
        # [T, 3] actions: [linear_x, linear_y, angular_z]
        normalized_actions = actions.copy()
        
        # ê° ì¶•ë³„ë¡œ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        normalized_actions[:, 0] = actions[:, 0] / self.action_bounds["linear_x"]    # linear_x
        normalized_actions[:, 1] = actions[:, 1] / self.action_bounds["linear_y"]    # linear_y  
        normalized_actions[:, 2] = actions[:, 2] / self.action_bounds["angular_z"]   # angular_z
        
        # í´ë¨í•‘ [-1, 1]
        normalized_actions = np.clip(normalized_actions, -1.0, 1.0)
        
        return torch.FloatTensor(normalized_actions)
    
    def _pad_or_truncate_sequence(
        self, 
        images: torch.Tensor, 
        actions: torch.Tensor, 
        events: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ í‘œì¤€ ê¸¸ì´(18)ì— ë§ì¶”ê¸°"""
        current_length = len(images)
        
        if current_length == self.sequence_length:
            return images, actions, events
        elif current_length < self.sequence_length:
            # íŒ¨ë”©: ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
            pad_length = self.sequence_length - current_length
            
            # ì´ë¯¸ì§€ íŒ¨ë”©
            last_image = images[-1:].repeat(pad_length, 1, 1, 1)
            padded_images = torch.cat([images, last_image], dim=0)
            
            # ì•¡ì…˜ íŒ¨ë”© (ì •ì§€ ì•¡ì…˜ìœ¼ë¡œ)
            stop_action = torch.zeros(pad_length, 3)
            padded_actions = torch.cat([actions, stop_action], dim=0)
            
            # ì´ë²¤íŠ¸ íŒ¨ë”© (stop_actionìœ¼ë¡œ)
            stop_events = torch.full((pad_length,), 2, dtype=torch.long)  # stop_action = 2
            padded_events = torch.cat([events, stop_events], dim=0)
            
            return padded_images, padded_actions, padded_events
        else:
            # ìë¥´ê¸°: ì²˜ìŒ sequence_lengthë§Œ ì‚¬ìš©
            return images[:self.sequence_length], actions[:self.sequence_length], events[:self.sequence_length]
    
    def _get_empty_sample(self, scenario: str) -> Dict:
        """ë¹ˆ ìƒ˜í”Œ ë°˜í™˜ (ì—ëŸ¬ ë°œìƒì‹œ)"""
        return {
            "images": torch.zeros(self.sequence_length, 3, *self.image_size),
            "actions": torch.zeros(self.sequence_length, 3),
            "action_events": torch.zeros(self.sequence_length, dtype=torch.long),
            "scenario": scenario,
            "instruction": self.scenario_instructions.get(scenario, "ì»µê¹Œì§€ ê°€ì„¸ìš”"),
            "episode_name": "error_episode",
            "num_frames": 0,
            "duration": 0.0,
            "sequence_mask": torch.zeros(self.sequence_length, dtype=torch.bool)
        }
    
    def denormalize_actions(self, normalized_actions: torch.Tensor) -> torch.Tensor:
        """ì •ê·œí™”ëœ ì•¡ì…˜ì„ ì›ë˜ ë²”ìœ„ë¡œ ë³µì›"""
        # [-1, 1] â†’ mobile_vla_data_collector.py ë²”ìœ„
        denormalized = normalized_actions.clone()
        denormalized[:, 0] *= self.action_bounds["linear_x"]    # linear_x
        denormalized[:, 1] *= self.action_bounds["linear_y"]    # linear_y
        denormalized[:, 2] *= self.action_bounds["angular_z"]   # angular_z
        return denormalized
    
    def get_scenario_statistics(self) -> Dict[str, int]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í†µê³„ ë°˜í™˜"""
        scenario_counts = defaultdict(int)
        for scenario in self.scenarios:
            scenario_counts[scenario] += 1
        return dict(scenario_counts)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile VLA Dataset í…ŒìŠ¤íŠ¸")
    
    dataset = MobileVLADataset()
    print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ í˜•íƒœ: {sample['images'].shape}")
        print(f"ğŸ® ì•¡ì…˜ í˜•íƒœ: {sample['actions'].shape}")
        print(f"âš¡ ì´ë²¤íŠ¸ í˜•íƒœ: {sample['action_events'].shape}")
        print(f"ğŸ¯ ì‹œë‚˜ë¦¬ì˜¤: {sample['scenario']}")
        print(f"ğŸ—£ï¸ ëª…ë ¹ì–´: {sample['instruction']}")
        print(f"ğŸ“‹ ì—í”¼ì†Œë“œëª…: {sample['episode_name']}")
    
    # ì‹œë‚˜ë¦¬ì˜¤ í†µê³„
    stats = dataset.get_scenario_statistics()
    print(f"ğŸ“ˆ ì‹œë‚˜ë¦¬ì˜¤ í†µê³„: {stats}")
