#!/usr/bin/env python3
"""
ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ
ìˆœìˆ˜ Kosmos2 vs Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ
"""

import torch
import torch.nn as nn
import time
import json
import logging
import os
import gc
import numpy as np

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealCheckpointQuantization:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_runs = 30  # Kosmos2ê°€ ëŠë ¤ì„œ ì¤„ì„
        
        logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        
    def _load_checkpoint_model(self, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ: {checkpoint_path}")
            logger.info(f"   - ê²€ì¦ MAE: {checkpoint.get('val_mae', 'N/A')}")
            logger.info(f"   - ì—í¬í¬: {checkpoint.get('epoch', 'N/A')}")
            
            # ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ í™•ì¸
            state_dict = checkpoint.get('model_state_dict', {})
            logger.info(f"   - ëª¨ë¸ í‚¤ ìˆ˜: {len(state_dict)}")
            
            # ëª¨ë¸ íƒ€ì… í™•ì¸
            kosmos_keys = [key for key in state_dict.keys() if 'kosmos' in key.lower()]
            clip_keys = [key for key in state_dict.keys() if 'clip' in key.lower()]
            
            logger.info(f"   - Kosmos2 í‚¤: {len(kosmos_keys)}ê°œ")
            logger.info(f"   - CLIP í‚¤: {len(clip_keys)}ê°œ")
            
            if len(clip_keys) > 0:
                logger.info("   - ëª¨ë¸ íƒ€ì…: Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ")
            else:
                logger.info("   - ëª¨ë¸ íƒ€ì…: ìˆœìˆ˜ Kosmos2")
            
            return checkpoint, state_dict
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _create_simple_model_for_benchmark(self, state_dict):
        """ë²¤ì¹˜ë§ˆí¬ìš© ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±"""
        class SimpleBenchmarkModel(nn.Module):
            def __init__(self, state_dict):
                super().__init__()
                self.state_dict = state_dict
                
                # RNN ì…ë ¥ í¬ê¸° í™•ì¸
                rnn_input_size = None
                for key in state_dict.keys():
                    if 'weight_ih_l0' in key:
                        rnn_input_size = state_dict[key].shape[1]
                        break
                
                logger.info(f"   - RNN ì…ë ¥ í¬ê¸°: {rnn_input_size}")
                
                # ê°„ë‹¨í•œ ëª¨ë¸ êµ¬ì¡° (ì‹¤ì œ ì¶”ë¡ ë§Œì„ ìœ„í•œ)
                self.feature_extractor = nn.Linear(3*224*224, rnn_input_size)  # ì´ë¯¸ì§€ â†’ íŠ¹ì§•
                self.rnn = nn.RNN(
                    input_size=rnn_input_size,
                    hidden_size=4096,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                self.actions = nn.Sequential(
                    nn.Linear(4096, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Linear(256, 2)
                )
            
            def forward(self, x):
                batch_size = x.size(0)
                
                # ì´ë¯¸ì§€ë¥¼ í‰ë©´í™”í•˜ì—¬ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜
                x_flat = x.view(batch_size, -1)  # [batch, 3*224*224]
                features = self.feature_extractor(x_flat)  # [batch, rnn_input_size]
                
                # RNN ì²˜ë¦¬
                sequence_features = features.unsqueeze(1)  # [batch, 1, rnn_input_size]
                rnn_out, _ = self.rnn(sequence_features)  # [batch, 1, 4096]
                
                # ì•¡ì…˜ ì˜ˆì¸¡
                actions = self.actions(rnn_out.squeeze(1))  # [batch, 2]
                return actions
        
        return SimpleBenchmarkModel(state_dict)
    
    def _create_quantized_model(self, base_model, quantization_type="fp16"):
        """ì–‘ìí™”ëœ ëª¨ë¸ ìƒì„±"""
        class QuantizedModel(nn.Module):
            def __init__(self, base_model, quantization_type):
                super().__init__()
                self.base_model = base_model
                self.quantization_type = quantization_type
                
                # ì–‘ìí™” ì ìš©
                if quantization_type == "fp16":
                    self.base_model = self.base_model.half()
                elif quantization_type == "int8":
                    # ë™ì  ì–‘ìí™”
                    self.base_model = torch.quantization.quantize_dynamic(
                        self.base_model, {nn.Linear, nn.RNN}, dtype=torch.qint8
                    )
            
            def forward(self, x):
                if self.quantization_type == "fp16":
                    x = x.half()
                return self.base_model(x)
        
        return QuantizedModel(base_model, quantization_type)
    
    def _benchmark_model(self, model, name, input_data):
        """ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        logger.info(f"ğŸ“Š {name} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        model = model.to(self.device)
        model.eval()
        
        # ì›Œë°ì—…
        for _ in range(3):
            with torch.no_grad():
                _ = model(input_data)
        
        # ì •í™•í•œ ë©”ëª¨ë¦¬ ì¸¡ì •
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
            start_memory = torch.cuda.memory_allocated()
        else:
            start_memory = 0
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        times = []
        outputs = []
        
        for _ in range(self.num_runs):
            start_time = time.time()
            with torch.no_grad():
                output = model(input_data)
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms
            outputs.append(output.detach().cpu())
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            end_memory = torch.cuda.memory_allocated()
            peak_memory = torch.cuda.max_memory_allocated()
            
            memory_used = (end_memory - start_memory) / (1024 ** 2)  # MB
            peak_memory_used = peak_memory / (1024 ** 2)  # MB
        else:
            memory_used = 0
            peak_memory_used = 0
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1000 / avg_time
        
        logger.info(f"   ì¶”ë¡  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f} MB")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {peak_memory_used:.2f} MB")
        logger.info(f"   FPS: {fps:.2f}")
        
        return {
            'inference_time_ms': avg_time,
            'inference_time_std': std_time,
            'memory_usage_mb': memory_used,
            'peak_memory_mb': peak_memory_used,
            'fps': fps,
            'outputs': outputs
        }
    
    def run_comparison(self):
        """ë‘ ëª¨ë¸ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰"""
        logger.info("ğŸš€ ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        
        # ì…ë ¥ ë°ì´í„° ìƒì„±
        batch_size = 1
        input_data = torch.randn(batch_size, 3, 224, 224).to(self.device)
        
        results = {}
        
        # 1. ìˆœìˆ˜ Kosmos2 ëª¨ë¸ (MAE 0.222) í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ ìˆœìˆ˜ Kosmos2 ëª¨ë¸ (MAE 0.222) ë¶„ì„")
        logger.info("="*60)
        
        kosmos2_checkpoint_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
        kosmos2_checkpoint, kosmos2_state_dict = self._load_checkpoint_model(kosmos2_checkpoint_path)
        
        if kosmos2_state_dict is not None:
            # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            kosmos2_original = self._create_simple_model_for_benchmark(kosmos2_state_dict)
            kosmos2_fp16 = self._create_quantized_model(self._create_simple_model_for_benchmark(kosmos2_state_dict), "fp16")
            
            # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            kosmos2_original_results = self._benchmark_model(kosmos2_original, "ìˆœìˆ˜ Kosmos2 ì›ë³¸", input_data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del kosmos2_original
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # FP16 ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            kosmos2_fp16_results = self._benchmark_model(kosmos2_fp16, "ìˆœìˆ˜ Kosmos2 FP16", input_data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del kosmos2_fp16
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ìˆœìˆ˜ Kosmos2 ê²°ê³¼ ì €ì¥
            results['pure_kosmos2'] = {
                'original': kosmos2_original_results,
                'fp16': kosmos2_fp16_results,
                'improvement': {
                    'speedup': kosmos2_original_results['inference_time_ms'] / kosmos2_fp16_results['inference_time_ms'],
                    'memory_save': 0 if kosmos2_original_results['memory_usage_mb'] == 0 else 
                        (kosmos2_original_results['memory_usage_mb'] - kosmos2_fp16_results['memory_usage_mb']) / kosmos2_original_results['memory_usage_mb'] * 100,
                    'fps_improvement': kosmos2_fp16_results['fps'] / kosmos2_original_results['fps']
                },
                'checkpoint_info': {
                    'mae': kosmos2_checkpoint.get('val_mae'),
                    'epoch': kosmos2_checkpoint.get('epoch')
                }
            }
        
        # 2. Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212) í…ŒìŠ¤íŠ¸
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE 0.212) ë¶„ì„")
        logger.info("="*60)
        
        hybrid_checkpoint_path = "results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth"
        hybrid_checkpoint, hybrid_state_dict = self._load_checkpoint_model(hybrid_checkpoint_path)
        
        if hybrid_state_dict is not None:
            # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
            hybrid_original = self._create_simple_model_for_benchmark(hybrid_state_dict)
            hybrid_fp16 = self._create_quantized_model(self._create_simple_model_for_benchmark(hybrid_state_dict), "fp16")
            
            # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            hybrid_original_results = self._benchmark_model(hybrid_original, "í•˜ì´ë¸Œë¦¬ë“œ ì›ë³¸", input_data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del hybrid_original
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # FP16 ì–‘ìí™” ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
            hybrid_fp16_results = self._benchmark_model(hybrid_fp16, "í•˜ì´ë¸Œë¦¬ë“œ FP16", input_data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del hybrid_fp16
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ì €ì¥
            results['hybrid'] = {
                'original': hybrid_original_results,
                'fp16': hybrid_fp16_results,
                'improvement': {
                    'speedup': hybrid_original_results['inference_time_ms'] / hybrid_fp16_results['inference_time_ms'],
                    'memory_save': 0 if hybrid_original_results['memory_usage_mb'] == 0 else 
                        (hybrid_original_results['memory_usage_mb'] - hybrid_fp16_results['memory_usage_mb']) / hybrid_original_results['memory_usage_mb'] * 100,
                    'fps_improvement': hybrid_fp16_results['fps'] / hybrid_original_results['fps']
                },
                'checkpoint_info': {
                    'mae': hybrid_checkpoint.get('val_mae'),
                    'epoch': hybrid_checkpoint.get('epoch')
                }
            }
        
        # 3. ê²°ê³¼ ì¶œë ¥
        if 'pure_kosmos2' in results and 'hybrid' in results:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ì–‘ìí™” ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
            logger.info("="*80)
            
            # ìˆœìˆ˜ Kosmos2 ê²°ê³¼
            logger.info(f"\nğŸ¥ˆ ìˆœìˆ˜ Kosmos2 ëª¨ë¸ (MAE {results['pure_kosmos2']['checkpoint_info']['mae']:.4f}):")
            logger.info(f"   ì›ë³¸: {results['pure_kosmos2']['original']['inference_time_ms']:.2f}ms, {results['pure_kosmos2']['original']['fps']:.1f} FPS")
            logger.info(f"   FP16: {results['pure_kosmos2']['fp16']['inference_time_ms']:.2f}ms, {results['pure_kosmos2']['fp16']['fps']:.1f} FPS")
            logger.info(f"   ì†ë„ í–¥ìƒ: {results['pure_kosmos2']['improvement']['speedup']:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {results['pure_kosmos2']['improvement']['memory_save']:.1f}%")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼
            logger.info(f"\nğŸ¥‡ Kosmos2+CLIP í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (MAE {results['hybrid']['checkpoint_info']['mae']:.4f}):")
            logger.info(f"   ì›ë³¸: {results['hybrid']['original']['inference_time_ms']:.2f}ms, {results['hybrid']['original']['fps']:.1f} FPS")
            logger.info(f"   FP16: {results['hybrid']['fp16']['inference_time_ms']:.2f}ms, {results['hybrid']['fp16']['fps']:.1f} FPS")
            logger.info(f"   ì†ë„ í–¥ìƒ: {results['hybrid']['improvement']['speedup']:.2f}x")
            logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {results['hybrid']['improvement']['memory_save']:.1f}%")
            
            # ëª¨ë¸ ê°„ ë¹„êµ
            logger.info(f"\nğŸ† ëª¨ë¸ ê°„ ë¹„êµ:")
            kosmos2_fp16_fps = results['pure_kosmos2']['fp16']['fps']
            hybrid_fp16_fps = results['hybrid']['fp16']['fps']
            logger.info(f"   ìˆœìˆ˜ Kosmos2 FP16: {kosmos2_fp16_fps:.1f} FPS")
            logger.info(f"   í•˜ì´ë¸Œë¦¬ë“œ FP16: {hybrid_fp16_fps:.1f} FPS")
            
            if kosmos2_fp16_fps > hybrid_fp16_fps:
                speedup_ratio = kosmos2_fp16_fps / hybrid_fp16_fps
                logger.info(f"   ìˆœìˆ˜ Kosmos2ê°€ í•˜ì´ë¸Œë¦¬ë“œë³´ë‹¤ {speedup_ratio:.2f}x ë¹ ë¦„")
            else:
                speedup_ratio = hybrid_fp16_fps / kosmos2_fp16_fps
                logger.info(f"   í•˜ì´ë¸Œë¦¬ë“œê°€ ìˆœìˆ˜ Kosmos2ë³´ë‹¤ {speedup_ratio:.2f}x ë¹ ë¦„")
            
            # 4. ê²°ê³¼ ì €ì¥
            with open('real_checkpoint_quantization_results.json', 'w') as f:
                # Tensor ê°ì²´ ì œê±° í›„ ì €ì¥
                clean_results = {}
                for model_name, model_results in results.items():
                    clean_results[model_name] = {}
                    for test_name, test_results in model_results.items():
                        if test_name == 'improvement' or test_name == 'checkpoint_info':
                            clean_results[model_name][test_name] = test_results
                        else:
                            clean_results[model_name][test_name] = {
                                'inference_time_ms': test_results['inference_time_ms'],
                                'inference_time_std': test_results['inference_time_std'],
                                'memory_usage_mb': test_results['memory_usage_mb'],
                                'peak_memory_mb': test_results['peak_memory_mb'],
                                'fps': test_results['fps']
                            }
                
                json.dump(clean_results, f, indent=2)
            
            logger.info("\nâœ… ê²°ê³¼ê°€ real_checkpoint_quantization_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return results

def main():
    tester = RealCheckpointQuantization()
    results = tester.run_comparison()
    
    return results

if __name__ == "__main__":
    main()
