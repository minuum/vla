"""
ğŸš€ Enhanced 2D Action Model with RoboVLMs Advanced Features
RoboVLMsì˜ ìµœì‹  ê¸°ëŠ¥ë“¤ì„ ëª¨ë‘ í¬í•¨í•œ í–¥ìƒëœ 2D ì•¡ì…˜ ëª¨ë¸

ì¶”ê°€ëœ ê¸°ëŠ¥:
- Vision Resampler (PerceiverResampler)
- CLIP Normalization
- State Embedding (ì„ íƒì )
- Enhanced Attention Mechanisms
"""

import torch
import torch.nn as nn
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import json
from einops import rearrange, repeat
from einops_exts import rearrange_many

from transformers import AutoModel
from PIL import Image

def exists(val):
    return val is not None

def FeedForward(dim, mult=4):
    inner_dim = int(dim * mult)
    return nn.Sequential(
        nn.LayerNorm(dim),
        nn.Linear(dim, inner_dim, bias=False),
        nn.GELU(),
        nn.Linear(inner_dim, dim, bias=False),
    )

class PerceiverAttention(nn.Module):
    """Perceiver Attention for Vision Resampling"""
    def __init__(self, *, dim, dim_head=64, heads=8):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        inner_dim = dim_head * heads

        self.norm_media = nn.LayerNorm(dim)
        self.norm_latents = nn.LayerNorm(dim)

        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

    def forward(self, x, latents):
        """
        Args:
            x (torch.Tensor): image features
                shape (b, T, n1, D)
            latent (torch.Tensor): latent features
                shape (b, T, n2, D)
        """
        x = self.norm_media(x)
        latents = self.norm_latents(latents)

        h = self.heads
        q = self.to_q(latents)
        kv_input = torch.cat((x, latents), dim=-2)
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)
        q, k, v = rearrange_many((q, k, v), "b t n (h d) -> b h t n d", h=h)
        q = q * self.scale

        # attention
        sim = torch.einsum("... i d, ... j d  -> ... i j", q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        out = torch.einsum("... i j, ... j d -> ... i d", attn, v)
        out = rearrange(out, "b h t n d -> b t n (h d)", h=h)
        return self.to_out(out)

class PerceiverResampler(nn.Module):
    """Vision Resampler for reducing image token count"""
    def __init__(
        self,
        *,
        dim,
        depth=6,
        dim_head=64,
        heads=8,
        num_latents=64,
        max_num_media=None,
        max_num_frames=None,
        ff_mult=4,
    ):
        super().__init__()
        self.latents = nn.Parameter(torch.randn(num_latents, dim))
        self.frame_embs = (
            nn.Parameter(torch.randn(max_num_frames, dim))
            if exists(max_num_frames)
            else None
        )
        self.media_time_embs = (
            nn.Parameter(torch.randn(max_num_media, 1, dim))
            if exists(max_num_media)
            else None
        )

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),
                        FeedForward(dim=dim, mult=ff_mult),
                    ]
                )
            )

        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): image features
                shape (b, T, F, v, D)
        Returns:
            shape (b, T, n, D) where n is self.num_latents
        """
        b, T, F, v = x.shape[:4]

        # frame and media time embeddings
        if exists(self.frame_embs):
            frame_embs = repeat(self.frame_embs[:F], "F d -> b T F v d", b=b, T=T, v=v)
            x = x + frame_embs
        x = rearrange(
            x, "b T F v d -> b T (F v) d"
        )  # flatten the frame and spatial dimensions
        if exists(self.media_time_embs):
            x = x + self.media_time_embs[:T]

        # blocks
        latents = repeat(self.latents, "n d -> b T n d", b=b, T=T)
        for attn, ff in self.layers:
            latents = attn(x, latents) + latents
            latents = ff(latents) + latents
        return self.norm(latents)

class CLIPNormalizationHead(nn.Module):
    """CLIP Normalization for better feature alignment"""
    def __init__(self, hidden_size, clip_dim=512):
        super().__init__()
        self.hidden_size = hidden_size
        self.clip_dim = clip_dim
        
        # Projection to CLIP space
        self.clip_projection = nn.Linear(hidden_size, clip_dim)
        
        # CLIP text encoder (frozen)
        try:
            import open_clip
            self.clip_model, _, _ = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')
            self.clip_tokenizer = open_clip.get_tokenizer('ViT-L-14')
            # Freeze CLIP
            for param in self.clip_model.parameters():
                param.requires_grad = False
        except ImportError:
            print("Warning: open_clip not available. CLIP normalization disabled.")
            self.clip_model = None
            self.clip_tokenizer = None

    def forward(self, features, raw_text):
        """Compute CLIP normalization loss"""
        if self.clip_model is None:
            return torch.tensor(0.0, device=features.device)
        
        # Project features to CLIP space
        projected_features = self.clip_projection(features)
        projected_features = projected_features / projected_features.norm(dim=-1, keepdim=True)
        
        # Encode text with CLIP
        text_tokens = self.clip_tokenizer(raw_text).to(features.device)
        with torch.no_grad():
            text_features = self.clip_model.encode_text(text_tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # Compute cosine similarity loss
        similarity = torch.cosine_similarity(projected_features, text_features, dim=-1)
        loss = 1.0 - similarity.mean()
        
        return loss

class StateEmbedding(nn.Module):
    """State embedding for robot state information"""
    def __init__(self, state_dim=7, hidden_size=512):
        super().__init__()
        self.state_dim = state_dim
        self.hidden_size = hidden_size
        
        # Arm state embedding (6D)
        self.embed_arm_state = nn.Linear(state_dim - 1, hidden_size)
        
        # Gripper state embedding (1D binary)
        self.embed_gripper_state = nn.Embedding(2, hidden_size)
        
        # Combined state embedding
        self.embed_state = nn.Linear(2 * hidden_size, hidden_size)
        
    def forward(self, state):
        """
        Args:
            state: [batch_size, seq_len, state_dim]
        Returns:
            state_embeddings: [batch_size, seq_len, hidden_size]
        """
        arm_state = state[..., :6]  # First 6 dimensions
        gripper_state = state[..., -1].long()  # Last dimension as binary
        
        arm_embeddings = self.embed_arm_state(arm_state)
        gripper_embeddings = self.embed_gripper_state(gripper_state)
        
        # Combine embeddings
        combined = torch.cat([arm_embeddings, gripper_embeddings], dim=-1)
        state_embeddings = self.embed_state(combined)
        
        return state_embeddings

class Enhanced2DActionDataset(Dataset):
    """Enhanced 2D Action Dataset with state information"""
    
    def __init__(self, data_path, processor, split='train', frame_selection='random', 
                 use_state=False, use_vision_resampler=False):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        self.frame_selection = frame_selection
        self.use_state = use_state
        self.use_vision_resampler = use_vision_resampler
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} Enhanced 2D ì•¡ì…˜ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
        print(f"   - í”„ë ˆì„ ì„ íƒ: {frame_selection}")
        print(f"   - Zì¶• ì œì™¸: True")
        print(f"   - ìƒíƒœ ì •ë³´ ì‚¬ìš©: {use_state}")
        print(f"   - ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬: {use_vision_resampler}")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ (Zì¶• ì œì™¸, 2D ì•¡ì…˜ë§Œ)"""
        if os.path.isdir(self.data_path):
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ìƒíƒœ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ë¡œë“œ
                        robot_state = None
                        if self.use_state and 'robot_state' in f:
                            robot_state = f['robot_state'][:]  # [18, 15]
                        
                        # ì²« í”„ë ˆì„ ì œì™¸ (í”„ë ˆì„ 1-17ë§Œ ì‚¬ìš©)
                        valid_frames = list(range(1, 18))  # 1, 2, 3, ..., 17
                        
                        if self.frame_selection == 'random':
                            frame_idx = np.random.choice(valid_frames)
                        elif self.frame_selection == 'middle':
                            frame_idx = valid_frames[len(valid_frames)//2]  # 9
                        elif self.frame_selection == 'all':
                            for frame_idx in valid_frames:
                                single_image = images[frame_idx]  # [H, W, 3]
                                single_action = actions[frame_idx]  # [3]
                                
                                # 2D ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (Zì¶• ì œì™¸)
                                action_2d = single_action[:2]  # [linear_x, linear_y]ë§Œ ì‚¬ìš©
                                
                                episode_data = {
                                    'image': single_image,
                                    'action': action_2d,  # 2D ì•¡ì…˜
                                    'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                                    'frame_idx': frame_idx,
                                    'original_file': h5_file.name
                                }
                                
                                if robot_state is not None:
                                    episode_data['robot_state'] = robot_state[frame_idx]
                                
                                self.episodes.append(episode_data)
                            continue
                        
                        # ë‹¨ì¼ í”„ë ˆì„ ì„ íƒ
                        single_image = images[frame_idx]  # [H, W, 3]
                        single_action = actions[frame_idx]  # [3]
                        
                        # 2D ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (Zì¶• ì œì™¸)
                        action_2d = single_action[:2]  # [linear_x, linear_y]ë§Œ ì‚¬ìš©
                        
                        episode_data = {
                            'image': single_image,
                            'action': action_2d,  # 2D ì•¡ì…˜
                            'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                            'frame_idx': frame_idx,
                            'original_file': h5_file.name
                        }
                        
                        if robot_state is not None:
                            episode_data['robot_state'] = robot_state[frame_idx]
                        
                        self.episodes.append(episode_data)
                        
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image = Image.fromarray(image.transpose(1, 2, 0))
        inputs = self.processor(images=image, return_tensors="pt")
        image_tensor = inputs['pixel_values'].squeeze(0)  # [3, H, W]
        
        # ì•¡ì…˜
        action = torch.FloatTensor(episode['action'])  # [2]
        
        # í…ìŠ¤íŠ¸ (ë”ë¯¸)
        text = "ë¡œë´‡ì„ ì œì–´í•˜ì„¸ìš”"
        
        result = {
            'image': image_tensor,
            'action': action,
            'text': text,
            'episode_id': episode['episode_id']
        }
        
        # ìƒíƒœ ì •ë³´ ì¶”ê°€
        if 'robot_state' in episode:
            result['robot_state'] = torch.FloatTensor(episode['robot_state'])
        
        return result

class Enhanced2DActionModel(nn.Module):
    """Enhanced 2D Action Model with RoboVLMs features"""
    
    def __init__(self, processor, vision_dim=1024, language_dim=1024, action_dim=2, 
                 hidden_dim=512, dropout=0.2, use_claw_matrix=True, use_hierarchical=True, 
                 use_advanced_attention=True, use_vision_resampler=False, use_clip_norm=False, 
                 use_state=False):
        super().__init__()
        
        # ê¸°ë³¸ ì„¤ì •
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim  # 2D ì•¡ì…˜
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # ê¸°ëŠ¥ í”Œë˜ê·¸
        self.use_claw_matrix = use_claw_matrix
        self.use_hierarchical = use_hierarchical
        self.use_advanced_attention = use_advanced_attention
        self.use_vision_resampler = use_vision_resampler
        self.use_clip_norm = use_clip_norm
        self.use_state = use_state
        
        # Kosmos2 ëª¨ë¸ ë¡œë“œ
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos_processor = processor
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = next(self.kosmos.parameters()).device
        self.kosmos = self.kosmos.to(self.device)
        self.kosmos.eval()
        
        # ë™ì  ì–´ëŒ‘í„°ë“¤
        self.language_adapter = None
        self.fusion_adapter = None
        
        # íŠ¹ì§• ì¶”ì¶œê¸°
        self.feature_adapter = nn.Linear(vision_dim, hidden_dim)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        self.layer_norm_vision = nn.LayerNorm(hidden_dim)
        self.layer_norm_language = nn.LayerNorm(language_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)
        
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        self.dropout_fusion = nn.Dropout(dropout)
        
        # Vision Resampler
        if self.use_vision_resampler:
            self.vision_resampler = PerceiverResampler(
                dim=hidden_dim,
                depth=6,
                dim_head=64,
                heads=8,
                num_latents=64
            )
        else:
            self.vision_resampler = None
        
        # State Embedding
        if self.use_state:
            self.state_embedding = StateEmbedding(state_dim=15, hidden_size=hidden_dim)
        else:
            self.state_embedding = None
        
        # CLIP Normalization
        if self.use_clip_norm:
            self.clip_norm_head = CLIPNormalizationHead(hidden_dim)
        else:
            self.clip_norm_head = None
        
        # ê³ ê¸‰ ê¸°ëŠ¥ë“¤
        if self.use_claw_matrix:
            self.claw_matrix = EnhancedClawMatrixFusion(
                vision_dim, language_dim, action_dim, hidden_dim, dropout
            )
        
        if self.use_hierarchical:
            self.hierarchical_planner = EnhancedHierarchicalPlanner(
                hidden_dim, action_dim, dropout
            )
        
        if self.use_advanced_attention:
            self.advanced_attention = EnhancedAdvancedAttention(
                hidden_dim, dropout
            )
        
        # 2D ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)  # 2D ì•¡ì…˜ë§Œ
        )
        
        print(f"âœ… Enhanced 2D Action Model ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì•¡ì…˜ ì°¨ì›: {action_dim}D")
        print(f"   - ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬: {use_vision_resampler}")
        print(f"   - CLIP ì •ê·œí™”: {use_clip_norm}")
        print(f"   - ìƒíƒœ ì„ë² ë”©: {use_state}")
        print(f"   - Claw Matrix: {use_claw_matrix}")
        print(f"   - Hierarchical Planning: {use_hierarchical}")
        print(f"   - Advanced Attention: {use_advanced_attention}")
    
    def extract_vision_features(self, images):
        """ë¹„ì „ íŠ¹ì§• ì¶”ì¶œ (ë¦¬ìƒ˜í”ŒëŸ¬ í¬í•¨)"""
        batch_size = images.shape[0]
        
        # Kosmos2 í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì¤€ë¹„
        inputs = self.kosmos_processor(
            images=images, 
            return_tensors="pt",
            padding=True
        )
        
        # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
        
        # Kosmos2 vision ëª¨ë¸ ì‚¬ìš©
        with torch.no_grad():
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output  # [batch_size, 1024]
            else:
                vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
        
        # ì°¨ì› ì¡°ì •
        vision_features = self.feature_adapter(vision_features)
        
        # Vision Resampler ì ìš©
        if self.use_vision_resampler and self.vision_resampler is not None:
            # Resampler ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜: [batch_size, 1, 1, num_tokens, hidden_dim]
            vision_features = vision_features.unsqueeze(1).unsqueeze(1).unsqueeze(1)
            vision_features = self.vision_resampler(vision_features)
            vision_features = vision_features.squeeze(1).squeeze(1)  # [batch_size, num_latents, hidden_dim]
            vision_features = vision_features.mean(dim=1)  # [batch_size, hidden_dim]
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, text: str, batch_size: int = 1):
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ"""
        with torch.no_grad():
            inputs = self.kosmos_processor(text=text, return_tensors="pt")
            inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # Kosmos2 í…ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš©
            language_outputs = self.kosmos.text_model(**inputs)
            language_features = language_outputs.last_hidden_state.mean(dim=1)  # [1, hidden_size]
        
        # ë°°ì¹˜ ì°¨ì› í™•ì¥
        language_features = language_features.expand(batch_size, -1)
        
        # ì°¨ì› ì¡°ì • (ë™ì  ì–´ëŒ‘í„° ìƒì„±)
        if language_features.shape[-1] != self.language_dim:
            if self.language_adapter is None:
                self.language_adapter = nn.Linear(
                    language_features.shape[-1], 
                    self.language_dim
                ).to(language_features.device)
            language_features = self.language_adapter(language_features)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, single_image: torch.Tensor, text: str, robot_state: torch.Tensor = None):
        """í–¥ìƒëœ 2D ì•¡ì…˜ ì˜ˆì¸¡"""
        batch_size = single_image.shape[0]
        
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(single_image)
        language_features = self.extract_language_features(text, batch_size)
        
        # ìƒíƒœ ì„ë² ë”© ì¶”ê°€
        if self.use_state and robot_state is not None and self.state_embedding is not None:
            state_features = self.state_embedding(robot_state)
            # ìƒíƒœ íŠ¹ì§•ì„ ë¹„ì „ íŠ¹ì§•ì— ì¶”ê°€
            vision_features = vision_features + state_features.mean(dim=1)
        
        # ê¸°ë³¸ ìœµí•©
        fused_features = torch.cat([vision_features, language_features], dim=-1)
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ì ìš©
        if self.use_claw_matrix and hasattr(self, 'claw_matrix'):
            # 2D ì•¡ì…˜ìš© ë”ë¯¸ ì•¡ì…˜ ìƒì„±
            dummy_actions = torch.zeros(batch_size, self.hidden_dim).to(vision_features.device)
            fused_features = self.claw_matrix(vision_features, language_features, dummy_actions)
        else:
            # Claw Matrixë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ìœµí•©
            if fused_features.shape[-1] != self.hidden_dim:
                if not hasattr(self, 'fusion_adapter'):
                    self.fusion_adapter = nn.Linear(fused_features.shape[-1], self.hidden_dim).to(fused_features.device)
                fused_features = self.fusion_adapter(fused_features)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        fused_features = self.layer_norm_fusion(fused_features)
        fused_features = self.dropout_fusion(fused_features)
        
        # Advanced Attention ì ìš©
        if self.use_advanced_attention and hasattr(self, 'advanced_attention'):
            fused_features = self.advanced_attention(fused_features)
        
        # Hierarchical Planning ì ìš©
        if self.use_hierarchical and hasattr(self, 'hierarchical_planner'):
            fused_features = self.hierarchical_planner(fused_features)
        
        # CLIP ì •ê·œí™” ì†ì‹¤ ê³„ì‚°
        clip_loss = 0.0
        if self.use_clip_norm and self.clip_norm_head is not None:
            clip_loss = self.clip_norm_head(fused_features, text)
        
        # 2D ì•¡ì…˜ ì˜ˆì¸¡ (Zì¶• ì œì™¸)
        actions_2d = self.action_head(fused_features)  # [batch_size, 2]
        
        return actions_2d, clip_loss

class EnhancedClawMatrixFusion(nn.Module):
    """í–¥ìƒëœ Claw Matrix ìœµí•©"""
    
    def __init__(self, vision_dim, language_dim, action_dim, hidden_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # í”„ë¡œì ì…˜ ë ˆì´ì–´ë“¤
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        self.language_proj = nn.Linear(language_dim, hidden_dim)
        self.action_proj = nn.Linear(hidden_dim, hidden_dim)  # ë”ë¯¸ ì•¡ì…˜ìš©
        
        # Cross-attention ë©”ì»¤ë‹ˆì¦˜ë“¤
        self.vl_cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True
        )
        self.la_cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True
        )
        self.av_cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True
        )
        
        # ì •ê·œí™” ë ˆì´ì–´ë“¤
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        # Feed-forward ë„¤íŠ¸ì›Œí¬
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        self.norm4 = nn.LayerNorm(hidden_dim)
    
    def forward(self, vision_features, language_features, dummy_actions):
        """í–¥ìƒëœ Claw Matrix ìœµí•©"""
        # í”„ë¡œì ì…˜
        vision_proj = self.vision_proj(vision_features)
        language_proj = self.language_proj(language_features)
        action_proj = self.action_proj(dummy_actions)
        
        # Vision-Language Cross-attention
        vl_out, _ = self.vl_cross_attention(
            query=vision_proj.unsqueeze(1),
            key=language_proj.unsqueeze(1),
            value=language_proj.unsqueeze(1)
        )
        vl_out = self.norm1(vl_out.squeeze(1) + vision_proj)
        
        # Language-Action Cross-attention
        la_out, _ = self.la_cross_attention(
            query=language_proj.unsqueeze(1),
            key=action_proj.unsqueeze(1),
            value=action_proj.unsqueeze(1)
        )
        la_out = self.norm2(la_out.squeeze(1) + language_proj)
        
        # Action-Vision Cross-attention
        av_out, _ = self.av_cross_attention(
            query=action_proj.unsqueeze(1),
            key=vl_out.unsqueeze(1),
            value=vl_out.unsqueeze(1)
        )
        av_out = self.norm3(av_out.squeeze(1) + action_proj)
        
        # ìµœì¢… ìœµí•©
        fused = vl_out + la_out + av_out
        
        # Feed-forward
        ffn_out = self.ffn(fused)
        fused = self.norm4(fused + ffn_out)
        
        return fused

class EnhancedHierarchicalPlanner(nn.Module):
    """í–¥ìƒëœ Hierarchical Planning"""
    
    def __init__(self, hidden_dim, action_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        
        # ëª©í‘œ ë¶„í•´ê¸°
        self.goal_decomposer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # ì•¡ì…˜ ê³„íšê¸°
        self.action_planner = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # ì •ê·œí™”
        self.norm = nn.LayerNorm(hidden_dim)
    
    def forward(self, features):
        """í–¥ìƒëœ Hierarchical Planning"""
        # ëª©í‘œ ë¶„í•´
        decomposed = self.goal_decomposer(features)
        
        # ì•¡ì…˜ ê³„íš
        planned_actions = self.action_planner(decomposed)
        
        # íŠ¹ì§• ì—…ë°ì´íŠ¸
        updated_features = self.norm(features + decomposed)
        
        return updated_features

class EnhancedAdvancedAttention(nn.Module):
    """í–¥ìƒëœ Advanced Attention"""
    
    def __init__(self, hidden_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # Self-attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True
        )
        
        # Temporal attention
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=4, dropout=dropout, batch_first=True
        )
        
        # Spatial attention
        self.spatial_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=4, dropout=dropout, batch_first=True
        )
        
        # ì •ê·œí™” ë ˆì´ì–´ë“¤
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        # Feed-forward
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        self.norm4 = nn.LayerNorm(hidden_dim)
    
    def forward(self, features):
        """í–¥ìƒëœ Advanced Attention"""
        # Self Attention
        attn_out, _ = self.self_attention(features, features, features)
        features = self.norm1(features + attn_out)
        
        # Temporal Attention (ì‹œí€€ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
        if features.dim() == 3:
            temp_out, _ = self.temporal_attention(features, features, features)
            features = self.norm2(features + temp_out)
        
        # Spatial Attention (ê³µê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if features.dim() == 3:
            spatial_out, _ = self.spatial_attention(features, features, features)
            features = self.norm3(features + spatial_out)
        
        # Feedforward
        ffn_out = self.ffn(features)
        features = self.norm4(features + ffn_out)
        
        return features

def create_enhanced_data_loaders(data_path, processor, batch_size=4, train_split=0.8, 
                                frame_selection='random', use_state=False, use_vision_resampler=False):
    """í–¥ìƒëœ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = Enhanced2DActionDataset(
        data_path, processor, 'full', frame_selection, use_state, use_vision_resampler
    )
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=1,
        pin_memory=True
    )
    
    print(f"ğŸ“Š í–¥ìƒëœ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - ì•¡ì…˜ ì°¨ì›: 2D (Zì¶• ì œì™¸)")
    print(f"   - ìƒíƒœ ì •ë³´: {use_state}")
    print(f"   - ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬: {use_vision_resampler}")
    
    return train_loader, val_loader

def train_enhanced_2d_model(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,
    weight_decay=1e-4,
    early_stopping_patience=5,
    device='cuda',
    clip_loss_weight=0.1
):
    """í–¥ìƒëœ 2D ì•¡ì…˜ ëª¨ë¸ í›ˆë ¨"""
    
    model = model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.MSELoss()
    
    # ì¡°ê¸° ì¢…ë£Œ
    best_val_loss = float('inf')
    patience_counter = 0
    
    print(f"ğŸš€ í–¥ìƒëœ 2D ì•¡ì…˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    print(f"   - ì—í¬í¬: {num_epochs}")
    print(f"   - í•™ìŠµë¥ : {learning_rate}")
    print(f"   - CLIP ì†ì‹¤ ê°€ì¤‘ì¹˜: {clip_loss_weight}")
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0
        train_clip_loss = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]"):
            images = batch['image'].to(device)
            actions = batch['action'].to(device)
            texts = batch['text']
            
            # ìƒíƒœ ì •ë³´
            robot_states = None
            if 'robot_state' in batch:
                robot_states = batch['robot_state'].to(device)
            
            # ìˆœì „íŒŒ
            predictions, clip_loss = model(images, texts, robot_states)
            
            # ì†ì‹¤ ê³„ì‚°
            action_loss = criterion(predictions, actions)
            total_loss = action_loss + clip_loss_weight * clip_loss
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += action_loss.item()
            train_clip_loss += clip_loss.item()
            num_batches += 1
        
        # ê²€ì¦
        model.eval()
        val_loss = 0
        val_clip_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]"):
                images = batch['image'].to(device)
                actions = batch['action'].to(device)
                texts = batch['text']
                
                # ìƒíƒœ ì •ë³´
                robot_states = None
                if 'robot_state' in batch:
                    robot_states = batch['robot_state'].to(device)
                
                # ìˆœì „íŒŒ
                predictions, clip_loss = model(images, texts, robot_states)
                
                # ì†ì‹¤ ê³„ì‚°
                action_loss = criterion(predictions, actions)
                
                val_loss += action_loss.item()
                val_clip_loss += clip_loss.item()
                val_batches += 1
        
        # í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_train_loss = train_loss / num_batches
        avg_train_clip_loss = train_clip_loss / num_batches
        avg_val_loss = val_loss / val_batches
        avg_val_clip_loss = val_clip_loss / val_batches
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"Epoch {epoch+1}/{num_epochs}:")
        print(f"  Train - Action Loss: {avg_train_loss:.6f}, CLIP Loss: {avg_train_clip_loss:.6f}")
        print(f"  Val   - Action Loss: {avg_val_loss:.6f}, CLIP Loss: {avg_val_clip_loss:.6f}")
        print(f"  LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': avg_val_loss,
                'config': {
                    'use_vision_resampler': model.use_vision_resampler,
                    'use_clip_norm': model.use_clip_norm,
                    'use_state': model.use_state,
                    'action_dim': model.action_dim
                }
            }, f'enhanced_2d_model_epoch_{epoch+1}.pth')
            print(f"  âœ… ëª¨ë¸ ì €ì¥ë¨ (Val Loss: {avg_val_loss:.6f})")
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"  ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ (Patience: {early_stopping_patience})")
                break
    
    print(f"ğŸ‰ í–¥ìƒëœ 2D ì•¡ì…˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    print(f"   ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
    
    return model

if __name__ == "__main__":
    # ì„¤ì •
    data_path = "path/to/your/h5/data"
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # í–¥ìƒëœ ëª¨ë¸ ìƒì„±
    model = Enhanced2DActionModel(
        processor=processor,
        vision_dim=1024,
        language_dim=1024,
        action_dim=2,
        hidden_dim=512,
        dropout=0.2,
        use_claw_matrix=True,
        use_hierarchical=True,
        use_advanced_attention=True,
        use_vision_resampler=True,  # ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬ í™œì„±í™”
        use_clip_norm=True,         # CLIP ì •ê·œí™” í™œì„±í™”
        use_state=False             # ìƒíƒœ ì •ë³´ëŠ” ì„ íƒì 
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader, val_loader = create_enhanced_data_loaders(
        data_path=data_path,
        processor=processor,
        batch_size=4,
        train_split=0.8,
        frame_selection='random',
        use_state=False,
        use_vision_resampler=True
    )
    
    # ëª¨ë¸ í›ˆë ¨
    trained_model = train_enhanced_2d_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=15,
        learning_rate=1e-4,
        weight_decay=1e-4,
        early_stopping_patience=5,
        device=device,
        clip_loss_weight=0.1
    )
    
    print("âœ… í–¥ìƒëœ 2D ì•¡ì…˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
