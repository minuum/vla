#!/usr/bin/env python3
"""
Mobile Image Encoder - mobile_vla_data_collector.pyì˜ 720p ì´ë¯¸ì§€ ì²˜ë¦¬ íŠ¹í™”
RoboVLMsì˜ ì´ë¯¸ì§€ ì¸ì½”ë”© ê¸°ìˆ ì„ Mobile VLAì— ì ìš©
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class MobileImageEncoder(nn.Module):
    """
    Mobile VLA íŠ¹í™” ì´ë¯¸ì§€ ì¸ì½”ë”
    - ì…ë ¥: [B, T, 3, 224, 224] (mobile_vla_data_collector.pyì—ì„œ 720pâ†’224p ë¦¬ì‚¬ì´ì¦ˆë¨)
    - ì¶œë ¥: [B, T, hidden_size] ì‹œê°„ì  íŠ¹ì§•
    """
    
    def __init__(
        self,
        backbone: str = "efficientnet_v2_s",
        hidden_size: int = 768,
        num_lstm_layers: int = 2,
        dropout: float = 0.1,
        freeze_backbone: bool = False
    ):
        super().__init__()
        
        self.backbone_name = backbone
        self.hidden_size = hidden_size
        self.num_lstm_layers = num_lstm_layers
        
        # ë°±ë³¸ CNN (EfficientNet V2 - ëª¨ë°”ì¼ ìµœì í™”)
        if backbone == "efficientnet_v2_s":
            self.backbone = models.efficientnet_v2_s(pretrained=True)
            backbone_output_size = 1000
        elif backbone == "resnet50":
            self.backbone = models.resnet50(pretrained=True)
            backbone_output_size = 2048
        elif backbone == "mobilenet_v3_large":
            self.backbone = models.mobilenet_v3_large(pretrained=True)  
            backbone_output_size = 1000
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°±ë³¸: {backbone}")
        
        # ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì • ì˜µì…˜
        if freeze_backbone:
            for param in self.backbone.parameters():
                param.requires_grad = False
            logger.info(f"ğŸ”’ {backbone} ë°±ë³¸ ê°€ì¤‘ì¹˜ ê³ ì •ë¨")
        
        # CNN íŠ¹ì§•ì„ hidden_sizeë¡œ ë§¤í•‘
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ (18í”„ë ˆì„ ì‹œí€€ìŠ¤)
        self.temporal_encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,  # ì–‘ë°©í–¥ì´ë¯€ë¡œ ì ˆë°˜
            num_layers=num_lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸ–¼ï¸ Mobile Image Encoder ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ë°±ë³¸: {backbone}, Hidden: {hidden_size}, LSTM Layers: {num_lstm_layers}")
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_sequence: [B, T, 3, 224, 224] - ë°°ì¹˜ í¬ê¸° B, ì‹œí€€ìŠ¤ ê¸¸ì´ T
            
        Returns:
            temporal_features: [B, T, hidden_size] - ì‹œê°„ì  ì´ë¯¸ì§€ íŠ¹ì§•
        """
        B, T, C, H, W = image_sequence.shape
        
        # ë°°ì¹˜ì™€ ì‹œê°„ ì°¨ì›ì„ í•©ì³ì„œ CNNì— ì…ë ¥
        images_flat = image_sequence.view(B * T, C, H, W)  # [B*T, 3, 224, 224]
        
        # CNNìœ¼ë¡œ ê° í”„ë ˆì„ íŠ¹ì§• ì¶”ì¶œ
        with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
            frame_features = self.backbone(images_flat)  # [B*T, backbone_output_size]
        
        # íŠ¹ì§• ì°¨ì›ì„ hidden_sizeë¡œ ë§¤í•‘
        frame_features = self.feature_projection(frame_features)  # [B*T, hidden_size]
        
        # ì‹œê°„ ì°¨ì› ë³µì›
        frame_features = frame_features.view(B, T, self.hidden_size)  # [B, T, hidden_size]
        
        # LSTMìœ¼ë¡œ ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ
        temporal_features, (hidden, cell) = self.temporal_encoder(frame_features)  # [B, T, hidden_size]
        
        # ë ˆì´ì–´ ì •ê·œí™”
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        # single_image: [B, 3, 224, 224] ë˜ëŠ” [3, 224, 224]
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        with torch.cuda.amp.autocast(enabled=True):
            spatial_features = self.backbone(single_image)  # [B, backbone_output_size]
        
        spatial_features = self.feature_projection(spatial_features)  # [B, hidden_size]
        
        return spatial_features
    
    def get_feature_maps(self, image_sequence: torch.Tensor) -> dict:
        """ì¤‘ê°„ íŠ¹ì§• ë§µë“¤ì„ ë°˜í™˜ (ë””ë²„ê¹…/ë¶„ì„ìš©)"""
        B, T, C, H, W = image_sequence.shape
        images_flat = image_sequence.view(B * T, C, H, W)
        
        features = {}
        
        # EfficientNetì˜ ì¤‘ê°„ íŠ¹ì§•ë“¤ ì¶”ì¶œ
        if self.backbone_name == "efficientnet_v2_s":
            x = images_flat
            for i, layer in enumerate(self.backbone.features):
                x = layer(x)
                if i in [2, 4, 6]:  # ì„ íƒëœ ë ˆì´ì–´ì˜ íŠ¹ì§• ì €ì¥
                    features[f"stage_{i}"] = x.view(B, T, *x.shape[1:])
        
        return features


class MobileImageEncoderLite(nn.Module):
    """
    ê²½ëŸ‰í™”ëœ Mobile Image Encoder (Jetsonìš©)
    ë” ì‘ì€ ëª¨ë¸ì´ì§€ë§Œ í•µì‹¬ ê¸°ëŠ¥ ìœ ì§€
    """
    
    def __init__(
        self,
        hidden_size: int = 512,
        num_lstm_layers: int = 1,
        dropout: float = 0.1
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        
        # ê²½ëŸ‰í™”ëœ CNN ë°±ë³¸ (MobileNet V3)
        self.backbone = models.mobilenet_v3_small(pretrained=True)
        backbone_output_size = 1000
        
        # íŠ¹ì§• ë§¤í•‘ (ë” ì‘ì€ hidden_size)
        self.feature_projection = nn.Sequential(
            nn.Linear(backbone_output_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ê°„ë‹¨í•œ GRU (LSTMë³´ë‹¤ íŒŒë¼ë¯¸í„° ì ìŒ)
        self.temporal_encoder = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout if num_lstm_layers > 1 else 0
        )
        
        self.layer_norm = nn.LayerNorm(hidden_size)
        
        logger.info(f"ğŸš€ Mobile Image Encoder Lite ì´ˆê¸°í™” (Hidden: {hidden_size})")
    
    def extract_spatial_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³µê°„ì  íŠ¹ì§•ë§Œ ì¶”ì¶œ (ì‹¤ì‹œê°„ ì¶”ë¡ ìš©)"""
        if single_image.dim() == 3:
            single_image = single_image.unsqueeze(0)  # [1, 3, 224, 224]
        
        # ê²½ëŸ‰í™”ëœ CNN
        spatial_features = self.backbone(single_image)
        spatial_features = self.feature_projection(spatial_features)
        
        return spatial_features
    
    def forward(self, image_sequence: torch.Tensor) -> torch.Tensor:
        B, T, C, H, W = image_sequence.shape
        
        images_flat = image_sequence.view(B * T, C, H, W)
        
        # ê²½ëŸ‰í™”ëœ CNN
        frame_features = self.backbone(images_flat)
        frame_features = self.feature_projection(frame_features)
        frame_features = frame_features.view(B, T, self.hidden_size)
        
        # GRUë¡œ ì‹œê°„ì  íŠ¹ì§•
        temporal_features, hidden = self.temporal_encoder(frame_features)
        temporal_features = self.layer_norm(temporal_features)
        
        return temporal_features


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile Image Encoder í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    encoder = MobileImageEncoder(hidden_size=768)
    encoder_lite = MobileImageEncoderLite(hidden_size=512)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (mobile_vla_data_collector.py í˜•ì‹)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    
    print(f"ğŸ“Š ì…ë ¥ ì´ë¯¸ì§€: {test_images.shape}")
    
    # ì¼ë°˜ ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        features = encoder(test_images)
        print(f"ğŸ–¼ï¸ ì¸ì½”ë” ì¶œë ¥: {features.shape}")
        
        features_lite = encoder_lite(test_images)
        print(f"ğŸš€ Lite ì¸ì½”ë” ì¶œë ¥: {features_lite.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in encoder.parameters())
    lite_params = sum(p.numel() for p in encoder_lite.parameters())
    
    print(f"ğŸ“Š ì¼ë°˜ ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
    print(f"ğŸš€ Lite ì¸ì½”ë” íŒŒë¼ë¯¸í„°: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"ğŸ’¡ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨: {(1 - lite_params/total_params)*100:.1f}%")
