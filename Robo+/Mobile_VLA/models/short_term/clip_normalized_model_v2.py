#!/usr/bin/env python3
"""
Case 2: ë‹¨ê¸° ì ìš© (Short-term Optimization) - V2
CLIP Normalizationì´ ì ìš©ëœ 2D ì•¡ì…˜ ëª¨ë¸ (ì™„ì „íˆ ìƒˆë¡œìš´ íŒŒì¼)
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AutoProcessor
from PIL import Image
import logging
import open_clip

logger = logging.getLogger(__name__)

class CLIPNormalization(nn.Module):
    """CLIP Normalization ë ˆì´ì–´"""
    
    def __init__(self, input_dim, clip_dim=512, temperature=0.07):
        super().__init__()
        self.input_dim = input_dim
        self.clip_dim = clip_dim
        self.temperature = temperature
        
        # CLIP ê³µê°„ìœ¼ë¡œì˜ í”„ë¡œì ì…˜
        self.clip_proj = nn.Linear(input_dim, clip_dim)
        self.inverse_proj = nn.Linear(clip_dim, input_dim)
        
        # ì •ê·œí™” ë ˆì´ì–´
        self.norm = nn.LayerNorm(input_dim)
        
        logger.info(f"âœ… CLIP Normalization ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - input_dim: {input_dim}")
        logger.info(f"   - clip_dim: {clip_dim}")
        logger.info(f"   - temperature: {temperature}")
    
    def forward(self, features, clip_features=None):
        """
        CLIP Normalization ì ìš©
        Args:
            features: ì…ë ¥ íŠ¹ì§• [batch_size, feature_dim]
            clip_features: CLIP íŠ¹ì§• (ì„ íƒì‚¬í•­) [batch_size, clip_dim]
        Returns:
            normalized_features: ì •ê·œí™”ëœ íŠ¹ì§• [batch_size, feature_dim]
        """
        batch_size = features.shape[0]
        
        # CLIP ê³µê°„ìœ¼ë¡œ í”„ë¡œì ì…˜
        clip_projected = self.clip_proj(features)  # [batch_size, clip_dim]
        
        if clip_features is not None:
            # CLIP íŠ¹ì§•ê³¼ ì •ë ¬
            clip_projected = F.normalize(clip_projected, dim=-1)
            clip_features = F.normalize(clip_features, dim=-1)
            
            # Cosine similarity ê³„ì‚°
            similarity = torch.matmul(clip_projected, clip_features.T) / self.temperature
            
            # Attention ê°€ì¤‘ì¹˜ ê³„ì‚°
            attention_weights = F.softmax(similarity, dim=-1)
            
            # CLIP íŠ¹ì§•ê³¼ì˜ ê°€ì¤‘ í‰ê· 
            aligned_features = torch.matmul(attention_weights, clip_features)
        else:
            # CLIP íŠ¹ì§•ì´ ì—†ëŠ” ê²½ìš° ë‹¨ìˆœ ì •ê·œí™”
            aligned_features = F.normalize(clip_projected, dim=-1)
        
        # ì›ë˜ ê³µê°„ìœ¼ë¡œ ì—­í”„ë¡œì ì…˜
        normalized_features = self.inverse_proj(aligned_features)
        
        # Residual connection + ì •ê·œí™”
        normalized_features = self.norm(features + normalized_features)
        
        return normalized_features

class CLIPNormalized2DActionModelV2(nn.Module):
    """
    CLIP Normalizationì´ ì ìš©ëœ 2D ì•¡ì…˜ ëª¨ë¸ V2
    - CLIP íŠ¹ì§•ê³¼ ì •ë ¬
    - ë” ë‚˜ì€ feature alignment
    """
    
    def __init__(self, processor, vision_dim=1024, language_dim=2048, action_dim=2, 
                 hidden_dim=256, dropout=0.4, use_vision_resampler=True, 
                 use_clip_normalization=True):
        super().__init__()
        
        self.processor = processor
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.dropout_rate = dropout
        self.use_vision_resampler = use_vision_resampler
        self.use_clip_normalization = use_clip_normalization
        
        # CLIP ëª¨ë¸ ë¡œë“œ (RoboVLMs ë°©ì‹)
        if use_clip_normalization:
            try:
                self.clip_model, self.clip_preprocess, _ = open_clip.create_model_and_transforms(
                    "ViT-B-32", pretrained="openai"
                )
                self.clip_tokenizer = open_clip.get_tokenizer("ViT-B-32")
                # CLIP ëª¨ë¸ì„ GPUë¡œ ì´ë™
                if torch.cuda.is_available():
                    self.clip_model = self.clip_model.cuda()
                logger.info("âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (open_clip)")
            except Exception as e:
                logger.warning(f"âš ï¸ CLIP ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.clip_model = None
                self.clip_preprocess = None
                self.use_clip_normalization = False
        else:
            self.clip_model = None
            self.clip_preprocess = None
            self.use_clip_normalization = False
        
        # íŠ¹ì§• ì–´ëŒ‘í„°
        self.feature_adapter = nn.Linear(vision_dim, hidden_dim)
        self.language_adapter = nn.Linear(language_dim, hidden_dim)  # 2048 â†’ 256
        
        # CLIP Normalization ë ˆì´ì–´
        if use_clip_normalization:
            self.clip_norm_vision = CLIPNormalization(hidden_dim, clip_dim=512)
            self.clip_norm_language = CLIPNormalization(hidden_dim, clip_dim=512)
        
        # ì •ê·œí™” ë ˆì´ì–´
        self.layer_norm_vision = nn.LayerNorm(hidden_dim)
        self.layer_norm_language = nn.LayerNorm(hidden_dim)
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        
        # Vision Resampler (Case 2ì—ì„œ ìµœì í™”ëœ ë²„ì „ ì‚¬ìš©)
        if use_vision_resampler:
            from optimized_vision_resampler import OptimizedVisionResampler
            self.vision_resampler = OptimizedVisionResampler(
                input_dim=hidden_dim,
                output_dim=hidden_dim,
                num_latents=16,
                num_heads=4,
                dropout=dropout
            )
        else:
            self.vision_resampler = None
        
        # ì•¡ì…˜ í—¤ë“œ
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # ìµœì¢… ì •ê·œí™”
        self.final_norm = nn.LayerNorm(action_dim)
        
        logger.info(f"âœ… CLIP Normalized 2D Model V2 ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - hidden_dim: {hidden_dim}")
        logger.info(f"   - action_dim: {action_dim}")
        logger.info(f"   - use_clip_normalization: {use_clip_normalization}")
        logger.info(f"   - use_vision_resampler: {use_vision_resampler}")
    
    def extract_clip_features_from_pil(self, images):
        """PIL ì´ë¯¸ì§€ì—ì„œ CLIP íŠ¹ì§• ì¶”ì¶œ"""
        if self.clip_model is None:
            return None
        
        batch_size = len(images)
        clip_features = []
        
        for i in range(batch_size):
            # PIL ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì‚¬ìš©
            pil_img = images[i]
            
            # CLIP ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ
            with torch.no_grad():
                clip_input = self.clip_preprocess(pil_img).unsqueeze(0)
                # CLIP ì…ë ¥ì„ GPUë¡œ ì´ë™
                if torch.cuda.is_available():
                    clip_input = clip_input.cuda()
                clip_output = self.clip_model.encode_image(clip_input)
                clip_features.append(clip_output)
        
        return torch.cat(clip_features, dim=0)
    
    def extract_clip_features(self, images):
        """í…ì„œ ì´ë¯¸ì§€ì—ì„œ CLIP íŠ¹ì§• ì¶”ì¶œ (ê¸°ì¡´ ë©”ì„œë“œ)"""
        if self.clip_model is None:
            return None
        
        batch_size = images.shape[0]
        clip_features = []
        
        for i in range(batch_size):
            # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
            img = images[i].permute(1, 2, 0).cpu().numpy()
            img = (img + 1) / 2.0  # [-1,1] â†’ [0,1]
            img = (img * 255).astype('uint8')
            pil_img = Image.fromarray(img)
            
            # CLIP ì „ì²˜ë¦¬ ë° íŠ¹ì§• ì¶”ì¶œ
            with torch.no_grad():
                clip_input = self.clip_preprocess(pil_img).unsqueeze(0)
                clip_output = self.clip_model.encode_image(clip_input)
                clip_features.append(clip_output)
        
        return torch.cat(clip_features, dim=0)
    
    def extract_vision_features(self, images):
        """ì‹œê° íŠ¹ì§• ì¶”ì¶œ (CLIP Normalization í¬í•¨)"""
        batch_size = len(images)  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        
        # PIL ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì²˜ë¦¬
        inputs = self.processor(images=images, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
        
        # Kosmos2 vision ëª¨ë¸ ì‚¬ìš©
        with torch.no_grad():
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output
            else:
                vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
        
        # íŠ¹ì§• ì–´ëŒ‘í„°
        vision_features = self.feature_adapter(vision_features)
        
        # CLIP Normalization ì ìš© (PIL ì´ë¯¸ì§€ì—ì„œ CLIP íŠ¹ì§• ì¶”ì¶œ)
        if self.use_clip_normalization:
            clip_features = self.extract_clip_features_from_pil(images)
            vision_features = self.clip_norm_vision(vision_features, clip_features)
        
        # Vision Resampler ì ìš©
        if self.use_vision_resampler and self.vision_resampler is not None:
            vision_features = vision_features.unsqueeze(1)  # [batch_size, 1, hidden_dim]
            vision_features = self.vision_resampler(vision_features)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, texts):
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ (CLIP Normalization í¬í•¨)"""
        batch_size = len(texts)
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        inputs = self.processor(text=texts, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items()}
        
        # Kosmos2 text ëª¨ë¸ ì‚¬ìš©
        with torch.no_grad():
            if 'input_ids' in inputs:
                text_outputs = self.kosmos.text_model(inputs['input_ids'])
                # pooler_outputì´ ì—†ìœ¼ë¯€ë¡œ ë§ˆì§€ë§‰ hidden stateì˜ í‰ê·  ì‚¬ìš©
                language_features = text_outputs.last_hidden_state.mean(dim=1)
            else:
                language_features = torch.zeros(batch_size, 2048).to(self.kosmos.device)
        
        # íŠ¹ì§• ì–´ëŒ‘í„°
        language_features = self.language_adapter(language_features)
        
        # CLIP Normalization ì ìš© (í…ìŠ¤íŠ¸ì˜ ê²½ìš° ë‹¨ìˆœ ì •ê·œí™”)
        if self.use_clip_normalization:
            language_features = self.clip_norm_language(language_features)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, images, texts):
        """ìˆœì „íŒŒ"""
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(images)
        language_features = self.extract_language_features(texts)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([vision_features, language_features], dim=1)
        
        # ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.action_head(combined_features)
        actions = self.final_norm(actions)
        
        return actions

class CLIPNormalizedTrainerV2:
    """CLIP Normalization í›ˆë ¨ê¸° V2"""
    
    def __init__(self, model, device, learning_rate=3e-5, weight_decay=1e-3):
        self.model = model.to(device)
        self.device = device
        
        # ìµœì í™”ê¸°
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=100, eta_min=1e-6
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.HuberLoss(delta=0.1)
        
        logger.info(f"âœ… CLIP Normalized Trainer V2 ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - learning_rate: {learning_rate}")
        logger.info(f"   - weight_decay: {weight_decay}")
    
    def train_step(self, batch):
        """í›ˆë ¨ ìŠ¤í…"""
        self.model.train()
        
        images = batch['image']  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        actions = batch['action'].to(self.device)
        texts = batch['text']
        
        # ìˆœì „íŒŒ
        predicted_actions = self.model(images, texts)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = self.criterion(predicted_actions, actions)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def validate_step(self, batch):
        """ê²€ì¦ ìŠ¤í…"""
        self.model.eval()
        
        with torch.no_grad():
            images = batch['image']  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            actions = batch['action'].to(self.device)
            texts = batch['text']
            
            predicted_actions = self.model(images, texts)
            loss = self.criterion(predicted_actions, actions)
            
            # MAE ê³„ì‚°
            mae = torch.mean(torch.abs(predicted_actions - actions))
            
            return loss.item(), mae.item()
    
    def save_checkpoint(self, path, epoch, loss, mae):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'loss': loss,
            'mae': mae,
            'model_config': {
                'hidden_dim': self.model.hidden_dim,
                'action_dim': self.model.action_dim,
                'dropout': self.model.dropout_rate,
                'use_clip_normalization': self.model.use_clip_normalization,
                'use_vision_resampler': self.model.use_vision_resampler
            }
        }, path)
        
        logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {path}")

def create_clip_normalized_model_v2(processor, device, use_clip_normalization=True, use_vision_resampler=True):
    """CLIP Normalization ëª¨ë¸ ìƒì„± V2"""
    
    model = CLIPNormalized2DActionModelV2(
        processor=processor,
        vision_dim=1024,
        language_dim=2048,  # Kosmos2 text ëª¨ë¸ ì¶œë ¥ ì°¨ì›
        action_dim=2,
        hidden_dim=256,
        dropout=0.4,
        use_vision_resampler=use_vision_resampler,
        use_clip_normalization=use_clip_normalization
    )
    
    trainer = CLIPNormalizedTrainerV2(
        model=model,
        device=device,
        learning_rate=3e-5,
        weight_decay=1e-3
    )
    
    return model, trainer

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    model, trainer = create_clip_normalized_model_v2(processor, device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
    logger.info(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    logger.info(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    logger.info(f"   - ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1024 / 1024:.2f} MB")
