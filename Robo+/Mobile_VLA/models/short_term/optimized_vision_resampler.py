#!/usr/bin/env python3
"""
ğŸš€ Case 2: ë‹¨ê¸° ì ìš© - ìµœì í™”ëœ Vision Resampler
ëª©í‘œ: MAE 0.5 â†’ 0.3, ì •í™•ë„ 15% â†’ 35%
íŠ¹ì§•: Vision Resampler ìµœì í™” (latents 64â†’16, heads 8â†’4)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedVisionResampler(nn.Module):
    """
    ìµœì í™”ëœ Vision Resampler
    - latents ìˆ˜: 64 â†’ 16 (75% ê°ì†Œ)
    - attention heads: 8 â†’ 4 (50% ê°ì†Œ)
    - FFN í¬ê¸°: 2x â†’ 1.5x (25% ê°ì†Œ)
    - ì ì€ ë°ì´í„°ì…‹ì— ë§ê²Œ ìµœì í™”
    """
    
    def __init__(self, input_dim, output_dim, num_latents=16, num_heads=4, dropout=0.3):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_latents = num_latents
        self.num_heads = num_heads
        self.dropout = dropout
        
        # ìµœì í™”ëœ íŒŒë¼ë¯¸í„°
        self.latents = nn.Parameter(torch.randn(num_latents, output_dim))
        self.input_proj = nn.Linear(input_dim, output_dim)
        self.output_proj = nn.Linear(output_dim, output_dim)
        
        # ìµœì í™”ëœ attention (heads 8â†’4)
        self.attention = nn.MultiheadAttention(
            embed_dim=output_dim, 
            num_heads=num_heads, 
            dropout=dropout, 
            batch_first=True
        )
        
        # ì •ê·œí™” ë ˆì´ì–´
        self.norm1 = nn.LayerNorm(output_dim)
        self.norm2 = nn.LayerNorm(output_dim)
        
        # ìµœì í™”ëœ FFN (2x â†’ 1.5x)
        ffn_hidden_dim = int(output_dim * 1.5)
        self.ffn = nn.Sequential(
            nn.Linear(output_dim, ffn_hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(ffn_hidden_dim, output_dim)
        )
        
        # ì¶”ê°€ ì •ê·œí™”
        self.final_norm = nn.LayerNorm(output_dim)
        
        logger.info(f"âœ… Optimized Vision Resampler ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - input_dim: {input_dim}")
        logger.info(f"   - output_dim: {output_dim}")
        logger.info(f"   - num_latents: {num_latents} (64â†’16, 75% ê°ì†Œ)")
        logger.info(f"   - num_heads: {num_heads} (8â†’4, 50% ê°ì†Œ)")
        logger.info(f"   - ffn_hidden_dim: {ffn_hidden_dim} (2xâ†’1.5x, 25% ê°ì†Œ)")
        logger.info(f"   - dropout: {dropout}")
    
    def forward(self, x):
        """
        ìˆœì „íŒŒ
        Args:
            x: ì…ë ¥ íŠ¹ì§• [batch_size, seq_len, input_dim]
        Returns:
            output: ì••ì¶•ëœ íŠ¹ì§• [batch_size, output_dim]
        """
        batch_size = x.shape[0]
        
        # ì…ë ¥ í”„ë¡œì ì…˜
        x = self.input_proj(x)  # [batch_size, seq_len, output_dim]
        
        # í•™ìŠµ ê°€ëŠ¥í•œ latents í™•ì¥
        latents = self.latents.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, num_latents, output_dim]
        
        # Cross-attention: latentsê°€ query, xê°€ key/value
        attn_out, _ = self.attention(latents, x, x)
        latents = self.norm1(latents + attn_out)
        
        # Self-attention: latents ê°„ì˜ attention
        self_attn_out, _ = self.attention(latents, latents, latents)
        latents = self.norm2(latents + self_attn_out)
        
        # Feed-forward network
        ffn_out = self.ffn(latents)
        latents = latents + ffn_out
        
        # ìµœì¢… ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.output_proj(latents)
        output = self.final_norm(output)
        
        # í‰ê·  í’€ë§ìœ¼ë¡œ ì••ì¶•
        output = output.mean(dim=1)  # [batch_size, output_dim]
        
        return output

class OptimizedVisionResamplerWithSkip(nn.Module):
    """
    Skip connectionì´ ì¶”ê°€ëœ ìµœì í™”ëœ Vision Resampler
    - ì •ë³´ ì†ì‹¤ ìµœì†Œí™”
    - ë” ì•ˆì •ì ì¸ í›ˆë ¨
    """
    
    def __init__(self, input_dim, output_dim, num_latents=16, num_heads=4, dropout=0.3):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_latents = num_latents
        
        # ê¸°ë³¸ resampler
        self.resampler = OptimizedVisionResampler(
            input_dim=input_dim,
            output_dim=output_dim,
            num_latents=num_latents,
            num_heads=num_heads,
            dropout=dropout
        )
        
        # Skip connectionì„ ìœ„í•œ í”„ë¡œì ì…˜
        self.skip_proj = nn.Linear(input_dim, output_dim)
        
        # Skip connection ê°€ì¤‘ì¹˜ (í•™ìŠµ ê°€ëŠ¥)
        self.skip_weight = nn.Parameter(torch.tensor(0.5))
        
        logger.info(f"âœ… Optimized Vision Resampler with Skip ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - skip_weight: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°")
    
    def forward(self, x):
        """
        Skip connectionì´ í¬í•¨ëœ ìˆœì „íŒŒ
        """
        # ì›ë³¸ íŠ¹ì§•ì˜ í‰ê·  (skip connection)
        original_mean = x.mean(dim=1)  # [batch_size, input_dim]
        skip_features = self.skip_proj(original_mean)  # [batch_size, output_dim]
        
        # Resampler ì¶œë ¥
        resampled_features = self.resampler(x)  # [batch_size, output_dim]
        
        # Skip connection ê²°í•©
        output = self.skip_weight * resampled_features + (1 - self.skip_weight) * skip_features
        
        return output

def test_optimized_vision_resampler():
    """ìµœì í™”ëœ Vision Resampler í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸ§ª Optimized Vision Resampler í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
    batch_size = 4
    seq_len = 100
    input_dim = 1024
    output_dim = 256
    num_latents = 16
    num_heads = 4
    
    # ëª¨ë¸ ìƒì„±
    resampler = OptimizedVisionResampler(
        input_dim=input_dim,
        output_dim=output_dim,
        num_latents=num_latents,
        num_heads=num_heads,
        dropout=0.3
    )
    
    resampler_with_skip = OptimizedVisionResamplerWithSkip(
        input_dim=input_dim,
        output_dim=output_dim,
        num_latents=num_latents,
        num_heads=num_heads,
        dropout=0.3
    )
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    x = torch.randn(batch_size, seq_len, input_dim)
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    with torch.no_grad():
        output1 = resampler(x)
        output2 = resampler_with_skip(x)
    
    # ê²°ê³¼ í™•ì¸
    logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    logger.info(f"   - ì…ë ¥ í¬ê¸°: {x.shape}")
    logger.info(f"   - ê¸°ë³¸ ì¶œë ¥ í¬ê¸°: {output1.shape}")
    logger.info(f"   - Skip ì¶œë ¥ í¬ê¸°: {output2.shape}")
    logger.info(f"   - ì••ì¶• ë¹„ìœ¨: {seq_len} â†’ {num_latents} ({(1-num_latents/seq_len)*100:.1f}% ê°ì†Œ)")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in resampler.parameters())
    trainable_params = sum(p.numel() for p in resampler.parameters() if p.requires_grad)
    
    logger.info(f"ğŸ“ˆ íŒŒë¼ë¯¸í„° ì •ë³´:")
    logger.info(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    logger.info(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
    logger.info(f"   - ëª¨ë¸ í¬ê¸°: {total_params * 4 / 1024 / 1024:.2f} MB")
    
    # ê¸°ì¡´ Vision Resamplerì™€ ë¹„êµ
    original_latents = 64
    original_heads = 8
    original_ffn_ratio = 2.0
    
    original_params = (
        original_latents * output_dim +  # latents
        input_dim * output_dim +  # input_proj
        output_dim * output_dim +  # output_proj
        output_dim * output_dim * 3 +  # attention (Q, K, V)
        output_dim * (output_dim * original_ffn_ratio) * 2  # FFN
    )
    
    compression_ratio = original_params / total_params
    logger.info(f"ğŸ“Š ì••ì¶• íš¨ìœ¨ì„±:")
    logger.info(f"   - ê¸°ì¡´ íŒŒë¼ë¯¸í„° (ì¶”ì •): {original_params:,}")
    logger.info(f"   - ìµœì í™” íŒŒë¼ë¯¸í„°: {total_params:,}")
    logger.info(f"   - ì••ì¶• ë¹„ìœ¨: {compression_ratio:.2f}x")
    logger.info(f"   - íŒŒë¼ë¯¸í„° ê°ì†Œ: {(1-1/compression_ratio)*100:.1f}%")
    
    logger.info("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_optimized_vision_resampler()
