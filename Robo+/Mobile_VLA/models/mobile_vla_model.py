#!/usr/bin/env python3
"""
Mobile VLA Model - Pure Mobile VLM without Calvin dependencies
RoboVLMsì˜ VLM ê¸°ìˆ ì„ mobile_vla_data_collector.pyì— ì™„ì „ ì ì‘
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
import logging

try:
    from .encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from .encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from .policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite
except ImportError:
    # í…ŒìŠ¤íŠ¸ìš© ì ˆëŒ€ ì„í¬íŠ¸
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from encoders.mobile_image_encoder import MobileImageEncoder, MobileImageEncoderLite
    from encoders.korean_text_encoder import KoreanTextEncoder, KoreanTextEncoderLite
    from policy_heads.mobile_policy_head import MobilePolicyHead, MobilePolicyHeadLite

logger = logging.getLogger(__name__)


class MobileVLAModel(nn.Module):
    """
    Pure Mobile VLA Model
    - ì…ë ¥: mobile_vla_data_collector.py ë°ì´í„° í˜•ì‹
    - ì¶œë ¥: 3D ì•¡ì…˜ + ì´ë²¤íŠ¸ ì˜ˆì¸¡
    - Calvin ì˜ì¡´ì„± ì—†ëŠ” ìˆœìˆ˜ Mobile VLM
    """
    
    def __init__(
        self,
        # ëª¨ë¸ í¬ê¸° ì„¤ì •
        hidden_size: int = 768,
        
        # ì´ë¯¸ì§€ ì¸ì½”ë” ì„¤ì •
        image_backbone: str = "efficientnet_v2_s",
        freeze_image_backbone: bool = False,
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë” ì„¤ì •  
        text_model: str = "klue/roberta-base",
        freeze_text_encoder: bool = False,
        
        # ì •ì±… í—¤ë“œ ì„¤ì •
        use_policy_lstm: bool = True,
        policy_lstm_layers: int = 2,
        
        # ì¼ë°˜ ì„¤ì •
        dropout: float = 0.1,
        use_lite_mode: bool = False  # Jetsonìš© ê²½ëŸ‰í™” ëª¨ë“œ
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.use_lite_mode = use_lite_mode
        
        # ê²½ëŸ‰í™” ëª¨ë“œì— ë”°ë¥¸ ì»´í¬ë„ŒíŠ¸ ì„ íƒ
        if use_lite_mode:
            # Jetsonìš© ê²½ëŸ‰í™” ëª¨ë¸
            self.image_encoder = MobileImageEncoderLite(
                hidden_size=hidden_size // 2,  # ë” ì‘ì€ hidden_size
                dropout=dropout
            )
            self.text_encoder = KoreanTextEncoderLite(
                hidden_size=hidden_size // 2
            )
            self.policy_head = MobilePolicyHeadLite(
                hidden_size=hidden_size,  # ìœµí•© í›„ì—ëŠ” ì›ë˜ í¬ê¸°
                dropout=dropout
            )
            logger.info("ğŸš€ Lite ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨ (Jetson ìµœì í™”)")
        else:
            # ì¼ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸
            self.image_encoder = MobileImageEncoder(
                backbone=image_backbone,
                hidden_size=hidden_size,
                dropout=dropout,
                freeze_backbone=freeze_image_backbone
            )
            self.text_encoder = KoreanTextEncoder(
                model_name=text_model,
                hidden_size=hidden_size,
                freeze_encoder=freeze_text_encoder
            )
            self.policy_head = MobilePolicyHead(
                hidden_size=hidden_size,
                dropout=dropout,
                use_lstm=use_policy_lstm,
                lstm_layers=policy_lstm_layers
            )
            logger.info("ğŸ’ª Full ëª¨ë“œë¡œ ì´ˆê¸°í™”ë¨ (ê³ ì„±ëŠ¥)")
        
        # ë©€í‹°ëª¨ë‹¬ ìœµí•© ë ˆì´ì–´
        if use_lite_mode:
            # ê²½ëŸ‰í™”ëœ ìœµí•©
            self.multimodal_fusion = nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
        else:
            # ì–´í…ì…˜ ê¸°ë°˜ ìœµí•©
            self.multimodal_fusion = nn.MultiheadAttention(
                embed_dim=hidden_size,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.output_norm = nn.LayerNorm(hidden_size)
        
        # ëª¨ë¸ í†µê³„
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ¤– Mobile VLA Model ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ ({total_params/1e6:.1f}M)")
        logger.info(f"   Hidden Size: {hidden_size}, Lite Mode: {use_lite_mode}")
    
    def forward(
        self,
        images: torch.Tensor,
        scenarios: List[str],
        instructions: Optional[List[str]] = None,
        return_intermediate: bool = False
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            images: [B, T, 3, 224, 224] - ì´ë¯¸ì§€ ì‹œí€€ìŠ¤  
            scenarios: List[str] - ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ë“¤ ["1box_vert_left", ...]
            instructions: List[str] - í•œêµ­ì–´ ëª…ë ¹ì–´ (ì˜µì…˜, scenariosì—ì„œ ìë™ ìƒì„± ê°€ëŠ¥)
            return_intermediate: ì¤‘ê°„ íŠ¹ì§•ë“¤ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict with:
                - actions: [B, T, 3] - ì •ê·œí™”ëœ ì•¡ì…˜
                - actions_denorm: [B, T, 3] - ì‹¤ì œ ë²”ìœ„ ì•¡ì…˜
                - event_logits: [B, T, 3] - ì´ë²¤íŠ¸ ë¶„ë¥˜ ë¡œì§“
                - predicted_events: [B, T] - ì˜ˆì¸¡ëœ ì´ë²¤íŠ¸
        """
        batch_size = images.shape[0]
        
        # 1. ì´ë¯¸ì§€ ì¸ì½”ë”©
        image_features = self.image_encoder(images)  # [B, T, hidden_size//2 or hidden_size]
        
        # 2. í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        if self.use_lite_mode:
            # Lite ëª¨ë“œ: ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì‚¬ìš©
            text_features = self.text_encoder(scenarios)  # [B, hidden_size//2]
            # ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í™•ì¥
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size//2]
        else:
            # Full ëª¨ë“œ: í•œêµ­ì–´ ëª…ë ¹ì–´ ì‚¬ìš©
            if instructions is None:
                # ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í•œêµ­ì–´ ëª…ë ¹ì–´ ìë™ ìƒì„±
                instructions = [
                    self.text_encoder.get_instruction_for_scenario(scenario) 
                    for scenario in scenarios
                ]
            
            text_result = self.text_encoder(instructions, scenarios)
            text_features = text_result["fused_features"]  # [B, hidden_size]
            # ì‹œê°„ ì°¨ì›ìœ¼ë¡œ í™•ì¥
            text_features = text_features.unsqueeze(1).repeat(1, images.shape[1], 1)  # [B, T, hidden_size]
        
        # 3. ë©€í‹°ëª¨ë‹¬ ìœµí•©
        if self.use_lite_mode:
            # ê²½ëŸ‰í™”ëœ ìœµí•©: ë‹¨ìˆœ concatenation + MLP
            multimodal_features = torch.cat([image_features, text_features], dim=-1)  # [B, T, hidden_size]
            multimodal_features = self.multimodal_fusion(multimodal_features)
        else:
            # ì–´í…ì…˜ ê¸°ë°˜ ìœµí•©
            # ì´ë¯¸ì§€ë¥¼ ì¿¼ë¦¬ë¡œ, í…ìŠ¤íŠ¸ë¥¼ í‚¤-ë°¸ë¥˜ë¡œ ì‚¬ìš©
            fused_features, attention_weights = self.multimodal_fusion(
                query=image_features,     # [B, T, hidden_size]
                key=text_features,        # [B, T, hidden_size]  
                value=text_features       # [B, T, hidden_size]
            )
            multimodal_features = fused_features
        
        # ì¶œë ¥ ì •ê·œí™”
        multimodal_features = self.output_norm(multimodal_features)  # [B, T, hidden_size]
        
        # 4. ì •ì±… í—¤ë“œë¡œ ì•¡ì…˜ ì˜ˆì¸¡
        policy_output = self.policy_head(multimodal_features)
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "actions": policy_output["actions"],                    # [B, T, 3] ì •ê·œí™”ëœ
            "actions_denorm": policy_output["actions_denorm"],      # [B, T, 3] ì‹¤ì œ ë²”ìœ„
            "event_logits": policy_output["event_logits"],          # [B, T, 3]
            "event_probs": policy_output.get("event_probs"),        # [B, T, 3]
            "predicted_events": policy_output["predicted_events"]   # [B, T]
        }
        
        # ì¤‘ê°„ íŠ¹ì§•ë“¤ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        if return_intermediate:
            result.update({
                "image_features": image_features,
                "text_features": text_features,
                "multimodal_features": multimodal_features,
                "attention_weights": attention_weights if not self.use_lite_mode else None
            })
        
        return result
    
    def inference_single_step(
        self,
        current_image: torch.Tensor,
        scenario: str,
        hidden_state: Optional[Tuple] = None
    ) -> Tuple[Dict[str, torch.Tensor], Optional[Tuple]]:
        """
        ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  (ì‹¤ì‹œê°„ mobile_vla_data_collector ì—°ë™ìš©)
        
        Args:
            current_image: [1, 3, 224, 224] - í˜„ì¬ ì´ë¯¸ì§€
            scenario: str - í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤
            hidden_state: LSTM hidden state (ìˆë‹¤ë©´)
            
        Returns:
            (ì•¡ì…˜ ì˜ˆì¸¡ ê²°ê³¼, ìƒˆë¡œìš´ hidden_state)
        """
        with torch.no_grad():
            # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì‹œí€€ìŠ¤ë¡œ í™•ì¥
            image_sequence = current_image.unsqueeze(1)  # [1, 1, 3, 224, 224]
            
            # ì¸ì½”ë”©
            image_features = self.image_encoder.extract_spatial_features(current_image)  # [1, hidden_size]
            
            if self.use_lite_mode:
                text_features = self.text_encoder([scenario])  # [1, hidden_size//2]
                # ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§•ì˜ ì°¨ì›ì„ ë§ì¶¤
                image_features_lite = image_features  # [1, hidden_size//2] (256)
                multimodal_features = torch.cat([image_features_lite, text_features], dim=-1)  # [1, 512]
                multimodal_features = self.multimodal_fusion(multimodal_features)
            else:
                instruction = self.text_encoder.get_instruction_for_scenario(scenario)
                text_result = self.text_encoder([instruction], [scenario])
                text_features = text_result["fused_features"]  # [1, hidden_size]
                
                # ê°„ë‹¨í•œ ìœµí•© (ì–´í…ì…˜ ì—†ì´)
                multimodal_features = (image_features + text_features) / 2
            
            multimodal_features = self.output_norm(multimodal_features)
            
            # ì •ì±… í—¤ë“œë¡œ ë‹¨ì¼ ìŠ¤í… ì˜ˆì¸¡
            if hasattr(self.policy_head, 'predict_single_step') and not self.use_lite_mode:
                action_result, new_hidden_state = self.policy_head.predict_single_step(
                    multimodal_features, hidden_state
                )
            else:
                action_result = self.policy_head(multimodal_features.unsqueeze(1))
                # ë‹¨ì¼ ìŠ¤í…ìœ¼ë¡œ ì••ì¶•
                for key in action_result:
                    if action_result[key].dim() > 1:
                        action_result[key] = action_result[key].squeeze(1)
                new_hidden_state = None
        
        return action_result, new_hidden_state
    
    def get_mobile_vla_action(
        self,
        current_image: torch.Tensor,
        scenario: str
    ) -> Dict[str, float]:
        """
        mobile_vla_data_collector.py í˜¸í™˜ ì•¡ì…˜ ë°˜í™˜
        
        Returns:
            Dict with keys: linear_x, linear_y, angular_z, event_type
        """
        action_result, _ = self.inference_single_step(current_image, scenario)
        
        # ì•¡ì…˜ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°°ì¹˜)
        actions = action_result["actions_denorm"][0].cpu().numpy()  # [3]
        predicted_event = action_result["predicted_events"][0].cpu().item()
        
        # ì´ë²¤íŠ¸ íƒ€ì… ë§¤í•‘
        event_types = ["episode_start", "start_action", "stop_action"]
        event_type = event_types[predicted_event]
        
        return {
            "linear_x": float(actions[0]),
            "linear_y": float(actions[1]),
            "angular_z": float(actions[2]),
            "event_type": event_type
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Mobile VLA Model í…ŒìŠ¤íŠ¸")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (Full & Lite)
    model_full = MobileVLAModel(hidden_size=768, use_lite_mode=False)
    model_lite = MobileVLAModel(hidden_size=512, use_lite_mode=True)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (mobile_vla_data_collector.py í˜•ì‹)
    batch_size, seq_len = 2, 18
    test_images = torch.randn(batch_size, seq_len, 3, 224, 224)
    test_scenarios = ["1box_vert_left", "2box_hori_right"]
    
    print(f"ğŸ“Š ì…ë ¥ ì´ë¯¸ì§€: {test_images.shape}")
    print(f"ğŸ¯ ì…ë ¥ ì‹œë‚˜ë¦¬ì˜¤: {test_scenarios}")
    
    # Full ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ’ª Full Model í…ŒìŠ¤íŠ¸:")
    with torch.no_grad():
        result_full = model_full(test_images, test_scenarios, return_intermediate=True)
        print(f"   ì•¡ì…˜: {result_full['actions'].shape}")
        print(f"   ì‹¤ì œ ì•¡ì…˜: {result_full['actions_denorm'].shape}")
        print(f"   ì´ë²¤íŠ¸: {result_full['predicted_events'].shape}")
    
    # Lite ëª¨ë¸ í…ŒìŠ¤íŠ¸  
    print("\nğŸš€ Lite Model í…ŒìŠ¤íŠ¸:")
    with torch.no_grad():
        result_lite = model_lite(test_images, test_scenarios)
        print(f"   ì•¡ì…˜: {result_lite['actions'].shape}")
        print(f"   ì‹¤ì œ ì•¡ì…˜: {result_lite['actions_denorm'].shape}")
        print(f"   ì´ë²¤íŠ¸: {result_lite['predicted_events'].shape}")
    
    # ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ë‹¨ì¼ ìŠ¤í… ì¶”ë¡  í…ŒìŠ¤íŠ¸:")
    single_image = torch.randn(1, 3, 224, 224)
    scenario = "1box_vert_left"
    
    mobile_action = model_full.get_mobile_vla_action(single_image, scenario)
    print(f"   Mobile ì•¡ì…˜: {mobile_action}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
    full_params = sum(p.numel() for p in model_full.parameters())
    lite_params = sum(p.numel() for p in model_lite.parameters())
    
    print(f"\nğŸ“Š ëª¨ë¸ í¬ê¸° ë¹„êµ:")
    print(f"   Full ëª¨ë¸: {full_params:,}ê°œ ({full_params/1e6:.1f}M)")
    print(f"   Lite ëª¨ë¸: {lite_params:,}ê°œ ({lite_params/1e6:.1f}M)")
    print(f"   ê²½ëŸ‰í™”ìœ¨: {(1 - lite_params/full_params)*100:.1f}%")
