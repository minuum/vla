#!/usr/bin/env python3
"""
ğŸ¯ Case 1: ì¦‰ì‹œ ì ìš© - ë‹¨ìˆœí™”ëœ ëª¨ë¸ í‰ê°€
ëª©í‘œ: MAE 0.8 â†’ 0.5, ì •í™•ë„ 0% â†’ 15%
íŠ¹ì§•: ìƒì„¸í•œ ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import json
import logging
import argparse
from pathlib import Path
from tqdm import tqdm
import pandas as pd

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
import sys
sys.path.append('..')
from simplified_2d_model import Simplified2DActionModel
from basic_augmentation_dataset import create_basic_augmentation_data_loaders
from transformers import AutoProcessor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_simplified_model(checkpoint_path, processor, device):
    """ë‹¨ìˆœí™”ëœ ëª¨ë¸ ë¡œë“œ"""
    
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    
    # ëª¨ë¸ ìƒì„±
    model = Simplified2DActionModel(
        processor=processor,
        vision_dim=1024,
        language_dim=1024,
        action_dim=2,
        hidden_dim=256,
        dropout=0.4,
        use_vision_resampler=False
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"   - ì—í¬í¬: {checkpoint.get('epoch', 'N/A')}")
    logger.info(f"   - ì†ì‹¤: {checkpoint.get('loss', 'N/A'):.6f}")
    logger.info(f"   - MAE: {checkpoint.get('mae', 'N/A'):.6f}")
    
    return model, checkpoint

def calculate_detailed_metrics(predictions, targets):
    """ìƒì„¸í•œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mse = mean_squared_error(targets, predictions)
    mae = mean_absolute_error(targets, predictions)
    r2 = r2_score(targets, predictions)
    rmse = np.sqrt(mse)
    
    # ê° ì°¨ì›ë³„ ì„±ëŠ¥
    mse_x = mean_squared_error(targets[:, 0], predictions[:, 0])
    mse_y = mean_squared_error(targets[:, 1], predictions[:, 1])
    
    mae_x = mean_absolute_error(targets[:, 0], predictions[:, 0])
    mae_y = mean_absolute_error(targets[:, 1], predictions[:, 1])
    
    rmse_x = np.sqrt(mse_x)
    rmse_y = np.sqrt(mse_y)
    
    # ì •í™•ë„ ê³„ì‚° (ì„ê³„ê°’ë³„)
    thresholds = [0.1, 0.05, 0.01]
    accuracies = {}
    
    for threshold in thresholds:
        # ì „ì²´ ì •í™•ë„ (ëª¨ë“  ì¶•ì´ ì„ê³„ê°’ ë‚´)
        all_axes_success = np.all(np.abs(predictions - targets) < threshold, axis=1)
        accuracies[f'accuracy_{threshold}'] = np.mean(all_axes_success) * 100
        
        # ê°œë³„ ì¶• ì •í™•ë„
        for i, axis_name in enumerate(['linear_x', 'linear_y']):
            axis_success = np.abs(predictions[:, i] - targets[:, i]) < threshold
            accuracies[f'{axis_name}_{threshold}'] = np.mean(axis_success) * 100
    
    # ì¶”ê°€ ë©”íŠ¸ë¦­
    # í‰ê·  ì˜¤ì°¨ ê¸°ë°˜ ì„±ê³µë¥ 
    mean_error = np.mean(np.abs(predictions - targets), axis=1)
    for threshold in thresholds:
        accuracies[f'mean_error_{threshold}'] = np.mean(mean_error < threshold) * 100
    
    # ê°€ì¤‘ í‰ê·  ì˜¤ì°¨ (linear_xì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
    weighted_error = 0.7 * np.abs(predictions[:, 0] - targets[:, 0]) + 0.3 * np.abs(predictions[:, 1] - targets[:, 1])
    for threshold in thresholds:
        accuracies[f'weighted_error_{threshold}'] = np.mean(weighted_error < threshold) * 100
    
    return {
        'mse': mse,
        'mae': mae,
        'r2': r2,
        'rmse': rmse,
        'mse_x': mse_x,
        'mse_y': mse_y,
        'mae_x': mae_x,
        'mae_y': mae_y,
        'rmse_x': rmse_x,
        'rmse_y': rmse_y,
        'accuracies': accuracies,
        'mean_error': np.mean(mean_error),
        'weighted_error': np.mean(weighted_error)
    }

def evaluate_simplified_model(model, data_loader, device, model_name="Simplified 2D Model"):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_episode_ids = []
    
    logger.info(f"ğŸ” {model_name} í‰ê°€ ì¤‘...")
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc=f"Evaluating {model_name}"):
            images = batch['image'].to(device)
            actions = batch['action'].to(device)
            texts = batch['text']
            episode_ids = batch['episode_id']
            
            # ëª¨ë¸ ì˜ˆì¸¡
            predicted_actions = model(images, texts)
            
            # ê²°ê³¼ ì €ì¥
            all_predictions.extend(predicted_actions.cpu().numpy())
            all_targets.extend(actions.cpu().numpy())
            all_episode_ids.extend(episode_ids)
    
    # numpy ë°°ì—´ë¡œ ë³€í™˜
    predictions = np.array(all_predictions)
    targets = np.array(all_targets)
    
    # ìƒì„¸ ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = calculate_detailed_metrics(predictions, targets)
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info(f"ğŸ“Š {model_name} í‰ê°€ ê²°ê³¼:")
    logger.info(f"   MSE: {metrics['mse']:.6f}")
    logger.info(f"   MAE: {metrics['mae']:.6f}")
    logger.info(f"   RÂ²: {metrics['r2']:.6f}")
    logger.info(f"   RMSE: {metrics['rmse']:.6f}")
    logger.info(f"   MSE X: {metrics['mse_x']:.6f}")
    logger.info(f"   MSE Y: {metrics['mse_y']:.6f}")
    logger.info(f"   MAE X: {metrics['mae_x']:.6f}")
    logger.info(f"   MAE Y: {metrics['mae_y']:.6f}")
    
    # ì •í™•ë„ ì¶œë ¥
    for threshold in [0.1, 0.05, 0.01]:
        logger.info(f"   ì •í™•ë„ ({threshold}): {metrics['accuracies'][f'accuracy_{threshold}']:.2f}%")
        logger.info(f"     - linear_x: {metrics['accuracies'][f'linear_x_{threshold}']:.2f}%")
        logger.info(f"     - linear_y: {metrics['accuracies'][f'linear_y_{threshold}']:.2f}%")
    
    return {
        'predictions': predictions,
        'targets': targets,
        'episode_ids': all_episode_ids,
        'metrics': metrics
    }

def create_detailed_visualizations(results, save_path):
    """ìƒì„¸í•œ ì‹œê°í™” ìƒì„±"""
    
    predictions = results['predictions']
    targets = results['targets']
    metrics = results['metrics']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Simplified 2D Action Model - Detailed Evaluation Results', fontsize=16, fontweight='bold')
    
    # 1. Scatter plot: Predicted vs Actual
    axes[0, 0].scatter(targets[:, 0], predictions[:, 0], alpha=0.6, label='X-axis', s=50)
    axes[0, 0].scatter(targets[:, 1], predictions[:, 1], alpha=0.6, label='Y-axis', s=50)
    axes[0, 0].plot([targets.min(), targets.max()], [targets.min(), targets.max()], 'r--', lw=2)
    axes[0, 0].set_xlabel('Actual Actions')
    axes[0, 0].set_ylabel('Predicted Actions')
    axes[0, 0].set_title('Predicted vs Actual Actions')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Error distribution
    errors = predictions - targets
    axes[0, 1].hist(errors[:, 0], bins=30, alpha=0.7, label='X-axis Error', color='blue')
    axes[0, 1].hist(errors[:, 1], bins=30, alpha=0.7, label='Y-axis Error', color='orange')
    axes[0, 1].set_xlabel('Prediction Error')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title('Error Distribution')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Action space visualization
    axes[0, 2].scatter(targets[:, 0], targets[:, 1], alpha=0.6, label='Actual', s=50, color='blue')
    axes[0, 2].scatter(predictions[:, 0], predictions[:, 1], alpha=0.6, label='Predicted', s=50, color='red')
    axes[0, 2].set_xlabel('X-axis Action')
    axes[0, 2].set_ylabel('Y-axis Action')
    axes[0, 2].set_title('Action Space: Actual vs Predicted')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Metrics comparison
    metric_names = ['MSE', 'MAE', 'RÂ²', 'RMSE']
    metric_values = [metrics['mse'], metrics['mae'], metrics['r2'], metrics['rmse']]
    
    bars = axes[1, 0].bar(metric_names, metric_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
    axes[1, 0].set_title('Overall Performance Metrics')
    axes[1, 0].set_ylabel('Value')
    
    # Add value labels on bars
    for bar, value in zip(bars, metric_values):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.001,
                        f'{value:.4f}', ha='center', va='bottom')
    
    # 5. Accuracy comparison
    thresholds = [0.1, 0.05, 0.01]
    accuracy_values = [metrics['accuracies'][f'accuracy_{t}'] for t in thresholds]
    
    bars2 = axes[1, 1].bar([f'{t}' for t in thresholds], accuracy_values, color=['green', 'orange', 'red'])
    axes[1, 1].set_title('Accuracy by Threshold')
    axes[1, 1].set_xlabel('Threshold')
    axes[1, 1].set_ylabel('Accuracy (%)')
    
    for bar, value in zip(bars2, accuracy_values):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{value:.1f}%', ha='center', va='bottom')
    
    # 6. Axis-wise performance
    axis_metrics = ['MAE', 'MSE', 'RMSE']
    x_values = [metrics['mae_x'], metrics['mse_x'], metrics['rmse_x']]
    y_values = [metrics['mae_y'], metrics['mse_y'], metrics['rmse_y']]
    
    x = np.arange(len(axis_metrics))
    width = 0.35
    
    bars3 = axes[1, 2].bar(x - width/2, x_values, width, label='X-axis', color='blue', alpha=0.7)
    bars4 = axes[1, 2].bar(x + width/2, y_values, width, label='Y-axis', color='orange', alpha=0.7)
    
    axes[1, 2].set_title('Axis-wise Performance')
    axes[1, 2].set_xlabel('Metric')
    axes[1, 2].set_ylabel('Value')
    axes[1, 2].set_xticks(x)
    axes[1, 2].set_xticklabels(axis_metrics)
    axes[1, 2].legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ğŸ“ˆ ìƒì„¸ ì‹œê°í™” ì €ì¥: {save_path}")

def create_performance_summary(results, save_path):
    """ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
    
    metrics = results['metrics']
    
    # ìš”ì•½ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    summary_data = {
        'Metric': [
            'Overall MAE', 'Overall MSE', 'Overall RMSE', 'RÂ² Score',
            'X-axis MAE', 'X-axis MSE', 'X-axis RMSE',
            'Y-axis MAE', 'Y-axis MSE', 'Y-axis RMSE',
            'Accuracy (0.1)', 'Accuracy (0.05)', 'Accuracy (0.01)',
            'X-axis Accuracy (0.1)', 'X-axis Accuracy (0.05)', 'X-axis Accuracy (0.01)',
            'Y-axis Accuracy (0.1)', 'Y-axis Accuracy (0.05)', 'Y-axis Accuracy (0.01)'
        ],
        'Value': [
            f"{metrics['mae']:.6f}",
            f"{metrics['mse']:.6f}",
            f"{metrics['rmse']:.6f}",
            f"{metrics['r2']:.6f}",
            f"{metrics['mae_x']:.6f}",
            f"{metrics['mse_x']:.6f}",
            f"{metrics['rmse_x']:.6f}",
            f"{metrics['mae_y']:.6f}",
            f"{metrics['mse_y']:.6f}",
            f"{metrics['rmse_y']:.6f}",
            f"{metrics['accuracies']['accuracy_0.1']:.2f}%",
            f"{metrics['accuracies']['accuracy_0.05']:.2f}%",
            f"{metrics['accuracies']['accuracy_0.01']:.2f}%",
            f"{metrics['accuracies']['linear_x_0.1']:.2f}%",
            f"{metrics['accuracies']['linear_x_0.05']:.2f}%",
            f"{metrics['accuracies']['linear_x_0.01']:.2f}%",
            f"{metrics['accuracies']['linear_y_0.1']:.2f}%",
            f"{metrics['accuracies']['linear_y_0.05']:.2f}%",
            f"{metrics['accuracies']['linear_y_0.01']:.2f}%"
        ]
    }
    
    df = pd.DataFrame(summary_data)
    
    # HTML í…Œì´ë¸”ë¡œ ì €ì¥
    html_content = f"""
    <html>
    <head>
        <title>Simplified 2D Model Performance Summary</title>
        <style>
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ border: 1px solid black; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            .metric {{ font-weight: bold; }}
            .value {{ font-family: monospace; }}
        </style>
    </head>
    <body>
        <h1>Simplified 2D Action Model Performance Summary</h1>
        {df.to_html(index=False, classes='performance-table')}
    </body>
    </html>
    """
    
    with open(save_path, 'w') as f:
        f.write(html_content)
    
    logger.info(f"ğŸ“‹ ì„±ëŠ¥ ìš”ì•½ ì €ì¥: {save_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Evaluate Simplified 2D Action Model')
    parser.add_argument('--model_path', type=str, required=True, help='Path to trained model checkpoint')
    parser.add_argument('--data_path', type=str, required=True, help='Path to evaluation data')
    parser.add_argument('--output_dir', type=str, default='evaluation_results', help='Output directory for results')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for evaluation')
    parser.add_argument('--device', type=str, default='cuda', help='Device to use (cuda/cpu)')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu'
    logger.info(f"Using device: {device}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    logger.info("Loading Kosmos2 processor...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ëª¨ë¸ ë¡œë“œ
    model, checkpoint = load_simplified_model(args.model_path, processor, device)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    logger.info("Creating evaluation data loader...")
    _, val_loader, test_loader = create_basic_augmentation_data_loaders(
        data_path=args.data_path,
        processor=processor,
        batch_size=args.batch_size,
        train_split=0.7,
        val_split=0.15,
        test_split=0.15
    )
    
    # ëª¨ë¸ í‰ê°€
    logger.info("Starting evaluation...")
    results = evaluate_simplified_model(model, test_loader, device, "Simplified 2D Model")
    
    # ê²°ê³¼ ì €ì¥
    results_path = output_path / 'simplified_model_evaluation_results.json'
    with open(results_path, 'w') as f:
        json.dump(results['metrics'], f, indent=2)
    
    # ì‹œê°í™” ìƒì„±
    plot_path = output_path / 'simplified_model_evaluation_plots.png'
    create_detailed_visualizations(results, plot_path)
    
    # ì„±ëŠ¥ ìš”ì•½ ìƒì„±
    summary_path = output_path / 'simplified_model_performance_summary.html'
    create_performance_summary(results, summary_path)
    
    logger.info(f"âœ… í‰ê°€ ì™„ë£Œ!")
    logger.info(f"   ê²°ê³¼ ì €ì¥: {results_path}")
    logger.info(f"   ì‹œê°í™” ì €ì¥: {plot_path}")
    logger.info(f"   ì„±ëŠ¥ ìš”ì•½ ì €ì¥: {summary_path}")
    logger.info(f"   ìµœì¢… MAE: {results['metrics']['mae']:.6f}")
    logger.info(f"   ìµœì¢… RÂ² Score: {results['metrics']['r2']:.6f}")

if __name__ == "__main__":
    main()
