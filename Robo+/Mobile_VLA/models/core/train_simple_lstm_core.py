#!/usr/bin/env python3
"""
ğŸ¯ Simple LSTM Model Training (RoboVLMs Style)
RoboVLMs ì•¡ì…˜ í—¤ë“œ ìŠ¤íƒ€ì¼ì˜ ë‹¨ìˆœí•œ LSTM ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import argparse
import logging
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from models.simple_lstm_model import create_simple_lstm_model
from data.mobile_dataset import MobileVLADataset

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def train_simple_lstm(args):
    """ë‹¨ìˆœí•œ LSTM ëª¨ë¸ í›ˆë ¨ (RoboVLMs ìŠ¤íƒ€ì¼)"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸš€ ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_dataset = MobileVLADataset(
        data_dir=args.data_path,
        sequence_length=18,
        image_size=(224, 224),
        normalize_actions=True
    )
    
    val_dataset = MobileVLADataset(
        data_dir=args.data_path,
        sequence_length=18,
        image_size=(224, 224),
        normalize_actions=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
    logger.info(f"   - í›ˆë ¨ì…‹: {len(train_dataset)} ìƒ˜í”Œ")
    logger.info(f"   - ê²€ì¦ì…‹: {len(val_dataset)} ìƒ˜í”Œ")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    
    # ëª¨ë¸ ìƒì„±
    logger.info("ğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
    model, trainer = create_simple_lstm_model()
    model = model.to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
    logger.info(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    logger.info(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
    logger.info(f"   - í›ˆë ¨ ë¹„ìœ¨: {trainable_params/total_params*100:.1f}%")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses = []
    train_maes = []
    val_losses = []
    val_maes = []
    best_val_mae = float('inf')
    
    # í›ˆë ¨ ë£¨í”„
    logger.info("ğŸ‹ï¸ í›ˆë ¨ ì‹œì‘...")
    for epoch in range(args.num_epochs):
        # í›ˆë ¨
        model.train()
        epoch_train_loss = 0.0
        epoch_train_mae = 0.0
        
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.num_epochs} [Train]")
        for batch_idx, batch in enumerate(train_pbar):
            # í›ˆë ¨ ìŠ¤í…
            loss, mae = trainer.train_step(batch)
            
            epoch_train_loss += loss
            epoch_train_mae += mae
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            train_pbar.set_postfix({
                'Loss': f'{loss:.4f}',
                'MAE': f'{mae:.4f}'
            })
        
        # í‰ê·  ê³„ì‚°
        avg_train_loss = epoch_train_loss / len(train_loader)
        avg_train_mae = epoch_train_mae / len(train_loader)
        
        # ê²€ì¦
        model.eval()
        epoch_val_loss = 0.0
        epoch_val_mae = 0.0
        
        val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{args.num_epochs} [Val]")
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_pbar):
                # ê²€ì¦ ìŠ¤í…
                loss, mae = trainer.validate_step(batch)
                
                epoch_val_loss += loss
                epoch_val_mae += mae
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                val_pbar.set_postfix({
                    'Loss': f'{loss:.4f}',
                    'MAE': f'{mae:.4f}'
                })
        
        # í‰ê·  ê³„ì‚°
        avg_val_loss = epoch_val_loss / len(val_loader)
        avg_val_mae = epoch_val_mae / len(val_loader)
        
        # ê¸°ë¡ ì €ì¥
        train_losses.append(avg_train_loss)
        train_maes.append(avg_train_mae)
        val_losses.append(avg_val_loss)
        val_maes.append(avg_val_mae)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“Š Epoch {epoch+1}/{args.num_epochs}:")
        logger.info(f"   - Train Loss: {avg_train_loss:.4f}")
        logger.info(f"   - Train MAE: {avg_train_mae:.4f}")
        logger.info(f"   - Val Loss: {avg_val_loss:.4f}")
        logger.info(f"   - Val MAE: {avg_val_mae:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (MAE ê¸°ì¤€)
        if avg_val_mae < best_val_mae:
            best_val_mae = avg_val_mae
            best_model_path = os.path.join(args.output_dir, 'best_simple_lstm_model.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'val_mae': best_val_mae,
                'args': args
            }, best_model_path)
            logger.info(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_model_path}")
        
        # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % args.save_interval == 0:
            checkpoint_path = os.path.join(args.output_dir, f'simple_lstm_epoch_{epoch+1}.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'val_mae': avg_val_mae,
                'args': args
            }, checkpoint_path)
            logger.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(args.output_dir, 'final_simple_lstm_model.pth')
    torch.save({
        'epoch': args.num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': trainer.optimizer.state_dict(),
        'val_mae': avg_val_mae,
        'args': args
    }, final_model_path)
    
    # í›ˆë ¨ ê²°ê³¼ ì €ì¥
    results = {
        'train_losses': train_losses,
        'train_maes': train_maes,
        'val_losses': val_losses,
        'val_maes': val_maes,
        'best_val_mae': best_val_mae,
        'final_val_mae': avg_val_mae,
        'args': vars(args)
    }
    
    results_path = os.path.join(args.output_dir, 'simple_lstm_training_results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # í›ˆë ¨ ê³¡ì„  í”Œë¡¯
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.plot(train_maes, label='Train MAE')
    plt.plot(val_maes, label='Val MAE')
    plt.title('Training and Validation MAE (2D Navigation)')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plot_path = os.path.join(args.output_dir, 'simple_lstm_training_curves.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    logger.info("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    logger.info(f"   - ìµœê³  ê²€ì¦ MAE: {best_val_mae:.4f}")
    logger.info(f"   - ìµœì¢… ê²€ì¦ MAE: {avg_val_mae:.4f}")
    logger.info(f"   - ê²°ê³¼ ì €ì¥: {results_path}")
    logger.info(f"   - í”Œë¡¯ ì €ì¥: {plot_path}")

def main():
    parser = argparse.ArgumentParser(description="Simple LSTM Model Training (RoboVLMs Style)")
    
    # ë°ì´í„° ê´€ë ¨
    parser.add_argument('--data_path', type=str, required=True,
                       help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    
    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--batch_size', type=int, default=2,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_epochs', type=int, default=15,
                       help='í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    
    # ì‹œìŠ¤í…œ ê´€ë ¨
    parser.add_argument('--device', type=str, default='cuda',
                       help='ë””ë°”ì´ìŠ¤ (cuda/cpu)')
    parser.add_argument('--num_workers', type=int, default=2,
                       help='ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜')
    
    # ì¶œë ¥ ê´€ë ¨
    parser.add_argument('--output_dir', type=str, default='simple_lstm_results',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--save_interval', type=int, default=5,
                       help='ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©')
    
    args = parser.parse_args()
    
    # í›ˆë ¨ ì‹¤í–‰
    train_simple_lstm(args)

if __name__ == "__main__":
    main()