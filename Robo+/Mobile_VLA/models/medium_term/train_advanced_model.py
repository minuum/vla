#!/usr/bin/env python3
"""
Case 3: ì¤‘ê¸° ì ìš© (Medium-term Optimization)
ê³ ê¸‰ RoboVLMs ê¸°ëŠ¥ + ë©€í‹°ëª¨ë‹¬ ìœµí•© ëª¨ë¸ í›ˆë ¨
"""
import os
import sys
import argparse
import logging
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoProcessor, AutoModel
from tqdm import tqdm
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import h5py
from PIL import Image

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

from advanced_multimodal_model import AdvancedMultimodalModel, AdvancedMultimodalTrainer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedDataset:
    """ê³ ê¸‰ ë°ì´í„°ì…‹ - ë©€í‹°ëª¨ë‹¬ ìœµí•©ì„ ìœ„í•œ ë°ì´í„° ë¡œë”"""
    
    def __init__(self, data_path, processor, frame_selection='first'):
        self.data_path = data_path
        self.processor = processor
        self.frame_selection = frame_selection
        self.data = self._load_data()
        logger.info(f"âœ… Advanced Dataset ì´ˆê¸°í™” ì™„ë£Œ:")
        logger.info(f"   - ë°ì´í„° ê²½ë¡œ: {data_path}")
        logger.info(f"   - ìƒ˜í”Œ ìˆ˜: {len(self.data)}")

    def _load_data(self):
        """H5 íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        data = []
        data_path = Path(self.data_path)
        h5_files = list(data_path.glob("*.h5"))
        logger.info(f"ğŸ“ H5 íŒŒì¼ ìˆ˜: {len(h5_files)}")

        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]
                        actions = f['actions'][:]
                        
                        for frame_idx in range(len(images)):
                            if self.frame_selection == 'first' and frame_idx != 0:
                                continue
                            elif self.frame_selection == 'random' and frame_idx != np.random.randint(0, len(images) - 1):
                                continue
                            
                            data.append({
                                'image': images[frame_idx],
                                'action': actions[frame_idx][:2],  # 2D ì•¡ì…˜ë§Œ
                                'episode_id': len(data),
                                'frame_id': frame_idx
                            })
                            if self.frame_selection == 'first':
                                break
            except Exception as e:
                logger.error(f"âŒ {h5_file} ë¡œë“œ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ğŸ“Š ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: {len(data)}")
        return data

    def __getitem__(self, idx):
        item = self.data[idx]
        image = Image.fromarray(item['image']).convert('RGB')
        action = torch.tensor(item['action'], dtype=torch.float32)
        
        # ê³ ê¸‰ ì‹œë‚˜ë¦¬ì˜¤ ëª…ë ¹ì–´ ìƒì„±
        scenario = self._extract_scenario_from_filename(item.get('filename', ''))
        text = f"Navigate the robot to {scenario} location."
        
        return {
            'image': image,
            'action': action,
            'text': text,
            'episode_id': item['episode_id']
        }

    def _extract_scenario_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ"""
        if 'hori' in filename:
            return 'horizontal'
        elif 'vert' in filename:
            return 'vertical'
        elif 'close' in filename:
            return 'close'
        elif 'medium' in filename:
            return 'medium'
        elif 'far' in filename:
            return 'far'
        else:
            return 'target'

    def __len__(self):
        return len(self.data)

def custom_collate_fn(batch):
    """PIL ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì»¤ìŠ¤í…€ collate í•¨ìˆ˜"""
    images = [item['image'] for item in batch]
    actions = torch.stack([item['action'] for item in batch])
    texts = [item['text'] for item in batch]
    episode_ids = [item['episode_id'] for item in batch]
    
    return {
        'image': images,  # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        'action': actions,
        'text': texts,
        'episode_id': episode_ids
    }

def create_advanced_data_loaders(data_path, processor, batch_size=2,
                                train_split=0.7, val_split=0.15, test_split=0.15):
    """ê³ ê¸‰ ë°ì´í„° ë¡œë” ìƒì„±"""
    full_dataset = AdvancedDataset(
        data_path=data_path,
        processor=processor,
        frame_selection='first'
    )
    
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = int(total_size * val_split)
    test_size = total_size - train_size - val_size
    
    logger.info(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :")
    logger.info(f"   - ì „ì²´: {total_size}")
    logger.info(f"   - í›ˆë ¨: {train_size}")
    logger.info(f"   - ê²€ì¦: {val_size}")
    logger.info(f"   - í…ŒìŠ¤íŠ¸: {test_size}")
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size, test_size]
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        collate_fn=custom_collate_fn
    )
    
    logger.info(f"âœ… Advanced Data Loaders ìƒì„± ì™„ë£Œ")
    return train_loader, val_loader, test_loader

def train_advanced_model(data_path, output_dir, num_epochs=50, batch_size=2,
                        learning_rate=3e-5, weight_decay=1e-4, patience=5):
    """ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨"""
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸš€ Case 3 í›ˆë ¨ ì‹œì‘ - ë””ë°”ì´ìŠ¤: {device}")
    
    # Kosmos2 í”„ë¡œì„¸ì„œ ë¡œë“œ
    logger.info("ğŸ“¥ Kosmos2 í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ê³ ê¸‰ ë°ì´í„° ë¡œë” ìƒì„±
    logger.info("ğŸ“Š ê³ ê¸‰ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader, test_loader = create_advanced_data_loaders(
        data_path=data_path,
        processor=processor,
        batch_size=batch_size
    )
    
    # ëª¨ë¸ ë° í›ˆë ¨ê¸° ìƒì„±
    logger.info("ğŸ¤– ê³ ê¸‰ ëª¨ë¸ ë° í›ˆë ¨ê¸° ìƒì„± ì¤‘...")
    model = AdvancedMultimodalModel(
        processor=processor,
        vision_dim=1024,
        language_dim=2048,  # Kosmos2 text ëª¨ë¸ ì¶œë ¥ ì°¨ì›
        action_dim=2,
        hidden_dim=512,  # ë” í° hidden_dim
        dropout=0.3,
        use_hierarchical_planning=True
    ).to(device)
    
    trainer = AdvancedMultimodalTrainer(
        model=model,
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        device=device
    )
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # í›ˆë ¨ ì„¤ì •
    logger.info("ğŸ¯ í›ˆë ¨ ì„¤ì •:")
    logger.info(f"   - ì—í¬í¬: {num_epochs}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   - í•™ìŠµë¥ : {learning_rate}")
    logger.info(f"   - Weight Decay: {weight_decay}")
    logger.info(f"   - ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬: {patience}")
    
    # í›ˆë ¨ ë£¨í”„
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        logger.info(f"\nğŸ“ˆ Epoch {epoch+1}/{num_epochs}")
        
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        train_losses = []
        train_pbar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")
        
        for batch in train_pbar:
            try:
                loss = trainer.train_step(batch)
                train_losses.append(loss)
                train_pbar.set_postfix({'loss': f'{loss:.4f}'})
            except Exception as e:
                logger.error(f"âŒ í›ˆë ¨ ë°°ì¹˜ ì˜¤ë¥˜: {e}")
                continue
        
        avg_train_loss = np.mean(train_losses)
        
        # ê²€ì¦ ë‹¨ê³„
        model.eval()
        val_losses = []
        val_pbar = tqdm(val_loader, desc=f"Validation Epoch {epoch+1}")
        
        with torch.no_grad():
            for batch in val_pbar:
                try:
                    loss, mae = trainer.validate_step(batch)
                    val_losses.append(loss)
                    val_pbar.set_postfix({'loss': f'{loss:.4f}', 'mae': f'{mae:.4f}'})
                except Exception as e:
                    logger.error(f"âŒ ê²€ì¦ ë°°ì¹˜ ì˜¤ë¥˜: {e}")
                    continue
        
        avg_val_loss = np.mean(val_losses)
        
        # ë¡œê¹…
        logger.info(f"ğŸ“Š Epoch {epoch+1} ê²°ê³¼:")
        logger.info(f"   - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
        logger.info(f"   - ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        
        # ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            model_path = os.path.join(output_dir, f"best_model_epoch_{epoch+1}.pth")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'best_val_loss': best_val_loss
            }, model_path)
            logger.info(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {model_path}")
        else:
            patience_counter += 1
            logger.info(f"â³ ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°: {patience_counter}/{patience}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if patience_counter >= patience:
            logger.info(f"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ! {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
        
        # ì¤‘ê°„ ëª¨ë¸ ì €ì¥ (10 ì—í¬í¬ë§ˆë‹¤)
        if (epoch + 1) % 10 == 0:
            checkpoint_path = os.path.join(output_dir, f"checkpoint_epoch_{epoch+1}.pth")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss
            }, checkpoint_path)
            logger.info(f"ğŸ’¾ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = os.path.join(output_dir, "final_model.pth")
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': trainer.optimizer.state_dict(),
        'train_loss': avg_train_loss,
        'val_loss': avg_val_loss,
        'best_val_loss': best_val_loss
    }, final_model_path)
    logger.info(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # í›ˆë ¨ ê²°ê³¼ ì €ì¥
    results = {
        'final_epoch': epoch + 1,
        'best_val_loss': best_val_loss,
        'final_train_loss': avg_train_loss,
        'final_val_loss': avg_val_loss,
        'training_completed': True,
        'early_stopped': patience_counter >= patience
    }
    
    results_path = os.path.join(output_dir, "training_results.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"ğŸ“Š í›ˆë ¨ ê²°ê³¼ ì €ì¥: {results_path}")
    
    logger.info("ğŸ‰ Case 3 í›ˆë ¨ ì™„ë£Œ!")
    return model, trainer

def main():
    parser = argparse.ArgumentParser(description="Case 3: ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í›ˆë ¨")
    parser.add_argument("--data_path", type=str, required=True,
                       help="ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="case3_results",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--num_epochs", type=int, default=50,
                       help="í›ˆë ¨ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=2,
                       help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=3e-5,
                       help="í•™ìŠµë¥ ")
    parser.add_argument("--weight_decay", type=float, default=1e-4,
                       help="Weight decay")
    parser.add_argument("--patience", type=int, default=5,
                       help="ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬")
    
    args = parser.parse_args()
    
    # í›ˆë ¨ ì‹¤í–‰
    model, trainer = train_advanced_model(
        data_path=args.data_path,
        output_dir=args.output_dir,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        patience=args.patience
    )

if __name__ == "__main__":
    main()
