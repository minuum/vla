"""
ğŸš€ ì™„ì „íˆ ìˆ˜ì •ëœ RoboVLMs Style Single Image VLA Model
ì°¨ì›ê³¼ ë°ì´í„°íƒ€ì… ë¬¸ì œë¥¼ ëª¨ë‘ í•´ê²°í•œ ì™„ì „í•œ ëª¨ë¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel
from typing import Optional, Tuple, Dict, Any
import numpy as np
import PIL.Image

# ìˆ˜ì •ëœ ì»´í¬ë„ŒíŠ¸ë“¤ ì„í¬íŠ¸
from fixed_claw_matrix import (
    FixedClawMatrixFusion, 
    FixedHierarchicalPlanner, 
    FixedAdvancedAttention
)

class FixedRoboVLMStyleSingleImageModel(nn.Module):
    """
    ğŸ¯ ì™„ì „íˆ ìˆ˜ì •ëœ RoboVLMs ìŠ¤íƒ€ì¼ ëª¨ë¸
    - ì…ë ¥: ë‹¨ì¼ ì´ë¯¸ì§€ 1ì¥
    - ì¶œë ¥: ë‹¨ì¼ ì•¡ì…˜ (3D)
    - ê³ ê¸‰ ê¸°ëŠ¥: Fixed Claw Matrix, Hierarchical Planning, Advanced Attention
    - ëª¨ë“  ì°¨ì› ë¬¸ì œ í•´ê²°ë¨
    """
    
    def __init__(
        self,
        processor,
        vision_dim: int = 1024,
        language_dim: int = 1024,
        action_dim: int = 3,
        hidden_dim: int = 512,
        dropout: float = 0.2,
        use_claw_matrix: bool = True,
        use_hierarchical: bool = True,
        use_advanced_attention: bool = True,
        z_axis_weight: float = 0.05
    ):
        super().__init__()
        
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        self.z_axis_weight = z_axis_weight
        
        # ê¸°ëŠ¥ ì‚¬ìš© í”Œë˜ê·¸
        self.use_claw_matrix = use_claw_matrix
        self.use_hierarchical = use_hierarchical
        self.use_advanced_attention = use_advanced_attention
        
        # Kosmos2 ëª¨ë¸ (RoboVLMs ìŠ¤íƒ€ì¼)
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        for param in self.kosmos.parameters():
            param.requires_grad = True
        
        # ë™ì  feature adapterë“¤
        self.vision_adapter = nn.Linear(1024, vision_dim)  # Kosmos2 vision ì¶œë ¥ -> vision_dim
        self.language_adapter = None  # ë™ì  ìƒì„±
        
        # ê°•í™”ëœ ì •ê·œí™”
        self.layer_norm_vision = nn.LayerNorm(vision_dim)
        self.layer_norm_language = nn.LayerNorm(language_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        self.dropout_fusion = nn.Dropout(dropout)
        
        # ê¸°ë³¸ ìœµí•© ì–´ëŒ‘í„°
        self.fusion_adapter = nn.Linear(vision_dim + language_dim, hidden_dim)
        
        # Fixed Claw Matrix (ìˆ˜ì •ëœ ë²„ì „)
        if use_claw_matrix:
            self.claw_matrix = FixedClawMatrixFusion(
                input_dim=vision_dim + language_dim,
                hidden_dim=hidden_dim,
                dropout=dropout
            )
        
        # Fixed Hierarchical Planning (ìˆ˜ì •ëœ ë²„ì „)
        if use_hierarchical:
            self.hierarchical_planner = FixedHierarchicalPlanner(
                hidden_dim=hidden_dim,
                action_dim=action_dim,
                dropout=dropout
            )
        
        # Fixed Advanced Attention (ìˆ˜ì •ëœ ë²„ì „)
        if use_advanced_attention:
            self.advanced_attention = FixedAdvancedAttention(
                hidden_dim=hidden_dim,
                dropout=dropout
            )
        
        # ìµœì¢… ì•¡ì…˜ í—¤ë“œ (RoboVLMs ìŠ¤íƒ€ì¼)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        # Zì¶• ê°€ì¤‘ì¹˜ (Final Fixed ìŠ¤íƒ€ì¼ ìœ ì§€)
        self.z_axis_weight_layer = nn.Parameter(torch.tensor([1.0, 1.0, z_axis_weight]))
        
    def _preprocess_image(self, images: torch.Tensor) -> list:
        """ì´ë¯¸ì§€ë¥¼ PIL í˜•íƒœë¡œ ì „ì²˜ë¦¬"""
        # images: [batch_size, 3, H, W] í˜•íƒœì˜ í…ì„œ
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™” (í•„ìš”í•œ ê²½ìš°)
        if images.min() < 0 or images.max() > 1:
            images = (images - images.min()) / (images.max() - images.min())
        
        # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pil_images = []
        for i in range(images.shape[0]):
            # [3, H, W] -> [H, W, 3]
            img_np = images[i].permute(1, 2, 0).cpu().numpy()
            # [0, 1] -> [0, 255]
            img_np = (img_np * 255).astype(np.uint8)
            # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pil_img = PIL.Image.fromarray(img_np)
            pil_images.append(pil_img)
        
        return pil_images
        
    def extract_vision_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ë‹¨ì¼ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (RoboVLMs ìŠ¤íƒ€ì¼)"""
        # single_image: [batch_size, 3, H, W] - ë‹¨ì¼ ì´ë¯¸ì§€
        
        batch_size = single_image.shape[0]
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        pil_images = self._preprocess_image(single_image)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        vision_features_list = []
        
        for pil_img in pil_images:
            with torch.no_grad():
                inputs = self.processor(images=pil_img, return_tensors="pt")
                inputs = {k: v.to(single_image.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
                
                # Kosmos2 vision ëª¨ë¸ ì‚¬ìš©
                if 'pixel_values' in inputs:
                    vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                    vision_feat = vision_outputs.pooler_output  # [1, 1024]
                else:
                    # fallback
                    vision_feat = torch.zeros(1, 1024).to(single_image.device)
                
                vision_features_list.append(vision_feat)
        
        # ë°°ì¹˜ë¡œ ê²°í•©
        vision_features = torch.cat(vision_features_list, dim=0)  # [batch_size, 1024]
        
        # íŠ¹ì§• ì°¨ì› ì¡°ì •
        vision_features = self.vision_adapter(vision_features)  # [batch_size, vision_dim]
        
        # ê°•í™”ëœ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, text: str, batch_size: int = 1) -> torch.Tensor:
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ (RoboVLMs ìŠ¤íƒ€ì¼)"""
        
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬
        with torch.no_grad():
            inputs = self.processor(text=text, return_tensors="pt")
            inputs = {k: v.to(next(self.parameters()).device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # Kosmos2 í…ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš©
            language_outputs = self.kosmos.text_model(**inputs)
            language_features = language_outputs.last_hidden_state.mean(dim=1)  # [1, hidden_size]
        
        # ë°°ì¹˜ ì°¨ì› í™•ì¥
        language_features = language_features.expand(batch_size, -1)
        
        # ì°¨ì› ì¡°ì • (ë™ì  ì–´ëŒ‘í„° ìƒì„±)
        if language_features.shape[-1] != self.language_dim:
            if self.language_adapter is None:
                self.language_adapter = nn.Linear(
                    language_features.shape[-1], 
                    self.language_dim
                ).to(language_features.device)
            language_features = self.language_adapter(language_features)
        
        # ê°•í™”ëœ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, single_image: torch.Tensor, text: str) -> torch.Tensor:
        """ìˆœì „íŒŒ (RoboVLMs ìŠ¤íƒ€ì¼)"""
        # single_image: [batch_size, 3, H, W] - ë‹¨ì¼ ì´ë¯¸ì§€
        # text: ë¬¸ìì—´
        
        batch_size = single_image.shape[0]
        
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(single_image)
        language_features = self.extract_language_features(text, batch_size)
        
        # ê¸°ë³¸ ìœµí•©
        fused_features = torch.cat([vision_features, language_features], dim=-1)
        
        # Claw Matrix ì ìš© (ìˆ˜ì •ëœ ë²„ì „)
        if self.use_claw_matrix and hasattr(self, 'claw_matrix'):
            fused_features = self.claw_matrix(fused_features)  # input_dim -> hidden_dim
        else:
            # Claw Matrixë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ìœµí•©
            fused_features = self.fusion_adapter(fused_features)  # input_dim -> hidden_dim
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        fused_features = self.layer_norm_fusion(fused_features)
        fused_features = self.dropout_fusion(fused_features)
        
        # Advanced Attention ì ìš© (ìˆ˜ì •ëœ ë²„ì „)
        if self.use_advanced_attention and hasattr(self, 'advanced_attention'):
            fused_features = self.advanced_attention(fused_features)  # hidden_dim -> hidden_dim
        
        # Hierarchical Planning ì ìš© (ìˆ˜ì •ëœ ë²„ì „)
        if self.use_hierarchical and hasattr(self, 'hierarchical_planner'):
            fused_features = self.hierarchical_planner(fused_features)  # hidden_dim -> hidden_dim
        
        # ìµœì¢… ì•¡ì…˜ ì˜ˆì¸¡
        actions = self.action_head(fused_features)  # hidden_dim -> action_dim
        
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš© (Final Fixed ìŠ¤íƒ€ì¼)
        actions = actions * self.z_axis_weight_layer.unsqueeze(0)
        
        return actions

def train_fixed_robovlms_model(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,
    weight_decay=1e-4,
    early_stopping_patience=5,
    device='cuda'
):
    """ìˆ˜ì •ëœ RoboVLMs ìŠ¤íƒ€ì¼ ëª¨ë¸ í›ˆë ¨"""
    
    model = model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì € (RoboVLMs ìŠ¤íƒ€ì¼)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (RoboVLMs ìŠ¤íƒ€ì¼)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # ì†ì‹¤ í•¨ìˆ˜ (RoboVLMs ìŠ¤íƒ€ì¼)
    def compute_loss(predicted_actions, target_actions):
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš©
        z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
        weighted_target = target_actions * z_weight.unsqueeze(0)
        weighted_pred = predicted_actions * z_weight.unsqueeze(0)
        
        return F.mse_loss(weighted_pred, weighted_target)
    
    # ì¡°ê¸° ì¢…ë£Œ
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0.0
        
        for batch_idx, batch in enumerate(train_loader):
            # ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥
            images = batch['image']  # [batch, 3, H, W] - ì´ë¯¸ ë‹¨ì¼ ì´ë¯¸ì§€
            actions = batch['action']  # [batch, 3] - ì´ë¯¸ ë‹¨ì¼ ì•¡ì…˜
            
            images = images.float().to(device)
            actions = actions.float().to(device)
            
            optimizer.zero_grad()
            
            try:
                # ì˜ˆì¸¡ (ë‹¨ì¼ ì´ë¯¸ì§€ â†’ ë‹¨ì¼ ì•¡ì…˜)
                predicted_actions = model(images, "Navigate to target")
                
                # ì†ì‹¤ ê³„ì‚°
                loss = compute_loss(predicted_actions, actions)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if batch_idx % 10 == 0:
                    print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    # ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥
                    images = batch['image']  # [batch, 3, H, W]
                    actions = batch['action']  # [batch, 3]
                    
                    images = images.float().to(device)
                    actions = actions.float().to(device)
                    
                    predicted_actions = model(images, "Navigate to target")
                    loss = compute_loss(predicted_actions, actions)
                    val_loss += loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # í‰ê·  ì†ì‹¤
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / max(val_batches, 1)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step()
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs} ì™„ë£Œ:")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        print(f"   - í•™ìŠµë¥ : {scheduler.get_last_lr()[0]:.6f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': {
                    'learning_rate': learning_rate,
                    'weight_decay': weight_decay,
                    'dropout': model.dropout,
                    'z_axis_weight': model.z_axis_weight,
                    'use_claw_matrix': model.use_claw_matrix,
                    'use_hierarchical': model.use_hierarchical,
                    'use_advanced_attention': model.use_advanced_attention
                }
            }, 'fixed_robovlms_model_best.pth')
            print(f"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"   â° ì¡°ê¸° ì¢…ë£Œ (Patience: {early_stopping_patience})")
                break
    
    return model

# ğŸš€ ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ì´ˆê¸°í™”
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    model = FixedRoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=0.2,
        use_claw_matrix=True,
        use_hierarchical=True,
        use_advanced_attention=True,
        z_axis_weight=0.05
    )
    
    print("ğŸš€ Fixed RoboVLMs Style Single Image VLA Model ì´ˆê¸°í™” ì™„ë£Œ!")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ¯ ì‚¬ìš© ê¸°ëŠ¥:")
    print(f"   - Fixed Claw Matrix: {model.use_claw_matrix}")
    print(f"   - Fixed Hierarchical Planning: {model.use_hierarchical}")
    print(f"   - Fixed Advanced Attention: {model.use_advanced_attention}")
    print(f"   - Zì¶• ê°€ì¤‘ì¹˜: {model.z_axis_weight}")
    print(f"   - ë“œë¡­ì•„ì›ƒ: {model.dropout}")
    print(f"ğŸ¯ ì…ë ¥: ë‹¨ì¼ ì´ë¯¸ì§€ [batch, 3, H, W]")
    print(f"ğŸ¯ ì¶œë ¥: ë‹¨ì¼ ì•¡ì…˜ [batch, 3]")
    print(f"ğŸ¯ ìš©ë„: ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ (RoboVLMs ìŠ¤íƒ€ì¼)")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ§ª ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸:")
    test_image = torch.randn(2, 3, 224, 224)
    test_text = "Navigate to target"
    
    try:
        with torch.no_grad():
            output = model(test_image, test_text)
        print(f"   âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì¶œë ¥: {output.shape}")
    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
