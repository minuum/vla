"""
ğŸš€ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸
ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Zì¶•ì„ ì œì™¸í•˜ê³  2D ì•¡ì…˜ì— ìµœì í™”ëœ ëª¨ë¸
"""

import torch
import torch.nn as nn
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import json

from transformers import AutoModel
from PIL import Image

class Optimized2DActionDataset(Dataset):
    """2D ì•¡ì…˜ì— ìµœì í™”ëœ ë°ì´í„°ì…‹ (Zì¶• ì œì™¸)"""
    
    def __init__(self, data_path, processor, split='train', frame_selection='random'):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        self.frame_selection = frame_selection
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} 2D ì•¡ì…˜ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
        print(f"   - í”„ë ˆì„ ì„ íƒ: {frame_selection}")
        print(f"   - Zì¶• ì œì™¸: True")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ (Zì¶• ì œì™¸, 2D ì•¡ì…˜ë§Œ)"""
        if os.path.isdir(self.data_path):
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ì²« í”„ë ˆì„ ì œì™¸ (í”„ë ˆì„ 1-17ë§Œ ì‚¬ìš©)
                        valid_frames = list(range(1, 18))  # 1, 2, 3, ..., 17
                        
                        if self.frame_selection == 'random':
                            frame_idx = np.random.choice(valid_frames)
                        elif self.frame_selection == 'middle':
                            frame_idx = valid_frames[len(valid_frames)//2]  # 9
                        elif self.frame_selection == 'all':
                            for frame_idx in valid_frames:
                                single_image = images[frame_idx]  # [H, W, 3]
                                single_action = actions[frame_idx]  # [3]
                                
                                # 2D ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (Zì¶• ì œì™¸)
                                action_2d = single_action[:2]  # [linear_x, linear_y]ë§Œ ì‚¬ìš©
                                
                                self.episodes.append({
                                    'image': single_image,
                                    'action': action_2d,  # 2D ì•¡ì…˜
                                    'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                                    'frame_idx': frame_idx,
                                    'original_file': h5_file.name
                                })
                            continue
                        
                        # ë‹¨ì¼ í”„ë ˆì„ ì„ íƒ
                        single_image = images[frame_idx]  # [H, W, 3]
                        single_action = actions[frame_idx]  # [3]
                        
                        # 2D ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (Zì¶• ì œì™¸)
                        action_2d = single_action[:2]  # [linear_x, linear_y]ë§Œ ì‚¬ìš©
                        
                        self.episodes.append({
                            'image': single_image,
                            'action': action_2d,  # 2D ì•¡ì…˜
                            'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                            'frame_idx': frame_idx,
                            'original_file': h5_file.name
                        })
                        
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if image.max() > 1:
            image = image.astype(np.float32) / 255.0
        
        # ì•¡ì…˜: 2D [linear_x, linear_y]
        action = episode['action']  # [2]
        
        return {
            'image': image,  # [3, H, W]
            'action': action,  # [2] - 2D ì•¡ì…˜
            'episode_id': episode['episode_id'],
            'frame_idx': episode['frame_idx']
        }

class Optimized2DActionModel(nn.Module):
    """2D ì•¡ì…˜ì— ìµœì í™”ëœ ëª¨ë¸ (Zì¶• ì œì™¸)"""
    
    def __init__(self, processor, vision_dim=1024, language_dim=1024, action_dim=2, hidden_dim=512, dropout=0.2, use_claw_matrix=True, use_hierarchical=True, use_advanced_attention=True):
        super().__init__()
        
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim  # 2D ì•¡ì…˜
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # ê³ ê¸‰ ê¸°ëŠ¥ í”Œë˜ê·¸
        self.use_claw_matrix = use_claw_matrix
        self.use_hierarchical = use_hierarchical
        self.use_advanced_attention = use_advanced_attention
        
        # Kosmos2 ë°±ë³¸
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        self.kosmos_processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # Kosmos2 ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.kosmos.eval()
        
        # ì°¨ì› ì–´ëŒ‘í„°ë“¤ (ë™ì  ìƒì„±)
        self.feature_adapter = nn.Linear(1024, vision_dim)
        self.language_adapter = None  # ë™ì  ìƒì„±
        
        # ë ˆì´ì–´ ì •ê·œí™”
        self.layer_norm_vision = nn.LayerNorm(vision_dim)
        self.layer_norm_language = nn.LayerNorm(language_dim)
        self.layer_norm_fusion = nn.LayerNorm(hidden_dim)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout_vision = nn.Dropout(dropout)
        self.dropout_language = nn.Dropout(dropout)
        self.dropout_fusion = nn.Dropout(dropout)
        
        # ê³ ê¸‰ ê¸°ëŠ¥ë“¤ (2D ì•¡ì…˜ì— ë§ê²Œ ì¡°ì •)
        if use_claw_matrix:
            self.claw_matrix = OptimizedClawMatrixFusion(vision_dim, language_dim, action_dim, hidden_dim, dropout)
        if use_hierarchical:
            self.hierarchical_planner = OptimizedHierarchicalPlanner(hidden_dim, action_dim, dropout)
        if use_advanced_attention:
            self.advanced_attention = OptimizedAdvancedAttention(hidden_dim, dropout)
        
        # 2D ì•¡ì…˜ í—¤ë“œ (Zì¶• ì œì™¸)
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)  # 2D ì•¡ì…˜ ì¶œë ¥
        )
        
        print(f"ğŸ¤– 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f"   - ì•¡ì…˜ ì°¨ì›: {action_dim}D (Zì¶• ì œì™¸)")
        print(f"   - Claw Matrix: {use_claw_matrix}")
        print(f"   - Hierarchical Planning: {use_hierarchical}")
        print(f"   - Advanced Attention: {use_advanced_attention}")
    
    def to(self, device):
        """ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•˜ê³  Kosmos2 ëª¨ë¸ë„ í•¨ê»˜ ì´ë™"""
        super().to(device)
        self.kosmos = self.kosmos.to(device)
        return self
    
    def extract_vision_features(self, single_image: torch.Tensor) -> torch.Tensor:
        """ì‹œê° íŠ¹ì§• ì¶”ì¶œ (2D ìµœì í™”)"""
        batch_size = single_image.shape[0]
        
        # Kosmos2 í”„ë¡œì„¸ì„œë¥¼ ìœ„í•œ í˜•ì‹ ë³€í™˜
        if single_image.max() > 1:
            single_image = single_image / 255.0
        
        # ì´ë¯¸ì§€ë¥¼ PIL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        images = []
        for i in range(single_image.shape[0]):
            img = single_image[i].permute(1, 2, 0).cpu().numpy()
            img = (img * 255).astype(np.uint8)
            pil_img = Image.fromarray(img)
            images.append(pil_img)
        
        # Kosmos2 í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì¤€ë¹„
        inputs = self.kosmos_processor(
            images=images, 
            return_tensors="pt",
            padding=True
        )
        
        # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
        
        # Kosmos2 vision ëª¨ë¸ ì‚¬ìš© (ì´ì „ ì„±ê³µ ì½”ë“œ ë°©ì‹)
        with torch.no_grad():
            if 'pixel_values' in inputs:
                vision_outputs = self.kosmos.vision_model(inputs['pixel_values'])
                vision_features = vision_outputs.pooler_output  # [batch_size, 1024]
            else:
                # fallback
                vision_features = torch.zeros(batch_size, 1024).to(self.kosmos.device)
        
        # ì°¨ì› ì¡°ì •
        vision_features = self.feature_adapter(vision_features)
        vision_features = self.layer_norm_vision(vision_features)
        vision_features = self.dropout_vision(vision_features)
        
        return vision_features
    
    def extract_language_features(self, text: str, batch_size: int = 1) -> torch.Tensor:
        """ì–¸ì–´ íŠ¹ì§• ì¶”ì¶œ (2D ìµœì í™”)"""
        # í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì´ì „ ì„±ê³µ ì½”ë“œ ë°©ì‹)
        with torch.no_grad():
            inputs = self.kosmos_processor(text=text, return_tensors="pt")
            inputs = {k: v.to(self.kosmos.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # Kosmos2 í…ìŠ¤íŠ¸ ëª¨ë¸ ì‚¬ìš©
            language_outputs = self.kosmos.text_model(**inputs)
            language_features = language_outputs.last_hidden_state.mean(dim=1)  # [1, hidden_size]
        
        # ë°°ì¹˜ ì°¨ì› í™•ì¥
        language_features = language_features.expand(batch_size, -1)
        
        # ì°¨ì› ì¡°ì • (ë™ì  ì–´ëŒ‘í„° ìƒì„±)
        if language_features.shape[-1] != self.language_dim:
            if self.language_adapter is None:
                self.language_adapter = nn.Linear(
                    language_features.shape[-1], 
                    self.language_dim
                ).to(language_features.device)
            language_features = self.language_adapter(language_features)
        
        # ê°•í™”ëœ ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        language_features = self.layer_norm_language(language_features)
        language_features = self.dropout_language(language_features)
        
        return language_features
    
    def forward(self, single_image: torch.Tensor, text: str) -> torch.Tensor:
        """2D ì•¡ì…˜ ì˜ˆì¸¡ (Zì¶• ì œì™¸)"""
        batch_size = single_image.shape[0]
        
        # íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(single_image)
        language_features = self.extract_language_features(text, batch_size)
        
        # ê¸°ë³¸ ìœµí•©
        fused_features = torch.cat([vision_features, language_features], dim=-1)
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ì ìš©
        if self.use_claw_matrix and hasattr(self, 'claw_matrix'):
            # 2D ì•¡ì…˜ìš© ë”ë¯¸ ì•¡ì…˜ ìƒì„±
            dummy_actions = torch.zeros(batch_size, self.hidden_dim).to(vision_features.device)
            fused_features = self.claw_matrix(vision_features, language_features, dummy_actions)
        else:
            # Claw Matrixë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ìœµí•©
            if fused_features.shape[-1] != self.hidden_dim:
                if not hasattr(self, 'fusion_adapter'):
                    self.fusion_adapter = nn.Linear(fused_features.shape[-1], self.hidden_dim).to(fused_features.device)
                fused_features = self.fusion_adapter(fused_features)
        
        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ
        fused_features = self.layer_norm_fusion(fused_features)
        fused_features = self.dropout_fusion(fused_features)
        
        # Advanced Attention ì ìš©
        if self.use_advanced_attention and hasattr(self, 'advanced_attention'):
            fused_features = self.advanced_attention(fused_features)
        
        # Hierarchical Planning ì ìš©
        if self.use_hierarchical and hasattr(self, 'hierarchical_planner'):
            fused_features = self.hierarchical_planner(fused_features)
        
        # 2D ì•¡ì…˜ ì˜ˆì¸¡ (Zì¶• ì œì™¸)
        actions_2d = self.action_head(fused_features)  # [batch_size, 2]
        
        return actions_2d

class OptimizedClawMatrixFusion(nn.Module):
    """2D ì•¡ì…˜ì— ìµœì í™”ëœ Claw Matrix"""
    
    def __init__(self, vision_dim, language_dim, action_dim, hidden_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # 2D ì•¡ì…˜ì— ë§ê²Œ ì¡°ì •
        self.vl_cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True)
        self.la_cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True)
        self.av_cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True)
        
        # í”„ë¡œì ì…˜ ë ˆì´ì–´ë“¤
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        self.language_proj = nn.Linear(language_dim, hidden_dim)
        self.action_proj = nn.Linear(hidden_dim, hidden_dim)  # 2D ì•¡ì…˜ìš©
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        self.vision_out_proj = nn.Linear(hidden_dim, hidden_dim)
        self.language_out_proj = nn.Linear(hidden_dim, hidden_dim)
        self.action_out_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # ë ˆì´ì–´ ì •ê·œí™”
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        # í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        
        self.norm4 = nn.LayerNorm(hidden_dim)
    
    def forward(self, vision_features, language_features, dummy_actions):
        """2D ì•¡ì…˜ì— ìµœì í™”ëœ Claw Matrix ìœµí•©"""
        # í”„ë¡œì ì…˜
        v_proj = self.vision_proj(vision_features)
        l_proj = self.language_proj(language_features)
        a_proj = self.action_proj(dummy_actions)
        
        # Vision-Language í¬ë¡œìŠ¤ ì–´í…ì…˜
        vl_out, _ = self.vl_cross_attention(v_proj, l_proj, l_proj)
        vl_out = self.vision_out_proj(vl_out)
        v_proj = self.norm1(v_proj + vl_out)
        
        # Language-Action í¬ë¡œìŠ¤ ì–´í…ì…˜
        la_out, _ = self.la_cross_attention(l_proj, a_proj, a_proj)
        la_out = self.language_out_proj(la_out)
        l_proj = self.norm2(l_proj + la_out)
        
        # Action-Vision í¬ë¡œìŠ¤ ì–´í…ì…˜
        av_out, _ = self.av_cross_attention(a_proj, v_proj, v_proj)
        av_out = self.action_out_proj(av_out)
        a_proj = self.norm3(a_proj + av_out)
        
        # ìœµí•©
        fused = v_proj + l_proj + a_proj
        
        # í”¼ë“œí¬ì›Œë“œ
        ffn_out = self.ffn(fused)
        fused = self.norm4(fused + ffn_out)
        
        return fused

class OptimizedHierarchicalPlanner(nn.Module):
    """2D ì•¡ì…˜ì— ìµœì í™”ëœ ê³„ì¸µì  ê³„íš"""
    
    def __init__(self, hidden_dim, action_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        
        # 2D ì•¡ì…˜ì— ë§ê²Œ ì¡°ì •
        self.goal_decomposer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, action_dim)
        )
        
        self.subgoal_generator = nn.Sequential(
            nn.Linear(hidden_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
    
    def forward(self, features):
        """2D ì•¡ì…˜ ê³„ì¸µì  ê³„íš"""
        # ëª©í‘œ ë¶„í•´
        goal = self.goal_decomposer(features)
        
        # ì„œë¸Œê³¨ ìƒì„±
        goal_features = torch.cat([features, goal], dim=-1)
        subgoals = self.subgoal_generator(goal_features)
        
        return subgoals

class OptimizedAdvancedAttention(nn.Module):
    """2D ì•¡ì…˜ì— ìµœì í™”ëœ ê³ ê¸‰ ì–´í…ì…˜"""
    
    def __init__(self, hidden_dim, dropout):
        super().__init__()
        self.hidden_dim = hidden_dim
        
        # 2D ì•¡ì…˜ì— ë§ê²Œ ì¡°ì •
        self.self_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, dropout=dropout, batch_first=True)
        self.temporal_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, dropout=dropout, batch_first=True)
        self.spatial_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, dropout=dropout, batch_first=True)
        
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        self.norm4 = nn.LayerNorm(hidden_dim)
    
    def forward(self, features):
        """2D ì•¡ì…˜ ê³ ê¸‰ ì–´í…ì…˜"""
        # Self Attention
        attn_out, _ = self.self_attention(features, features, features)
        features = self.norm1(features + attn_out)
        
        # Temporal Attention (ì‹œí€€ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)
        if features.dim() == 3:
            temp_out, _ = self.temporal_attention(features, features, features)
            features = self.norm2(features + temp_out)
        
        # Spatial Attention (ê³µê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if features.dim() == 3:
            spatial_out, _ = self.spatial_attention(features, features, features)
            features = self.norm3(features + spatial_out)
        
        # Feedforward
        ffn_out = self.ffn(features)
        features = self.norm4(features + ffn_out)
        
        return features

def create_2d_data_loaders(data_path, processor, batch_size=4, train_split=0.8, frame_selection='random'):
    """2D ì•¡ì…˜ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = Optimized2DActionDataset(data_path, processor, 'full', frame_selection)
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=1,
        pin_memory=True
    )
    
    print(f"ğŸ“Š 2D ì•¡ì…˜ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - ì•¡ì…˜ ì°¨ì›: 2D (Zì¶• ì œì™¸)")
    
    return train_loader, val_loader

def train_2d_optimized_model(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,
    weight_decay=1e-4,
    early_stopping_patience=5,
    device='cuda'
):
    """2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í›ˆë ¨"""
    
    model = model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # 2D ì•¡ì…˜ ì†ì‹¤ í•¨ìˆ˜ (Zì¶• ì œì™¸)
    def compute_2d_loss(predicted_actions, target_actions):
        # 2D ì•¡ì…˜ë§Œ ì‚¬ìš© (linear_x, linear_y)
        return nn.functional.mse_loss(predicted_actions, target_actions)
    
    # ì¡°ê¸° ì¢…ë£Œ
    best_val_loss = float('inf')
    patience_counter = 0
    
    print(f"ğŸš€ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {num_epochs} ì—í¬í¬, í•™ìŠµë¥ : {learning_rate}")
    print(f"ğŸ¯ ì•¡ì…˜ ì°¨ì›: 2D (Zì¶• ì œì™¸)")
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            images = batch['image']  # [batch, 3, H, W]
            actions = batch['action']  # [batch, 2] - 2D ì•¡ì…˜
            
            images = images.float().to(device)
            actions = actions.float().to(device)
            
            optimizer.zero_grad()
            
            try:
                # 2D ì•¡ì…˜ ì˜ˆì¸¡
                predicted_actions = model(images, "Navigate to target")
                
                # 2D ì†ì‹¤ ê³„ì‚°
                loss = compute_2d_loss(predicted_actions, actions)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    images = batch['image'].float().to(device)
                    actions = batch['action'].float().to(device)
                    
                    predicted_actions = model(images, "Navigate to target")
                    loss = compute_2d_loss(predicted_actions, actions)
                    val_loss += loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # í‰ê·  ì†ì‹¤
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / max(val_batches, 1)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step()
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs} ì™„ë£Œ:")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        print(f"   - í•™ìŠµë¥ : {scheduler.get_last_lr()[0]:.6f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': {
                    'learning_rate': learning_rate,
                    'weight_decay': weight_decay,
                    'dropout': model.dropout,
                    'action_dim': model.action_dim,
                    'use_claw_matrix': model.use_claw_matrix,
                    'use_hierarchical': model.use_hierarchical,
                    'use_advanced_attention': model.use_advanced_attention,
                    'training_type': '2d_optimized'
                }
            }, 'optimized_2d_action_model_best.pth')
            print(f"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"   â° ì¡°ê¸° ì¢…ë£Œ (Patience: {early_stopping_patience})")
                break
    
    return model

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = {
        'data_path': '../../ROS_action/mobile_vla_dataset',
        'batch_size': 4,
        'num_epochs': 15,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'dropout': 0.2,
        'action_dim': 2,  # 2D ì•¡ì…˜
        'use_claw_matrix': True,
        'use_hierarchical': True,
        'use_advanced_attention': True,
        'early_stopping_patience': 5,
        'frame_selection': 'all',
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print("ğŸš€ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {json.dumps(config, indent=2)}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“Š 2D ì•¡ì…˜ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader = create_2d_data_loaders(
        config['data_path'],
        processor,
        batch_size=config['batch_size'],
        frame_selection=config['frame_selection']
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = Optimized2DActionModel(
        processor=processor,
        action_dim=config['action_dim'],
        dropout=config['dropout'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention']
    )
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ¯ 2D ì•¡ì…˜ ìµœì í™”:")
    print(f"   - ì•¡ì…˜ ì°¨ì›: {model.action_dim}D (Zì¶• ì œì™¸)")
    print(f"   - Claw Matrix: {model.use_claw_matrix}")
    print(f"   - Hierarchical Planning: {model.use_hierarchical}")
    print(f"   - Advanced Attention: {model.use_advanced_attention}")
    
    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ¯ 2D ì•¡ì…˜ ìµœì í™” í›ˆë ¨ ì‹œì‘!")
    try:
        trained_model = train_2d_optimized_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=config['num_epochs'],
            learning_rate=config['learning_rate'],
            weight_decay=config['weight_decay'],
            early_stopping_patience=config['early_stopping_patience'],
            device=config['device']
        )
        
        print("âœ… 2D ì•¡ì…˜ ìµœì í™” í›ˆë ¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_type': 'Optimized_2D_Action_Model',
        'training_type': '2d_optimized',
        'action_dimension': 2,
        'z_axis_excluded': True,
        'data_size': len(train_loader.dataset) + len(val_loader.dataset),
        'config': config,
        'advanced_features': {
            'optimized_claw_matrix': config['use_claw_matrix'],
            'optimized_hierarchical_planning': config['use_hierarchical'],
            'optimized_advanced_attention': config['use_advanced_attention']
        },
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'training_status': 'completed'
    }
    
    with open('optimized_2d_action_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: optimized_2d_action_training_results.json")
    
    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if os.path.exists('optimized_2d_action_model_best.pth'):
        checkpoint = torch.load('optimized_2d_action_model_best.pth', map_location='cpu')
        print(f"ğŸ“Š ìµœê³  ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - ì—í¬í¬: {checkpoint['epoch']}")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {checkpoint['train_loss']:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.4f}")
        print(f"   - ì•¡ì…˜ ì°¨ì›: {checkpoint['config']['action_dim']}D")

if __name__ == "__main__":
    main()
