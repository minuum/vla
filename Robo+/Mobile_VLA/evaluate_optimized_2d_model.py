"""
ğŸ¯ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
í›ˆë ¨ëœ 2D ì•¡ì…˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ë¹„êµ
"""

import torch
import torch.nn as nn
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import json
from PIL import Image

from optimized_2d_action_model import Optimized2DActionModel, Optimized2DActionDataset

class Optimized2DEvaluationDataset(Dataset):
    """2D ì•¡ì…˜ í‰ê°€ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, data_path, processor, split='val'):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} 2D ì•¡ì…˜ í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ (ì²« í”„ë ˆì„ ì œì™¸)"""
        if os.path.isdir(self.data_path):
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ì²« í”„ë ˆì„ ì œì™¸ (í”„ë ˆì„ 1-17ë§Œ ì‚¬ìš©)
                        valid_frames = list(range(1, 18))  # 1, 2, 3, ..., 17
                        
                        # ëª¨ë“  ìœ íš¨í•œ í”„ë ˆì„ì„ í‰ê°€ì— ì‚¬ìš©
                        for frame_idx in valid_frames:
                            single_image = images[frame_idx]  # [H, W, 3]
                            single_action = actions[frame_idx]  # [3]
                            
                            # 2D ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (Zì¶• ì œì™¸)
                            action_2d = single_action[:2]  # [linear_x, linear_y]ë§Œ ì‚¬ìš©
                            
                            self.episodes.append({
                                'image': single_image,
                                'action': action_2d,  # 2D ì•¡ì…˜
                                'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                                'frame_idx': frame_idx,
                                'original_file': h5_file.name
                            })
                        
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if image.max() > 1:
            image = image.astype(np.float32) / 255.0
        
        # 2D ì•¡ì…˜: [2]
        action = episode['action']  # [2]
        
        return {
            'image': image,  # [3, H, W]
            'action': action,  # [2]
            'episode_id': episode['episode_id']
        }

def evaluate_2d_model(model, data_loader, device):
    """2D ì•¡ì…˜ ëª¨ë¸ í‰ê°€"""
    model.eval()
    
    total_loss = 0.0
    total_mae = 0.0
    total_rmse = 0.0
    total_samples = 0
    
    # ì„±ê³µë¥  ê³„ì‚°ì„ ìœ„í•œ ì„ê³„ê°’
    success_thresholds = [0.1, 0.05, 0.01]
    success_counts = {f"accuracy_{int(t*100)}": 0 for t in success_thresholds}
    
    predictions = []
    targets = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="í‰ê°€ ì¤‘"):
            images = batch['image'].float().to(device)
            actions = batch['action'].float().to(device)
            
            # 2D ì•¡ì…˜ ì˜ˆì¸¡
            predicted_actions = model(images, "Navigate to target")
            
            # ì†ì‹¤ ê³„ì‚°
            loss = nn.functional.mse_loss(predicted_actions, actions)
            total_loss += loss.item()
            
            # MAE ê³„ì‚°
            mae = nn.functional.l1_loss(predicted_actions, actions)
            total_mae += mae.item()
            
            # RMSE ê³„ì‚°
            rmse = torch.sqrt(nn.functional.mse_loss(predicted_actions, actions))
            total_rmse += rmse.item()
            
            # ì„±ê³µë¥  ê³„ì‚°
            for threshold in success_thresholds:
                accuracy_key = f"accuracy_{int(threshold*100)}"
                # ëª¨ë“  ì•¡ì…˜ ì°¨ì›ì—ì„œ ì„ê³„ê°’ ì´ë‚´ì¸ì§€ í™•ì¸
                within_threshold = torch.all(torch.abs(predicted_actions - actions) < threshold, dim=1)
                success_counts[accuracy_key] += within_threshold.sum().item()
            
            total_samples += images.shape[0]
            
            # ì˜ˆì¸¡ê°’ê³¼ íƒ€ê²Ÿ ì €ì¥
            predictions.extend(predicted_actions.cpu().numpy())
            targets.extend(actions.cpu().numpy())
    
    # í‰ê·  ê³„ì‚°
    avg_loss = total_loss / len(data_loader)
    avg_mae = total_mae / len(data_loader)
    avg_rmse = total_rmse / len(data_loader)
    
    # ì„±ê³µë¥  ê³„ì‚°
    success_rates = {}
    for key, count in success_counts.items():
        success_rates[key] = (count / total_samples) * 100
    
    # ì˜ˆì¸¡ê°’ê³¼ íƒ€ê²Ÿì„ numpy ë°°ì—´ë¡œ ë³€í™˜
    predictions = np.array(predictions)
    targets = np.array(targets)
    
    # ê° ì•¡ì…˜ ì°¨ì›ë³„ ì„±ëŠ¥ ë¶„ì„
    action_dim_performance = {}
    for i, dim_name in enumerate(['linear_x', 'linear_y']):
        dim_predictions = predictions[:, i]
        dim_targets = targets[:, i]
        
        dim_mae = np.mean(np.abs(dim_predictions - dim_targets))
        dim_rmse = np.sqrt(np.mean((dim_predictions - dim_targets) ** 2))
        
        action_dim_performance[dim_name] = {
            'mae': float(dim_mae),
            'rmse': float(dim_rmse)
        }
    
    results = {
        'model_type': 'Optimized_2D_Action_Model',
        'total_samples': total_samples,
        'avg_loss': float(avg_loss),
        'avg_mae': float(avg_mae),
        'avg_rmse': float(avg_rmse),
        'success_rates': success_rates,
        'action_dim_performance': action_dim_performance,
        'predictions_shape': predictions.shape,
        'targets_shape': targets.shape
    }
    
    return results

def compare_with_existing_models():
    """ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ"""
    print("ğŸ” ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ ì¤‘...")
    
    # ê¸°ì¡´ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë“¤ í™•ì¸
    checkpoint_files = [
        'optimized_2d_action_model_best.pth',
        'fixed_robovlms_model_best.pth',
        'train_without_first_frame_model_best.pth',
        'train_fixed_robovlms_model_best.pth'
    ]
    
    existing_results = {}
    
    for checkpoint_file in checkpoint_files:
        if os.path.exists(checkpoint_file):
            try:
                checkpoint = torch.load(checkpoint_file, map_location='cpu')
                if 'val_loss' in checkpoint:
                    existing_results[checkpoint_file] = {
                        'val_loss': checkpoint['val_loss'],
                        'train_loss': checkpoint.get('train_loss', 'N/A'),
                        'epoch': checkpoint.get('epoch', 'N/A'),
                        'config': checkpoint.get('config', {})
                    }
                    print(f"ğŸ“Š {checkpoint_file}: ê²€ì¦ ì†ì‹¤ = {checkpoint['val_loss']:.4f}")
            except Exception as e:
                print(f"âŒ {checkpoint_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return existing_results

def main():
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""
    print("ğŸ¯ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í‰ê°€ ì‹œì‘!")
    
    # ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    data_path = '../../ROS_action/mobile_vla_dataset'
    batch_size = 8
    
    print(f"ğŸ“Š ì„¤ì •: ë””ë°”ì´ìŠ¤={device}, ë°°ì¹˜í¬ê¸°={batch_size}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“Š í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    eval_dataset = Optimized2DEvaluationDataset(data_path, processor, 'eval')
    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)
    
    # ëª¨ë¸ ë¡œë“œ
    print("ğŸ¤– 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = Optimized2DActionModel(
        processor=processor,
        action_dim=2,
        dropout=0.2,
        use_claw_matrix=True,
        use_hierarchical=True,
        use_advanced_attention=True
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_path = 'optimized_2d_action_model_best.pth'
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # ë™ì  ì–´ëŒ‘í„° ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë”ë¯¸ í¬ì›Œë“œ íŒ¨ìŠ¤
        dummy_input = torch.zeros(1, 3, 224, 224).to(device)
        with torch.no_grad():
            _ = model(dummy_input, "Navigate to target")
        
        # í˜¸í™˜ë˜ëŠ” í‚¤ë§Œ ë¡œë“œ
        model_state_dict = model.state_dict()
        checkpoint_state_dict = checkpoint['model_state_dict']
        
        # í˜¸í™˜ë˜ëŠ” í‚¤ë§Œ í•„í„°ë§
        compatible_state_dict = {}
        for key in checkpoint_state_dict.keys():
            if key in model_state_dict and model_state_dict[key].shape == checkpoint_state_dict[key].shape:
                compatible_state_dict[key] = checkpoint_state_dict[key]
            else:
                print(f"âš ï¸ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” í‚¤ ê±´ë„ˆë›°ê¸°: {key}")
        
        model.load_state_dict(compatible_state_dict, strict=False)
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_path}")
        print(f"   - ì—í¬í¬: {checkpoint['epoch']}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.4f}")
        print(f"   - ë¡œë“œëœ íŒŒë¼ë¯¸í„°: {len(compatible_state_dict)}/{len(checkpoint_state_dict)}")
    else:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    model = model.to(device)
    
    # ëª¨ë¸ í‰ê°€
    print("ğŸ¯ ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    results = evaluate_2d_model(model, eval_loader, device)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í‰ê°€ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {results['total_samples']:,}")
    print(f"ğŸ“Š í‰ê·  ì†ì‹¤: {results['avg_loss']:.4f}")
    print(f"ğŸ“Š í‰ê·  MAE: {results['avg_mae']:.4f}")
    print(f"ğŸ“Š í‰ê·  RMSE: {results['avg_rmse']:.4f}")
    
    print("\nğŸ¯ ì„±ê³µë¥ :")
    for threshold, rate in results['success_rates'].items():
        print(f"   - {threshold}: {rate:.2f}%")
    
    print("\nğŸ“Š ì•¡ì…˜ ì°¨ì›ë³„ ì„±ëŠ¥:")
    for dim_name, performance in results['action_dim_performance'].items():
        print(f"   - {dim_name}:")
        print(f"     MAE: {performance['mae']:.4f}")
        print(f"     RMSE: {performance['rmse']:.4f}")
    
    # ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ë¹„êµ
    print("\n" + "="*60)
    print("ğŸ” ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ")
    print("="*60)
    existing_results = compare_with_existing_models()
    
    if existing_results:
        print("\nğŸ“Š ëª¨ë¸ë³„ ê²€ì¦ ì†ì‹¤ ë¹„êµ:")
        sorted_models = sorted(existing_results.items(), key=lambda x: x[1]['val_loss'])
        for i, (model_name, result) in enumerate(sorted_models):
            rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{rank} {model_name}: {result['val_loss']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    results['existing_models_comparison'] = existing_results
    results['evaluation_config'] = {
        'device': device,
        'batch_size': batch_size,
        'data_path': data_path
    }
    
    with open('optimized_2d_action_evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: optimized_2d_action_evaluation_results.json")
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ ìµœì¢… ìš”ì•½")
    print("="*60)
    print(f"âœ… 2D ì•¡ì…˜ ìµœì í™” ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“Š ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   - í‰ê·  MAE: {results['avg_mae']:.4f}")
    print(f"   - í‰ê·  RMSE: {results['avg_rmse']:.4f}")
    print(f"   - 0.1 ì„ê³„ê°’ ì„±ê³µë¥ : {results['success_rates']['accuracy_10']:.2f}%")
    print(f"   - 0.05 ì„ê³„ê°’ ì„±ê³µë¥ : {results['success_rates']['accuracy_5']:.2f}%")
    print(f"   - 0.01 ì„ê³„ê°’ ì„±ê³µë¥ : {results['success_rates']['accuracy_1']:.2f}%")

if __name__ == "__main__":
    main()
