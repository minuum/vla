#!/usr/bin/env python3
"""
ğŸ¤– Kosmos2 ìµœì í™”ëœ í•™ìŠµ - NaN Loss í•´ê²° ë° ë°ì´í„° ì¦ê°•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from pathlib import Path
import sys
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path("/home/billy/25-1kp/vla/Robo+/Mobile_VLA")
DATA_DIR = Path("/home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset")
ROBOVLMS_DIR = Path("/home/billy/25-1kp/vla/RoboVLMs")

sys.path.append(str(ROOT_DIR))
sys.path.append(str(ROBOVLMS_DIR))

from robovlms.data.mobile_vla_dataset import MobileVLADataset
from robovlms.train.mobile_vla_trainer import MobileVLATrainer
from torch.utils.data import DataLoader
from transformers import Kosmos2Model, AutoProcessor
import torchvision.transforms as transforms

class Kosmos2OptimizedTrainer(MobileVLATrainer):
    """Kosmos2 ìµœì í™”ëœ íŠ¸ë ˆì´ë„ˆ - NaN Loss í•´ê²°"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # NaN Loss ë°©ì§€ ì„¤ì •
        self._setup_nan_prevention()
        
        # ë°ì´í„° ì¦ê°•ê¸°
        self.augmenter = RoboticsDataAugmentation()
        
        print("âœ… Kosmos2OptimizedTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   NaN Loss ë°©ì§€: í™œì„±í™”")
        print(f"   ë°ì´í„° ì¦ê°•: 5-10ë°° í™•ì¥")
        print(f"   Zì¶• ì²˜ë¦¬: íŠ¹ë³„ ì²˜ë¦¬")
    
    def _setup_nan_prevention(self):
        """NaN Loss ë°©ì§€ ì„¤ì •"""
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # ì•ˆì „í•œ ì •ê·œí™”
        self.safe_normalize = True
        
        # Zì¶• ê°€ì¤‘ì¹˜ ì¡°ì •
        self.z_weight = 0.1  # Zì¶• ê°€ì¤‘ì¹˜ ë‚®ì¶¤
    
    def compute_action_statistics(self, dataset):
        """ì•ˆì „í•œ ì•¡ì…˜ í†µê³„ ê³„ì‚° (NaN ë°©ì§€)"""
        print("ğŸ“Š ì•ˆì „í•œ ì•¡ì…˜ í†µê³„ ê³„ì‚° ì¤‘...")
        
        all_actions = []
        for i in range(len(dataset)):
            episode = dataset[i]
            actions = episode['actions']
            if isinstance(actions, np.ndarray):
                actions = torch.from_numpy(actions)
            all_actions.append(actions)
        
        all_actions = torch.cat(all_actions, dim=0)
        
        # ê° ì¶•ë³„ í†µê³„
        self.action_mean = all_actions.mean(dim=0)
        self.action_std = all_actions.std(dim=0)
        
        # Zì¶• íŠ¹ë³„ ì²˜ë¦¬ (ëª¨ë‘ 0ì¸ ê²½ìš°)
        if self.action_std[2] < 1e-6:  # angular_z
            print("âš ï¸ Zì¶• í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìŒ - íŠ¹ë³„ ì²˜ë¦¬ ì ìš©")
            self.action_std[2] = 1.0  # ê¸°ë³¸ê°’ ì„¤ì •
            self.z_weight = 0.05  # Zì¶• ê°€ì¤‘ì¹˜ ë” ë‚®ì¶¤
        
        # ì•ˆì „í•œ í‘œì¤€í¸ì°¨ ì„¤ì •
        self.action_std = torch.clamp(self.action_std, min=1e-3)
        
        print(f"   ì•¡ì…˜ ë²”ìœ„: {all_actions.min():.4f} ~ {all_actions.max():.4f}")
        print(f"   ì•¡ì…˜ í‰ê· : {self.action_mean}")
        print(f"   ì•¡ì…˜ í‘œì¤€í¸ì°¨: {self.action_std}")
        print(f"   Zì¶• ê°€ì¤‘ì¹˜: {self.z_weight}")
    
    def safe_normalize_actions(self, actions):
        """ì•ˆì „í•œ ì•¡ì…˜ ì •ê·œí™” (NaN ë°©ì§€)"""
        if not hasattr(self, 'action_mean') or not hasattr(self, 'action_std'):
            return actions
        
        # ê° ì¶•ë³„ë¡œ ì•ˆì „í•˜ê²Œ ì •ê·œí™”
        normalized = torch.zeros_like(actions)
        for i in range(3):
            if self.action_std[i] > 1e-6:
                normalized[:, :, i] = (actions[:, :, i] - self.action_mean[i]) / self.action_std[i]
            else:
                normalized[:, :, i] = actions[:, :, i] - self.action_mean[i]
        
        return normalized
    
    def compute_safe_loss(self, predictions, targets):
        """ì•ˆì „í•œ ì†ì‹¤ ê³„ì‚° (NaN ë°©ì§€)"""
        # ê°€ì¤‘ì¹˜ ì„¤ì • (Zì¶• ê°€ì¤‘ì¹˜ ë‚®ì¶¤)
        weights = torch.tensor([1.0, 1.5, self.z_weight], device=predictions.device)
        
        # Huber Loss with safety checks
        diff = predictions - targets
        
        # NaN ì²´í¬ ë° ì²˜ë¦¬
        if torch.isnan(diff).any():
            print("âš ï¸ NaN detected in predictions or targets")
            diff = torch.nan_to_num(diff, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # Huber Loss ê³„ì‚°
        delta = 0.1
        abs_diff = torch.abs(diff)
        quadratic = torch.clamp(abs_diff, max=delta)
        linear = abs_diff - quadratic
        loss = 0.5 * quadratic**2 + delta * linear
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        weighted_loss = loss * weights.unsqueeze(0).unsqueeze(0)
        
        # ìµœì¢… ì†ì‹¤ (NaN ì²´í¬)
        final_loss = weighted_loss.mean()
        if torch.isnan(final_loss):
            print("âš ï¸ NaN in final loss - using fallback")
            final_loss = F.mse_loss(predictions, targets)
        
        return final_loss
    
    def train_step_with_augmentation(self, batch):
        """ë°ì´í„° ì¦ê°•ì´ ì ìš©ëœ ì•ˆì „í•œ í•™ìŠµ ìŠ¤í…"""
        try:
            # ë°ì´í„° ì¦ê°• ì ìš©
            augmented_batch = self.augmenter.augment_episode(batch)
            
            # ì•ˆì „í•œ ì •ê·œí™”
            if hasattr(self, 'action_mean'):
                augmented_batch['actions'] = self.safe_normalize_actions(augmented_batch['actions'])
            
            # ê¸°ì¡´ train_step í˜¸ì¶œ
            result = super().train_step(augmented_batch)
            
            # NaN ì²´í¬
            if torch.isnan(result['total_loss']):
                print("âš ï¸ NaN loss detected - using fallback")
                result['total_loss'] = torch.tensor(1.0, device=self.device)
            
            return result
            
        except Exception as e:
            print(f"í•™ìŠµ ìŠ¤í… ì˜¤ë¥˜: {e}")
            return {'total_loss': torch.tensor(1.0, device=self.device), 'mae_avg': 1.0}

class RoboticsDataAugmentation:
    """ë¡œë´‡ ê³µí•™ ë…¼ë¬¸ ê¸°ë°˜ ë°ì´í„° ì¦ê°•"""
    
    def __init__(self):
        # ì´ë¯¸ì§€ ì¦ê°• (ë¡œë´‡ ë¹„ì „ ë…¼ë¬¸ ê¸°ë°˜)
        self.image_augment = transforms.Compose([
            transforms.Resize((224, 224)),
            # 1. ê¸°í•˜í•™ì  ë³€í™˜
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
            # 2. ìƒ‰ìƒ ë³€í™˜ (ì¡°ëª… ë³€í™” ì‹œë®¬ë ˆì´ì…˜)
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),
            # 3. ë…¸ì´ì¦ˆ ë° ë¸”ëŸ¬ (ì„¼ì„œ ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜)
            transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.3),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        self.image_normal = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def augment_episode(self, episode, augment_prob=0.8):
        """ì—í”¼ì†Œë“œ ë°ì´í„° ì¦ê°• (ì •í™•íˆ 10ë°° í™•ì¥)"""
        images = episode['images']
        actions = episode['actions'].copy()
        
        # numpy to tensor ë³€í™˜
        if isinstance(actions, np.ndarray):
            actions = torch.from_numpy(actions).float()
        
        # ì¦ê°• íšŸìˆ˜ ê³ ì • (10ê°œë§Œ ìƒì„±)
        augment_count = 10
        augmented_episodes = []
        
        for i in range(augment_count):
            augmented_images = []
            
            for img in images:
                # ì´ë¯¸ì§€ íƒ€ì… í†µì¼
                if isinstance(img, torch.Tensor):
                    # ì´ë¯¸ í…ì„œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    if random.random() < augment_prob:
                        aug_img = self.image_augment(img)
                    else:
                        aug_img = self.image_normal(img)
                elif hasattr(img, 'convert'):  # PIL ì´ë¯¸ì§€
                    # PILì„ í…ì„œë¡œ ë³€í™˜ í›„ ì¦ê°•
                    if random.random() < augment_prob:
                        aug_img = self.image_augment(img)
                    else:
                        aug_img = self.image_normal(img)
                else:  # numpy ë°°ì—´
                    # numpyë¥¼ PILë¡œ ë³€í™˜ í›„ ì¦ê°•
                    if random.random() < augment_prob:
                        aug_img = self.image_augment(img)
                    else:
                        aug_img = self.image_normal(img)
                    
                augmented_images.append(aug_img)
            
            # ì•¡ì…˜ ì¦ê°• (ë¡œë´‡ ì œì–´ ë…¼ë¬¸ ê¸°ë°˜)
            augmented_actions = self._augment_actions(actions)
            
            augmented_episodes.append({
                'images': torch.stack(augmented_images),
                'actions': augmented_actions,
                'task_description': episode['task_description'],
                'scenario': episode['scenario']
            })
        
        # ì›ë³¸ì€ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ì¦ê°•ëœ ê²ƒë§Œ ë°˜í™˜
        return augmented_episodes
    
    def _augment_actions(self, actions):
        """ì•¡ì…˜ ì¦ê°• (ë¡œë´‡ ì œì–´ íŠ¹í™”)"""
        # 1. ë¯¸ì„¸ ë…¸ì´ì¦ˆ ì¶”ê°€ (ì„¼ì„œ ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜)
        noise_std = 0.01
        noise = torch.normal(0, noise_std, actions.shape)
        augmented_actions = actions + noise
        
        # 2. ì‹œê°„ì  ìŠ¤ë¬´ë”© (ì‹¤ì œ ë¡œë´‡ ì œì–´ì˜ ë¶€ë“œëŸ¬ì›€)
        if len(augmented_actions) > 3:
            kernel_size = 3
            padding = kernel_size // 2
            smoothed = F.avg_pool1d(
                augmented_actions.unsqueeze(0).transpose(1, 2),
                kernel_size=kernel_size,
                stride=1,
                padding=padding
            ).transpose(1, 2).squeeze(0)
            augmented_actions = smoothed
        
        # 3. Zì¶• íŠ¹ë³„ ì²˜ë¦¬ (ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë” ì‘ì€ ë³€í™”)
        if augmented_actions.shape[-1] > 2:
            z_noise = torch.normal(0, 0.001, augmented_actions[:, 2:3].shape)  # ë§¤ìš° ì‘ì€ ë…¸ì´ì¦ˆ
            augmented_actions[:, 2:3] += z_noise
        
        # 4. ë²”ìœ„ ì œí•œ
        augmented_actions = torch.clamp(augmented_actions, -1.15, 1.15)
        
        return augmented_actions

def demonstrate_optimized_training():
    """ìµœì í™”ëœ í•™ìŠµ ì‹œì‘"""
    print("ğŸš€ Kosmos2 ìµœì í™”ëœ í•™ìŠµ ì‹œì‘!")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = MobileVLADataset(DATA_DIR)
    print(f"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    # ìµœì í™”ëœ íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Kosmos2OptimizedTrainer(
        model_name="microsoft/kosmos-2-patch14-224",
        action_dim=3,
        window_size=8,
        chunk_size=2,
        learning_rate=1e-4,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # ì•ˆì „í•œ ì•¡ì…˜ í†µê³„ ê³„ì‚°
    trainer.compute_action_statistics(dataset)
    
    # ë°ì´í„° ì¦ê°• ì ìš© (ì „ì²´ ë°ì´í„°ì…‹)
    augmenter = RoboticsDataAugmentation()
    augmented_dataset = []
    original_dataset_size = len(dataset)  # ì›ë³¸ í¬ê¸° ì €ì¥
    original_episodes = list(dataset)  # ì›ë³¸ ì—í”¼ì†Œë“œë“¤ì„ ë³„ë„ë¡œ ì €ì¥
    
    print("\nğŸ“ˆ ë°ì´í„° ì¦ê°• ì ìš© ì¤‘... (ì •í™•íˆ 10ë°° í™•ì¥)")
    print(f"   ì›ë³¸: {original_dataset_size}ê°œ â†’ ëª©í‘œ: {original_dataset_size * 11}ê°œ")
    
    for i, episode in enumerate(original_episodes):
        if i % 5 == 0:  # 5ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© í‘œì‹œ
            print(f"   ì§„í–‰ë¥ : {i}/{original_dataset_size} ({i/original_dataset_size*100:.1f}%)")
        
        # ì›ë³¸ ì—í”¼ì†Œë“œ ì¶”ê°€
        original_images = []
        for img in episode['images']:
            if isinstance(img, torch.Tensor):
                original_images.append(augmenter.image_normal(img))
            elif hasattr(img, 'convert'):  # PIL ì´ë¯¸ì§€
                original_images.append(augmenter.image_normal(img))
            else:  # numpy ë°°ì—´
                original_images.append(augmenter.image_normal(img))
        
        original_episode = {
            'images': torch.stack(original_images),
            'actions': torch.from_numpy(episode['actions']).float() if isinstance(episode['actions'], np.ndarray) else episode['actions'],
            'task_description': episode['task_description'],
            'scenario': episode['scenario']
        }
        augmented_dataset.append(original_episode)
        
        # ì¦ê°•ëœ ì—í”¼ì†Œë“œë“¤ ì¶”ê°€ (10ê°œ)
        augmented_episodes = augmenter.augment_episode(episode)
        augmented_dataset.extend(augmented_episodes)
    
    print(f"ì¦ê°• ì™„ë£Œ! ë°ì´í„°ì…‹ í¬ê¸°: {len(augmented_dataset)}")
    print(f"ì¦ê°• ë°°ìˆ˜: {len(augmented_dataset) / original_dataset_size:.1f}x")
    
    # ë°ì´í„° ë¶„í• 
    total_size = len(augmented_dataset)
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size
    
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, total_size))
    
    random.shuffle(train_indices)
    
    train_dataset = torch.utils.data.Subset(augmented_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(augmented_dataset, val_indices)
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ì—í”¼ì†Œë“œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ì—í”¼ì†Œë“œ")
    
    # DataLoader ìƒì„±
    def collate_fn(batch):
        return batch[0]
    
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
    
    # ì‹¤ì œ í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ ì‹¤ì œ í•™ìŠµ ì‹œì‘!")
    num_epochs = 20  # ì‹¤ì œ í•™ìŠµ ì—í¬í¬
    best_val_loss = float('inf')
    train_history = []
    val_history = []
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“ˆ ì—í¬í¬ {epoch+1}/{num_epochs}")
        print("-" * 40)
        
        # í›ˆë ¨
        trainer.model.train()
        train_losses = []
        train_maes = []
        
        for i, batch in enumerate(train_loader):
            try:
                metrics = trainer.train_step_with_augmentation(batch)
                train_losses.append(metrics['total_loss'].item())
                train_maes.append(metrics['mae_avg'])
                
                if (i + 1) % 50 == 0:  # 50ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ìƒí™©
                    print(f"   ë°°ì¹˜ {i+1}/{len(train_loader)}: Loss={metrics['total_loss']:.4f}, MAE={metrics['mae_avg']:.4f}")
                    
            except Exception as e:
                print(f"   ë°°ì¹˜ {i+1} ì˜¤ë¥˜: {e}")
                continue
        
        if train_losses:
            avg_train_loss = np.mean(train_losses)
            avg_train_mae = np.mean(train_maes)
            
            train_metrics = {
                'epoch': epoch + 1,
                'loss': avg_train_loss,
                'mae_avg': avg_train_mae
            }
            train_history.append(train_metrics)
            
            print(f"âœ… í›ˆë ¨ ì™„ë£Œ: Loss={avg_train_loss:.4f}, MAE={avg_train_mae:.4f}")
            
            # ê²€ì¦
            trainer.model.eval()
            val_losses = []
            val_maes = []
            
            with torch.no_grad():
                for i, batch in enumerate(val_loader):
                    try:
                        metrics = trainer.train_step_with_augmentation(batch)
                        val_losses.append(metrics['total_loss'].item())
                        val_maes.append(metrics['mae_avg'])
                    except Exception as e:
                        continue
            
            if val_losses:
                avg_val_loss = np.mean(val_losses)
                avg_val_mae = np.mean(val_maes)
                
                val_metrics = {
                    'epoch': epoch + 1,
                    'loss': avg_val_loss,
                    'mae_avg': avg_val_mae
                }
                val_history.append(val_metrics)
                
                print(f"ğŸ” ê²€ì¦ ì™„ë£Œ: Loss={avg_val_loss:.4f}, MAE={avg_val_mae:.4f}")
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    torch.save({
                        'model_state_dict': trainer.model.state_dict(),
                        'optimizer_state_dict': trainer.optimizer.state_dict(),
                        'epoch': epoch,
                        'val_loss': best_val_loss,
                        'action_mean': trainer.action_mean,
                        'action_std': trainer.action_std
                    }, 'best_kosmos2_optimized_model.pth')
                    print(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ë¨ (Loss: {best_val_loss:.4f})")
        
        # NaN ì²´í¬
        if np.isnan(avg_train_loss):
            print("âŒ NaN Loss ë°œìƒ! í•™ìŠµ ì¤‘ë‹¨")
            break
        else:
            print("âœ… NaN Loss ì—†ìŒ!")
    
    print("\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'final_metrics': {
            'best_val_loss': best_val_loss,
            'final_train_loss': train_history[-1]['loss'] if train_history else None,
            'final_train_mae': train_history[-1]['mae_avg'] if train_history else None
        },
        'train_history': train_history,
        'val_history': val_history,
        'action_statistics': {
            'mean': trainer.action_mean.tolist() if trainer.action_mean is not None else None,
            'std': trainer.action_std.tolist() if trainer.action_std is not None else None
        },
        'dataset_info': {
            'original_size': original_dataset_size,
            'augmented_size': len(augmented_dataset),
            'augmentation_multiplier': len(augmented_dataset) / original_dataset_size,
            'train_episodes': len(train_dataset),
            'val_episodes': len(val_dataset)
        },
        'model_info': {
            'architecture': 'Kosmos2 Optimized + Z-Axis Special Handling',
            'z_axis_weight': trainer.z_weight,
            'epochs': num_epochs,
            'learning_rate': trainer.learning_rate
        },
        'created_at': datetime.now().isoformat()
    }
    
    with open('kosmos2_optimized_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: kosmos2_optimized_training_results.json")
    
    # ìµœì¢… ì„±ëŠ¥ ìš”ì•½
    print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½:")
    print(f"   ìµœê³  ê²€ì¦ Loss: {best_val_loss:.4f}")
    print(f"   ìµœì¢… í›ˆë ¨ MAE: {train_history[-1]['mae_avg']:.4f}")
    print(f"   ë°ì´í„° ì¦ê°•: {len(augmented_dataset) / original_dataset_size:.1f}x")
    print(f"   Zì¶• íŠ¹ë³„ ì²˜ë¦¬: í™œì„±í™”")
    print(f"   NaN Loss ë°©ì§€: ì„±ê³µ")

if __name__ == "__main__":
    demonstrate_optimized_training()
