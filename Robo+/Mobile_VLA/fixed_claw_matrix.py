"""
ğŸ”§ ì™„ì „íˆ ìˆ˜ì •ëœ Claw Matrix êµ¬í˜„
ì°¨ì›ê³¼ ë°ì´í„°íƒ€ì… ë¬¸ì œë¥¼ ëª¨ë‘ í•´ê²°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class FixedClawMatrixFusion(nn.Module):
    """ìˆ˜ì •ëœ Claw Matrix Fusion (ì°¨ì› ë¬¸ì œ í•´ê²°)"""
    
    def __init__(self, input_dim, hidden_dim, dropout=0.2):
        super().__init__()
        
        self.input_dim = input_dim  # ì…ë ¥ ì°¨ì› (vision_dim + language_dim)
        self.hidden_dim = hidden_dim  # ìˆ¨ê²¨ì§„ ì°¨ì›
        self.dropout = dropout
        
        # ì…ë ¥ì„ hidden_dimìœ¼ë¡œ ë³€í™˜
        self.input_adapter = nn.Linear(input_dim, hidden_dim)
        
        # Multi-head Self Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Cross-modal Attention
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Vision-Language ìœµí•©
        self.vl_fusion = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # Language-Action ìœµí•©
        self.la_fusion = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # Action-Vision ìœµí•©
        self.av_fusion = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )
        
        # ì •ê·œí™” ë ˆì´ì–´ë“¤
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        self.norm_final = nn.LayerNorm(hidden_dim)
        
        # ìµœì¢… ìœµí•©
        self.final_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
    def forward(self, features):
        """
        ì…ë ¥: features [batch_size, input_dim]
        ì¶œë ¥: fused_features [batch_size, hidden_dim]
        """
        batch_size = features.shape[0]
        
        # 1. ì…ë ¥ì„ hidden_dimìœ¼ë¡œ ë³€í™˜
        adapted_features = self.input_adapter(features)  # [batch, hidden_dim]
        
        # 2. Self Attention
        features_expanded = adapted_features.unsqueeze(1)  # [batch, 1, hidden_dim]
        self_attended, _ = self.self_attention(
            query=features_expanded,
            key=features_expanded,
            value=features_expanded
        )
        self_attended = self_attended.squeeze(1)  # [batch, hidden_dim]
        
        # Residual connection + norm
        features_self = self.norm1(adapted_features + self_attended)
        
        # 3. Vision-Language ìœµí•©
        vl_fused = self.vl_fusion(features_self)
        vl_output = self.norm1(features_self + vl_fused)
        
        # 4. Language-Action ìœµí•©  
        la_fused = self.la_fusion(vl_output)
        la_output = self.norm2(vl_output + la_fused)
        
        # 5. Action-Vision ìœµí•©
        av_fused = self.av_fusion(la_output)
        av_output = self.norm3(la_output + av_fused)
        
        # 6. ìµœì¢… ìœµí•© (3ê°œ ê²½ë¡œ ê²°í•©)
        combined = torch.cat([vl_output, la_output, av_output], dim=-1)
        final_output = self.final_fusion(combined)
        
        # ìµœì¢… ì •ê·œí™”
        final_output = self.norm_final(final_output)
        
        return final_output

class FixedHierarchicalPlanner(nn.Module):
    """ìˆ˜ì •ëœ Hierarchical Planning"""
    
    def __init__(self, hidden_dim, action_dim, dropout=0.2):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.dropout = dropout
        
        # ê³ ìˆ˜ì¤€ ëª©í‘œ ê³„íš
        self.high_level_planner = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # ì¤‘ê°„ ìˆ˜ì¤€ ì„œë¸Œê³¨ ê³„íš
        self.mid_level_planner = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim)
        )
        
        # ì €ìˆ˜ì¤€ ì•¡ì…˜ ê³„íš
        self.low_level_planner = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, hidden_dim)
        )
        
        # ê³„ì¸µë³„ ìœµí•©
        self.hierarchical_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # ì •ê·œí™”
        self.norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, features):
        """ê³„ì¸µì  ê³„íš ìˆ˜í–‰"""
        
        # ê° ìˆ˜ì¤€ì—ì„œ ê³„íš ìˆ˜í–‰
        high_level = self.high_level_planner(features)
        mid_level = self.mid_level_planner(features)
        low_level = self.low_level_planner(features)
        
        # ê³„ì¸µë³„ ê²°ê³¼ ìœµí•©
        hierarchical_combined = torch.cat([high_level, mid_level, low_level], dim=-1)
        hierarchical_output = self.hierarchical_fusion(hierarchical_combined)
        
        # Residual connection + norm
        output = self.norm(features + hierarchical_output)
        
        return output

class FixedAdvancedAttention(nn.Module):
    """ìˆ˜ì •ëœ Advanced Attention"""
    
    def __init__(self, hidden_dim, dropout=0.2):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.dropout = dropout
        
        # Multi-head Self Attention
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        # Temporal Attention (ì‹œê°„ì  ì˜ì¡´ì„±)
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # Spatial Attention (ê³µê°„ì  ì˜ì¡´ì„±)
        self.spatial_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # ì •ê·œí™”
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.norm3 = nn.LayerNorm(hidden_dim)
        
        # FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )
        
        # ìµœì¢… ìœµí•©
        self.attention_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
    def forward(self, features):
        """ê³ ê¸‰ ì–´í…ì…˜ ìˆ˜í–‰"""
        
        # ì…ë ¥ í™•ì¥ (ì–´í…ì…˜ì„ ìœ„í•´)
        features_expanded = features.unsqueeze(1)  # [batch, 1, hidden_dim]
        
        # Self Attention
        self_attended, _ = self.self_attention(
            query=features_expanded,
            key=features_expanded, 
            value=features_expanded
        )
        self_attended = self_attended.squeeze(1)
        self_output = self.norm1(features + self_attended)
        
        # Temporal Attention
        temporal_attended, _ = self.temporal_attention(
            query=features_expanded,
            key=features_expanded,
            value=features_expanded
        )
        temporal_attended = temporal_attended.squeeze(1)
        temporal_output = self.norm2(features + temporal_attended)
        
        # Spatial Attention
        spatial_attended, _ = self.spatial_attention(
            query=features_expanded,
            key=features_expanded,
            value=features_expanded
        )
        spatial_attended = spatial_attended.squeeze(1)
        spatial_output = self.norm3(features + spatial_attended)
        
        # ëª¨ë“  ì–´í…ì…˜ ê²°ê³¼ ìœµí•©
        attention_combined = torch.cat([self_output, temporal_output, spatial_output], dim=-1)
        attention_fused = self.attention_fusion(attention_combined)
        
        # FFN ì ìš©
        ffn_output = self.ffn(attention_fused)
        
        # ìµœì¢… residual connection
        final_output = features + attention_fused + ffn_output
        
        return final_output

def test_fixed_components():
    """ìˆ˜ì •ëœ ì»´í¬ë„ŒíŠ¸ë“¤ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª ìˆ˜ì •ëœ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    batch_size = 8
    input_dim = 2048  # vision_dim + language_dim
    hidden_dim = 512
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    test_input = torch.randn(batch_size, input_dim)
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì…ë ¥: {test_input.shape}")
    
    # 1. Fixed Claw Matrix í…ŒìŠ¤íŠ¸
    print(f"\n1. Fixed Claw Matrix í…ŒìŠ¤íŠ¸:")
    try:
        claw = FixedClawMatrixFusion(input_dim, hidden_dim)
        claw_output = claw(test_input)
        print(f"   âœ… ì„±ê³µ! ì¶œë ¥: {claw_output.shape}")
    except Exception as e:
        print(f"   âŒ ì‹¤íŒ¨: {e}")
    
    # 2. Fixed Hierarchical Planner í…ŒìŠ¤íŠ¸
    print(f"\n2. Fixed Hierarchical Planner í…ŒìŠ¤íŠ¸:")
    try:
        hierarchical = FixedHierarchicalPlanner(hidden_dim, 3)
        # hidden_dim ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        hidden_input = torch.randn(batch_size, hidden_dim)
        hierarchical_output = hierarchical(hidden_input)
        print(f"   âœ… ì„±ê³µ! ì¶œë ¥: {hierarchical_output.shape}")
    except Exception as e:
        print(f"   âŒ ì‹¤íŒ¨: {e}")
    
    # 3. Fixed Advanced Attention í…ŒìŠ¤íŠ¸
    print(f"\n3. Fixed Advanced Attention í…ŒìŠ¤íŠ¸:")
    try:
        attention = FixedAdvancedAttention(hidden_dim)
        # hidden_dim ì…ë ¥ìœ¼ë¡œ ë³€í™˜
        hidden_input = torch.randn(batch_size, hidden_dim)
        attention_output = attention(hidden_input)
        print(f"   âœ… ì„±ê³µ! ì¶œë ¥: {attention_output.shape}")
    except Exception as e:
        print(f"   âŒ ì‹¤íŒ¨: {e}")
    
    # 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    print(f"\n4. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:")
    try:
        claw = FixedClawMatrixFusion(input_dim, hidden_dim)
        hierarchical = FixedHierarchicalPlanner(hidden_dim, 3)
        attention = FixedAdvancedAttention(hidden_dim)
        
        # ìˆœì°¨ ì²˜ë¦¬
        x = test_input
        x = claw(x)  # input_dim -> hidden_dim
        x = attention(x)  # hidden_dim -> hidden_dim  
        x = hierarchical(x)  # hidden_dim -> hidden_dim
        
        print(f"   âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
        print(f"   ğŸ“Š ìµœì¢… ì¶œë ¥: {x.shape}")
        
    except Exception as e:
        print(f"   âŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_fixed_components()
