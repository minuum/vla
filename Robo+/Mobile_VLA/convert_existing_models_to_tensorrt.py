#!/usr/bin/env python3
"""
ê¸°ì¡´ ì–‘ìí™”ëœ ëª¨ë¸ë“¤ì„ TensorRTë¡œ ë³€í™˜
- ì´ë¯¸ ìˆëŠ” ONNX ëª¨ë¸ë“¤ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜
- FP16/INT8 ì–‘ìí™” ì§€ì›
"""

import os
import subprocess
import json
import time
from typing import Dict, Any

class ExistingModelsTensorRTConverter:
    """ê¸°ì¡´ ëª¨ë¸ë“¤ì„ TensorRTë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_dir = "Robo+/Mobile_VLA"
        self.output_dir = "Robo+/Mobile_VLA/tensorrt_engines"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ê¸°ì¡´ ONNX ëª¨ë¸ë“¤
        self.existing_models = {
            'accurate_gpu': 'accurate_gpu_quantized/accurate_gpu_model.onnx',
            'simple_gpu': 'simple_gpu_quantized/simple_gpu_model.onnx',
            'cpu_mae0222': 'quantized_models_cpu/mae0222_model_cpu.onnx'
        }
        
    def check_trtexec_availability(self):
        """trtexec ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            result = subprocess.run(['trtexec', '--help'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… trtexec is available")
                return True
            else:
                print("âŒ trtexec is not working properly")
                return False
        except FileNotFoundError:
            print("âŒ trtexec not found. Please install TensorRT.")
            return False
    
    def convert_onnx_to_tensorrt(self, model_name: str, onnx_path: str, precision: str = "fp16"):
        """ONNX ëª¨ë¸ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜"""
        print(f"ğŸ”¨ Converting {model_name} to TensorRT {precision.upper()}")
        
        # ì¶œë ¥ ê²½ë¡œ
        engine_path = os.path.join(self.output_dir, f"{model_name}_{precision}.engine")
        
        # trtexec ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            'trtexec',
            '--onnx=' + onnx_path,
            '--saveEngine=' + engine_path,
            '--workspace=1024',
            '--verbose'
        ]
        
        # ì •ë°€ë„ ì„¤ì •
        if precision == "fp16":
            cmd.append('--fp16')
        elif precision == "int8":
            cmd.append('--int8')
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì • (í•„ìš”ì‹œ)
        cmd.extend([
            '--minShapes=pixel_values:1x3x224x224',
            '--optShapes=pixel_values:1x3x224x224',
            '--maxShapes=pixel_values:4x3x224x224'
        ])
        
        print(f"Running command: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… TensorRT engine created: {engine_path}")
                
                # ì—”ì§„ íŒŒì¼ í¬ê¸° í™•ì¸
                if os.path.exists(engine_path):
                    size_mb = os.path.getsize(engine_path) / (1024 * 1024)
                    print(f"ğŸ“Š Engine size: {size_mb:.1f} MB")
                
                return engine_path
            else:
                print(f"âŒ TensorRT conversion failed for {model_name}")
                print(f"Error: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ Error converting {model_name}: {e}")
            return None
    
    def convert_all_models(self):
        """ëª¨ë“  ê¸°ì¡´ ëª¨ë¸ì„ TensorRTë¡œ ë³€í™˜"""
        print("ğŸš€ Starting conversion of existing models to TensorRT")
        
        # trtexec í™•ì¸
        if not self.check_trtexec_availability():
            print("âš ï¸ Skipping TensorRT conversion due to missing trtexec")
            return
        
        results = {}
        
        for model_name, onnx_relative_path in self.existing_models.items():
            onnx_path = os.path.join(self.base_dir, onnx_relative_path)
            
            if not os.path.exists(onnx_path):
                print(f"âš ï¸ ONNX model not found: {onnx_path}")
                continue
            
            print(f"\nğŸ“ Processing {model_name}: {onnx_path}")
            
            # ONNX íŒŒì¼ í¬ê¸° í™•ì¸
            onnx_size_mb = os.path.getsize(onnx_path) / (1024 * 1024)
            print(f"ğŸ“Š ONNX size: {onnx_size_mb:.1f} MB")
            
            model_results = {}
            
            # FP16 ë³€í™˜
            fp16_engine = self.convert_onnx_to_tensorrt(model_name, onnx_path, "fp16")
            if fp16_engine:
                model_results['fp16'] = fp16_engine
            
            # INT8 ë³€í™˜ (ì„ íƒì )
            try:
                int8_engine = self.convert_onnx_to_tensorrt(model_name, onnx_path, "int8")
                if int8_engine:
                    model_results['int8'] = int8_engine
            except Exception as e:
                print(f"âš ï¸ INT8 conversion failed for {model_name}: {e}")
            
            results[model_name] = model_results
        
        return results
    
    def create_benchmark_script(self, results: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        print("ğŸ”§ Creating benchmark script")
        
        benchmark_script = '''#!/usr/bin/env python3
"""
TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
- ìƒì„±ëœ TensorRT ì—”ì§„ë“¤ì˜ ì„±ëŠ¥ ì¸¡ì •
"""

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
import time
import json
import os

def benchmark_engine(engine_path: str, num_runs: int = 100):
    """TensorRT ì—”ì§„ ë²¤ì¹˜ë§ˆí¬"""
    if not os.path.exists(engine_path):
        print(f"âŒ Engine not found: {engine_path}")
        return None
    
    try:
        # ì—”ì§„ ë¡œë“œ
        with open(engine_path, "rb") as f:
            engine_data = f.read()
        
        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
        engine = runtime.deserialize_cuda_engine(engine_data)
        context = engine.create_execution_context()
        
        # ë©”ëª¨ë¦¬ í• ë‹¹
        input_size = 1 * 3 * 224 * 224 * 4  # float32
        output_size = 1 * 3 * 4  # float32
        
        input_mem = cuda.mem_alloc(input_size)
        output_mem = cuda.mem_alloc(output_size)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_input = np.random.randn(1, 3, 224, 224).astype(np.float32)
        
        # ì›Œë°ì—…
        for _ in range(10):
            cuda.memcpy_htod(input_mem, test_input)
            context.execute_v2(bindings=[int(input_mem), int(output_mem)])
        
        # ë²¤ì¹˜ë§ˆí¬
        times = []
        for i in range(num_runs):
            start_time = time.time()
            
            cuda.memcpy_htod(input_mem, test_input)
            context.execute_v2(bindings=[int(input_mem), int(output_mem)])
            
            inference_time = time.time() - start_time
            times.append(inference_time)
            
            if (i + 1) % 20 == 0:
                print(f"Progress: {i + 1}/{num_runs}")
        
        # ê²°ê³¼ ë¶„ì„
        avg_time = np.mean(times)
        std_time = np.std(times)
        min_time = np.min(times)
        max_time = np.max(times)
        fps = 1.0 / avg_time
        
        return {
            "average_time_ms": avg_time * 1000,
            "std_time_ms": std_time * 1000,
            "min_time_ms": min_time * 1000,
            "max_time_ms": max_time * 1000,
            "fps": fps,
            "num_runs": num_runs
        }
        
    except Exception as e:
        print(f"âŒ Benchmark failed for {engine_path}: {e}")
        return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting TensorRT Engine Benchmark")
    
    # ì—”ì§„ ë””ë ‰í† ë¦¬
    engine_dir = "Robo+/Mobile_VLA/tensorrt_engines"
    
    if not os.path.exists(engine_dir):
        print(f"âŒ Engine directory not found: {engine_dir}")
        return
    
    # ëª¨ë“  ì—”ì§„ íŒŒì¼ ì°¾ê¸°
    engine_files = []
    for file in os.listdir(engine_dir):
        if file.endswith('.engine'):
            engine_files.append(os.path.join(engine_dir, file))
    
    if not engine_files:
        print("âŒ No TensorRT engines found")
        return
    
    print(f"ğŸ“ Found {len(engine_files)} TensorRT engines")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = {}
    for engine_path in engine_files:
        engine_name = os.path.basename(engine_path)
        print(f"\nğŸ§ª Benchmarking {engine_name}")
        
        benchmark_result = benchmark_engine(engine_path, num_runs=50)
        if benchmark_result:
            results[engine_name] = benchmark_result
            
            print(f"ğŸ“Š Results for {engine_name}:")
            print(f"  Average: {benchmark_result['average_time_ms']:.2f} ms")
            print(f"  FPS: {benchmark_result['fps']:.1f}")
    
    # ê²°ê³¼ ì €ì¥
    if results:
        results_path = os.path.join(engine_dir, "benchmark_results.json")
        with open(results_path, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ… Benchmark results saved: {results_path}")
        
        # ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ“Š Performance Comparison:")
        for engine_name, result in results.items():
            print(f"  {engine_name}: {result['average_time_ms']:.2f} ms ({result['fps']:.1f} FPS)")

if __name__ == "__main__":
    main()
'''
        
        benchmark_path = os.path.join(self.output_dir, "benchmark_engines.py")
        with open(benchmark_path, "w") as f:
            f.write(benchmark_script)
        
        os.chmod(benchmark_path, 0o755)
        print(f"âœ… Benchmark script created: {benchmark_path}")
        
        return benchmark_path
    
    def create_usage_guide(self, results: Dict[str, Any]):
        """ì‚¬ìš© ê°€ì´ë“œ ìƒì„±"""
        print("ğŸ“– Creating usage guide")
        
        guide = f"""# ê¸°ì¡´ ëª¨ë¸ TensorRT ë³€í™˜ ê°€ì´ë“œ

## ê°œìš”
ê¸°ì¡´ì— ì–‘ìí™”ëœ ONNX ëª¨ë¸ë“¤ì„ TensorRT ì—”ì§„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³ ì„±ëŠ¥ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ë³€í™˜ëœ ì—”ì§„ë“¤
"""
        
        for model_name, model_results in results.items():
            guide += f"\n### {model_name}\n"
            for precision, engine_path in model_results.items():
                if os.path.exists(engine_path):
                    size_mb = os.path.getsize(engine_path) / (1024 * 1024)
                    guide += f"- **{precision.upper()}**: `{os.path.basename(engine_path)}` ({size_mb:.1f} MB)\n"
        
        guide += f"""
## ì‚¬ìš© ë°©ë²•

### 1. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
```bash
cd {self.output_dir}
python benchmark_engines.py
```

### 2. ROS ë…¸ë“œì—ì„œ ì‚¬ìš©
```bash
# FP16 ì—”ì§„ ì‚¬ìš©
ros2 run mobile_vla_package tensorrt_inference_node --ros-args \\
    -p engine_path:={self.output_dir}/accurate_gpu_fp16.engine

# INT8 ì—”ì§„ ì‚¬ìš©
ros2 run mobile_vla_package tensorrt_inference_node --ros-args \\
    -p engine_path:={self.output_dir}/simple_gpu_int8.engine
```

### 3. ì„±ëŠ¥ ì˜ˆìƒ
- **FP16**: 2-5x ì†ë„ í–¥ìƒ
- **INT8**: 5-10x ì†ë„ í–¥ìƒ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 50-80% ê°ì†Œ

## ìš”êµ¬ì‚¬í•­
- NVIDIA GPU
- TensorRT 8.x
- CUDA 11.x ì´ìƒ

## ë¬¸ì œ í•´ê²°
1. TensorRT ì„¤ì¹˜: `pip install tensorrt`
2. ê¶Œí•œ ë¬¸ì œ: `chmod +x benchmark_engines.py`
3. ë©”ëª¨ë¦¬ ë¶€ì¡±: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ í¬ê¸° ì¡°ì •
"""
        
        guide_path = os.path.join(self.output_dir, "README.md")
        with open(guide_path, "w") as f:
            f.write(guide)
        
        print(f"âœ… Usage guide created: {guide_path}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Starting Existing Models TensorRT Conversion")
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = ExistingModelsTensorRTConverter()
    
    try:
        # ëª¨ë“  ëª¨ë¸ ë³€í™˜
        results = converter.convert_all_models()
        
        if results:
            # ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
            print("\nğŸ”§ Creating benchmark script...")
            converter.create_benchmark_script(results)
            
            # ì‚¬ìš© ê°€ì´ë“œ ìƒì„±
            print("\nğŸ“– Creating usage guide...")
            converter.create_usage_guide(results)
            
            print("\nâœ… TensorRT conversion completed!")
            print(f"\nğŸ“ Output directory: {converter.output_dir}")
            print("ğŸ”§ Next steps:")
            print("  1. cd Robo+/Mobile_VLA/tensorrt_engines")
            print("  2. python benchmark_engines.py")
            print("  3. Use the generated TensorRT engines in ROS nodes")
        else:
            print("\nâš ï¸ No models were successfully converted")
        
    except Exception as e:
        print(f"âŒ TensorRT conversion failed: {e}")
        raise

if __name__ == "__main__":
    main()
