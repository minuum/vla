#!/usr/bin/env python3
"""
ğŸ—ï¸ Hierarchical Planning Implementation
18í”„ë ˆì„ ì˜ˆì¸¡ì„ ìœ„í•œ ê³„ì¸µì  ê³„íš ëª¨ë¸
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List
import math

class GoalDecomposition(nn.Module):
    """
    ëª©í‘œ ë¶„í•´: ì¥ê¸° ëª©í‘œë¥¼ ë‹¨ê¸° ëª©í‘œë¡œ ë¶„í•´
    """
    def __init__(self, 
                 input_dim: int = 768,
                 goal_dim: int = 256,
                 num_subgoals: int = 6,  # 18í”„ë ˆì„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
                 num_heads: int = 8):
        super().__init__()
        
        self.input_dim = input_dim
        self.goal_dim = goal_dim
        self.num_subgoals = num_subgoals
        self.num_heads = num_heads
        
        # ëª©í‘œ ì¸ì½”ë”
        self.goal_encoder = nn.Sequential(
            nn.Linear(input_dim, goal_dim * 2),
            nn.LayerNorm(goal_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(goal_dim * 2, goal_dim)
        )
        
        # í•˜ìœ„ ëª©í‘œ ìƒì„±ê¸°
        self.subgoal_generator = nn.MultiheadAttention(
            embed_dim=goal_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # í•˜ìœ„ ëª©í‘œ í”„ë¡œì ì…˜
        self.subgoal_proj = nn.Linear(goal_dim, goal_dim)
        
        # ì‹œê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©
        self.temporal_embedding = nn.Embedding(num_subgoals, goal_dim)
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        ëª©í‘œ ë¶„í•´
        
        Args:
            features: [batch_size, seq_len, input_dim]
            
        Returns:
            subgoals: [batch_size, num_subgoals, goal_dim]
        """
        batch_size = features.size(0)
        
        # ëª©í‘œ ì¸ì½”ë”©
        goal_features = self.goal_encoder(features.mean(dim=1))  # [batch_size, goal_dim]
        goal_features = goal_features.unsqueeze(1).expand(-1, self.num_subgoals, -1)
        
        # ì‹œê°„ì  ìœ„ì¹˜ ì¸ì½”ë”©
        temporal_pos = torch.arange(self.num_subgoals, device=features.device)
        temporal_emb = self.temporal_embedding(temporal_pos).unsqueeze(0).expand(batch_size, -1, -1)
        
        # í•˜ìœ„ ëª©í‘œ ìƒì„±
        subgoals, _ = self.subgoal_generator(
            query=goal_features + temporal_emb,
            key=goal_features + temporal_emb,
            value=goal_features + temporal_emb
        )
        
        subgoals = self.subgoal_proj(subgoals)
        
        return subgoals

class HierarchicalPlanner(nn.Module):
    """
    ê³„ì¸µì  ê³„íšì: ì¥ê¸° ê³„íšê³¼ ë‹¨ê¸° ì‹¤í–‰ì„ ë¶„ë¦¬
    """
    def __init__(self, 
                 input_dim: int = 768,
                 plan_dim: int = 256,
                 action_dim: int = 3,
                 num_subgoals: int = 6,
                 frames_per_subgoal: int = 3,  # ê° í•˜ìœ„ ëª©í‘œë‹¹ 3í”„ë ˆì„
                 num_heads: int = 8):
        super().__init__()
        
        self.input_dim = input_dim
        self.plan_dim = plan_dim
        self.action_dim = action_dim
        self.num_subgoals = num_subgoals
        self.frames_per_subgoal = frames_per_subgoal
        self.total_frames = num_subgoals * frames_per_subgoal  # 18í”„ë ˆì„
        
        # ëª©í‘œ ë¶„í•´
        self.goal_decomposition = GoalDecomposition(
            input_dim=input_dim,
            goal_dim=plan_dim,
            num_subgoals=num_subgoals,
            num_heads=num_heads
        )
        
        # ì¥ê¸° ê³„íšì (High-level planner)
        self.high_level_planner = nn.TransformerEncoderLayer(
            d_model=plan_dim,
            nhead=num_heads,
            dim_feedforward=plan_dim * 4,
            dropout=0.1,
            batch_first=True
        )
        
        # ë‹¨ê¸° ì‹¤í–‰ì (Low-level executor)
        self.low_level_executor = nn.ModuleList([
            nn.Sequential(
                nn.Linear(plan_dim + input_dim, plan_dim),
                nn.LayerNorm(plan_dim),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(plan_dim, plan_dim // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(plan_dim // 2, action_dim)
            ) for _ in range(frames_per_subgoal)
        ])
        
        # ê³„íš-ì‹¤í–‰ ì—°ê²°
        self.plan_execution_fusion = nn.MultiheadAttention(
            embed_dim=plan_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # ì¶œë ¥ ì •ê·œí™”
        self.output_norm = nn.LayerNorm(action_dim)
        
    def forward(self, 
                features: torch.Tensor,
                current_actions: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ê³„ì¸µì  ê³„íš ë° ì‹¤í–‰
        
        Args:
            features: [batch_size, seq_len, input_dim]
            current_actions: [batch_size, seq_len, action_dim] (ì„ íƒì‚¬í•­)
            
        Returns:
            predicted_actions: [batch_size, total_frames, action_dim]
        """
        batch_size = features.size(0)
        
        # 1. ëª©í‘œ ë¶„í•´
        subgoals = self.goal_decomposition(features)  # [batch_size, num_subgoals, plan_dim]
        
        # 2. ì¥ê¸° ê³„íš ìˆ˜ë¦½
        high_level_plan = self.high_level_planner(subgoals)  # [batch_size, num_subgoals, plan_dim]
        
        # 3. ë‹¨ê¸° ì‹¤í–‰ ê³„íš
        all_actions = []
        
        for subgoal_idx in range(self.num_subgoals):
            subgoal = high_level_plan[:, subgoal_idx:subgoal_idx+1, :]  # [batch_size, 1, plan_dim]
            
            # í˜„ì¬ ìƒíƒœì™€ í•˜ìœ„ ëª©í‘œ ìœµí•©
            current_state = features.mean(dim=1, keepdim=True)  # [batch_size, 1, input_dim]
            plan_state = torch.cat([subgoal, current_state], dim=-1)  # [batch_size, 1, plan_dim + input_dim]
            
            # ê° í”„ë ˆì„ë³„ ì•¡ì…˜ ìƒì„±
            subgoal_actions = []
            for frame_idx in range(self.frames_per_subgoal):
                executor = self.low_level_executor[frame_idx]
                action = executor(plan_state)  # [batch_size, 1, action_dim]
                subgoal_actions.append(action)
            
            subgoal_actions = torch.cat(subgoal_actions, dim=1)  # [batch_size, frames_per_subgoal, action_dim]
            all_actions.append(subgoal_actions)
        
        # ëª¨ë“  ì•¡ì…˜ ê²°í•©
        predicted_actions = torch.cat(all_actions, dim=1)  # [batch_size, total_frames, action_dim]
        
        # ì¶œë ¥ ì •ê·œí™”
        predicted_actions = self.output_norm(predicted_actions)
        
        return predicted_actions

class HierarchicalPlanningModel(nn.Module):
    """
    ì™„ì „í•œ ê³„ì¸µì  ê³„íš ëª¨ë¸
    """
    def __init__(self, 
                 vision_dim: int = 768,
                 language_dim: int = 768,
                 action_dim: int = 3,
                 plan_dim: int = 256,
                 num_subgoals: int = 6,
                 frames_per_subgoal: int = 3,
                 num_heads: int = 8):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim
        self.plan_dim = plan_dim
        self.num_subgoals = num_subgoals
        self.frames_per_subgoal = frames_per_subgoal
        self.total_frames = num_subgoals * frames_per_subgoal
        
        # íŠ¹ì§• ìœµí•©
        self.feature_fusion = nn.Sequential(
            nn.Linear(vision_dim + language_dim, plan_dim * 2),
            nn.LayerNorm(plan_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(plan_dim * 2, plan_dim)
        )
        
        # ê³„ì¸µì  ê³„íšì
        self.hierarchical_planner = HierarchicalPlanner(
            input_dim=plan_dim,
            plan_dim=plan_dim,
            action_dim=action_dim,
            num_subgoals=num_subgoals,
            frames_per_subgoal=frames_per_subgoal,
            num_heads=num_heads
        )
        
        # ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥
        self.temporal_consistency = nn.LSTM(
            input_size=action_dim,
            hidden_size=action_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.1,
            bidirectional=True
        )
        
        # ìµœì¢… ì•¡ì…˜ í—¤ë“œ
        self.final_action_head = nn.Sequential(
            nn.Linear(action_dim * 2, action_dim),
            nn.LayerNorm(action_dim),
            nn.Tanh()  # ì•¡ì…˜ ë²”ìœ„ ì œí•œ
        )
        
    def forward(self, 
                vision_features: torch.Tensor,
                language_features: torch.Tensor,
                current_actions: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ê³„ì¸µì  ê³„íš ëª¨ë¸ ìˆœì „íŒŒ
        
        Args:
            vision_features: [batch_size, seq_len, vision_dim]
            language_features: [batch_size, seq_len, language_dim]
            current_actions: [batch_size, seq_len, action_dim] (ì„ íƒì‚¬í•­)
            
        Returns:
            predicted_actions: [batch_size, total_frames, action_dim]
        """
        # íŠ¹ì§• ìœµí•©
        fused_features = torch.cat([vision_features, language_features], dim=-1)
        fused_features = self.feature_fusion(fused_features)
        
        # ê³„ì¸µì  ê³„íš
        planned_actions = self.hierarchical_planner(fused_features, current_actions)
        
        # ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥
        lstm_out, _ = self.temporal_consistency(planned_actions)
        
        # ìµœì¢… ì•¡ì…˜ ìƒì„±
        final_actions = self.final_action_head(lstm_out)
        
        return final_actions

def create_hierarchical_model(config: dict) -> HierarchicalPlanningModel:
    """
    ê³„ì¸µì  ê³„íš ëª¨ë¸ ìƒì„±
    
    Args:
        config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        HierarchicalPlanningModel ì¸ìŠ¤í„´ìŠ¤
    """
    return HierarchicalPlanningModel(
        vision_dim=config.get('vision_dim', 768),
        language_dim=config.get('language_dim', 768),
        action_dim=config.get('action_dim', 3),
        plan_dim=config.get('plan_dim', 256),
        num_subgoals=config.get('num_subgoals', 6),
        frames_per_subgoal=config.get('frames_per_subgoal', 3),
        num_heads=config.get('num_heads', 8)
    )

def test_hierarchical_model():
    """ê³„ì¸µì  ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê³„ì¸µì  ê³„íš ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ëª¨ë¸ ì„¤ì •
    config = {
        'vision_dim': 768,
        'language_dim': 768,
        'action_dim': 3,
        'plan_dim': 256,
        'num_subgoals': 6,
        'frames_per_subgoal': 3,
        'num_heads': 8
    }
    
    # ëª¨ë¸ ìƒì„±
    model = create_hierarchical_model(config)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 2
    seq_len = 8
    
    vision_features = torch.randn(batch_size, seq_len, config['vision_dim'])
    language_features = torch.randn(batch_size, seq_len, config['language_dim'])
    
    print(f"ì…ë ¥ í˜•íƒœ:")
    print(f"  Vision: {vision_features.shape}")
    print(f"  Language: {language_features.shape}")
    
    # ìˆœì „íŒŒ
    with torch.no_grad():
        predicted_actions = model(vision_features, language_features)
    
    print(f"ì¶œë ¥ í˜•íƒœ: {predicted_actions.shape}")
    print(f"ì˜ˆìƒ ì¶œë ¥: [batch_size, {config['num_subgoals'] * config['frames_per_subgoal']}, {config['action_dim']}]")
    print(f"ì‹¤ì œ ì¶œë ¥: {predicted_actions.shape}")
    
    # ê²€ì¦
    expected_frames = config['num_subgoals'] * config['frames_per_subgoal']
    assert predicted_actions.shape == (batch_size, expected_frames, config['action_dim']), \
        f"ì¶œë ¥ í˜•íƒœê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {predicted_actions.shape} vs ({batch_size}, {expected_frames}, {config['action_dim']})"
    
    print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print(f"ğŸ¯ 18í”„ë ˆì„ ì˜ˆì¸¡ ê°€ëŠ¥: {expected_frames} í”„ë ˆì„")
    
    return model

if __name__ == "__main__":
    test_hierarchical_model()
