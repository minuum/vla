#!/usr/bin/env python3
"""
ğŸ¯ ìµœì¢… ìˆ˜ì •: ì´ë¯¸ì§€+ì•¡ì…˜ í…ì„œ ë³€í™˜ + ê°„ë‹¨í•œ ì¦ê°•
"""

import torch
import numpy as np
import random
from pathlib import Path
import sys
import json
from datetime import datetime
import torchvision.transforms as transforms
from PIL import Image

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
ROOT_DIR = Path("/home/billy/25-1kp/vla/Robo+/Mobile_VLA")
DATA_DIR = Path("/home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset")
ROBOVLMS_DIR = Path("/home/billy/25-1kp/vla/RoboVLMs")

sys.path.append(str(ROOT_DIR))
sys.path.append(str(ROBOVLMS_DIR))

from robovlms.data.mobile_vla_dataset import MobileVLADataset
from robovlms.train.mobile_vla_trainer import MobileVLATrainer
from torch.utils.data import DataLoader

class FinalFixedTrainer(MobileVLATrainer):
    """ìµœì¢… ìˆ˜ì •: ì´ë¯¸ì§€+ì•¡ì…˜ í…ì„œ ë³€í™˜ + ê°„ë‹¨í•œ ì¦ê°•"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.z_weight = 0.05
        self.action_noise_std = 0.005  # ë§¤ìš° ì‘ì€ ë…¸ì´ì¦ˆ
        
        # ì´ë¯¸ì§€ ë³€í™˜
        self.image_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        print("âœ… FinalFixedTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   Zì¶• ê°€ì¤‘ì¹˜: {self.z_weight}")
        print(f"   ì•¡ì…˜ ë…¸ì´ì¦ˆ: Ïƒ={self.action_noise_std}")
        print(f"   ì´ë¯¸ì§€ ë³€í™˜: í™œì„±í™”")
    
    def compute_action_statistics(self, dataset):
        """ì•ˆì „í•œ ì•¡ì…˜ í†µê³„ ê³„ì‚°"""
        print("ğŸ“Š ì•¡ì…˜ í†µê³„ ê³„ì‚° ì¤‘...")
        
        all_actions = []
        for i in range(len(dataset)):
            episode = dataset[i]
            actions = episode['actions']
            if isinstance(actions, np.ndarray):
                actions = torch.from_numpy(actions)
            all_actions.append(actions)
        
        all_actions = torch.cat(all_actions, dim=0)
        
        self.action_mean = all_actions.mean(dim=0)
        self.action_std = all_actions.std(dim=0)
        
        # Zì¶• íŠ¹ë³„ ì²˜ë¦¬
        if self.action_std[2] < 1e-6:
            print("âš ï¸ Zì¶• í‘œì¤€í¸ì°¨ê°€ ë„ˆë¬´ ì‘ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            self.action_std[2] = 1.0
        
        self.action_std = torch.clamp(self.action_std, min=1e-3)
        
        print(f"   ì•¡ì…˜ ë²”ìœ„: {all_actions.min():.4f} ~ {all_actions.max():.4f}")
        print(f"   ì•¡ì…˜ í‰ê· : {self.action_mean}")
        print(f"   ì•¡ì…˜ í‘œì¤€í¸ì°¨: {self.action_std}")
    
    def safe_normalize_actions(self, actions):
        """ì•ˆì „í•œ ì•¡ì…˜ ì •ê·œí™” (ë°°ì¹˜ ì°¨ì› ìœ ì§€)"""
        # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
        if isinstance(actions, list):
            actions = torch.tensor(actions, dtype=torch.float32)
        elif isinstance(actions, np.ndarray):
            actions = torch.from_numpy(actions).float()
        
        # ì°¨ì› í™•ì¸ ë° ì¡°ì •
        if actions.dim() == 2:  # [T, 3]
            actions = actions.unsqueeze(0)  # [1, T, 3]
        
        normalized = torch.zeros_like(actions)
        for i in range(3):
            if self.action_std[i] > 1e-6:
                normalized[:, :, i] = (actions[:, :, i] - self.action_mean[i]) / self.action_std[i]
            else:
                normalized[:, :, i] = actions[:, :, i] - self.action_mean[i]
        
        # ë°°ì¹˜ ì°¨ì› ìœ ì§€ (squeeze ì œê±°)
        return normalized
    
    def process_images_to_tensor(self, images):
        """ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜"""
        if isinstance(images, list):
            # PIL ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            tensor_images = []
            for img in images:
                if isinstance(img, Image.Image):
                    tensor_img = self.image_transform(img)
                elif isinstance(img, np.ndarray):
                    # numpyë¥¼ PILë¡œ ë³€í™˜ í›„ í…ì„œë¡œ
                    pil_img = Image.fromarray(img.astype(np.uint8))
                    tensor_img = self.image_transform(pil_img)
                elif isinstance(img, torch.Tensor):
                    tensor_img = img
                else:
                    print(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì´ë¯¸ì§€ íƒ€ì…: {type(img)}")
                    continue
                tensor_images.append(tensor_img)
            
            if tensor_images:
                return torch.stack(tensor_images)  # [T, C, H, W]
            else:
                return None
        else:
            return images
    
    def train_step_final_fixed(self, batch):
        """ìµœì¢… ìˆ˜ì •ëœ í•™ìŠµ ìŠ¤í…"""
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            images = batch['images']
            images_tensor = self.process_images_to_tensor(images)
            
            if images_tensor is None:
                print("âš ï¸ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
                return {'total_loss': torch.tensor(1.0, device=self.device), 'mae_avg': 1.0}
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if images_tensor.dim() == 3:  # [T, C, H, W]
                images_tensor = images_tensor.unsqueeze(0)  # [1, T, C, H, W]
            
            print(f"   ì´ë¯¸ì§€ í…ì„œ shape: {images_tensor.shape}")
            
            # ì•¡ì…˜ ì²˜ë¦¬
            actions = batch['actions']
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            if isinstance(actions, list):
                actions = torch.tensor(actions, dtype=torch.float32)
            elif isinstance(actions, np.ndarray):
                actions = torch.from_numpy(actions).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if actions.dim() == 2:  # [T, 3]
                actions = actions.unsqueeze(0)  # [1, T, 3]
            
            print(f"   ì•¡ì…˜ í…ì„œ shape: {actions.shape}")
            
            # ğŸ¯ íƒœìŠ¤í¬ íŠ¹ì„± ê¸°ë°˜ ë§ì¶¤í˜• ì¦ê°•
            # 1. ì¢Œìš° ëŒ€ì¹­ (2D ì´ë™ì´ë¯€ë¡œ ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹)
            if random.random() < 0.5:
                images_tensor = torch.flip(images_tensor, [-1])  # ê°€ë¡œ ë’¤ì§‘ê¸°
                actions[:, :, 1] = -actions[:, :, 1]  # Yì¶• ë¶€í˜¸ ë³€ê²½ (ì¸¡ë©´ ì´ë™)
            
            # 2. ì „ì§„/í›„ì§„ ë’¤ì§‘ê¸° (Xì¶• ìš°ì„¸ì´ë¯€ë¡œ íš¨ê³¼ì )
            if random.random() < 0.3:
                images_tensor = torch.flip(images_tensor, [1])  # ì‹œê°„ì¶• ë’¤ì§‘ê¸°
                actions[:, :, 0] = -actions[:, :, 0]  # Xì¶• ë¶€í˜¸ ë³€ê²½ (ì „ì§„/í›„ì§„)
            
            # 3. ì•¡ì…˜ ë…¸ì´ì¦ˆ (ì„¼ì„œ ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜)
            if random.random() < 0.8:
                # Xì¶• (ì£¼ìš” ì´ë™ì¶•)ì— ì‘ì€ ë…¸ì´ì¦ˆ
                x_noise = torch.normal(0, self.action_noise_std, actions[:, :, 0:1].shape)
                actions[:, :, 0:1] += x_noise
                
                # Yì¶• (ë³´ì¡° ì´ë™ì¶•)ì— ë” ì‘ì€ ë…¸ì´ì¦ˆ
                y_noise = torch.normal(0, self.action_noise_std * 0.5, actions[:, :, 1:2].shape)
                actions[:, :, 1:2] += y_noise
                
                # Zì¶•ì€ 0ì´ë¯€ë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            
            # 4. ì†ë„ ë³€í™” (ë‹¤ì–‘í•œ ì†ë„ë¡œ ì´ë™)
            if random.random() < 0.3:
                speed_scale = random.uniform(0.8, 1.2)
                actions[:, :, 0] *= speed_scale  # Xì¶•ë§Œ ìŠ¤ì¼€ì¼ë§
            
            # 5. ì‹œì‘-ì •ì§€ íŒ¨í„´ (ì •ì§€ ìƒíƒœê°€ ì ìœ¼ë¯€ë¡œ í•™ìŠµ)
            if random.random() < 0.2:
                # ì—í”¼ì†Œë“œ ì‹œì‘ ë¶€ë¶„ì— ì •ì§€ íŒ¨í„´ ì¶”ê°€
                if random.random() < 0.5:
                    stop_frames = random.randint(1, 3)
                    actions[:, :stop_frames, :] = 0
                
                # ì—í”¼ì†Œë“œ ì¤‘ê°„ì— ì§§ì€ ì •ì§€ ì¶”ê°€
                if random.random() < 0.3:
                    mid_point = actions.shape[1] // 2
                    actions[:, mid_point:mid_point+1, :] = 0
            
            # ë²”ìœ„ ì œí•œ
            actions = torch.clamp(actions, -1.15, 1.15)
            
            # ì •ê·œí™”
            if hasattr(self, 'action_mean'):
                actions = self.safe_normalize_actions(actions)
            
            # ë°°ì¹˜ ì—…ë°ì´íŠ¸
            batch['images'] = images_tensor
            batch['actions'] = actions
            
            # ê¸°ì¡´ train_step í˜¸ì¶œ
            result = super().train_step(batch)
            
            # NaN ì²´í¬
            if torch.isnan(result['total_loss']):
                print("âš ï¸ NaN loss detected - using fallback")
                result['total_loss'] = torch.tensor(1.0, device=self.device)
            
            return result
            
        except Exception as e:
            print(f"í•™ìŠµ ìŠ¤í… ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {'total_loss': torch.tensor(1.0, device=self.device), 'mae_avg': 1.0}
    
    def validation_step(self, batch):
        """ê²€ì¦ ìŠ¤í… (gradient ê³„ì‚° ì—†ìŒ)"""
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬
            images = batch['images']
            images_tensor = self.process_images_to_tensor(images)
            
            if images_tensor is None:
                return {'total_loss': 1.0, 'mae_avg': 1.0}
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if images_tensor.dim() == 3:  # [T, C, H, W]
                images_tensor = images_tensor.unsqueeze(0)  # [1, T, C, H, W]
            
            # ì•¡ì…˜ ì²˜ë¦¬
            actions = batch['actions']
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            if isinstance(actions, list):
                actions = torch.tensor(actions, dtype=torch.float32)
            elif isinstance(actions, np.ndarray):
                actions = torch.from_numpy(actions).float()
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            if actions.dim() == 2:  # [T, 3]
                actions = actions.unsqueeze(0)  # [1, T, 3]
            
            # ì •ê·œí™” (ë…¸ì´ì¦ˆ ì—†ì´)
            if hasattr(self, 'action_mean'):
                actions = self.safe_normalize_actions(actions)
            
            # ë°°ì¹˜ ì—…ë°ì´íŠ¸
            batch['images'] = images_tensor
            batch['actions'] = actions
            
            # ëª¨ë¸ ì˜ˆì¸¡ (gradient ì—†ì´)
            images = batch["images"]
            actions = batch["actions"]
            
            # Window/Chunk ë¶„í• 
            batch_size, sequence_length = images.shape[:2]
            
            if sequence_length >= self.window_size + self.chunk_size:
                window_images = images[:, :self.window_size]
                chunk_actions = actions[:, self.window_size:self.window_size + self.chunk_size]
            else:
                window_images = images[:, :min(sequence_length, self.window_size)]
                chunk_actions = actions[:, -self.chunk_size:] if sequence_length >= self.chunk_size else actions
            
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            task_descriptions = ["Navigate around obstacles to track the target cup"] * batch_size
            text_inputs = self.processor(text=task_descriptions, return_tensors="pt", padding=True, truncation=True)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            window_images = window_images.to(self.device)
            chunk_actions = chunk_actions.to(self.device)
            input_ids = text_inputs["input_ids"].to(self.device)
            attention_mask = text_inputs["attention_mask"].to(self.device)
            
            # Forward pass
            predictions = self.model(window_images, input_ids, attention_mask)
            targets = {"action_chunk": chunk_actions}
            
            # Loss ê³„ì‚°
            loss_dict = self.compute_loss(predictions, targets)
            
            return {
                'total_loss': loss_dict["total_loss"].item(),
                'mae_avg': loss_dict["mae_avg"]
            }
            
        except Exception as e:
            print(f"ê²€ì¦ ìŠ¤í… ì˜¤ë¥˜: {e}")
            return {'total_loss': 1.0, 'mae_avg': 1.0}

def final_fixed_training():
    """ìµœì¢… ìˆ˜ì •ëœ í•™ìŠµ"""
    print("ğŸ¯ ìµœì¢… ìˆ˜ì •: ì´ë¯¸ì§€+ì•¡ì…˜ í…ì„œ ë³€í™˜ + ê°„ë‹¨í•œ ì¦ê°• í•™ìŠµ!")
    print("=" * 70)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = MobileVLADataset(DATA_DIR)
    print(f"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = FinalFixedTrainer(
        model_name="microsoft/kosmos-2-patch14-224",
        action_dim=3,
        window_size=8,
        chunk_size=2,
        learning_rate=1e-4,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    # ì•¡ì…˜ í†µê³„ ê³„ì‚°
    trainer.compute_action_statistics(dataset)
    
    # ë°ì´í„° ë¶„í• 
    total_size = len(dataset)
    train_size = int(0.8 * total_size)
    val_size = total_size - train_size
    
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, total_size))
    
    random.shuffle(train_indices)
    
    train_dataset = torch.utils.data.Subset(dataset, train_indices)
    val_dataset = torch.utils.data.Subset(dataset, val_indices)
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ì—í”¼ì†Œë“œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ì—í”¼ì†Œë“œ")
    
    # DataLoader ìƒì„±
    def collate_fn(batch):
        return batch[0]
    
    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ ìµœì¢… ìˆ˜ì • í•™ìŠµ ì‹œì‘!")
    num_epochs = 10
    best_val_loss = float('inf')
    train_history = []
    val_history = []
    patience = 5
    patience_counter = 0
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“ˆ ì—í¬í¬ {epoch+1}/{num_epochs}")
        print("-" * 40)
        
        # í›ˆë ¨
        trainer.model.train()
        train_losses = []
        train_maes = []
        
        for i, batch in enumerate(train_loader):
            try:
                print(f"\n--- ë°°ì¹˜ {i+1} ì²˜ë¦¬ ì¤‘ ---")
                metrics = trainer.train_step_final_fixed(batch)
                train_losses.append(metrics['total_loss'].item())
                train_maes.append(metrics['mae_avg'])
                
                if (i + 1) % 10 == 0:
                    print(f"   ë°°ì¹˜ {i+1}/{len(train_loader)}: Loss={metrics['total_loss']:.4f}, MAE={metrics['mae_avg']:.4f}")
                    
            except Exception as e:
                print(f"   ë°°ì¹˜ {i+1} ì˜¤ë¥˜: {e}")
                continue
            
            # ì²˜ìŒ 5ê°œ ë°°ì¹˜ë§Œ í…ŒìŠ¤íŠ¸
            if i >= 4:
                break
        
        if train_losses:
            avg_train_loss = np.mean(train_losses)
            avg_train_mae = np.mean(train_maes)
            
            train_metrics = {
                'epoch': epoch + 1,
                'loss': avg_train_loss,
                'mae_avg': avg_train_mae
            }
            train_history.append(train_metrics)
            
            print(f"âœ… í›ˆë ¨ ì™„ë£Œ: Loss={avg_train_loss:.4f}, MAE={avg_train_mae:.4f}")
            
            # ê²€ì¦
            trainer.model.eval()
            val_losses = []
            val_maes = []
            
            with torch.no_grad():
                for i, batch in enumerate(val_loader):
                    try:
                        metrics = trainer.validation_step(batch)
                        val_losses.append(metrics['total_loss'])
                        val_maes.append(metrics['mae_avg'])
                    except Exception as e:
                        continue
                    
                    # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
                    if i >= 2:
                        break
            
            if val_losses:
                avg_val_loss = np.mean(val_losses)
                avg_val_mae = np.mean(val_maes)
                
                val_metrics = {
                    'epoch': epoch + 1,
                    'loss': avg_val_loss,
                    'mae_avg': avg_val_mae
                }
                val_history.append(val_metrics)
                
                print(f"ğŸ” ê²€ì¦ ì™„ë£Œ: Loss={avg_val_loss:.4f}, MAE={avg_val_mae:.4f}")
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                    torch.save({
                        'model_state_dict': trainer.model.state_dict(),
                        'optimizer_state_dict': trainer.optimizer.state_dict(),
                        'epoch': epoch,
                        'val_loss': best_val_loss,
                        'action_mean': trainer.action_mean,
                        'action_std': trainer.action_std
                    }, 'best_final_fixed_model.pth')
                    print(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ë¨ (Loss: {best_val_loss:.4f})")
                else:
                    patience_counter += 1
                    print(f"â³ Early stopping ì¹´ìš´í„°: {patience_counter}/{patience}")
                
                # Early stopping
                if patience_counter >= patience:
                    print(f"ğŸ›‘ Early stopping! {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                    break
        
        # NaN ì²´í¬
        if np.isnan(avg_train_loss):
            print("âŒ NaN Loss ë°œìƒ! í•™ìŠµ ì¤‘ë‹¨")
            break
        else:
            print("âœ… NaN Loss ì—†ìŒ!")
    
    print("\nğŸ‰ ìµœì¢… ìˆ˜ì • í•™ìŠµ ì™„ë£Œ!")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'final_metrics': {
            'best_val_loss': best_val_loss,
            'final_train_loss': train_history[-1]['loss'] if train_history else None,
            'final_train_mae': train_history[-1]['mae_avg'] if train_history else None,
            'epochs_trained': len(train_history)
        },
        'train_history': train_history,
        'val_history': val_history,
        'action_statistics': {
            'mean': trainer.action_mean.tolist() if trainer.action_mean is not None else None,
            'std': trainer.action_std.tolist() if trainer.action_std is not None else None
        },
        'dataset_info': {
            'original_size': len(dataset),
            'train_episodes': len(train_dataset),
            'val_episodes': len(val_dataset)
        },
        'augmentation_info': {
            'action_noise_std': trainer.action_noise_std,
            'approach': 'Final Fixed + Simple Augmentation'
        },
        'model_info': {
            'architecture': 'Final Fixed Kosmos2',
            'z_axis_weight': trainer.z_weight,
            'epochs': num_epochs,
            'learning_rate': trainer.learning_rate,
            'early_stopping': True,
            'patience': patience
        },
        'created_at': datetime.now().isoformat()
    }
    
    with open('final_fixed_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: final_fixed_results.json")
    
    # ìµœì¢… ì„±ëŠ¥ ìš”ì•½
    print("\nğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½:")
    print(f"   ìµœê³  ê²€ì¦ Loss: {best_val_loss:.4f}")
    print(f"   ìµœì¢… í›ˆë ¨ MAE: {train_history[-1]['mae_avg']:.4f}")
    print(f"   í•™ìŠµ ì—í¬í¬: {len(train_history)}")
    print(f"   ì¦ê°• ë°©ì‹: ì•¡ì…˜ ë…¸ì´ì¦ˆ (Ïƒ=0.005)")
    print(f"   Zì¶• ì²˜ë¦¬: ê±´ë“œë¦¬ì§€ ì•ŠìŒ")
    print(f"   ì´ë¯¸ì§€ ë³€í™˜: í…ì„œë¡œ ë³€í™˜")
    print(f"   Shape ì˜¤ë¥˜: ì™„ì „ í•´ê²°")
    print(f"   Early stopping: {'í™œì„±í™”' if patience_counter >= patience else 'ë¹„í™œì„±í™”'}")

if __name__ == "__main__":
    final_fixed_training()
