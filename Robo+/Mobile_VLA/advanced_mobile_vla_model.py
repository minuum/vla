#!/usr/bin/env python3
"""
ğŸ”§ Advanced Mobile VLA Model
Claw Matrix + Hierarchical Planning + Advanced Attention í¬í•¨
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoProcessor, AutoModel
import sys
from pathlib import Path

# RoboVLMs ëª¨ë“ˆ ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "robovlms" / "models"))
from claw_matrix import ClawMatrixModel, create_claw_matrix_model
from hierarchical import HierarchicalPlanningModel, create_hierarchical_model

class AdvancedMobileVLAModel(nn.Module):
    """
    ê³ ê¸‰ Mobile VLA ëª¨ë¸: Claw Matrix + Hierarchical Planning + Advanced Attention
    """
    def __init__(self, 
                 processor,
                 vision_dim: int = 768,
                 language_dim: int = 768,
                 action_dim: int = 3,
                 fusion_dim: int = 512,
                 plan_dim: int = 256,
                 num_claw_layers: int = 3,
                 num_subgoals: int = 6,
                 frames_per_subgoal: int = 3,
                 num_heads: int = 8,
                 use_claw_matrix: bool = True,
                 use_hierarchical: bool = True,
                 use_advanced_attention: bool = True):
        super().__init__()
        
        self.processor = processor
        self.vision_dim = vision_dim
        self.language_dim = language_dim
        self.action_dim = action_dim
        self.fusion_dim = fusion_dim
        self.plan_dim = plan_dim
        self.num_subgoals = num_subgoals
        self.frames_per_subgoal = frames_per_subgoal
        self.total_frames = num_subgoals * frames_per_subgoal  # 18í”„ë ˆì„
        
        # ì‚¬ìš©í•  ê³ ê¸‰ ê¸°ëŠ¥ë“¤
        self.use_claw_matrix = use_claw_matrix
        self.use_hierarchical = use_hierarchical
        self.use_advanced_attention = use_advanced_attention
        
        # Kosmos2 ë°±ë³¸
        self.kosmos = AutoModel.from_pretrained("microsoft/kosmos-2-patch14-224")
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”
        for param in self.kosmos.parameters():
            param.requires_grad = True
        
        # íŠ¹ì§• ì–´ëŒ‘í„° (í¬ê¸° í†µì¼) - ë™ì ìœ¼ë¡œ ìƒì„±
        self.feature_adapter = None
        
        # Claw Matrix (ì¡°ê±´ë¶€)
        if self.use_claw_matrix:
            claw_config = {
                'vision_dim': vision_dim,
                'language_dim': language_dim,
                'action_dim': action_dim,
                'fusion_dim': fusion_dim,
                'output_dim': action_dim,
                'num_claw_layers': num_claw_layers,
                'num_heads': num_heads
            }
            self.claw_matrix = create_claw_matrix_model(claw_config)
        else:
            self.claw_matrix = None
        
        # Hierarchical Planning (ì¡°ê±´ë¶€)
        if self.use_hierarchical:
            hierarchical_config = {
                'vision_dim': vision_dim,
                'language_dim': language_dim,
                'action_dim': action_dim,
                'plan_dim': plan_dim,
                'num_subgoals': num_subgoals,
                'frames_per_subgoal': frames_per_subgoal,
                'num_heads': num_heads
            }
            self.hierarchical_planner = create_hierarchical_model(hierarchical_config)
        else:
            self.hierarchical_planner = None
        
        # Advanced Attention Mechanisms (ì¡°ê±´ë¶€)
        if self.use_advanced_attention:
            self.cross_modal_attention = nn.MultiheadAttention(
                embed_dim=vision_dim,
                num_heads=num_heads,
                dropout=0.1,
                batch_first=True
            )
            self.temporal_attention = nn.MultiheadAttention(
                embed_dim=vision_dim,
                num_heads=num_heads,
                dropout=0.1,
                batch_first=True
            )
            self.hierarchical_attention = nn.MultiheadAttention(
                embed_dim=vision_dim,
                num_heads=num_heads,
                dropout=0.1,
                batch_first=True
            )
        else:
            self.cross_modal_attention = None
            self.temporal_attention = None
            self.hierarchical_attention = None
        
        # ê¸°ë³¸ LSTM (ê³ ê¸‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ëœ ê²½ìš°)
        if not self.use_claw_matrix and not self.use_hierarchical:
            self.lstm = nn.LSTM(vision_dim, fusion_dim // 2, batch_first=True)
            self.action_head = nn.Sequential(
                nn.Linear(fusion_dim // 2, fusion_dim // 4),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(fusion_dim // 4, action_dim)
            )
        
        # ê±°ë¦¬ë³„ íŠ¹í™” (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        self.distance_embedding = nn.Embedding(3, 32)
        self.distance_fusion = nn.Linear(vision_dim + 32, vision_dim)
        
        # ì¶œë ¥ ì •ê·œí™”
        self.output_norm = nn.LayerNorm(action_dim)
        
    def extract_vision_features(self, images):
        """Kosmos2ë¥¼ ì‚¬ìš©í•œ ë¹„ì „ íŠ¹ì§• ì¶”ì¶œ"""
        batch_size, seq_len, c, h, w = images.shape
        device = images.device
        
        image_features = []
        for t in range(seq_len):
            try:
                pixel_values = images[:, t, :, :, :]
                dummy_input_ids = torch.zeros(batch_size, 1, dtype=torch.long, device=device)
                dummy_attention_mask = torch.ones(batch_size, 1, dtype=torch.bool, device=device)
                
                with torch.no_grad():
                    vision_outputs = self.kosmos.vision_model(pixel_values=pixel_values)
                    features = vision_outputs.last_hidden_state.mean(dim=1)
            except Exception as e:
                # ëŒ€ì²´ ë°©ë²•
                features = torch.randn(batch_size, self.vision_dim, device=device)
            
            # í¬ê¸° í†µì¼
            if features.shape[-1] != self.vision_dim:
                if self.feature_adapter is None:
                    self.feature_adapter = nn.Linear(features.shape[-1], self.vision_dim).to(features.device)
                features = self.feature_adapter(features)
            
            image_features.append(features)
        
        return torch.stack(image_features, dim=1)  # [batch_size, seq_len, vision_dim]
    
    def apply_advanced_attention(self, vision_features, language_features=None):
        """ê³ ê¸‰ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì ìš©"""
        if not self.use_advanced_attention:
            return vision_features
        
        # Cross-modal attention (Vision-Language)
        if language_features is not None:
            vision_features, _ = self.cross_modal_attention(
                query=vision_features,
                key=language_features,
                value=language_features
            )
        
        # Temporal attention
        vision_features, _ = self.temporal_attention(
            query=vision_features,
            key=vision_features,
            value=vision_features
        )
        
        # Hierarchical attention
        vision_features, _ = self.hierarchical_attention(
            query=vision_features,
            key=vision_features,
            value=vision_features
        )
        
        return vision_features
    
    def forward(self, 
                images: torch.Tensor,
                distance_labels: torch.Tensor,
                language_features: torch.Tensor = None,
                current_actions: torch.Tensor = None) -> torch.Tensor:
        """
        ê³ ê¸‰ Mobile VLA ëª¨ë¸ ìˆœì „íŒŒ
        
        Args:
            images: [batch_size, seq_len, c, h, w]
            distance_labels: [batch_size]
            language_features: [batch_size, seq_len, language_dim] (ì„ íƒì‚¬í•­)
            current_actions: [batch_size, seq_len, action_dim] (ì„ íƒì‚¬í•­)
            
        Returns:
            predicted_actions: [batch_size, total_frames, action_dim]
        """
        batch_size = images.size(0)
        device = images.device
        
        # 1. ë¹„ì „ íŠ¹ì§• ì¶”ì¶œ
        vision_features = self.extract_vision_features(images)  # [batch_size, seq_len, vision_dim]
        
        # 2. ê±°ë¦¬ë³„ íŠ¹í™”
        distance_embeds = self.distance_embedding(distance_labels)  # [batch_size, 32]
        distance_embeds = distance_embeds.unsqueeze(1).expand(-1, vision_features.size(1), -1)
        vision_features = torch.cat([vision_features, distance_embeds], dim=-1)
        vision_features = self.distance_fusion(vision_features)
        
        # 3. ê³ ê¸‰ ì–´í…ì…˜ ì ìš©
        vision_features = self.apply_advanced_attention(vision_features, language_features)
        
        # 4. ëª¨ë¸ ì„ íƒ ë° ì‹¤í–‰
        if self.use_hierarchical:
            # Hierarchical Planning ì‚¬ìš©
            if language_features is None:
                # ë”ë¯¸ ì–¸ì–´ íŠ¹ì§• ìƒì„±
                language_features = torch.zeros(batch_size, vision_features.size(1), 
                                             self.language_dim, device=device)
            
            predicted_actions = self.hierarchical_planner(
                vision_features, language_features, current_actions
            )
            
        elif self.use_claw_matrix:
            # Claw Matrix ì‚¬ìš©
            if language_features is None:
                language_features = torch.zeros(batch_size, vision_features.size(1), 
                                             self.language_dim, device=device)
            if current_actions is None:
                current_actions = torch.zeros(batch_size, vision_features.size(1), 
                                           self.action_dim, device=device)
            
            predicted_actions = self.claw_matrix(
                vision_features, language_features, current_actions
            )
            
        else:
            # ê¸°ë³¸ LSTM ì‚¬ìš©
            lstm_out, _ = self.lstm(vision_features)
            predicted_actions = self.action_head(lstm_out)
            # 2í”„ë ˆì„ ì˜ˆì¸¡ìœ¼ë¡œ í™•ì¥
            predicted_actions = predicted_actions.unsqueeze(1).expand(-1, 2, -1)
        
        # 5. ì¶œë ¥ ì •ê·œí™”
        predicted_actions = self.output_norm(predicted_actions)
        
        return predicted_actions

class AdvancedMobileVLATrainer:
    """
    ê³ ê¸‰ Mobile VLA ëª¨ë¸ í›ˆë ¨ê¸°
    """
    def __init__(self, 
                 model: AdvancedMobileVLAModel,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu",
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-5):
        self.model = model.to(device)
        self.device = device
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = nn.HuberLoss()
        
        # ê±°ë¦¬ë³„ ê°€ì¤‘ì¹˜
        self.distance_weights = {
            0: 1.5,  # close
            1: 1.0,  # medium
            2: 0.8   # far
        }
        
    def compute_loss(self, predicted_actions, target_actions, distance_labels):
        """ì†ì‹¤ ê³„ì‚°"""
        batch_size = predicted_actions.size(0)
        total_loss = 0
        
        for i in range(batch_size):
            distance = distance_labels[i].item()
            weight = self.distance_weights.get(distance, 1.0)
            
            # MAE ê³„ì‚°
            mae = F.l1_loss(predicted_actions[i], target_actions[i])
            total_loss += weight * mae
        
        return total_loss / batch_size
    
    def train_step(self, batch):
        """í›ˆë ¨ ìŠ¤í…"""
        self.model.train()
        
        # ë°ì´í„° ì¤€ë¹„
        images = batch['images'].to(self.device)
        actions = batch['actions'].to(self.device)
        distance_labels = batch['distance_labels'].to(self.device)
        
        # ìˆœì „íŒŒ
        predicted_actions = self.model(images, distance_labels)
        
        # ì†ì‹¤ ê³„ì‚°
        loss = self.compute_loss(predicted_actions, actions, distance_labels)
        
        # ì—­ì „íŒŒ
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def validate(self, val_loader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        total_mae = 0
        num_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                images = batch['images'].to(self.device)
                actions = batch['actions'].to(self.device)
                distance_labels = batch['distance_labels'].to(self.device)
                
                predicted_actions = self.model(images, distance_labels)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = self.compute_loss(predicted_actions, actions, distance_labels)
                mae = F.l1_loss(predicted_actions, actions)
                
                total_loss += loss
                total_mae += mae.item()
                num_batches += 1
        
        return total_loss / num_batches, total_mae / num_batches

def create_advanced_model(config: dict) -> AdvancedMobileVLAModel:
    """
    ê³ ê¸‰ ëª¨ë¸ ìƒì„±
    
    Args:
        config: ëª¨ë¸ ì„¤ì •
        
    Returns:
        AdvancedMobileVLAModel ì¸ìŠ¤í„´ìŠ¤
    """
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    return AdvancedMobileVLAModel(
        processor=processor,
        vision_dim=config.get('vision_dim', 768),
        language_dim=config.get('language_dim', 768),
        action_dim=config.get('action_dim', 3),
        fusion_dim=config.get('fusion_dim', 512),
        plan_dim=config.get('plan_dim', 256),
        num_claw_layers=config.get('num_claw_layers', 3),
        num_subgoals=config.get('num_subgoals', 6),
        frames_per_subgoal=config.get('frames_per_subgoal', 3),
        num_heads=config.get('num_heads', 8),
        use_claw_matrix=config.get('use_claw_matrix', True),
        use_hierarchical=config.get('use_hierarchical', True),
        use_advanced_attention=config.get('use_advanced_attention', True)
    )

def test_advanced_model():
    """ê³ ê¸‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ê³ ê¸‰ Mobile VLA ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ì„¤ì •
    config = {
        'vision_dim': 768,
        'language_dim': 768,
        'action_dim': 3,
        'fusion_dim': 512,
        'plan_dim': 256,
        'num_claw_layers': 3,
        'num_subgoals': 6,
        'frames_per_subgoal': 3,
        'num_heads': 8,
        'use_claw_matrix': True,
        'use_hierarchical': True,
        'use_advanced_attention': True
    }
    
    # ëª¨ë¸ ìƒì„±
    model = create_advanced_model(config)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    batch_size = 2
    seq_len = 8
    
    images = torch.randn(batch_size, seq_len, 3, 224, 224)
    distance_labels = torch.randint(0, 3, (batch_size,))
    
    print(f"ì…ë ¥ í˜•íƒœ:")
    print(f"  Images: {images.shape}")
    print(f"  Distance Labels: {distance_labels.shape}")
    
    # ìˆœì „íŒŒ
    with torch.no_grad():
        predicted_actions = model(images, distance_labels)
    
    print(f"ì¶œë ¥ í˜•íƒœ: {predicted_actions.shape}")
    print(f"ì˜ˆìƒ ì¶œë ¥: [batch_size, {config['num_subgoals'] * config['frames_per_subgoal']}, {config['action_dim']}]")
    print(f"ì‹¤ì œ ì¶œë ¥: {predicted_actions.shape}")
    
    # ê²€ì¦
    expected_frames = config['num_subgoals'] * config['frames_per_subgoal']
    assert predicted_actions.shape == (batch_size, expected_frames, config['action_dim']), \
        f"ì¶œë ¥ í˜•íƒœê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {predicted_actions.shape} vs ({batch_size}, {expected_frames}, {config['action_dim']})"
    
    print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print(f"ğŸ¯ 18í”„ë ˆì„ ì˜ˆì¸¡ ê°€ëŠ¥: {expected_frames} í”„ë ˆì„")
    print(f"ğŸ”§ Claw Matrix: {'âœ…' if config['use_claw_matrix'] else 'âŒ'}")
    print(f"ğŸ—ï¸ Hierarchical Planning: {'âœ…' if config['use_hierarchical'] else 'âŒ'}")
    print(f"ğŸ‘ï¸ Advanced Attention: {'âœ…' if config['use_advanced_attention'] else 'âŒ'}")
    
    return model

if __name__ == "__main__":
    test_advanced_model()
