#!/usr/bin/env python3
"""
Mobile VLA λ¨λΈ μ–‘μν™” μ¤ν¬λ¦½νΈ (MAE 0.222 λ¨λΈ)
Jetson Orin NXμ—μ„ TensorRT 8.6.2.3μ„ ν™μ©ν• λ¨λΈ μµμ ν™”
"""

import torch
import torch.nn as nn
import numpy as np
import os
import json
import time
from pathlib import Path
import logging
from typing import Dict, Any, Optional

# ONNX imports
try:
    import onnx
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("β οΈ ONNX not available. Install with: pip install onnx onnxruntime")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MAE0222ModelQuantizer:
    """
    MAE 0.222 λ¨λΈ μ–‘μν™” ν΄λμ¤
    """
    
    def __init__(self, model_path: str, output_dir: str = "quantized_models_mae0222"):
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Jetson Orin NX μ„¤μ •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # λ¨λΈ λ΅λ“
        self.model = self._load_model()
        
    def _load_model(self) -> nn.Module:
        """λ¨λΈ λ΅λ“"""
        logger.info(f"λ¨λΈ λ΅λ“ μ¤‘: {self.model_path}")
        
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {self.model_path}")
        
        # μ²΄ν¬ν¬μΈνΈ λ΅λ“
        checkpoint = torch.load(self.model_path, map_location=self.device)
        
        # λ¨λΈ κµ¬μ΅° λ¶„μ„
        logger.info("λ¨λΈ κµ¬μ΅° λ¶„μ„ μ¤‘...")
        
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        logger.info(f"μ΄ νλΌλ―Έν„° μ: {len(state_dict)}")
        
        # Kosmos2 κΈ°λ° λ¨λΈ κµ¬μ΅° ν™•μΈ
        kosmos_keys = [key for key in state_dict.keys() if 'kosmos_model' in key]
        logger.info(f"Kosmos2 κ΄€λ ¨ νλΌλ―Έν„° μ: {len(kosmos_keys)}")
        
        # μ‹¤μ  λ¨λΈ κµ¬μ΅° μƒμ„±
        model = self._create_actual_model(state_dict)
        
        # νλΌλ―Έν„° λ΅λ“ (νΈν™λλ” λ¶€λ¶„λ§)
        try:
            model.load_state_dict(state_dict, strict=False)
            logger.info("λ¨λΈ νλΌλ―Έν„° λ΅λ“ μ™„λ£ (strict=False)")
        except Exception as e:
            logger.warning(f"λ¨λΈ νλΌλ―Έν„° λ΅λ“ μ¤‘ μ¤λ¥ (μΌλ¶€λ§ λ΅λ“): {e}")
        
        model.eval()
        model.to(self.device)
        
        logger.info("λ¨λΈ λ΅λ“ μ™„λ£")
        return model
    
    def _create_actual_model(self, state_dict: Dict[str, torch.Tensor]) -> nn.Module:
        """μ‹¤μ  λ¨λΈ κµ¬μ΅° μƒμ„±"""
        class MAE0222Model(nn.Module):
            def __init__(self):
                super().__init__()
                
                # Kosmos2 Vision Model (κ°„λ‹¨ν• λ²„μ „)
                self.vision_model = nn.Sequential(
                    # Vision embeddings
                    nn.Conv2d(3, 1280, kernel_size=14, stride=14, padding=0),  # patch_embedding
                    nn.LayerNorm(1280),  # pre_layrnorm
                    
                    # Vision encoder (κ°„λ‹¨ν• λ²„μ „)
                    nn.TransformerEncoder(
                        nn.TransformerEncoderLayer(
                            d_model=1280,
                            nhead=20,
                            dim_feedforward=5120,
                            dropout=0.1,
                            batch_first=True
                        ),
                        num_layers=24  # Kosmos2 vision encoder layers
                    ),
                    
                    # Post layer norm
                    nn.LayerNorm(1280),
                    
                    # Global average pooling
                    nn.AdaptiveAvgPool2d((1, 1)),
                    nn.Flatten()
                )
                
                # Image to text projection
                self.image_to_text_projection = nn.Sequential(
                    nn.Linear(1280, 768),  # dense projection
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
                
                # Text model (κ°„λ‹¨ν• λ²„μ „)
                self.text_model = nn.Sequential(
                    nn.Embedding(32000, 768),  # embed_tokens
                    nn.TransformerEncoder(
                        nn.TransformerEncoderLayer(
                            d_model=768,
                            nhead=12,
                            dim_feedforward=3072,
                            dropout=0.1,
                            batch_first=True
                        ),
                        num_layers=24  # Kosmos2 text encoder layers
                    ),
                    nn.LayerNorm(768)
                )
                
                # RNN for action prediction
                self.rnn = nn.RNN(
                    input_size=768,
                    hidden_size=512,
                    num_layers=4,
                    batch_first=True,
                    dropout=0.1
                )
                
                # Action head
                self.actions = nn.Sequential(
                    nn.Linear(512, 256),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(128, 2)  # action_dim = 2
                )
            
            def forward(self, pixel_values, input_ids=None, attention_mask=None):
                # Vision encoding
                batch_size = pixel_values.size(0)
                
                # Vision model
                vision_features = self.vision_model(pixel_values)  # [batch_size, 1280]
                
                # Image to text projection
                projected_features = self.image_to_text_projection(vision_features)  # [batch_size, 768]
                
                # Text processing (κ°„λ‹¨ν• λ²„μ „)
                if input_ids is None:
                    # λ”λ―Έ ν…μ¤νΈ μƒμ„±
                    input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=pixel_values.device)
                
                text_features = self.text_model(input_ids)  # [batch_size, seq_len, 768]
                
                # νΉμ§• κ²°ν•©
                combined_features = projected_features.unsqueeze(1)  # [batch_size, 1, 768]
                
                # RNN processing
                rnn_out, _ = self.rnn(combined_features)  # [batch_size, 1, 512]
                
                # Action prediction
                actions = self.actions(rnn_out.squeeze(1))  # [batch_size, 2]
                
                return actions
        
        return MAE0222Model()
    
    def export_to_onnx(self, onnx_path: str) -> bool:
        """λ¨λΈμ„ ONNX ν•μ‹μΌλ΅ λ‚΄λ³΄λ‚΄κΈ°"""
        if not ONNX_AVAILABLE:
            logger.error("ONNX not available")
            return False
        
        logger.info("ONNX λ¨λΈ λ‚΄λ³΄λ‚΄κΈ° μ‹μ‘...")
        
        # λ”λ―Έ μ…λ ¥ μƒμ„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        dummy_text = torch.zeros((1, 1), dtype=torch.long, device=self.device)
        
        try:
            # ONNX λ‚΄λ³΄λ‚΄κΈ°
            torch.onnx.export(
                self.model,
                (dummy_input, dummy_text),
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['pixel_values', 'input_ids'],
                output_names=['actions'],
                dynamic_axes={
                    'pixel_values': {0: 'batch_size'},
                    'input_ids': {0: 'batch_size'},
                    'actions': {0: 'batch_size'}
                }
            )
            
            # ONNX λ¨λΈ κ²€μ¦
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            logger.info(f"ONNX λ¨λΈ λ‚΄λ³΄λ‚΄κΈ° μ™„λ£: {onnx_path}")
            return True
            
        except Exception as e:
            logger.error(f"ONNX λ‚΄λ³΄λ‚΄κΈ° μ‹¤ν¨: {e}")
            return False
    
    def benchmark_model(self, model_type: str, model_path: str, num_runs: int = 50) -> Dict[str, float]:
        """λ¨λΈ μ„±λ¥ λ²¤μΉλ§ν¬"""
        logger.info(f"{model_type} λ¨λΈ λ²¤μΉλ§ν¬ μ‹μ‘...")
        
        # λ”λ―Έ μ…λ ¥ μƒμ„±
        dummy_input = torch.randn(1, 3, 224, 224, device=self.device)
        dummy_text = torch.zeros((1, 1), dtype=torch.long, device=self.device)
        
        if model_type == "PyTorch":
            # PyTorch λ¨λΈ λ²¤μΉλ§ν¬
            self.model.eval()
            
            # Warmup
            for _ in range(10):
                with torch.no_grad():
                    _ = self.model(dummy_input, dummy_text)
            
            # λ²¤μΉλ§ν¬
            torch.cuda.synchronize()
            start_time = time.time()
            
            for _ in range(num_runs):
                with torch.no_grad():
                    _ = self.model(dummy_input, dummy_text)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
        elif model_type == "ONNX":
            # ONNX λ¨λΈ λ²¤μΉλ§ν¬
            if not ONNX_AVAILABLE:
                return {"error": "ONNX not available"}
            
            session = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider'])
            input_names = [input.name for input in session.get_inputs()]
            
            # Warmup
            for _ in range(10):
                _ = session.run(None, {
                    'pixel_values': dummy_input.cpu().numpy(),
                    'input_ids': dummy_text.cpu().numpy()
                })
            
            # λ²¤μΉλ§ν¬
            start_time = time.time()
            
            for _ in range(num_runs):
                _ = session.run(None, {
                    'pixel_values': dummy_input.cpu().numpy(),
                    'input_ids': dummy_text.cpu().numpy()
                })
            
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
        
        else:
            return {"error": f"Unknown model type: {model_type}"}
        
        return {
            "avg_inference_time_ms": avg_time * 1000,
            "memory_used_mb": memory_used,
            "throughput_fps": 1.0 / avg_time
        }
    
    def quantize_model(self) -> Dict[str, Any]:
        """λ¨λΈ μ–‘μν™” μ‹¤ν–‰"""
        logger.info("MAE 0.222 λ¨λΈ μ–‘μν™” μ‹μ‘...")
        
        results = {
            "original_model": self.model_path,
            "model_info": {
                "mae": 0.222,
                "model_type": "Kosmos2 + LSTM",
                "action_dim": 2
            },
            "quantization_results": {}
        }
        
        # 1. PyTorch λ¨λΈ λ²¤μΉλ§ν¬
        logger.info("1. PyTorch λ¨λΈ λ²¤μΉλ§ν¬...")
        pytorch_benchmark = self.benchmark_model("PyTorch", self.model_path)
        results["quantization_results"]["pytorch"] = pytorch_benchmark
        
        # 2. ONNX λ¨λΈ μƒμ„± λ° λ²¤μΉλ§ν¬
        if ONNX_AVAILABLE:
            logger.info("2. ONNX λ¨λΈ μƒμ„±...")
            onnx_path = self.output_dir / "mae0222_model.onnx"
            
            if self.export_to_onnx(str(onnx_path)):
                onnx_benchmark = self.benchmark_model("ONNX", str(onnx_path))
                results["quantization_results"]["onnx"] = onnx_benchmark
                results["onnx_model"] = str(onnx_path)
        
        # κ²°κ³Ό μ €μ¥
        results_path = self.output_dir / "mae0222_quantization_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"μ–‘μν™” κ²°κ³Ό μ €μ¥: {results_path}")
        
        # κ²°κ³Ό μ¶λ ¥
        self._print_results(results)
        
        return results
    
    def _print_results(self, results: Dict[str, Any]):
        """μ–‘μν™” κ²°κ³Ό μ¶λ ¥"""
        print("\n" + "="*60)
        print("π¤– Mobile VLA λ¨λΈ μ–‘μν™” κ²°κ³Ό (MAE 0.222)")
        print("="*60)
        
        model_info = results.get("model_info", {})
        print(f"\nπ“ λ¨λΈ μ •λ³΄:")
        print(f"   MAE: {model_info.get('mae', 0.222)}")
        print(f"   λ¨λΈ νƒ€μ…: {model_info.get('model_type', 'Kosmos2 + LSTM')}")
        print(f"   μ•΅μ… μ°¨μ›: {model_info.get('action_dim', 2)}")
        
        quantization_results = results.get("quantization_results", {})
        
        if "pytorch" in quantization_results:
            pytorch = quantization_results["pytorch"]
            print(f"\nπ“ PyTorch λ¨λΈ:")
            print(f"   μ¶”λ΅  μ‹κ°„: {pytorch.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {pytorch.get('memory_used_mb', 0):.2f} MB")
            print(f"   μ²λ¦¬λ‰: {pytorch.get('throughput_fps', 0):.2f} FPS")
        
        if "onnx" in quantization_results:
            onnx = quantization_results["onnx"]
            print(f"\nπ“ ONNX λ¨λΈ:")
            print(f"   μ¶”λ΅  μ‹κ°„: {onnx.get('avg_inference_time_ms', 0):.2f} ms")
            print(f"   λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {onnx.get('memory_used_mb', 0):.2f} MB")
            print(f"   μ²λ¦¬λ‰: {onnx.get('throughput_fps', 0):.2f} FPS")
            
            # κ°μ„ μ¨ κ³„μ‚°
            if "pytorch" in quantization_results:
                pytorch_time = pytorch.get('avg_inference_time_ms', 0)
                onnx_time = onnx.get('avg_inference_time_ms', 0)
                if pytorch_time > 0:
                    speedup = pytorch_time / onnx_time
                    print(f"   μ†λ„ κ°μ„ : {speedup:.2f}x")
                
                pytorch_memory = pytorch.get('memory_used_mb', 0)
                onnx_memory = onnx.get('memory_used_mb', 0)
                if pytorch_memory > 0:
                    memory_reduction = (pytorch_memory - onnx_memory) / pytorch_memory * 100
                    print(f"   λ©”λ¨λ¦¬ μ μ•½: {memory_reduction:.1f}%")
        
        print("\n" + "="*60)

def main():
    """λ©”μΈ ν•¨μ"""
    print("π€ Mobile VLA λ¨λΈ μ–‘μν™” μ‹μ‘ (MAE 0.222)")
    
    # λ¨λΈ κ²½λ΅ μ„¤μ •
    model_path = "results/simple_lstm_results_extended/best_simple_lstm_model.pth"
    
    if not os.path.exists(model_path):
        print(f"β λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {model_path}")
        return
    
    # μ–‘μν™” μ‹¤ν–‰
    quantizer = MAE0222ModelQuantizer(model_path)
    results = quantizer.quantize_model()
    
    print("\nβ… μ–‘μν™” μ™„λ£!")

if __name__ == "__main__":
    main()
