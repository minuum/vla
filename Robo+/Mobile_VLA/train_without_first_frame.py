"""
ğŸš€ ì²« í”„ë ˆì„ ì œì™¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ì‹œì‘ í”„ë ˆì„ì´ 0ìœ¼ë¡œ ê³ ì •ì´ë¼ëŠ” ì ì„ ê³ ë ¤í•˜ì—¬ ì²« í”„ë ˆì„ì„ ì œì™¸í•˜ê³  í•™ìŠµ
ì‹¤ì œ ì˜ë¯¸ìˆëŠ” í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 
"""

import torch
import torch.nn as nn
import numpy as np
import h5py
import os
from pathlib import Path
from transformers import AutoProcessor
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import json

from fixed_robovlms_model import FixedRoboVLMStyleSingleImageModel

class NoFirstFrameDataset(Dataset):
    """ì²« í”„ë ˆì„ì„ ì œì™¸í•œ ë°ì´í„°ì…‹ (ì‹œì‘ í”„ë ˆì„ ê³ ì • ê³ ë ¤)"""
    
    def __init__(self, data_path, processor, split='train', frame_selection='random'):
        self.data_path = data_path
        self.processor = processor
        self.split = split
        self.frame_selection = frame_selection  # 'random', 'middle', 'all'
        
        # H5 íŒŒì¼ë“¤ ë¡œë“œ
        self.episodes = []
        self._load_episodes()
        
        print(f"ğŸ“Š {split} ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.episodes)}ê°œ ì—í”¼ì†Œë“œ")
        print(f"   - í”„ë ˆì„ ì„ íƒ ë°©ì‹: {frame_selection}")
        print(f"   - ì²« í”„ë ˆì„ ì œì™¸: True")
    
    def _load_episodes(self):
        """ì—í”¼ì†Œë“œ ë¡œë“œ (ì²« í”„ë ˆì„ ì œì™¸)"""
        if os.path.isdir(self.data_path):
            h5_files = list(Path(self.data_path).glob("*.h5"))
        else:
            h5_files = [self.data_path]
        
        for h5_file in h5_files:
            try:
                with h5py.File(h5_file, 'r') as f:
                    if 'images' in f and 'actions' in f:
                        images = f['images'][:]  # [18, H, W, 3]
                        actions = f['actions'][:]  # [18, 3]
                        
                        # ì²« í”„ë ˆì„ ì œì™¸ (í”„ë ˆì„ 1-17ë§Œ ì‚¬ìš©)
                        valid_frames = list(range(1, 18))  # 1, 2, 3, ..., 17
                        
                        if self.frame_selection == 'random':
                            # ëœë¤í•˜ê²Œ í”„ë ˆì„ ì„ íƒ
                            frame_idx = np.random.choice(valid_frames)
                        elif self.frame_selection == 'middle':
                            # ì¤‘ê°„ í”„ë ˆì„ ì„ íƒ
                            frame_idx = valid_frames[len(valid_frames)//2]  # 9
                        elif self.frame_selection == 'all':
                            # ëª¨ë“  ìœ íš¨í•œ í”„ë ˆì„ì„ ê°œë³„ ì—í”¼ì†Œë“œë¡œ ìƒì„±
                            for frame_idx in valid_frames:
                                single_image = images[frame_idx]  # [H, W, 3]
                                single_action = actions[frame_idx]  # [3]
                                
                                self.episodes.append({
                                    'image': single_image,
                                    'action': single_action,
                                    'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                                    'frame_idx': frame_idx,
                                    'original_file': h5_file.name
                                })
                            continue  # ë‹¤ìŒ íŒŒì¼ë¡œ
                        
                        # ë‹¨ì¼ í”„ë ˆì„ ì„ íƒ
                        single_image = images[frame_idx]  # [H, W, 3]
                        single_action = actions[frame_idx]  # [3]
                        
                        self.episodes.append({
                            'image': single_image,
                            'action': single_action,
                            'episode_id': f"{h5_file.stem}_frame_{frame_idx}",
                            'frame_idx': frame_idx,
                            'original_file': h5_file.name
                        })
                        
            except Exception as e:
                print(f"âŒ {h5_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        
        # ì´ë¯¸ì§€: [H, W, 3] â†’ [3, H, W] (PyTorch í˜•ì‹)
        image = episode['image']  # [H, W, 3]
        image = np.transpose(image, (2, 0, 1))  # [3, H, W]
        
        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        if image.max() > 1:
            image = image.astype(np.float32) / 255.0
        
        # ì•¡ì…˜: [3]
        action = episode['action']  # [3]
        
        return {
            'image': image,  # [3, H, W]
            'action': action,  # [3]
            'episode_id': episode['episode_id'],
            'frame_idx': episode['frame_idx']
        }

def create_no_first_frame_loaders(data_path, processor, batch_size=4, train_split=0.8, frame_selection='random'):
    """ì²« í”„ë ˆì„ ì œì™¸ ë°ì´í„° ë¡œë” ìƒì„±"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = NoFirstFrameDataset(data_path, processor, 'full', frame_selection)
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset, [train_size, val_size]
    )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=1,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=1,
        pin_memory=True
    )
    
    print(f"ğŸ“Š ì²« í”„ë ˆì„ ì œì™¸ ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {len(train_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ê²€ì¦: {len(val_dataset)}ê°œ ì—í”¼ì†Œë“œ")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   - í”„ë ˆì„ ì„ íƒ: {frame_selection}")
    
    return train_loader, val_loader

def train_without_first_frame(
    model,
    train_loader,
    val_loader,
    num_epochs=15,
    learning_rate=1e-4,
    weight_decay=1e-4,
    early_stopping_patience=5,
    device='cuda'
):
    """ì²« í”„ë ˆì„ ì œì™¸ í›ˆë ¨"""
    
    model = model.to(device)
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=num_epochs, eta_min=1e-6
    )
    
    # ì†ì‹¤ í•¨ìˆ˜
    def compute_loss(predicted_actions, target_actions):
        # Zì¶• ê°€ì¤‘ì¹˜ ì ìš©
        z_weight = torch.tensor([1.0, 1.0, 0.05]).to(device)
        weighted_target = target_actions * z_weight.unsqueeze(0)
        weighted_pred = predicted_actions * z_weight.unsqueeze(0)
        
        return nn.functional.mse_loss(weighted_pred, weighted_target)
    
    # ì¡°ê¸° ì¢…ë£Œ
    best_val_loss = float('inf')
    patience_counter = 0
    
    print(f"ğŸš€ ì²« í”„ë ˆì„ ì œì™¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {num_epochs} ì—í¬í¬, í•™ìŠµë¥ : {learning_rate}")
    
    for epoch in range(num_epochs):
        # í›ˆë ¨
        model.train()
        train_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            # ì‹¤ì œ ì•¡ì…˜ì´ ìˆëŠ” í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©
            images = batch['image']  # [batch, 3, H, W]
            actions = batch['action']  # [batch, 3] - ì‹¤ì œ ì•¡ì…˜ (0ì´ ì•„ë‹˜)
            
            images = images.float().to(device)
            actions = actions.float().to(device)
            
            optimizer.zero_grad()
            
            try:
                # ì˜ˆì¸¡ (ì‹¤ì œ ì•¡ì…˜ ì˜ˆì¸¡)
                predicted_actions = model(images, "Navigate to target")
                
                # ì†ì‹¤ ê³„ì‚°
                loss = compute_loss(predicted_actions, actions)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ê²€ì¦
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    images = batch['image'].float().to(device)
                    actions = batch['action'].float().to(device)
                    
                    predicted_actions = model(images, "Navigate to target")
                    loss = compute_loss(predicted_actions, actions)
                    val_loss += loss.item()
                    val_batches += 1
                    
                except Exception as e:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        # í‰ê·  ì†ì‹¤
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / max(val_batches, 1)
        
        # í•™ìŠµë¥  ì¡°ì •
        scheduler.step()
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs} ì™„ë£Œ:")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {avg_train_loss:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
        print(f"   - í•™ìŠµë¥ : {scheduler.get_last_lr()[0]:.6f}")
        
        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # ìµœê³  ëª¨ë¸ ì €ì¥
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'config': {
                    'learning_rate': learning_rate,
                    'weight_decay': weight_decay,
                    'dropout': model.dropout,
                    'z_axis_weight': model.z_axis_weight,
                    'use_claw_matrix': model.use_claw_matrix,
                    'use_hierarchical': model.use_hierarchical,
                    'use_advanced_attention': model.use_advanced_attention,
                    'training_type': 'without_first_frame'
                }
            }, 'no_first_frame_model_best.pth')
            print(f"   âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print(f"   â° ì¡°ê¸° ì¢…ë£Œ (Patience: {early_stopping_patience})")
                break
    
    return model

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = {
        'data_path': '../../ROS_action/mobile_vla_dataset',
        'batch_size': 4,
        'num_epochs': 15,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'dropout': 0.2,
        'z_axis_weight': 0.05,
        'use_claw_matrix': True,
        'use_hierarchical': True,
        'use_advanced_attention': True,
        'early_stopping_patience': 5,
        'frame_selection': 'all',  # 'all', 'random', 'middle'
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print("ğŸš€ ì²« í”„ë ˆì„ ì œì™¸ í•™ìŠµ ì‹œì‘!")
    print(f"ğŸ“Š ì„¤ì •: {json.dumps(config, indent=2)}")
    
    # í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë“œ ì¤‘...")
    processor = AutoProcessor.from_pretrained("microsoft/kosmos-2-patch14-224")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“Š ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    train_loader, val_loader = create_no_first_frame_loaders(
        config['data_path'],
        processor,
        batch_size=config['batch_size'],
        frame_selection=config['frame_selection']
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("ğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = FixedRoboVLMStyleSingleImageModel(
        processor=processor,
        dropout=config['dropout'],
        use_claw_matrix=config['use_claw_matrix'],
        use_hierarchical=config['use_hierarchical'],
        use_advanced_attention=config['use_advanced_attention'],
        z_axis_weight=config['z_axis_weight']
    )
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ¯ í™œì„±í™”ëœ ê³ ê¸‰ ê¸°ëŠ¥:")
    print(f"   - Fixed Claw Matrix: {model.use_claw_matrix}")
    print(f"   - Fixed Hierarchical Planning: {model.use_hierarchical}")
    print(f"   - Fixed Advanced Attention: {model.use_advanced_attention}")
    print(f"   - í›ˆë ¨ ë°©ì‹: ì²« í”„ë ˆì„ ì œì™¸")
    
    # í›ˆë ¨ ì‹¤í–‰
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    try:
        trained_model = train_without_first_frame(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            num_epochs=config['num_epochs'],
            learning_rate=config['learning_rate'],
            weight_decay=config['weight_decay'],
            early_stopping_patience=config['early_stopping_patience'],
            device=config['device']
        )
        
        print("âœ… ì²« í”„ë ˆì„ ì œì™¸ í›ˆë ¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_type': 'Fixed_RoboVLMs_Style_Without_First_Frame',
        'training_type': 'without_first_frame',
        'frame_selection': config['frame_selection'],
        'data_size': len(train_loader.dataset) + len(val_loader.dataset),
        'config': config,
        'advanced_features': {
            'fixed_claw_matrix': config['use_claw_matrix'],
            'fixed_hierarchical_planning': config['use_hierarchical'],
            'fixed_advanced_attention': config['use_advanced_attention']
        },
        'model_parameters': sum(p.numel() for p in model.parameters()),
        'training_status': 'completed'
    }
    
    with open('no_first_frame_training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: no_first_frame_training_results.json")
    
    # ëª¨ë¸ ìƒíƒœ í™•ì¸
    if os.path.exists('no_first_frame_model_best.pth'):
        checkpoint = torch.load('no_first_frame_model_best.pth', map_location='cpu')
        print(f"ğŸ“Š ìµœê³  ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - ì—í¬í¬: {checkpoint['epoch']}")
        print(f"   - í›ˆë ¨ ì†ì‹¤: {checkpoint['train_loss']:.4f}")
        print(f"   - ê²€ì¦ ì†ì‹¤: {checkpoint['val_loss']:.4f}")

if __name__ == "__main__":
    main()
