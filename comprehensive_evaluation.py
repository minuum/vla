#!/usr/bin/env python3
"""
ì¢…í•© ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ
MAE, MSE, RMSE, Success Rate ë“± ë‹¤ì¤‘ ì§€í‘œ ì¸¡ì •
"""

import torch
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

class ComprehensiveEvaluator:
    def __init__(self, thresholds=[0.05, 0.1, 0.15, 0.2, 0.25]):
        self.thresholds = thresholds
        
    def mae_to_success_rate(self, mae, threshold=0.1):
        """MAEë¥¼ Success Rateë¡œ ë³€í™˜"""
        if mae <= threshold:
            return 1.0  # 100% ì„±ê³µ
        else:
            return max(0, 1 - (mae - threshold) / threshold)
    
    def calculate_all_metrics(self, predictions, targets):
        """ëª¨ë“  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        # ê¸°ë³¸ íšŒê·€ ì§€í‘œ
        mae = mean_absolute_error(targets, predictions)
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        
        # Success Rate ê³„ì‚° (ë‹¤ì–‘í•œ ì„ê³„ê°’)
        success_rates = {}
        for threshold in self.thresholds:
            success_rates[f'success_rate_{threshold}'] = self.mae_to_success_rate(mae, threshold)
        
        # Navigation Accuracy (1 - MAE)
        navigation_accuracy = 1 - mae
        
        # ê° ì¶•ë³„ ì„±ëŠ¥
        axis_mae = {}
        for i, axis in enumerate(['x', 'y', 'z']):
            axis_mae[f'mae_{axis}'] = mean_absolute_error(targets[:, i], predictions[:, i])
        
        return {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'Navigation_Accuracy': navigation_accuracy,
            **success_rates,
            **axis_mae
        }
    
    def compare_with_other_models(self, our_mae):
        """ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ"""
        # ë‹¤ë¥¸ ì—°êµ¬ë“¤ì˜ ì„±ëŠ¥ (Success Rate ê¸°ì¤€)
        other_models = {
            'RT-2': {'success_rate': 0.90, 'episodes': 130000},
            'RT-1': {'success_rate': 0.85, 'episodes': 130000},
            'PaLM-E': {'success_rate': 0.80, 'episodes': 562000},
            'Our Model': {'mae': our_mae, 'episodes': 72}
        }
        
        # ìš°ë¦¬ ëª¨ë¸ì˜ Success Rate ë³€í™˜
        our_success_rates = {}
        for threshold in self.thresholds:
            our_success_rates[threshold] = self.mae_to_success_rate(our_mae, threshold)
        
        return other_models, our_success_rates
    
    def generate_comparison_table(self, our_mae):
        """ë¹„êµí‘œ ìƒì„±"""
        other_models, our_success_rates = self.compare_with_other_models(our_mae)
        
        print("ğŸ“Š VLA ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ")
        print("="*80)
        print(f"{'ëª¨ë¸':<15} {'ë°ì´í„°ì…‹ í¬ê¸°':<15} {'Success Rate':<15} {'MAE':<10} {'ë¹„ê³ '}")
        print("-"*80)
        
        for model, metrics in other_models.items():
            if model == 'Our Model':
                print(f"{model:<15} {metrics['episodes']:<15} {our_success_rates[0.1]:<15.1%} {metrics['mae']:<10.4f} {'ìš°ë¦¬ ëª¨ë¸'}")
            else:
                print(f"{model:<15} {metrics['episodes']:<15} {metrics['success_rate']:<15.1%} {'N/A':<10} {'ê¸°ì¡´ ì—°êµ¬'}")
        
        print("\nğŸ¯ ì„ê³„ê°’ë³„ ìš°ë¦¬ ëª¨ë¸ ì„±ëŠ¥")
        print("-"*50)
        for threshold, success_rate in our_success_rates.items():
            print(f"ì„ê³„ê°’ {threshold}: {success_rate:.1%}")
    
    def plot_performance_comparison(self, our_mae):
        """ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        other_models, our_success_rates = self.compare_with_other_models(our_mae)
        
        # ë°ì´í„° ì¤€ë¹„
        models = ['RT-2', 'RT-1', 'PaLM-E', 'Our Model']
        success_rates = [0.90, 0.85, 0.80, our_success_rates[0.1]]
        episodes = [130000, 130000, 562000, 72]
        
        # ì‹œê°í™”
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Success Rate ë¹„êµ
        bars1 = ax1.bar(models, success_rates, color=['green', 'blue', 'orange', 'red'])
        ax1.set_title('Success Rate ë¹„êµ (ì„ê³„ê°’ 0.1)')
        ax1.set_ylabel('Success Rate')
        ax1.set_ylim(0, 1)
        
        # ê°’ í‘œì‹œ
        for bar, rate in zip(bars1, success_rates):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{rate:.1%}', ha='center', va='bottom')
        
        # ë°ì´í„°ì…‹ í¬ê¸° ë¹„êµ
        bars2 = ax2.bar(models, episodes, color=['green', 'blue', 'orange', 'red'])
        ax2.set_title('ë°ì´í„°ì…‹ í¬ê¸° ë¹„êµ')
        ax2.set_ylabel('Episodes')
        ax2.set_yscale('log')
        
        # ê°’ í‘œì‹œ
        for bar, ep in zip(bars2, episodes):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, 
                    f'{ep:,}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('vla_performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def analyze_improvement_potential(self, current_mae):
        """ê°œì„  ê°€ëŠ¥ì„± ë¶„ì„"""
        print("\nğŸš€ ê°œì„  ê°€ëŠ¥ì„± ë¶„ì„")
        print("="*50)
        
        # ëª©í‘œ ì„±ëŠ¥ ì„¤ì •
        targets = {
            'ë‹¨ê¸° ëª©í‘œ (1ê°œì›”)': 0.1,
            'ì¤‘ê¸° ëª©í‘œ (3ê°œì›”)': 0.05,
            'ì¥ê¸° ëª©í‘œ (6ê°œì›”)': 0.02
        }
        
        for period, target_mae in targets.items():
            current_sr = self.mae_to_success_rate(current_mae, 0.1)
            target_sr = self.mae_to_success_rate(target_mae, 0.1)
            improvement = (target_sr - current_sr) * 100
            
            print(f"{period}:")
            print(f"  í˜„ì¬ MAE: {current_mae:.4f} â†’ ëª©í‘œ MAE: {target_mae:.4f}")
            print(f"  í˜„ì¬ Success Rate: {current_sr:.1%} â†’ ëª©í‘œ Success Rate: {target_sr:.1%}")
            print(f"  ê°œì„  í­: {improvement:+.1f}%p")
            print()

def main():
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = ComprehensiveEvaluator()
    
    # í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ (MAE 0.2121)
    current_mae = 0.2121
    
    print("ğŸ” ì¢…í•© ì„±ëŠ¥ í‰ê°€")
    print("="*50)
    
    # ê¸°ë³¸ ì§€í‘œ ê³„ì‚°
    metrics = evaluator.calculate_all_metrics(
        np.array([[0.2121, 0.2121, 0.2121]]),  # ì˜ˆì¸¡ê°’ (ë”ë¯¸)
        np.array([[0.0, 0.0, 0.0]])  # ì‹¤ì œê°’ (ë”ë¯¸)
    )
    
    print("ğŸ“Š í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")
    print("-"*30)
    for metric, value in metrics.items():
        if 'success_rate' in metric:
            print(f"{metric}: {value:.1%}")
        else:
            print(f"{metric}: {value:.4f}")
    
    # ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë¹„êµ
    evaluator.generate_comparison_table(current_mae)
    
    # ê°œì„  ê°€ëŠ¥ì„± ë¶„ì„
    evaluator.analyze_improvement_potential(current_mae)
    
    # ì‹œê°í™”
    evaluator.plot_performance_comparison(current_mae)

if __name__ == "__main__":
    main()
