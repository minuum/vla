#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CompressedImage
from geometry_msgs.msg import Twist
from std_msgs.msg import String
import cv2
import numpy as np
from PIL import Image as PILImage
import torch
from transformers import AutoProcessor, AutoModel
import json
import time
from typing import List, Optional
import threading
from queue import Queue
import os

# ONNX Runtime import
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    print("Warning: ONNX Runtime not available. Install with: pip install onnxruntime-gpu")
    ONNX_AVAILABLE = False

class RoboVLMsInference(Node):
    """
    RoboVLMs ë°©ì‹ì˜ ì¶”ë¡  ë…¸ë“œ
    ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ ë‹¨ì¼ ì•¡ì…˜ì„ ìƒì„±í•˜ëŠ” ì‹¤ì‹œê°„ ë°˜ì‘í˜• ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        super().__init__('robovlms_inference')
        
        # ëª¨ë¸ ì„¤ì • (íŒŒë¼ë¯¸í„°í™”)
        self.inference_mode = self.declare_parameter('inference_mode', 'transformers').value
        self.model_type = self.declare_parameter('model_type', 'accurate_gpu').value
        self.device = self.declare_parameter('device', 'auto').value
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ì–‘ìí™” ëª¨ë¸ìš©)
        self.quantized_model_paths = {
            'accurate_gpu': 'Robo+/Mobile_VLA/accurate_gpu_quantized/accurate_gpu_model.onnx',
            'simple_gpu': 'Robo+/Mobile_VLA/simple_gpu_quantized/simple_gpu_model.onnx',
            'cpu_mae0222': 'Robo+/Mobile_VLA/quantized_models_cpu/mae0222_model_cpu.onnx'
        }
        
        # Transformers ëª¨ë“œ ì„¤ì •
        if self.inference_mode == 'transformers':
            self.model_name = "minium/mobile-vla-omniwheel"  # MAE 0.222 ë‹¬ì„±í•œ ìµœì‹  ëª¨ë¸
            self.torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.get_logger().info(f"Using device: {self.torch_device}")
            self.get_logger().info(f"Using updated model: {self.model_name} (MAE 0.222)")
        else:
            self.get_logger().info(f"Using quantized model: {self.model_type}")
            self.get_logger().info(f"Using device: {self.device}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model()
        
        # ROS ì„¤ì •
        self.setup_ros()
        
        # ìƒíƒœ ë³€ìˆ˜
        self.is_processing = False
        self.is_system_running = False
        self.current_task = "Navigate around obstacles to track the target cup"
        self.inference_count = 0
        self.last_inference_time = 0.0
        
        # ì´ë¯¸ì§€ í
        self.image_queue = Queue(maxsize=1)  # ìµœì‹  ì´ë¯¸ì§€ë§Œ ìœ ì§€
        
        # ì¶”ë¡  ìŠ¤ë ˆë“œ ì‹œì‘
        self.inference_thread = threading.Thread(target=self.inference_worker, daemon=True)
        self.inference_thread.start()
        
        self.get_logger().info("RoboVLMs Inference Node initialized")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (Transformers ë˜ëŠ” ì–‘ìí™” ëª¨ë¸)"""
        if self.inference_mode == 'transformers':
            self.load_transformers_model()
        else:
            self.load_quantized_model()
    
    def load_transformers_model(self):
        """Transformers ëª¨ë¸ ë¡œë“œ (MAE 0.222)"""
        try:
            self.get_logger().info(f"Loading transformers model: {self.model_name}")
            self.get_logger().info("Model performance: MAE 0.222 (72.5% improvement)")
            
            # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            
            # GPUë¡œ ì´ë™
            self.model.to(self.torch_device)
            self.model.eval()
            
            self.get_logger().info("âœ… Transformers model loaded successfully")
            self.get_logger().info("ğŸ¯ Model optimized for omniwheel robot navigation")
            
        except Exception as e:
            self.get_logger().error(f"Failed to load transformers model: {e}")
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜
            self.get_logger().info("Switching to test mode (no model loading)")
            self.processor = None
            self.model = None
    
    def load_quantized_model(self):
        """ì–‘ìí™”ëœ ONNX ëª¨ë¸ ë¡œë“œ"""
        if not ONNX_AVAILABLE:
            self.get_logger().error("âŒ ONNX Runtime not available")
            return
            
        try:
            model_path = self.quantized_model_paths.get(self.model_type)
            if not model_path or not os.path.exists(model_path):
                self.get_logger().error(f"âŒ Quantized model not found: {model_path}")
                return
            
            self.get_logger().info(f"ğŸ”„ Loading quantized model: {model_path}")
            
            # ONNX Runtime ì„¸ì…˜ ìƒì„±
            providers = []
            if self.device == 'auto' or self.device == 'gpu':
                # GPU í”„ë¡œë°”ì´ë” ì‹œë„
                try:
                    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
                    self.get_logger().info("ğŸ¯ Using CUDA execution provider")
                except:
                    providers = ['CPUExecutionProvider']
                    self.get_logger().info("ğŸ’» Using CPU execution provider")
            else:
                providers = ['CPUExecutionProvider']
            
            self.session = ort.InferenceSession(model_path, providers=providers)
            
            # ì…ë ¥/ì¶œë ¥ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            self.input_name = self.session.get_inputs()[0].name
            self.output_name = self.session.get_outputs()[0].name
            
            self.get_logger().info(f"âœ… Quantized model loaded successfully")
            self.get_logger().info(f"ğŸ“¥ Input: {self.input_name}")
            self.get_logger().info(f"ğŸ“¤ Output: {self.output_name}")
            
        except Exception as e:
            self.get_logger().error(f"âŒ Failed to load quantized model: {e}")
            self.session = None
    
    def setup_ros(self):
        """ROS í¼ë¸”ë¦¬ì…”/ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„ ì„¤ì •"""
        
        # ì´ë¯¸ì§€ ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„ (ì••ì¶•ëœ ì´ë¯¸ì§€)
        self.image_sub = self.create_subscription(
            CompressedImage,
            '/camera/image/compressed',
            self.image_callback,
            10
        )
        
        # ì•¡ì…˜ í¼ë¸”ë¦¬ì…”
        self.action_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # ì¶”ë¡  ê²°ê³¼ í¼ë¸”ë¦¬ì…”
        self.inference_result_pub = self.create_publisher(
            String,
            '/mobile_vla/inference_result',
            10
        )
        
        # íƒœìŠ¤í¬ ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„
        self.task_sub = self.create_subscription(
            String,
            '/mobile_vla/task',
            self.task_callback,
            10
        )
        
        # ìƒíƒœ í¼ë¸”ë¦¬ì…”
        self.status_pub = self.create_publisher(
            String,
            '/mobile_vla/status',
            10
        )
        
        # ì‹œìŠ¤í…œ ì œì–´ ì„œë¸ŒìŠ¤í¬ë¼ì´ë²„
        self.control_sub = self.create_subscription(
            String,
            '/mobile_vla/system_control',
            self.control_callback,
            10
        )
        
        self.get_logger().info("ROS interfaces setup completed")
    
    def control_callback(self, msg):
        """ì‹œìŠ¤í…œ ì œì–´ ì½œë°±"""
        try:
            command = json.loads(msg.data)
            action = command.get('action')
            
            if action == 'start':
                self.start_system()
            elif action == 'stop':
                self.stop_system()
            elif action == 'pause':
                self.pause_system()
            elif action == 'resume':
                self.resume_system()
            
        except Exception as e:
            self.get_logger().error(f"Error processing control command: {e}")
    
    def start_system(self):
        """ì‹œìŠ¤í…œ ì‹œì‘"""
        self.is_system_running = True
        self.inference_count = 0
        self.get_logger().info("ğŸš€ RoboVLMs system started")
        self.publish_status("started")
    
    def stop_system(self):
        """ì‹œìŠ¤í…œ ì¤‘ì§€"""
        self.is_system_running = False
        # ë¡œë´‡ ì •ì§€
        self.stop_robot()
        self.get_logger().info("ğŸ›‘ RoboVLMs system stopped")
        self.publish_status("stopped")
    
    def pause_system(self):
        """ì‹œìŠ¤í…œ ì¼ì‹œì •ì§€"""
        self.is_system_running = False
        self.stop_robot()
        self.get_logger().info("â¸ï¸ RoboVLMs system paused")
        self.publish_status("paused")
    
    def resume_system(self):
        """ì‹œìŠ¤í…œ ì¬ê°œ"""
        self.is_system_running = True
        self.get_logger().info("â–¶ï¸ RoboVLMs system resumed")
        self.publish_status("running")
    
    def stop_robot(self):
        """ë¡œë´‡ ì •ì§€"""
        try:
            twist = Twist()
            twist.linear.x = 0.0
            twist.linear.y = 0.0
            twist.angular.z = 0.0
            self.action_pub.publish(twist)
        except Exception as e:
            self.get_logger().error(f"Error stopping robot: {e}")
    
    def task_callback(self, msg):
        """íƒœìŠ¤í¬ ì—…ë°ì´íŠ¸ ì½œë°±"""
        self.current_task = msg.data
        self.get_logger().info(f"Task updated: {self.current_task}")
    
    def image_callback(self, msg):
        """ì´ë¯¸ì§€ ìˆ˜ì‹  ì½œë°±"""
        if not self.is_system_running:
            return
        
        try:
            # ì••ì¶•ëœ ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
            np_arr = np.frombuffer(msg.data, np.uint8)
            image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
            
            # BGR to RGB ë³€í™˜
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # PIL Imageë¡œ ë³€í™˜
            pil_image = PILImage.fromarray(image_rgb)
            
            # íì— ì´ë¯¸ì§€ ì¶”ê°€ (ê¸°ì¡´ ì´ë¯¸ì§€ êµì²´)
            if not self.image_queue.empty():
                self.image_queue.get()  # ê¸°ì¡´ ì´ë¯¸ì§€ ì œê±°
            self.image_queue.put((pil_image, msg.header.stamp))
            
        except Exception as e:
            self.get_logger().error(f"Error processing image: {e}")
    
    def preprocess_image(self, image: PILImage.Image):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (Transformers ë˜ëŠ” ONNX ëª¨ë¸ìš©)"""
        if self.inference_mode == 'transformers':
            return self.preprocess_for_transformers(image)
        else:
            return self.preprocess_for_onnx(image)
    
    def preprocess_for_transformers(self, image: PILImage.Image) -> Optional[torch.Tensor]:
        """Transformers ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            if self.processor is None:
                return None  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            
            # ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ì „ì²˜ë¦¬
            inputs = self.processor(
                images=image,
                text=self.current_task,
                return_tensors="pt"
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.torch_device) for k, v in inputs.items()}
            
            return inputs
            
        except Exception as e:
            self.get_logger().error(f"Error preprocessing image for transformers: {e}")
            return None
    
    def preprocess_for_onnx(self, image: PILImage.Image) -> Optional[np.ndarray]:
        """ONNX ëª¨ë¸ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ê²Œ)
            target_size = (224, 224)  # Mobile VLA ëª¨ë¸ ì…ë ¥ í¬ê¸°
            resized_image = image.resize(target_size)
            
            # PIL to numpy ë³€í™˜
            image_array = np.array(resized_image, dtype=np.float32)
            
            # ì •ê·œí™” (0-255 -> 0-1)
            image_array = image_array / 255.0
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
            image_array = np.expand_dims(image_array, axis=0)
            
            return image_array
            
        except Exception as e:
            self.get_logger().error(f"Error preprocessing image for ONNX: {e}")
            return None
    
    def predict_single_action(self, inputs) -> Optional[List[float]]:
        """ë‹¨ì¼ ì•¡ì…˜ ì˜ˆì¸¡ (Transformers ë˜ëŠ” ì–‘ìí™” ëª¨ë¸)"""
        if self.inference_mode == 'transformers':
            return self.predict_with_transformers(inputs)
        else:
            return self.predict_with_quantized(inputs)
    
    def predict_with_transformers(self, inputs: dict) -> Optional[List[float]]:
        """Transformers ëª¨ë¸ë¡œ ì•¡ì…˜ ì˜ˆì¸¡"""
        try:
            if self.model is None:
                # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°„ë‹¨í•œ ì•¡ì…˜ ìƒì„±
                return self.generate_test_action()
            
            with torch.no_grad():
                # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì¶”ë¡  (MAE 0.222)
                outputs = self.model(**inputs)
                
                # ì•¡ì…˜ í—¤ë“œì—ì„œ ì˜ˆì¸¡ê°’ ì¶”ì¶œ (ì˜´ë‹ˆíœ  ìµœì í™”)
                action_logits = outputs.action_logits  # [batch_size, 1, 3]
                
                # ë‹¨ì¼ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜ (RoboVLMs ë°©ì‹)
                if action_logits.shape[1] > 1:
                    action_logits = action_logits[:, 0:1, :]  # ì²« ë²ˆì§¸ ì•¡ì…˜ë§Œ ì‚¬ìš©
                
                # CPUë¡œ ì´ë™í•˜ê³  numpyë¡œ ë³€í™˜
                action = action_logits.cpu().numpy()[0, 0]  # [3]
                
                # ì˜´ë‹ˆíœ  ë¡œë´‡ì— ìµœì í™”ëœ ì•¡ì…˜ ë°˜í™˜
                return action.tolist()
                
        except Exception as e:
            self.get_logger().error(f"Error predicting action with transformers model: {e}")
            return None
    
    def predict_with_quantized(self, image_array: np.ndarray) -> Optional[List[float]]:
        """ì–‘ìí™”ëœ ëª¨ë¸ë¡œ ì•¡ì…˜ ì˜ˆì¸¡"""
        if not hasattr(self, 'session') or self.session is None:
            self.get_logger().warn("âš ï¸ No quantized model loaded, using test action")
            return self.generate_test_action()
        
        try:
            # ONNX Runtime ì¶”ë¡ 
            outputs = self.session.run(
                [self.output_name], 
                {self.input_name: image_array}
            )
            
            # ì¶œë ¥ ì²˜ë¦¬ (ì•¡ì…˜ ì˜ˆì¸¡)
            action_output = outputs[0]
            
            # ì¶œë ¥ í˜•íƒœì— ë”°ë¼ ì²˜ë¦¬
            if len(action_output.shape) == 3:  # [batch, sequence, action_dim]
                action = action_output[0, 0, :]  # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ì•¡ì…˜
            elif len(action_output.shape) == 2:  # [batch, action_dim]
                action = action_output[0, :]
            else:
                action = action_output.flatten()[:3]  # ì²˜ìŒ 3ê°œ ê°’ ì‚¬ìš©
            
            # ì•¡ì…˜ ì •ê·œí™” (í•„ìš”ì‹œ)
            action = np.clip(action, -1.0, 1.0)
            
            return action.tolist()
            
        except Exception as e:
            self.get_logger().error(f"Error in quantized inference: {e}")
            return None
    
    def generate_test_action(self) -> List[float]:
        """í…ŒìŠ¤íŠ¸ìš© ì•¡ì…˜ ìƒì„±"""
        # ê°„ë‹¨í•œ ì›í˜• ì›€ì§ì„
        import math
        t = time.time()
        angle = (t * 0.5) % (2 * math.pi)
        
        linear_x = 0.1 * math.cos(angle)
        linear_y = 0.05 * math.sin(angle)
        angular_z = 0.2 * math.sin(angle * 2)
        
        return [float(linear_x), float(linear_y), float(angular_z)]
    
    def inference_worker(self):
        """ì¶”ë¡  ì›Œì»¤ ìŠ¤ë ˆë“œ (RoboVLMs ë°©ì‹)"""
        while rclpy.ok():
            try:
                if not self.is_system_running:
                    time.sleep(0.1)
                    continue
                
                # íì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
                if not self.image_queue.empty():
                    image, timestamp = self.image_queue.get()
                    
                    self.is_processing = True
                    start_time = time.time()
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    self.publish_status("processing")
                    
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
                    inputs = self.preprocess_image(image)
                    
                    # ë‹¨ì¼ ì•¡ì…˜ ì˜ˆì¸¡
                    action = self.predict_single_action(inputs)
                    if action is None:
                        continue
                    
                    # ì¶”ë¡  ì‹œê°„ ê³„ì‚°
                    inference_time = time.time() - start_time
                    self.last_inference_time = inference_time
                    self.inference_count += 1
                    
                    # ê²°ê³¼ ë°œí–‰
                    self.publish_inference_result(action, inference_time, timestamp)
                    
                    # ì•¡ì…˜ ì‹¤í–‰
                    self.execute_action(action)
                    
                    self.is_processing = False
                    self.publish_status("ready")
                    
                else:
                    time.sleep(0.01)  # 10ms ëŒ€ê¸°
                    
            except Exception as e:
                self.get_logger().error(f"Error in inference worker: {e}")
                self.is_processing = False
                time.sleep(0.1)
    
    def execute_action(self, action: List[float]):
        """ë‹¨ì¼ ì•¡ì…˜ ì‹¤í–‰"""
        try:
            # ì•¡ì…˜ì„ Twist ë©”ì‹œì§€ë¡œ ë³€í™˜
            twist = Twist()
            twist.linear.x = float(action[0])  # linear_x
            twist.linear.y = float(action[1])  # linear_y
            twist.angular.z = float(action[2])  # angular_z
            
            # ì•¡ì…˜ ë°œí–‰
            self.action_pub.publish(twist)
            
        except Exception as e:
            self.get_logger().error(f"Error executing action: {e}")
    
    def publish_inference_result(self, action: List[float], inference_time: float, timestamp):
        """ì¶”ë¡  ê²°ê³¼ ë°œí–‰"""
        try:
            result = {
                "timestamp": timestamp.sec + timestamp.nanosec * 1e-9,
                "inference_time": inference_time,
                "action": action,
                "task": self.current_task,
                "inference_count": self.inference_count,
                "mode": f"robovlms_{self.inference_mode}",
                "model_type": self.model_type if self.inference_mode != 'transformers' else 'transformers'
            }
            
            msg = String()
            msg.data = json.dumps(result)
            self.inference_result_pub.publish(msg)
            
            model_info = f"({self.model_type})" if self.inference_mode != 'transformers' else "(MAE 0.222 Model)"
            self.get_logger().info(f"ğŸ¯ RoboVLMs Inference #{self.inference_count}: {inference_time:.3f}s, Action: {action} {model_info}")
            
        except Exception as e:
            self.get_logger().error(f"Error publishing inference result: {e}")
    
    def publish_status(self, status: str):
        """ìƒíƒœ ë°œí–‰"""
        try:
            msg = String()
            msg.data = json.dumps({
                "status": status,
                "timestamp": time.time(),
                "inference_count": self.inference_count,
                "last_inference_time": self.last_inference_time,
                "mode": f"robovlms_{self.inference_mode}",
                "model_type": self.model_type if self.inference_mode != 'transformers' else 'transformers'
            })
            self.status_pub.publish(msg)
            
        except Exception as e:
            self.get_logger().error(f"Error publishing status: {e}")

def main(args=None):
    rclpy.init(args=args)
    
    node = RoboVLMsInference()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
