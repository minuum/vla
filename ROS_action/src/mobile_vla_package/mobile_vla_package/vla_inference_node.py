#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
import torch
import numpy as np
import cv2
from PIL import Image
import time
from typing import Optional, Dict, Any

from sensor_msgs.msg import Image
from std_msgs.msg import String, Float32MultiArray
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge

from camera_interfaces.srv import GetImage

try:
    from transformers import AutoProcessor, AutoModelForVision2Seq
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("Warning: transformers not available. Using mock inference.")
    TRANSFORMERS_AVAILABLE = False

class VLAInferenceNode(Node):
    def __init__(self):
        super().__init__('vla_inference_node')
        
        self.bridge = CvBridge()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = None
        self.processor = None
        self.model_loaded = False
        
        # ì¶”ë¡  ì„¤ì •
        self.inference_interval = 0.5  # 0.5ì´ˆë§ˆë‹¤ ì¶”ë¡ 
        self.last_inference_time = 0.0
        self.confidence_threshold = 0.7
        
        # ROS ì„¤ì •
        self.setup_ros_components()
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_vla_model()
        
        self.get_logger().info(f"ğŸš€ VLA ì¶”ë¡  ë…¸ë“œ ì‹œì‘ - Device: {self.device}")
        self.get_logger().info(f"ğŸ“Š ì¶”ë¡  ê°„ê²©: {self.inference_interval}ì´ˆ")
        
    def setup_ros_components(self):
        """ROS ì»´í¬ë„ŒíŠ¸ ì„¤ì •"""
        # ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ (ì¹´ë©”ë¼ì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°)
        self.get_image_client = self.create_client(GetImage, 'get_image_service')
        
        # ë°œí–‰ìë“¤
        self.inference_result_pub = self.create_publisher(
            String, 'vla_inference_result', 10
        )
        self.action_command_pub = self.create_publisher(
            Twist, 'vla_action_command', 10
        )
        self.confidence_pub = self.create_publisher(
            Float32MultiArray, 'vla_confidence', 10
        )
        
        # íƒ€ì´ë¨¸ (ì£¼ê¸°ì  ì¶”ë¡ )
        self.inference_timer = self.create_timer(
            self.inference_interval, self.inference_callback
        )
        
        self.get_logger().info("âœ… ROS ì»´í¬ë„ŒíŠ¸ ì„¤ì • ì™„ë£Œ")
        
    def load_vla_model(self):
        """VLA ëª¨ë¸ ë¡œë“œ"""
        if not TRANSFORMERS_AVAILABLE:
            self.get_logger().warn("âš ï¸ Transformers ì—†ìŒ - Mock ëª¨ë“œë¡œ ì‹¤í–‰")
            self.model_loaded = True
            return
            
        try:
            self.get_logger().info("ğŸ”„ VLA ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ë¡œì»¬ ë˜ëŠ” HuggingFace)
            model_name = "microsoft/kosmos-2-patch14-224"
            
            # í”„ë¡œì„¸ì„œì™€ ëª¨ë¸ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(model_name)
            self.model = AutoModelForVision2Seq.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                device_map="auto" if self.device.type == 'cuda' else None
            )
            
            if self.device.type == 'cuda':
                self.model = self.model.to(self.device)
            
            self.model.eval()
            self.model_loaded = True
            
            self.get_logger().info("âœ… VLA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            self.get_logger().error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model_loaded = False
            
    def get_latest_image(self) -> Optional[np.ndarray]:
        """ì¹´ë©”ë¼ ì„œë¹„ìŠ¤ì—ì„œ ìµœì‹  ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.get_image_client.wait_for_service(timeout_sec=1.0):
                self.get_logger().warn("âš ï¸ ì¹´ë©”ë¼ ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                return None
                
            request = GetImage.Request()
            future = self.get_image_client.call_async(request)
            
            rclpy.spin_until_future_complete(self, future, timeout_sec=2.0)
            
            if future.done():
                response = future.result()
                if response.image.data:
                    cv_image = self.bridge.imgmsg_to_cv2(response.image, "bgr8")
                    return cv_image
                    
        except Exception as e:
            self.get_logger().error(f"âŒ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            
        return None
        
    def preprocess_image(self, cv_image: np.ndarray) -> Optional[torch.Tensor]:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        try:
            # BGR to RGB
            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
            
            # PIL Imageë¡œ ë³€í™˜
            pil_image = Image.fromarray(rgb_image)
            
            # í”„ë¡œì„¸ì„œë¡œ ì „ì²˜ë¦¬
            if self.processor:
                inputs = self.processor(
                    images=pil_image,
                    return_tensors="pt"
                )
                
                # GPUë¡œ ì´ë™
                if self.device.type == 'cuda':
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                return inputs
                
        except Exception as e:
            self.get_logger().error(f"âŒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
        return None
        
    def run_inference(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """VLA ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰"""
        if not self.model_loaded or self.model is None:
            return self.mock_inference()
            
        try:
            with torch.no_grad():
                # ì¶”ë¡  ì‹¤í–‰
                outputs = self.model.generate(
                    **inputs,
                    max_length=50,
                    num_beams=5,
                    early_stopping=True,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
                
                # ê²°ê³¼ ë””ì½”ë”©
                generated_text = self.processor.tokenizer.decode(
                    outputs[0], skip_special_tokens=True
                )
                
                # ì‹ ë¢°ë„ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                confidence = self.calculate_confidence(outputs[0])
                
                return {
                    "text": generated_text,
                    "confidence": confidence,
                    "raw_output": outputs[0].cpu().numpy()
                }
                
        except Exception as e:
            self.get_logger().error(f"âŒ ì¶”ë¡  ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return self.mock_inference()
            
    def mock_inference(self) -> Dict[str, Any]:
        """Mock ì¶”ë¡  (ëª¨ë¸ ì—†ì„ ë•Œ)"""
        return {
            "text": "Mock VLA inference result",
            "confidence": 0.8,
            "raw_output": np.array([1, 2, 3, 4, 5])
        }
        
    def calculate_confidence(self, output_tokens: torch.Tensor) -> float:
        """ì¶”ë¡  ê²°ê³¼ ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: í† í° ê¸¸ì´ì™€ íŠ¹ìˆ˜ í† í° ë¹„ìœ¨
        token_length = len(output_tokens)
        special_tokens = torch.sum(output_tokens == self.processor.tokenizer.eos_token_id)
        
        # ê¸¸ì´ê°€ ì ë‹¹í•˜ê³  íŠ¹ìˆ˜ í† í°ì´ ì ìœ¼ë©´ ë†’ì€ ì‹ ë¢°ë„
        if 10 <= token_length <= 30 and special_tokens <= 2:
            return 0.9
        elif 5 <= token_length <= 50:
            return 0.7
        else:
            return 0.5
            
    def parse_action_from_text(self, text: str) -> Optional[Dict[str, float]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì•¡ì…˜ íŒŒì‹±"""
        text_lower = text.lower()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒŒì‹±
        action_mapping = {
            "forward": {"linear_x": 1.0, "linear_y": 0.0, "angular_z": 0.0},
            "backward": {"linear_x": -1.0, "linear_y": 0.0, "angular_z": 0.0},
            "left": {"linear_x": 0.0, "linear_y": 1.0, "angular_z": 0.0},
            "right": {"linear_x": 0.0, "linear_y": -1.0, "angular_z": 0.0},
            "turn left": {"linear_x": 0.0, "linear_y": 0.0, "angular_z": 1.0},
            "turn right": {"linear_x": 0.0, "linear_y": 0.0, "angular_z": -1.0},
            "stop": {"linear_x": 0.0, "linear_y": 0.0, "angular_z": 0.0}
        }
        
        for keyword, action in action_mapping.items():
            if keyword in text_lower:
                return action
                
        return None
        
    def publish_inference_result(self, result: Dict[str, Any]):
        """ì¶”ë¡  ê²°ê³¼ ë°œí–‰"""
        # í…ìŠ¤íŠ¸ ê²°ê³¼ ë°œí–‰
        text_msg = String()
        text_msg.data = result["text"]
        self.inference_result_pub.publish(text_msg)
        
        # ì‹ ë¢°ë„ ë°œí–‰
        confidence_msg = Float32MultiArray()
        confidence_msg.data = [result["confidence"]]
        self.confidence_pub.publish(confidence_msg)
        
        # ì•¡ì…˜ ëª…ë ¹ íŒŒì‹± ë° ë°œí–‰
        action = self.parse_action_from_text(result["text"])
        if action and result["confidence"] > self.confidence_threshold:
            twist_msg = Twist()
            twist_msg.linear.x = float(action["linear_x"])
            twist_msg.linear.y = float(action["linear_y"])
            twist_msg.angular.z = float(action["angular_z"])
            self.action_command_pub.publish(twist_msg)
            
            self.get_logger().info(f"ğŸ¯ ì¶”ë¡ : {result['text']} â†’ ì•¡ì…˜: {action}")
        else:
            self.get_logger().info(f"ğŸ“ ì¶”ë¡ : {result['text']} (ì‹ ë¢°ë„: {result['confidence']:.2f})")
            
    def inference_callback(self):
        """ì£¼ê¸°ì  ì¶”ë¡  ì½œë°±"""
        current_time = time.time()
        
        # ì¶”ë¡  ê°„ê²© ì²´í¬
        if current_time - self.last_inference_time < self.inference_interval:
            return
            
        # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        image = self.get_latest_image()
        if image is None:
            return
            
        # ì „ì²˜ë¦¬
        inputs = self.preprocess_image(image)
        if inputs is None:
            return
            
        # ì¶”ë¡  ì‹¤í–‰
        result = self.run_inference(inputs)
        
        # ê²°ê³¼ ë°œí–‰
        self.publish_inference_result(result)
        
        self.last_inference_time = current_time

def main(args=None):
    rclpy.init(args=args)
    node = VLAInferenceNode()
    
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
