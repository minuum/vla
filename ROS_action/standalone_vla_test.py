#!/usr/bin/env python3
"""
ROS2 ì—†ì´ VLA ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ë…ë¦½ì ì¸ íŒŒì¼
"""

import torch
from PIL import Image as PilImage
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
import cv2
import numpy as np
import os
import sys
import re
from pathlib import Path
import time
from typing import Tuple, List, Dict, Optional

class StandaloneVLAInference:
    def __init__(self, 
                 model_id: str = "google/paligemma-3b-mix-224",
                 device_preference: str = "cuda",
                 model_cache_dir: str = ".vla_models_cache",
                 max_new_tokens: int = 128):
        
        print("ğŸ¤– ë…ë¦½í˜• VLA ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        self.model_id = model_id
        self.max_new_tokens = max_new_tokens
        self.model_cache_dir = model_cache_dir
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device_preference == "cuda" and torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"ğŸ¯ CUDA ì‚¬ìš©: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device("cpu")
            print("ğŸ¯ CPU ì‚¬ìš©")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = None
        self.processor = None
        self.load_model()
        
        print("âœ… VLA ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_model(self):
        """VLA ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_id}")
            
            model_save_path = Path(self.model_cache_dir) / self.model_id.split('/')[-1]
            model_save_path.mkdir(parents=True, exist_ok=True)

            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                self.model_id, 
                cache_dir=model_save_path
            )

            # ëª¨ë¸ ë¡œë“œ
            model_kwargs = {
                "cache_dir": model_save_path,
                "low_cpu_mem_usage": True
            }
            
            if self.device.type == "cuda":
                model_kwargs["torch_dtype"] = torch.bfloat16
                model_kwargs["device_map"] = "auto"
            else:
                model_kwargs["torch_dtype"] = torch.float32

            self.model = PaliGemmaForConditionalGeneration.from_pretrained(
                self.model_id, 
                **model_kwargs
            )
            
            if self.device.type != "cuda":
                self.model.to(self.device)
            
            self.model.eval()
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def infer_from_image_and_text(self, image: np.ndarray, text_prompt: str) -> str:
        """ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¡œë¶€í„° VLA ì¶”ë¡  ìˆ˜í–‰"""
        if self.model is None or self.processor is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            print(f"ğŸ§  ì¶”ë¡  ì‹¤í–‰: '{text_prompt}'")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (BGR -> RGB)
            if len(image.shape) == 3 and image.shape[2] == 3:
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                rgb_image = image
            
            pil_image = PilImage.fromarray(rgb_image)
            
            # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            inputs = self.processor(
                images=pil_image, 
                text=text_prompt, 
                return_tensors="pt"
            ).to(self.device)
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs, 
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False
                )
                result = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            print(f"ğŸ¤– VLA ê²°ê³¼: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return ""

    def simple_command_inference(self, image: np.ndarray, command: str) -> Tuple[float, float, float]:
        """ê°„ë‹¨í•œ ëª…ë ¹ì–´ ì¶”ë¡ """
        command_lower = command.lower()
        
        # ì§ì ‘ ì²˜ë¦¬ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤
        if "stop" in command_lower or "halt" in command_lower:
            print("ğŸ›‘ ì •ì§€ ëª…ë ¹")
            return 0.0, 0.0, 0.0
        elif "move forward" in command_lower or "go forward" in command_lower:
            print("â¡ï¸ ì „ì§„ ëª…ë ¹")
            return 0.3, 0.0, 0.0
        elif "move backward" in command_lower or "go backward" in command_lower:
            print("â¬…ï¸ í›„ì§„ ëª…ë ¹")
            return -0.3, 0.0, 0.0
        elif "turn left" in command_lower:
            print("â†ªï¸ ì¢ŒíšŒì „ ëª…ë ¹")
            return 0.0, 0.0, 0.5
        elif "turn right" in command_lower:
            print("â†©ï¸ ìš°íšŒì „ ëª…ë ¹")
            return 0.0, 0.0, -0.5
        
        # VLA ëª¨ë¸ ì‚¬ìš©
        return self.vla_model_inference(image, command)

    def vla_model_inference(self, image: np.ndarray, command: str) -> Tuple[float, float, float]:
        """VLA ëª¨ë¸ì„ ì´ìš©í•œ ë³µì¡í•œ ì¶”ë¡ """
        try:
            # ë‹¤ì–‘í•œ ì¶”ë¡  íƒ€ì…
            if "navigate to" in command.lower() or "go to" in command.lower():
                return self.navigation_inference(image, command)
            elif "avoid" in command.lower() or "obstacle" in command.lower():
                return self.obstacle_avoidance_inference(image, command)
            else:
                return self.general_inference(image, command)
                
        except Exception as e:
            print(f"âŒ VLA ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return 0.0, 0.0, 0.0

    def navigation_inference(self, image: np.ndarray, command: str) -> Tuple[float, float, float]:
        """ë‚´ë¹„ê²Œì´ì…˜ ì¶”ë¡ """
        target = command.lower().replace("navigate to", "").replace("go to", "").strip()
        prompt = f"find {target} in the image and determine robot movement direction"
        
        result = self.infer_from_image_and_text(image, prompt)
        return self.parse_action_to_twist(result)

    def obstacle_avoidance_inference(self, image: np.ndarray, command: str) -> Tuple[float, float, float]:
        """ì¥ì• ë¬¼ íšŒí”¼ ì¶”ë¡ """
        prompt = "detect obstacles and suggest safe movement direction"
        result = self.infer_from_image_and_text(image, prompt)
        
        # ì¥ì• ë¬¼ ê°ì§€ ì‹œ ì•ˆì „í•œ í–‰ë™
        if "obstacle" in result.lower() or "blocked" in result.lower():
            print("ğŸ›‘ ì¥ì• ë¬¼ ê°ì§€ - ì •ì§€")
            return 0.0, 0.0, 0.0
        else:
            print("âœ… ê²½ë¡œ ì•ˆì „ - ì²œì²œíˆ ì „ì§„")
            return 0.1, 0.0, 0.0

    def general_inference(self, image: np.ndarray, command: str) -> Tuple[float, float, float]:
        """ì¼ë°˜ì ì¸ ì¶”ë¡ """
        prompt = f"Robot action for command: {command}"
        result = self.infer_from_image_and_text(image, prompt)
        return self.parse_action_to_twist(result)

    def parse_action_to_twist(self, action_text: str) -> Tuple[float, float, float]:
        """VLA ê²°ê³¼ë¥¼ ë¡œë´‡ ì œì–´ ëª…ë ¹ìœ¼ë¡œ ë³€í™˜"""
        linear_x, linear_y, angular_z = 0.0, 0.0, 0.0
        
        action_lower = action_text.lower()
        
        if "forward" in action_lower or "ahead" in action_lower:
            linear_x = 0.2
        elif "backward" in action_lower or "back" in action_lower:
            linear_x = -0.2
        elif "left" in action_lower:
            angular_z = 0.5
        elif "right" in action_lower:
            angular_z = -0.5
        elif "stop" in action_lower or "halt" in action_lower:
            linear_x, linear_y, angular_z = 0.0, 0.0, 0.0
            
        return linear_x, linear_y, angular_z

class CameraHandler:
    """ì¹´ë©”ë¼ ì…ë ¥ ì²˜ë¦¬"""
    
    def __init__(self, camera_id: int = 0):
        self.camera_id = camera_id
        self.cap = None
        
    def init_camera(self) -> bool:
        """ì¹´ë©”ë¼ ì´ˆê¸°í™”"""
        try:
            self.cap = cv2.VideoCapture(self.camera_id)
            if not self.cap.isOpened():
                print(f"âŒ ì¹´ë©”ë¼ {self.camera_id} ì—´ê¸° ì‹¤íŒ¨")
                return False
            
            # ì¹´ë©”ë¼ ì„¤ì •
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            print(f"âœ… ì¹´ë©”ë¼ {self.camera_id} ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False

    def capture_frame(self) -> Optional[np.ndarray]:
        """í”„ë ˆì„ ìº¡ì²˜"""
        if self.cap is None:
            return None
            
        ret, frame = self.cap.read()
        if not ret:
            return None
            
        return frame

    def load_test_image(self, image_path: str) -> Optional[np.ndarray]:
        """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
                return None
            
            print(f"âœ… í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ: {image_path}")
            return image
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def release(self):
        """ì¹´ë©”ë¼ í•´ì œ"""
        if self.cap:
            self.cap.release()
            print("ğŸ”’ ì¹´ë©”ë¼ í•´ì œ ì™„ë£Œ")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ
    print("ğŸš€ ë…ë¦½í˜• VLA í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # VLA ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    vla = StandaloneVLAInference()
    
    # ì¹´ë©”ë¼ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” 
    camera = CameraHandler()
    
    # í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ë“¤
    test_commands = [
        "move forward",
        "turn left", 
        "stop",
        "navigate to door",
        "avoid obstacle"
    ]
    
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ë“¤:")
    for i, cmd in enumerate(test_commands):
        print(f"{i+1}. {cmd}")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í•„ìš”)
    test_image_path = "../RoboVLMs/cat.jpg"  # ì˜ˆì‹œ ì´ë¯¸ì§€ ê²½ë¡œ
    if os.path.exists(test_image_path):
        test_image = camera.load_test_image(test_image_path)
        if test_image is not None:
            for cmd in test_commands:
                print(f"\nğŸ§  í…ŒìŠ¤íŠ¸: '{cmd}'")
                linear_x, linear_y, angular_z = vla.simple_command_inference(test_image, cmd)
                print(f"ğŸš€ ê²°ê³¼: linear_x={linear_x:.2f}, linear_y={linear_y:.2f}, angular_z={angular_z:.2f}")
                time.sleep(1)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 