#!/usr/bin/env python3
"""
Mobile VLA Dataset Classifier
í”„ë ˆì„ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ìë™ ë¶„ë¥˜í•˜ê³  íƒœê·¸ë¥¼ ë¶™ì´ëŠ” ìœ í‹¸ë¦¬í‹°
"""

import h5py
import json
import shutil
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
from datetime import datetime

class DatasetClassifier:
    def __init__(self, dataset_path: str = "mobile_vla_dataset"):
        """
        Args:
            dataset_path: ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ë¡œ
        """
        self.dataset_path = Path(dataset_path)
        self.categories = {
            "short": {"min": 1, "max": 10, "target": 50, "description": "ì§§ì€ ì—í”¼ì†Œë“œ (1-10 í”„ë ˆì„)"},
            "medium": {"min": 11, "max": 25, "target": 100, "description": "ì¤‘ê°„ ì—í”¼ì†Œë“œ (11-25 í”„ë ˆì„)"},
            "long": {"min": 26, "max": 50, "target": 30, "description": "ê¸´ ì—í”¼ì†Œë“œ (26-50 í”„ë ˆì„)"},
            "extra_long": {"min": 51, "max": float('inf'), "target": 10, "description": "ë§¤ìš° ê¸´ ì—í”¼ì†Œë“œ (51+ í”„ë ˆì„)"}
        }
        
        # ë¶„ë¥˜ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë“¤
        self.create_category_directories()
        
    def create_category_directories(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±"""
        for category in self.categories.keys():
            category_dir = self.dataset_path / f"classified_{category}"
            category_dir.mkdir(exist_ok=True)
            
    def analyze_h5_file(self, h5_file: Path) -> Dict:
        """H5 íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            with h5py.File(h5_file, 'r') as f:
                metadata = {
                    "file_path": str(h5_file),
                    "episode_name": f.attrs.get('episode_name', h5_file.stem),
                    "total_duration": f.attrs.get('total_duration', 0.0),
                    "num_frames": f.attrs.get('num_frames', 0),
                    "action_chunk_size": f.attrs.get('action_chunk_size', 0),
                    "images_shape": f['images'].shape if 'images' in f else None,
                    "actions_shape": f['actions'].shape if 'actions' in f else None,
                    "file_size_mb": h5_file.stat().st_size / (1024 * 1024)
                }
                
                # ì‹¤ì œ í”„ë ˆì„ ìˆ˜ í™•ì¸ (images ë°ì´í„°ì…‹ ê¸°ì¤€)
                if 'images' in f:
                    metadata["actual_frames"] = f['images'].shape[0]
                else:
                    metadata["actual_frames"] = 0
                    
                return metadata
        except Exception as e:
            print(f"âŒ {h5_file} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
            
    def classify_by_frames(self, num_frames: int) -> str:
        """í”„ë ˆì„ ìˆ˜ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        for category, config in self.categories.items():
            if config["min"] <= num_frames <= config["max"]:
                return category
        return "unknown"
        
    def scan_dataset(self) -> Dict:
        """ì „ì²´ ë°ì´í„°ì…‹ì„ ìŠ¤ìº”í•˜ì—¬ ë¶„ë¥˜"""
        print("ğŸ” ë°ì´í„°ì…‹ ìŠ¤ìº” ì¤‘...")
        
        h5_files = list(self.dataset_path.glob("*.h5"))
        results = {
            "total_files": len(h5_files),
            "classified": defaultdict(list),
            "statistics": defaultdict(int),
            "errors": []
        }
        
        for h5_file in h5_files:
            metadata = self.analyze_h5_file(h5_file)
            
            if metadata is None:
                results["errors"].append(str(h5_file))
                continue
                
            # í”„ë ˆì„ ìˆ˜ ê¸°ì¤€ ë¶„ë¥˜
            category = self.classify_by_frames(metadata["actual_frames"])
            metadata["category"] = category
            
            results["classified"][category].append(metadata)
            results["statistics"][category] += 1
            
        return results
        
    def generate_report(self, scan_results: Dict) -> str:
        """ìŠ¤ìº” ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ¤– Mobile VLA Dataset Classification Report")
        report.append("=" * 60)
        report.append(f"ğŸ“Š ì´ íŒŒì¼ ìˆ˜: {scan_results['total_files']}")
        report.append(f"âŒ ì—ëŸ¬ íŒŒì¼ ìˆ˜: {len(scan_results['errors'])}")
        report.append("")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        report.append("ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í˜„í™©:")
        report.append("-" * 60)
        
        total_current = 0
        total_target = 0
        
        for category, config in self.categories.items():
            current = scan_results["statistics"][category]
            target = config["target"]
            percentage = (current / target * 100) if target > 0 else 0
            
            total_current += current
            total_target += target
            
            status_emoji = "âœ…" if current >= target else "â³"
            progress_bar = self.create_progress_bar(current, target)
            
            report.append(f"{status_emoji} {category.upper()}: {current}/{target}ê°œ ({percentage:.1f}%)")
            report.append(f"   {config['description']}")
            report.append(f"   {progress_bar}")
            report.append("")
            
        # ì „ì²´ ì§„í–‰ë¥ 
        overall_percentage = (total_current / total_target * 100) if total_target > 0 else 0
        overall_progress = self.create_progress_bar(total_current, total_target, width=40)
        
        report.append("ğŸ¯ ì „ì²´ ì§„í–‰ë¥ :")
        report.append(f"   {total_current}/{total_target}ê°œ ({overall_percentage:.1f}%)")
        report.append(f"   {overall_progress}")
        report.append("")
        
        # í”„ë ˆì„ 18ê°œ ë°ì´í„° ê°•ì¡°
        frame_18_files = []
        for category_files in scan_results["classified"].values():
            for metadata in category_files:
                if metadata["actual_frames"] == 18:
                    frame_18_files.append(metadata)
                    
        if frame_18_files:
            report.append("ğŸ¯ í”„ë ˆì„ 18ê°œ ë°ì´í„° (íŠ¹ë³„ ê´€ì‹¬ ëŒ€ìƒ):")
            report.append("-" * 40)
            for metadata in frame_18_files:
                report.append(f"   â€¢ {metadata['episode_name']} (ì¹´í…Œê³ ë¦¬: {metadata['category']})")
            report.append("")
            
        # ì—ëŸ¬ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        if scan_results["errors"]:
            report.append("âŒ ì—ëŸ¬ íŒŒì¼ë“¤:")
            for error_file in scan_results["errors"]:
                report.append(f"   â€¢ {error_file}")
            report.append("")
            
        report.append("=" * 60)
        
        return "\n".join(report)
        
    def create_progress_bar(self, current: int, target: int, width: int = 20) -> str:
        """ì§„í–‰ë¥  ë°” ìƒì„±"""
        if target == 0:
            return "â–ˆ" * width + " (ë¬´ì œí•œ)"
            
        percentage = min(current / target, 1.0)
        filled = int(width * percentage)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"{bar} {current}/{target}"
        
    def organize_files(self, scan_results: Dict, copy_mode: bool = True):
        """íŒŒì¼ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬ (ë³µì‚¬ ë˜ëŠ” ì´ë™)"""
        print(f"ğŸ“ íŒŒì¼ ì •ë¦¬ ì¤‘... ({'ë³µì‚¬' if copy_mode else 'ì´ë™'} ëª¨ë“œ)")
        
        for category, files in scan_results["classified"].items():
            category_dir = self.dataset_path / f"classified_{category}"
            category_dir.mkdir(exist_ok=True)
            
            for metadata in files:
                source_path = Path(metadata["file_path"])
                dest_path = category_dir / source_path.name
                
                try:
                    if copy_mode:
                        shutil.copy2(source_path, dest_path)
                        print(f"ğŸ“‹ ë³µì‚¬: {source_path.name} â†’ {category}/")
                    else:
                        shutil.move(str(source_path), str(dest_path))
                        print(f"ğŸ“ ì´ë™: {source_path.name} â†’ {category}/")
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {source_path.name}: {e}")
                    
    def save_metadata(self, scan_results: Dict):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        metadata_file = self.dataset_path / "dataset_classification_metadata.json"
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë°ì´í„° ë³€í™˜
        json_data = {
            "timestamp": datetime.now().isoformat(),
            "total_files": scan_results["total_files"],
            "statistics": dict(scan_results["statistics"]),
            "categories": self.categories,
            "files": {}
        }
        
        for category, files in scan_results["classified"].items():
            json_data["files"][category] = [
                {k: int(v) if isinstance(v, np.integer) else float(v) if isinstance(v, np.floating) else v 
                 for k, v in metadata.items() if k != "file_path"}
                for metadata in files
            ]
            
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
        
    def create_tagged_symlinks(self, scan_results: Dict):
        """íƒœê·¸ëœ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±"""
        tagged_dir = self.dataset_path / "tagged_episodes"
        tagged_dir.mkdir(exist_ok=True)
        
        print("ğŸ·ï¸ íƒœê·¸ëœ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì¤‘...")
        
        for category, files in scan_results["classified"].items():
            for metadata in files:
                source_path = Path(metadata["file_path"])
                frames = metadata["actual_frames"]
                
                # íƒœê·¸ëœ íŒŒì¼ëª… ìƒì„±
                tagged_name = f"{category}_{frames}frames_{source_path.name}"
                tagged_path = tagged_dir / tagged_name
                
                try:
                    # ê¸°ì¡´ ì‹¬ë³¼ë¦­ ë§í¬ê°€ ìˆìœ¼ë©´ ì œê±°
                    if tagged_path.is_symlink():
                        tagged_path.unlink()
                        
                    # ìƒëŒ€ ê²½ë¡œë¡œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
                    relative_source = Path("..") / source_path.name
                    tagged_path.symlink_to(relative_source)
                    
                    # í”„ë ˆì„ 18ê°œ ë°ì´í„°ëŠ” íŠ¹ë³„ í‘œì‹œ
                    if frames == 18:
                        special_name = f"â˜…FRAME18â˜…_{tagged_name}"
                        special_path = tagged_dir / special_name
                        if special_path.is_symlink():
                            special_path.unlink()
                        special_path.symlink_to(relative_source)
                        
                except Exception as e:
                    print(f"âŒ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± ì‹¤íŒ¨ {tagged_name}: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– Mobile VLA Dataset Classifier ì‹œì‘!")
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    dataset_path = "mobile_vla_dataset"
    
    if not Path(dataset_path).exists():
        print(f"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        return
        
    classifier = DatasetClassifier(dataset_path)
    
    # 1. ë°ì´í„°ì…‹ ìŠ¤ìº”
    scan_results = classifier.scan_dataset()
    
    # 2. ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
    report = classifier.generate_report(scan_results)
    print(report)
    
    # 3. ë©”íƒ€ë°ì´í„° ì €ì¥
    classifier.save_metadata(scan_results)
    
    # 4. íƒœê·¸ëœ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
    classifier.create_tagged_symlinks(scan_results)
    
    # 5. ì‚¬ìš©ì ì„ íƒ: íŒŒì¼ ì •ë¦¬
    print("\nğŸ“ íŒŒì¼ ì •ë¦¬ ì˜µì…˜:")
    print("1. ë³µì‚¬ ëª¨ë“œ (ì›ë³¸ íŒŒì¼ ìœ ì§€)")
    print("2. ì´ë™ ëª¨ë“œ (ì›ë³¸ íŒŒì¼ ì´ë™)")
    print("3. ê±´ë„ˆë›°ê¸°")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1/2/3): ").strip()
    
    if choice == "1":
        classifier.organize_files(scan_results, copy_mode=True)
    elif choice == "2":
        classifier.organize_files(scan_results, copy_mode=False)
    else:
        print("íŒŒì¼ ì •ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
    print("\nâœ… ë¶„ë¥˜ ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“‚ íƒœê·¸ëœ íŒŒì¼ë“¤: {dataset_path}/tagged_episodes/")
    print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„°: {dataset_path}/dataset_classification_metadata.json")

if __name__ == "__main__":
    main()
