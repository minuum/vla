FROM nvidia/cuda:11.8-devel-ubuntu20.04

# 환경 변수 설정
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# 시스템 패키지 설치
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    git \
    curl \
    wget \
    vim \
    htop \
    && rm -rf /var/lib/apt/lists/*

# ROS2 Humble 설치
RUN curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg
RUN echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" | tee /etc/apt/sources.list.d/ros2.list > /dev/null
RUN apt-get update && apt-get install -y \
    ros-humble-ros-base \
    ros-humble-cv-bridge \
    ros-humble-image-transport \
    ros-humble-image-transport-plugins \
    python3-colcon-common-extensions \
    && rm -rf /var/lib/apt/lists/*

# Python 패키지 설치
COPY src/mobile_vla_package/requirements.txt /tmp/
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# 작업 디렉토리 설정
WORKDIR /workspace/vla

# ROS 환경 설정
RUN echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
RUN echo "export ROS_DOMAIN_ID=0" >> ~/.bashrc

# Mobile VLA 모델 사전 다운로드 (선택사항)
RUN python3 -c "
import torch
from transformers import AutoModel, AutoProcessor
print('Downloading Mobile VLA model...')
try:
    model = AutoModel.from_pretrained('minium/mobile-vla', cache_dir='/root/.cache/huggingface')
    processor = AutoProcessor.from_pretrained('minium/mobile-vla', cache_dir='/root/.cache/huggingface')
    print('Model downloaded successfully')
except Exception as e:
    print(f'Model download failed: {e}')
    print('Model will be downloaded at runtime')
"

# 컨테이너 시작 스크립트
COPY start_mobile_vla.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/start_mobile_vla.sh

# 기본 명령어
CMD ["/usr/local/bin/start_mobile_vla.sh"]
