#!/usr/bin/env python3
"""
Microsoft Kosmos-2 Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq
import time
import psutil
import os

def get_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss': memory_info.rss / 1024 / 1024,  # MB
        'vms': memory_info.vms / 1024 / 1024,  # MB
        'percent': process.memory_percent()
    }

def test_kosmos2_loading():
    """Kosmos-2 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Microsoft Kosmos-2 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
    initial_memory = get_memory_usage()
    print(f"ğŸ“Š ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory['rss']:.1f} MB (RSS)")
    
    try:
        # ëª¨ë¸ëª… ì„¤ì • (2B íŒŒë¼ë¯¸í„° ë²„ì „)
        model_name = "microsoft/kosmos-2-patch14-224"
        
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        start_time = time.time()
        
        # í”„ë¡œì„¸ì„œ ë¡œë”©
        print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
        processor = AutoProcessor.from_pretrained(model_name)
        processor_time = time.time() - start_time
        print(f"âœ… í”„ë¡œì„¸ì„œ ë¡œë”© ì™„ë£Œ: {processor_time:.2f}ì´ˆ")
        
        # ëª¨ë¸ ë¡œë”© (ë©”ëª¨ë¦¬ ìµœì í™”)
        print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            torch_dtype=torch.float16,  # FP16 ì‚¬ìš©
            device_map="auto",  # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘
            low_cpu_mem_usage=True,  # ë‚®ì€ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©
            trust_remote_code=True
        )
        
        model_time = time.time() - model_start_time
        total_time = time.time() - start_time
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_time:.2f}ì´ˆ")
        print(f"â±ï¸ ì´ ë¡œë”© ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
        loaded_memory = get_memory_usage()
        memory_increase = loaded_memory['rss'] - initial_memory['rss']
        print(f"ğŸ“Š ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {loaded_memory['rss']:.1f} MB (RSS)")
        print(f"ğŸ“ˆ ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.1f} MB")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        print(f"ğŸ“Š í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
        print(f"ğŸ“Š ëª¨ë¸ í¬ê¸° (ì˜ˆìƒ): {total_params * 2 / (1024**3):.2f} GB (FP16)")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def test_kosmos2_inference(model, processor):
    """Kosmos-2 ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ Kosmos-2 ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    try:
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ë”ë¯¸ ì´ë¯¸ì§€)
        print("ğŸ–¼ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        dummy_image = torch.randn(3, 224, 224)  # RGB ì´ë¯¸ì§€
        
        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        text_prompt = "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {text_prompt}")
        
        # ì¶”ë¡  ì‹œì‘
        print("ğŸ§  ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
        inference_start = time.time()
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = processor(
            images=dummy_image,
            text=text_prompt,
            return_tensors="pt"
        )
        
        # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            model = model.cuda()
            print("ğŸš€ GPU ì‚¬ìš© ì¤‘")
        else:
            print("ğŸ’» CPU ì‚¬ìš© ì¤‘")
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=100,
                num_beams=3,
                early_stopping=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        inference_time = time.time() - inference_start
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"âœ… ì¶”ë¡  ì™„ë£Œ: {inference_time:.2f}ì´ˆ")
        print(f"ğŸ“ ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = get_memory_usage()
        print(f"ğŸ“Š ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory['rss']:.1f} MB (RSS)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return False

def test_memory_optimized_inference():
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        model_name = "microsoft/kosmos-2-patch14-224"
        
        # 8ë¹„íŠ¸ ì–‘ìí™”ë¡œ ë¡œë”©
        print("ğŸ”§ 8ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()
        
        model = AutoModelForVision2Seq.from_pretrained(
            model_name,
            load_in_8bit=True,  # 8ë¹„íŠ¸ ì–‘ìí™”
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(model_name)
        
        load_time = time.time() - start_time
        print(f"âœ… 8ë¹„íŠ¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {load_time:.2f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = get_memory_usage()
        print(f"ğŸ“Š 8ë¹„íŠ¸ ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory['rss']:.1f} MB (RSS)")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        dummy_image = torch.randn(3, 224, 224)
        text_prompt = "ì´ë¯¸ì§€ ì„¤ëª…"
        
        inputs = processor(
            images=dummy_image,
            text=text_prompt,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=50,
                num_beams=2
            )
        
        generated_text = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ“ 8ë¹„íŠ¸ ëª¨ë¸ ê²°ê³¼: {generated_text}")
        
        return True
        
    except Exception as e:
        print(f"âŒ 8ë¹„íŠ¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ Microsoft Kosmos-2 Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print(f"ğŸ’» CPU ì½”ì–´ ìˆ˜: {psutil.cpu_count()}")
    print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        print(f"ğŸš€ CUDA ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
    
    # 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    model, processor = test_kosmos2_loading()
    
    if model is not None and processor is not None:
        # 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
        inference_success = test_kosmos2_inference(model, processor)
        
        if inference_success:
            print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print("\nâš ï¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    # 3. ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    test_memory_optimized_inference()
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
