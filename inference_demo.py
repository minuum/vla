#!/usr/bin/env python3
"""
ğŸ¯ Simple CLIP LSTM ëª¨ë¸ ëŒ€í™”í˜• ì¶”ë¡  ë°ëª¨
ì²´í¬í¬ì¸íŠ¸: best_simple_clip_lstm_model.pth
"""

import torch
import torch.nn as nn
import numpy as np
import time
import os
import sys

# Simple CLIP LSTM ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class SimpleCLIPLSTMModel(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512, num_layers=2, output_size=2):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Vision feature encoder
        self.vision_encoder = nn.Linear(input_size, hidden_size)
        
        # Text feature encoder  
        self.text_encoder = nn.Linear(input_size, hidden_size)
        
        # LSTM layer
        self.lstm = nn.LSTM(hidden_size * 2, hidden_size, num_layers, batch_first=True)
        
        # Action prediction head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, output_size)
        )
        
    def forward(self, vision_features, text_features, hidden=None):
        # Encode features
        vision_encoded = self.vision_encoder(vision_features)
        text_encoded = self.text_encoder(text_features)
        
        # Concatenate features
        combined = torch.cat([vision_encoded, text_encoded], dim=-1)
        
        # Add sequence dimension if needed
        if combined.dim() == 2:
            combined = combined.unsqueeze(1)  # [batch, 1, features]
        
        # LSTM forward pass
        lstm_out, hidden = self.lstm(combined, hidden)
        
        # Take the last output
        last_output = lstm_out[:, -1, :]
        
        # Predict actions
        actions = self.action_head(last_output)
        
        return actions, hidden

def load_model(checkpoint_path):
    """ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {checkpoint_path}")
    
    # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    model = SimpleCLIPLSTMModel()
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
    print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤: {list(checkpoint.keys())}")
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ (ë‹¤ì–‘í•œ í‚¤ ì´ë¦„ ì‹œë„)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
    elif 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        # ì§ì ‘ ë¡œë“œ ì‹œë„
        model.load_state_dict(checkpoint)
    
    # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, device

def generate_dummy_features(batch_size=1, device='cpu'):
    """ë”ë¯¸ visionê³¼ text featuresë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    vision_features = torch.randn(batch_size, 2048).to(device)
    text_features = torch.randn(batch_size, 2048).to(device)
    return vision_features, text_features

def run_inference(model, device, vision_features=None, text_features=None):
    """ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    if vision_features is None or text_features is None:
        vision_features, text_features = generate_dummy_features(device=device)
    
    start_time = time.time()
    
    with torch.no_grad():
        actions, hidden = model(vision_features, text_features)
    
    inference_time = (time.time() - start_time) * 1000  # ms
    fps = 1000 / inference_time
    
    return actions, hidden, inference_time, fps

def main():
    print("ğŸ¯ Simple CLIP LSTM ëª¨ë¸ ëŒ€í™”í˜• ì¶”ë¡  ë°ëª¨")
    print("=" * 50)
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
    checkpoint_path = './vla/Robo+/Mobile_VLA/simple_clip_lstm_model/best_simple_clip_lstm_model.pth'
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        return
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, device = load_model(checkpoint_path)
        
        print("\nğŸš€ ëŒ€í™”í˜• ì¶”ë¡  ëª¨ë“œ ì‹œì‘")
        print("ëª…ë ¹ì–´:")
        print("  'infer': ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰")
        print("  'benchmark': ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (100íšŒ)")
        print("  'continuous': ì—°ì† ì¶”ë¡  ëª¨ë“œ")
        print("  'quit': ì¢…ë£Œ")
        print("-" * 50)
        
        while True:
            try:
                command = input("\nğŸ’¬ ëª…ë ¹ì–´ ì…ë ¥: ").strip().lower()
                
                if command == 'quit':
                    print("ğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                elif command == 'infer':
                    print("ğŸ”„ ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
                    actions, hidden, inference_time, fps = run_inference(model, device)
                    
                    print(f"âœ… ì¶”ë¡  ì™„ë£Œ:")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ms")
                    print(f"   FPS: {fps:.0f}")
                    print(f"   ì•¡ì…˜ ê²°ê³¼: {actions[0].cpu().numpy()}")
                
                elif command == 'benchmark':
                    print("ğŸ”„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ (100íšŒ)...")
                    
                    times = []
                    for i in range(100):
                        _, _, inference_time, _ = run_inference(model, device)
                        times.append(inference_time)
                        
                        if (i + 1) % 20 == 0:
                            print(f"   ì§„í–‰ë¥ : {i + 1}/100")
                    
                    avg_time = sum(times) / len(times)
                    fps = 1000 / avg_time
                    min_time = min(times)
                    max_time = max(times)
                    
                    print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ:")
                    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.3f}ms")
                    print(f"   ìµœì†Œ ì¶”ë¡  ì‹œê°„: {min_time:.3f}ms")
                    print(f"   ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {max_time:.3f}ms")
                    print(f"   í‰ê·  FPS: {fps:.0f}")
                
                elif command == 'continuous':
                    print("ğŸ”„ ì—°ì† ì¶”ë¡  ëª¨ë“œ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ë‹¨)")
                    print("ì‹¤ì‹œê°„ FPS ëª¨ë‹ˆí„°ë§...")
                    
                    try:
                        count = 0
                        start_time = time.time()
                        
                        while True:
                            _, _, inference_time, fps = run_inference(model, device)
                            count += 1
                            
                            if count % 10 == 0:
                                elapsed = time.time() - start_time
                                avg_fps = count / elapsed
                                print(f"   {count}íšŒ ì™„ë£Œ - í˜„ì¬ FPS: {fps:.0f}, í‰ê·  FPS: {avg_fps:.0f}")
                    
                    except KeyboardInterrupt:
                        elapsed = time.time() - start_time
                        avg_fps = count / elapsed
                        print(f"\nâ¹ï¸ ì—°ì† ì¶”ë¡  ì¤‘ë‹¨")
                        print(f"   ì´ ì¶”ë¡  íšŸìˆ˜: {count}")
                        print(f"   ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
                        print(f"   í‰ê·  FPS: {avg_fps:.0f}")
                
                else:
                    print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. 'infer', 'benchmark', 'continuous', 'quit' ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
            
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì¶”ë¡  ë°ëª¨ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
