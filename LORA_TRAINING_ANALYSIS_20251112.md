# LoRA í•™ìŠµ ê³¼ì • ë¶„ì„ ë° ì´ìŠˆ ì •ë¦¬ (2025-11-12)

## ğŸ“‹ í•™ìŠµ í™˜ê²½

- **ë‚ ì§œ**: 2025-11-12
- **ëª¨ë¸**: Kosmos-2 + LoRA (r=32, alpha=16, dropout=0.1)
- **ë°ì´í„°ì…‹**: 20251106 ì—í”¼ì†Œë“œ 13ê°œ (train 10ê°œ, val 3ê°œ)
- **ë””ë°”ì´ìŠ¤**: NVIDIA RTX A5000
- **í”„ë ˆì„ì›Œí¬**: PyTorch Lightning + RoboVLMs

---

## ğŸ› ë°œìƒí•œ ì´ìŠˆ ë° í•´ê²° ê³¼ì •

### 1. KeyError: 'rgb' (í•´ê²°ë¨ âœ…)

**ë¬¸ì œ:**
```
KeyError: 'rgb'
File: robovlms/train/base_trainer.py:366
```

**ì›ì¸:**
- `MobileVLAH5Dataset`ì´ ë°ì´í„°ë¥¼ ë°˜í™˜í•  ë•Œ í‚¤ ì´ë¦„ì´ RoboVLMsì™€ ë¶ˆì¼ì¹˜
- RoboVLMsëŠ” `batch["rgb"]`ë¥¼ ê¸°ëŒ€í•˜ì§€ë§Œ ë°ì´í„°ì…‹ì´ ë‹¤ë¥¸ í‚¤ë¡œ ë°˜í™˜

**í•´ê²°:**
- ì´ë¯¸ í•´ê²°ë¨ (ì´ì „ ì‘ì—…ì—ì„œ MobileVLAH5Dataset ìˆ˜ì •)
- ë°ì´í„°ì…‹ì´ 'rgb' í‚¤ë¡œ ì˜¬ë°”ë¥´ê²Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • ì™„ë£Œ

**ê²€ì¦:**
- Sanity Check í†µê³¼ âœ…
- Validation step ì •ìƒ ë™ì‘ âœ…

---

### 2. Gradient ì—ëŸ¬ (í•´ê²°ë¨ âœ…)

**ë¬¸ì œ:**
```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

**ì›ì¸:**
- `base_trainer.py`ì˜ `_get_loss()` ë©”ì„œë“œì—ì„œ loss ì´ˆê¸°í™” ì‹œ ë¬¸ì œ
- `loss = torch.tensor(0.0).to(self.device)` â†’ gradient tracking ì—†ìŒ
- ì‹¤ì œ lossê°€ ì¶”ê°€ë˜ì§€ ì•Šìœ¼ë©´ backward()ì—ì„œ ì—ëŸ¬ ë°œìƒ

**í•´ê²°:**
```python
# ìˆ˜ì • ì „
loss = torch.tensor(0.0).to(self.device)

# ìˆ˜ì • í›„
loss = torch.tensor(0.0, requires_grad=True).to(self.device)
```

**ê²€ì¦:**
- Training stepì—ì„œ backward() ì •ìƒ ë™ì‘ âœ…
- 3/4 ë°°ì¹˜ê¹Œì§€ ì§„í–‰ë¨

---

### 3. Mixed Precision (16-mixed) ì—ëŸ¬ (í•´ê²°ë¨ âœ…)

**ë¬¸ì œ:**
```
AssertionError: No inf checks were recorded for this optimizer.
File: torch/cuda/amp/grad_scaler.py:449
```

**ì›ì¸:**
- Mixed precision (16-mixed) ì‚¬ìš© ì‹œ GradScalerê°€ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠìŒ
- LoRA + PyTorch Lightningì˜ í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¶”ì •
- Gradientê°€ 0ì´ê±°ë‚˜ ê³„ì‚°ë˜ì§€ ì•Šì•„ scalerê°€ inf checkë¥¼ ìˆ˜í–‰í•˜ì§€ ëª»í•¨

**í•´ê²°:**
```json
// Config ìˆ˜ì •: precision "16-mixed" â†’ "32"
{
  "trainer": {
    "precision": "32"
  },
  "train_setup": {
    "precision": "32"
  }
}
```

**íŠ¸ë ˆì´ë“œì˜¤í”„:**
- âœ… ì•ˆì •ì„± í–¥ìƒ
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ (ì•½ 2ë°°)
- âŒ í•™ìŠµ ì†ë„ ì•½ê°„ ê°ì†Œ

**ê²€ì¦:**
- 1 Epoch ì™„ë£Œ âœ…
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì„±ê³µ âœ…

---

### 4. Loss = 0.000 (ë¯¸í•´ê²° âš ï¸)

**í˜„ìƒ:**
```
train_loss=0.000
val_loss=0.000
```

**ê°€ëŠ¥í•œ ì›ì¸:**

#### 4-1. Gripper Loss ë¹„ìœ¨ì´ 0
```json
"arm_gripper_loss_ratio": 0.0
```
- Mobile VLAëŠ” gripperê°€ ì—†ìœ¼ë¯€ë¡œ gripper lossë¥¼ 0ìœ¼ë¡œ ì„¤ì •
- ê·¸ëŸ¬ë‚˜ arm action lossëŠ” ê³„ì‚°ë˜ì–´ì•¼ í•¨

#### 4-2. Action ì°¨ì› ë¶ˆì¼ì¹˜
```python
# ë°ì´í„°ì…‹: 2D action (linear_x, linear_y)
action[:2] = action_2d
action[6] = 0.0  # gripper

# Config: action_dim=2
"act_head": {
    "action_dim": 2
}
```
- RoboVLMsëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 7D action (6-DOF + gripper)ì„ ê°€ì •
- 2Dë¡œ ì„¤ì •í–ˆì§€ë§Œ loss ê³„ì‚° ë¶€ë¶„ì—ì„œ ë¬¸ì œ ê°€ëŠ¥ì„±

#### 4-3. Loss ê³„ì‚° ë¡œì§ ë¬¸ì œ
```python
# base_trainer.py:_get_loss()
loss = torch.tensor(0.0, requires_grad=True).to(self.device)
if self.act_pred:
    loss_act = (loss_arm_act if loss_arm_act is not None else 0)
    loss += loss_act
```
- `loss_arm_act`ê°€ Noneì´ê±°ë‚˜ 0ì¼ ê°€ëŠ¥ì„±
- Forward passì—ì„œ ì‹¤ì œ lossê°€ ê³„ì‚°ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

#### 4-4. ì•¡ì…˜ ì •ê·œí™” ë¬¸ì œ
```python
# ë°ì´í„°ì…‹ì—ì„œ ì•¡ì…˜ì„ [-1, 1]ë¡œ í´ë¨í”„
actions_tensor = torch.clamp(actions_tensor, -1.0, 1.0)
```
- ì‹¤ì œ ì•¡ì…˜ ê°’ì´ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ë§¤ìš° ì‘ì„ ê°€ëŠ¥ì„±
- Lossê°€ ê³„ì‚°ë˜ì–´ë„ ë§¤ìš° ì‘ì€ ê°’ì¼ ìˆ˜ ìˆìŒ

---

## ğŸ” ì‹¬ì¸µ ë¶„ì„

### LoRA íŒŒë¼ë¯¸í„° ì ìš© í™•ì¸

**í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°:**
- Total: 1.7B
- Trainable: 57.0M (<3.4%)
- LoRAê°€ ì˜¬ë°”ë¥´ê²Œ ì ìš©ë¨ âœ…

**ì ìš©ëœ ëª¨ë“ˆ:**
- Vision Encoder: ëª¨ë“  attention layer
- Text Encoder: ëª¨ë“  attention layer
- Image-to-Text Projection
- Action Head (LSTM + MLP)

### ë°ì´í„°ì…‹ êµ¬ì¡°

**Training:**
- Episodes: 10
- Total frames: 164
- Batches: 4 (batch_size=2, accumulate=4)

**Validation:**
- Episodes: 3
- Total frames: 54
- Batches: 1

**ë°ì´í„° í˜•ì‹:**
```python
{
    'rgb': (window_size, C, H, W),      # (8, 3, 224, 224)
    'action': (action_chunk_size, 7),   # (10, 7) - padded
    'action_chunck': (action_chunk_size, 7),
    'chunck_mask': (action_chunk_size,),  # all ones
    'text': (256,),
    'text_mask': (256,),
    'raw_text': str
}
```

---

## ğŸ“Š í•™ìŠµ ì„±ëŠ¥

### ì‹œê°„ ì¸¡ì •
- **1 Epoch**: ~7ì´ˆ
- **Per batch**: ~1.75ì´ˆ
- **Effective batch**: 8 (batch_size=2 Ã— accumulate=4)

### ë©”ëª¨ë¦¬ ì‚¬ìš©
- **Model size**: 6.5GB (ì²´í¬í¬ì¸íŠ¸)
- **Precision**: FP32
- **GPU**: NVIDIA RTX A5000

---

## âš ï¸ ê³ ë ¤ì‚¬í•­

### 1. Loss ê³„ì‚° ê²€ì¦ í•„ìš”
- [ ] Forward passì—ì„œ ì‹¤ì œ loss ê°’ ì¶œë ¥
- [ ] loss_arm_actê°€ Noneì´ ì•„ë‹Œì§€ í™•ì¸
- [ ] Prediction output êµ¬ì¡° í™•ì¸

### 2. Action ì°¨ì› í˜¸í™˜ì„±
- [ ] 2D actionì´ RoboVLMsì™€ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦
- [ ] Action headì˜ ì¶œë ¥ ì°¨ì› í™•ì¸
- [ ] Loss ê³„ì‚° ì‹œ action slicing í™•ì¸

### 3. ë°ì´í„° ê²€ì¦
- [ ] ì‹¤ì œ ì•¡ì…˜ ê°’ì´ valid rangeì¸ì§€ í™•ì¸
- [ ] ì•¡ì…˜ ê°’ì´ ëª¨ë‘ 0ì´ ì•„ë‹Œì§€ í™•ì¸
- [ ] Normalizationì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸

### 4. í•™ìŠµ ì•ˆì •ì„±
- [ ] Gradient clipping ë™ì‘ í™•ì¸
- [ ] Learning rate scheduler ë™ì‘ í™•ì¸
- [ ] Optimizer state í™•ì¸

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì¡°ì¹˜
1. **Loss ë””ë²„ê¹…**
   - Forward passì—ì„œ loss ê°’ print
   - Prediction output êµ¬ì¡° í™•ì¸
   - Action targetê³¼ prediction ë¹„êµ

2. **ë°ì´í„° ê²€ì¦**
   - ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
   - ì•¡ì…˜ ê°’ ë²”ìœ„ í™•ì¸
   - Mask ê°’ í™•ì¸

3. **ëª¨ë¸ ê²€ì¦**
   - Action head ì¶œë ¥ í™•ì¸
   - LoRA íŒŒë¼ë¯¸í„° gradient í™•ì¸

### ì¤‘ê¸° ê°œì„ 
1. **í•™ìŠµ ëª¨ë‹ˆí„°ë§ ê°•í™”**
   - TensorBoard í™œìš©
   - ë” ìƒì„¸í•œ ë¡œê¹…
   - Gradient í†µê³„ ì¶”ì 

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - Learning rate ì¡°ì •
   - Batch size ìµœì í™”
   - LoRA rank ì‹¤í—˜

3. **ë°ì´í„° ì¦ê°•**
   - ë” ë§ì€ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
   - Data augmentation ì ìš©

---

## ğŸ“ í•´ê²° ìš°ì„ ìˆœìœ„

1. **High Priority** ğŸ”´
   - Loss = 0 ë¬¸ì œ í•´ê²°
   - ì‹¤ì œ loss ê°’ í™•ì¸

2. **Medium Priority** ğŸŸ¡
   - ë” ê¸´ í•™ìŠµ ì‹¤í–‰ (50 epochs)
   - ì¶”ë¡  í…ŒìŠ¤íŠ¸

3. **Low Priority** ğŸŸ¢
   - Mixed precision ì¬ì‹œë„
   - ë©”ëª¨ë¦¬ ìµœì í™”

---

## ğŸ’¾ ì €ì¥ëœ íŒŒì¼

**ì²´í¬í¬ì¸íŠ¸:**
```
/home/billy/25-1kp/vla/RoboVLMs_upstream/runs/mobile_vla_lora_20251106/
â””â”€â”€ kosmos/mobile_vla_finetune/2025-11-12/mobile_vla_lora_20251106/
    â””â”€â”€ epoch=0-step=1.ckpt (6.5GB)
```

**ë¡œê·¸:**
```
/home/billy/25-1kp/vla/
â”œâ”€â”€ lora_training_20251112_final.log
â”œâ”€â”€ lora_1epoch_FINAL_RUN.log
â”œâ”€â”€ lora_1epoch_SUCCESS.log
â””â”€â”€ LORA_TRAINING_STATUS.md
```

---

---

## âœ… ìµœì¢… í•´ê²° ë° í•™ìŠµ ì„±ê³µ! (2025-11-12 12:18)

### í•µì‹¬ ë¬¸ì œ í•´ê²°

**ë¬¸ì œ 1: data_sourceì— 'action' ë¬¸ìì—´ ëˆ„ë½**
- `data_source='mobile_vla_h5'` â†’ `data_source='mobile_vla_action'`ìœ¼ë¡œ ë³€ê²½
- ì´ë¡œ ì¸í•´ `forward_action()` ë©”ì„œë“œê°€ í˜¸ì¶œë˜ì§€ ì•Šì•„ lossê°€ 0ì´ì—ˆìŒ

**ë¬¸ì œ 2: Action shape ë¶ˆì¼ì¹˜**
- ë°ì´í„°ì…‹ì´ `(chunk_size, 7)` ë°˜í™˜ â†’ `(window_size, chunk_size, 7)`ë¡œ ìˆ˜ì •
- ê° window frameë§ˆë‹¤ future action chunk ì œê³µí•˜ë„ë¡ ë³€ê²½

**ë¬¸ì œ 3: CUDA OOM**
- `window_size=8` â†’ `4`ë¡œ ì¶•ì†Œ
- `batch_size=2` â†’ `1`ë¡œ ì¶•ì†Œ  
- `accumulate_grad_batches=4` â†’ `8`ë¡œ ì¦ê°€
- `precision="32"` â†’ `"16-mixed"` (FP16 mixed precision)

### ìµœì¢… í•™ìŠµ ê²°ê³¼ (3 Epochs)

| Epoch | Train Loss (Mobile 2D) | Val Loss (Mobile 2D) | ê°œì„ ìœ¨ |
|-------|----------------------|---------------------|--------|
| 0     | 0.126                | 0.122               | -      |
| 1     | 0.114 (-9.5%)        | 0.107 (-12.3%)      | âœ…     |
| 2     | 0.083 (-27.2%)       | 0.075 (-29.9%)      | âœ…âœ…   |

**Train Loss:** 0.126 â†’ 0.083 (34% ê°ì†Œ) ğŸ‰  
**Val Loss:** 0.122 â†’ 0.075 (38% ê°ì†Œ) ğŸ‰

### ì¤‘ìš”: Loss ì´ë¦„ í•´ì„

**ì‚¬ìš©ì í˜¼ë€ í¬ì¸íŠ¸:**
```
train_loss_arm_act=0.083      # âœ… ì´ê²ƒì´ Mobile Robot 2D ì†ë„ [linear_x, linear_y]!
train_loss_gripper_act=0.697  # âŒ ë”ë¯¸ ê°’ (action[6]=0, í•­ìƒ ê³ ì •)
```

**ì‹¤ì œ ë°ì´í„° êµ¬ì¡°:**
```python
action[0] = linear_x   # Mobile robot X ë°©í–¥ ì†ë„
action[1] = linear_y   # Mobile robot Y ë°©í–¥ ì†ë„
action[2:6] = 0        # íŒ¨ë”© (ë¡œë´‡ íŒ” ì—†ìŒ)
action[6] = 0          # ê·¸ë¦¬í¼ (ì—†ìŒ, ë”ë¯¸)
```

**RoboVLMsëŠ” ì›ë˜ ë¡œë´‡ íŒ” + ê·¸ë¦¬í¼ìš©ìœ¼ë¡œ ì„¤ê³„**ë˜ì–´ì„œ:
- "arm_act" = ì²˜ìŒ 6ì°¨ì› (ìš°ë¦¬ëŠ” ì²« 2ê°œë§Œ ì˜ë¯¸ ìˆìŒ)
- "gripper_act" = 7ë²ˆì§¸ ì°¨ì› (ìš°ë¦¬ëŠ” í•­ìƒ 0)

**ë”°ë¼ì„œ `loss_arm_act`ê°€ ìš°ë¦¬ì˜ ì‹¤ì œ Mobile Robot ì œì–´ lossì…ë‹ˆë‹¤!**

### í•™ìŠµ ì„¤ì • (ìµœì¢…)

```json
{
  "window_size": 4,
  "action_chunk_size": 10,
  "batch_size": 1,
  "accumulate_grad_batches": 8,
  "precision": "16-mixed",
  "max_epochs": 3,
  
  "action_dim": 7,  // [linear_x, linear_y, 0, 0, 0, 0, gripper_dummy]
  "lora_r": 32,
  "lora_alpha": 16,
  "lora_dropout": 0.1,
  
  "learning_rate": 1e-4,
  "gradient_clip_val": 1.0
}
```

### ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸

```
/home/billy/25-1kp/vla/RoboVLMs_upstream/runs/mobile_vla_lora_20251106/kosmos/mobile_vla_finetune/2025-11-12/mobile_vla_lora_20251106/
â”œâ”€â”€ epoch=0-step=6.ckpt   (6.9GB)
â”œâ”€â”€ epoch=1-step=12.ckpt  (6.9GB)
â””â”€â”€ epoch=2-step=18.ckpt  (6.9GB) â­ ìµœê³  ì„±ëŠ¥
```

### í•™ìŠµ ì‹œê°„

- **1 Epoch**: ~19ì´ˆ (45 steps)
- **3 Epochs**: ~60ì´ˆ (ì´ 135 steps)
- **Per step**: ~0.4ì´ˆ

### ë‹¤ìŒ ë‹¨ê³„

1. **ì¶”ë¡  í…ŒìŠ¤íŠ¸** - í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ action ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
2. **50 Epoch ë³¸ê²© í•™ìŠµ** - ë” ê¸´ í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
3. **Jetson ë°°í¬** - ì‹¤ì‹œê°„ ì¶”ë¡  í…ŒìŠ¤íŠ¸
4. **ì„±ëŠ¥ í‰ê°€** - MAE, MSE ë“± ë©”íŠ¸ë¦­ ì¸¡ì •

---

**ì‘ì„±ì¼**: 2025-11-12 12:18 (ìµœì¢… ì—…ë°ì´íŠ¸)  
**ì‘ì„±ì**: Mobile VLA Team  
**ìƒíƒœ**: âœ… í•™ìŠµ ì„±ê³µ! Loss ì •ìƒ ê°ì†Œ ì¤‘

