# Mobile-VLA ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ¯ ìµœì¢… ì„ íƒ: Habitat-AI

### ì„ ì • ì´ìœ 

| ê¸°ì¤€ | Habitat-AI | Isaac Sim | PyBullet |
|------|-----------|-----------|----------|
| **ì„¤ì¹˜ ë‚œì´ë„** | â­â­ ë³´í†µ | â­â­â­â­ ë†’ìŒ | â­ ì‰¬ì›€ |
| **ë‚´ë¹„ê²Œì´ì…˜ ìµœì í™”** | âœ… **ë§¤ìš° ìš°ìˆ˜** | â­â­ ë³´í†µ | â­ ì œí•œì  |
| **ì‹¤ë‚´ í™˜ê²½ ë°ì´í„°** | âœ… **í’ë¶€** (Matterport3D) | ìì²´ ì œì‘ í•„ìš” | ìì²´ ì œì‘ í•„ìš” |
| **ë Œë”ë§ í’ˆì§ˆ** | â­â­â­ ìš°ìˆ˜ | â­â­â­â­ ìµœê³  | â­ ë‚®ìŒ |
| **Python API** | âœ… **ì§ê´€ì ** | ë³µì¡ | ì§ê´€ì  |
| **GPU ë³‘ë ¬í™”** | â­â­ ì§€ì› | â­â­â­â­ ìµœê³  | â­ ì œí•œì  |
| **ì»¤ë®¤ë‹ˆí‹°/ë¬¸ì„œ** | âœ… **í™œë°œ** | ì¤‘ê°„ | í™œë°œ |
| **Mobile Robot ì‚¬ë¡€** | âœ… **ë§ìŒ** | ìˆìŒ | ìˆìŒ |

**ê²°ë¡ **: Mobile-VLAëŠ” **ì‹¤ë‚´ ë‚´ë¹„ê²Œì´ì…˜**ì— íŠ¹í™”ëœ í”„ë¡œì íŠ¸ì´ë¯€ë¡œ, ì‹¤ì œ ê±´ë¬¼ ìŠ¤ìº” ë°ì´í„°ë¥¼ í™œìš©í•  ìˆ˜ ìˆê³  ë‚´ë¹„ê²Œì´ì…˜ íƒœìŠ¤í¬ì— ìµœì í™”ëœ **Habitat-AI**ê°€ ìµœì ì˜ ì„ íƒì…ë‹ˆë‹¤.

---

## ğŸ“¦ ì„¤ì¹˜ ê°€ì´ë“œ

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```yaml
OS: Ubuntu 20.04 / 22.04 (ê¶Œì¥), macOS (ì œí•œì  ì§€ì›)
Python: 3.9 - 3.10
GPU: NVIDIA GPU (CUDA 11.0+) - ì„ íƒì‚¬í•­ì´ì§€ë§Œ ê°•ë ¥ ê¶Œì¥
RAM: 16GB ì´ìƒ
Storage: 50GB ì´ìƒ (ì”¬ ë°ì´í„° í¬í•¨)
```

### 2. Conda í™˜ê²½ ìƒì„±

```bash
# Conda í™˜ê²½ ìƒì„±
conda create -n habitat python=3.10 cmake=3.14.0 -y
conda activate habitat

# ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜
conda install habitat-sim headless -c conda-forge -c aihabitat -y
```

### 3. Habitat-Lab ì„¤ì¹˜

```bash
# Habitat-Lab í´ë¡  ë° ì„¤ì¹˜
git clone --branch stable https://github.com/facebookresearch/habitat-lab.git
cd habitat-lab

# ê°œë°œ ëª¨ë“œë¡œ ì„¤ì¹˜
pip install -e habitat-lab
pip install -e habitat-baselines

# ì¶”ê°€ ì˜ì¡´ì„±
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install opencv-python matplotlib numpy scipy h5py pyyaml tqdm
```

### 4. ì”¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ

```bash
# Matterport3D ì”¬ ë‹¤ìš´ë¡œë“œ (ì•½ 15GB)
# í•™ìˆ  ëª©ì  ë¼ì´ì„¼ìŠ¤ í•„ìš”: https://niessner.github.io/Matterport/
python -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path data/

# Gibson ì”¬ (ëŒ€ì•ˆ)
python -m habitat_sim.utils.datasets_download --uids habitat_test_pointnav_dataset --data-path data/
```

### 5. ì„¤ì¹˜ ê²€ì¦

```bash
# Habitat-Sim í…ŒìŠ¤íŠ¸
python -c "import habitat_sim; print('Habitat-Sim version:', habitat_sim.__version__)"

# Habitat-Lab í…ŒìŠ¤íŠ¸
python -c "import habitat; print('Habitat-Lab version:', habitat.__version__)"

# ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
python examples/tutorials/habitat_lab_demo.py
```

---

## ğŸ› ï¸ Mobile-VLA í†µí•© ì„¤ì •

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì‹œë®¬ë ˆì´ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
cd /path/to/vla
mkdir -p simulation/{environments,configs,data_generator,utils}
```

**êµ¬ì¡°**:
```
simulation/
â”œâ”€â”€ environments/          # í™˜ê²½ ì •ì˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ office_nav_env.py # ì‚¬ë¬´ì‹¤ ë‚´ë¹„ê²Œì´ì…˜ í™˜ê²½
â”‚   â””â”€â”€ hallway_nav_env.py # ë³µë„ ë‚´ë¹„ê²Œì´ì…˜ í™˜ê²½
â”œâ”€â”€ configs/               # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ default.yaml      # ê¸°ë³¸ ì„¤ì •
â”‚   â””â”€â”€ domain_randomization.yaml
â”œâ”€â”€ data_generator/        # ë°ì´í„° ìƒì„±ê¸°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trajectory_generator.py
â”‚   â””â”€â”€ h5_exporter.py    # Mobile-VLA í˜•ì‹ ë³€í™˜
â”œâ”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ visualization.py  # ê¶¤ì  ì‹œê°í™”
â””â”€â”€ README.md
```

### í™˜ê²½ ì„¤ì • íŒŒì¼ ìƒì„±

**`simulation/configs/default.yaml`**:
```yaml
# Habitat-AI ê¸°ë³¸ ì„¤ì •
habitat:
  simulator:
    type: "Sim-v0"
    action_space_config: "v0"
    forward_step_size: 0.25  # ì´ë™ í¬ê¸° (ë¯¸í„°)
    turn_angle: 10           # íšŒì „ ê°ë„ (ë„)
    
  task:
    type: PointNav-v0
    sensors:
      - type: RGBSensor
        height: 224
        width: 224
        position: [0, 1.25, 0]  # ì¹´ë©”ë¼ ë†’ì´
        orientation: [0, 0, 0]
    
    measurements:
      - type: TopDownMap
      - type: DistanceToGoal
      - type: Success
      - type: SPL  # Success weighted by Path Length
    
    goal_sensor_uuid: pointgoal_with_gps_compass
    
# Mobile-VLA íŠ¹í™” ì„¤ì •
mobile_vla:
  robot:
    base_height: 0.5        # ë¡œë´‡ ë² ì´ìŠ¤ ë†’ì´ (m)
    max_linear_velocity: 1.0 # ìµœëŒ€ ì„ ì†ë„ (m/s)
    max_angular_velocity: 1.5 # ìµœëŒ€ ê°ì†ë„ (rad/s)
    
  data_collection:
    episodes_per_scene: 50   # ì”¬ë‹¹ ì—í”¼ì†Œë“œ ìˆ˜
    max_steps: 500          # ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í…
    success_distance: 0.2   # ì„±ê³µ ê±°ë¦¬ (m)
    
  domain_randomization:
    enabled: true
    lighting_range: [0.5, 1.5]
    camera_tilt_range: [-5, 5]  # ë„
    
# ì €ì¥ ê²½ë¡œ
output:
  dataset_dir: "./data/synthetic_episodes"
  format: "h5"
  compress: true
```

---

## ğŸš€ ì²« ë²ˆì§¸ í™˜ê²½ êµ¬í˜„

### `simulation/environments/office_nav_env.py`

```python
"""
Mobile-VLAë¥¼ ìœ„í•œ Habitat-AI ì‚¬ë¬´ì‹¤ ë‚´ë¹„ê²Œì´ì…˜ í™˜ê²½
"""
import habitat
from habitat.config.default import get_config
from habitat.core.env import Env
import numpy as np
import quaternion
from typing import Dict, Optional, Tuple

class OfficeNavigationEnv:
    """
    ì‚¬ë¬´ì‹¤ í™˜ê²½ ë‚´ë¹„ê²Œì´ì…˜ ì‹œë®¬ë ˆì´í„°
    Mobile-VLA ë°ì´í„° í¬ë§· (2DOF: linear_x, angular_z) ìƒì„±
    """
    
    def __init__(self, config_path: str = "simulation/configs/default.yaml"):
        # Habitat ì„¤ì • ë¡œë“œ
        self.config = get_config(config_path)
        self.env: Optional[Env] = None
        self.current_episode = 0
        
    def reset(self) -> Dict[str, np.ndarray]:
        """í™˜ê²½ ë¦¬ì…‹ ë° ì´ˆê¸° ê´€ì¸¡ ë°˜í™˜"""
        if self.env is None:
            self.env = habitat.Env(config=self.config)
        
        observations = self.env.reset()
        self.current_episode += 1
        
        return {
            'rgb': observations['rgb'],  # (224, 224, 3)
            'goal_position': self._get_goal_position(),
            'robot_position': self._get_robot_position()
        }
    
    def step(self, action: Dict[str, float]) -> Tuple[Dict, float, bool, Dict]:
        """
        Mobile-VLA ì•¡ì…˜ ì ìš© (2DOF)
        
        Args:
            action: {'linear_x': ì†ë„(m/s), 'angular_z': ê°ì†ë„(rad/s)}
        
        Returns:
            observation, reward, done, info
        """
        # 2DOF ì†ë„ë¥¼ Habitat ì´ì‚° ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
        habitat_action = self._velocity_to_habitat_action(
            action['linear_x'], 
            action['angular_z']
        )
        
        observations = self.env.step(habitat_action)
        
        # ë³´ìƒ ê³„ì‚°
        metrics = self.env.get_metrics()
        reward = self._compute_reward(metrics)
        
        done = self.env.episode_over
        
        info = {
            'distance_to_goal': metrics.get('distance_to_goal', -1),
            'success': metrics.get('success', 0),
            'spl': metrics.get('spl', 0)
        }
        
        return observations, reward, done, info
    
    def _velocity_to_habitat_action(self, linear: float, angular: float) -> int:
        """
        2DOF ì†ë„ ëª…ë ¹ì„ Habitat ì´ì‚° ì•¡ì…˜ìœ¼ë¡œ ë§¤í•‘
        
        Habitat Actions:
        0: STOP
        1: MOVE_FORWARD
        2: TURN_LEFT
        3: TURN_RIGHT
        """
        # ì„ê³„ê°’ ì„¤ì •
        lin_threshold = 0.1  # m/s
        ang_threshold = 0.2  # rad/s
        
        if abs(linear) < lin_threshold and abs(angular) < ang_threshold:
            return 0  # STOP
        
        if abs(angular) > abs(linear):
            # íšŒì „ ìš°ì„ 
            return 2 if angular > 0 else 3  # LEFT or RIGHT
        else:
            # ì „ì§„ ìš°ì„ 
            return 1 if linear > 0 else 0  # FORWARD or STOP
    
    def _get_goal_position(self) -> np.ndarray:
        """ëª©í‘œ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸° (x, y, z)"""
        if self.env and self.env.current_episode:
            goals = self.env.current_episode.goals
            if goals:
                return np.array(goals[0].position)
        return np.zeros(3)
    
    def _get_robot_position(self) -> np.ndarray:
        """ë¡œë´‡ í˜„ì¬ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸° (x, y, z)"""
        if self.env:
            agent_state = self.env.sim.get_agent_state()
            return agent_state.position
        return np.zeros(3)
    
    def _compute_reward(self, metrics: Dict) -> float:
        """
        ë³´ìƒ í•¨ìˆ˜ ì •ì˜
        - ëª©í‘œì— ê°€ê¹Œì›Œì§€ë©´ ì–‘ì˜ ë³´ìƒ
        - ì¶©ëŒ ì‹œ ìŒì˜ ë³´ìƒ
        - ëª©í‘œ ë„ë‹¬ ì‹œ í° ë³´ìƒ
        """
        reward = 0.0
        
        # ëª©í‘œê¹Œì§€ì˜ ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
        dist = metrics.get('distance_to_goal', 0)
        if hasattr(self, '_prev_distance'):
            reward += (self._prev_distance - dist) * 10.0  # ê°€ê¹Œì›Œì§€ë©´ ì–‘ìˆ˜
        self._prev_distance = dist
        
        # ì„±ê³µ ë³´ìƒ
        if metrics.get('success', 0) > 0:
            reward += 100.0
        
        # ì‹œê°„ í˜ë„í‹° (ë¹¨ë¦¬ ë„ë‹¬ ìœ ë„)
        reward -= 0.01
        
        return reward
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        if self.env:
            self.env.close()
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### `simulation/test_env.py`

```python
"""
Habitat-AI í™˜ê²½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import numpy as np
from environments.office_nav_env import OfficeNavigationEnv
import matplotlib.pyplot as plt

def test_basic_navigation():
    """ê¸°ë³¸ ë‚´ë¹„ê²Œì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Habitat-AI í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í™˜ê²½ ì´ˆê¸°í™”
    env = OfficeNavigationEnv()
    
    # ì—í”¼ì†Œë“œ ì‹¤í–‰
    num_episodes = 3
    
    for ep in range(num_episodes):
        print(f"\nğŸ“ Episode {ep + 1}/{num_episodes}")
        
        obs = env.reset()
        done = False
        step_count = 0
        total_reward = 0
        
        # ê°„ë‹¨í•œ ëœë¤ ì •ì±…
        while not done and step_count < 100:
            # ëœë¤ ì•¡ì…˜ ìƒì„±
            action = {
                'linear_x': np.random.uniform(-0.5, 1.0),
                'angular_z': np.random.uniform(-1.0, 1.0)
            }
            
            obs, reward, done, info = env.step(action)
            total_reward += reward
            step_count += 1
            
            if step_count % 10 == 0:
                print(f"  Step {step_count}: "
                      f"Dist={info['distance_to_goal']:.2f}m, "
                      f"Reward={reward:.2f}")
        
        print(f"âœ… Episode finished: "
              f"Steps={step_count}, "
              f"Total Reward={total_reward:.2f}, "
              f"Success={info['success']}")
    
    env.close()
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_basic_navigation()
```

---

## ğŸ“… ë‹¤ìŒ ë‹¨ê³„

### 1ì£¼ì°¨: í™˜ê²½ ê²€ì¦ (í˜„ì¬)
- [x] Habitat-AI ì„¤ì¹˜
- [x] ê¸°ë³¸ í™˜ê²½ í´ë˜ìŠ¤ êµ¬í˜„
- [ ] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ê²€ì¦

### 2ì£¼ì°¨: Domain Randomization
- [ ] ì¡°ëª…/í…ìŠ¤ì²˜ ëœë¤í™” êµ¬í˜„
- [ ] ì¥ì• ë¬¼ ìë™ ë°°ì¹˜
- [ ] ë‹¤ì–‘í•œ ì”¬ ë¡œë“œ

### 3ì£¼ì°¨: ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸
- [ ] ìë™ ê¶¤ì  ìˆ˜ì§‘ê¸° êµ¬í˜„
- [ ] H5 í¬ë§· ë³€í™˜ê¸° êµ¬í˜„
- [ ] ì²« 100ê°œ ì—í”¼ì†Œë“œ ìƒì„±

### 4ì£¼ì°¨: ê²€ì¦ ë° í†µí•©
- [ ] ìƒì„± ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- [ ] Mobile-VLA í•™ìŠµ íŒŒì´í”„ë¼ì¸ í†µí•©
- [ ] ì„±ëŠ¥ ë¹„êµ (synthetic vs real)

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: CUDA ê´€ë ¨ ì˜¤ë¥˜
```bash
# GPU ì—†ì´ CPU ëª¨ë“œë¡œ ì‹¤í–‰
export HABITAT_SIM_LOG=quiet
export MAGNUM_LOG=quiet
habitat-viewer --no-display
```

### ë¬¸ì œ 2: ì”¬ ë°ì´í„° ì ‘ê·¼ ì˜¤ë¥˜
```bash
# ë°ì´í„° ê²½ë¡œ í™•ì¸
ls -la data/scene_datasets/
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export HABITAT_DATA_PATH=/path/to/habitat-lab/data
```

### ë¬¸ì œ 3: macOSì—ì„œ ì‹¤í–‰ ì‹œ ë Œë”ë§ ë¬¸ì œ
- macOSëŠ” headless ëª¨ë“œ ì§€ì›ì´ ì œí•œì 
- Linux í™˜ê²½ ë˜ëŠ” Google Colab ì‚¬ìš© ê¶Œì¥

---

**ì—…ë°ì´íŠ¸**: 2025-11-25  
**ë‹¤ìŒ ë¬¸ì„œ**: `DATA_GENERATION_PIPELINE.md` (ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„¸)
