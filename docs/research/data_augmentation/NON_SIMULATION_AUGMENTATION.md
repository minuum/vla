# Mobile-VLA ë¹„ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì¦ê°• ì „ëµ

> **ëª©í‘œ**: ì‹œë®¬ë ˆì´ì…˜ ì—†ì´ ì‹¤ì œ 500ê°œ ë°ì´í„°ë¥¼ 5,000ê°œ ì´ìƒìœ¼ë¡œ í™•ì¥

---

## ğŸ¯ ì „ëµ ê°œìš”

ì‹œë®¬ë ˆì´ì…˜ êµ¬ì¶•ì˜ ë†’ì€ ë¹„ìš©ê³¼ Sim-to-Real Gapì„ í”¼í•˜ë©´ì„œë„ ëŒ€ê·œëª¨ ë°ì´í„° ì¦ê°•ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´, ìµœì‹  VLA/VLM ì—°êµ¬ì—ì„œ ê²€ì¦ëœ **5ê°€ì§€ ë¹„ì‹œë®¬ë ˆì´ì…˜ ì¦ê°• ë°©ë²•**ì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

## ğŸ“š ë°©ë²• 1: CAST - Counterfactual Augmentation (VLM í™œìš©)

### ê°œìš”
**CAST (Counterfactual Augmentation with Synthetic Trajectories)**: ê¸°ì¡´ ê¶¤ì ì—ì„œ VLMì„ í™œìš©í•´ "ë§Œì•½ ~í–ˆë‹¤ë©´?" ì‹ì˜ ëŒ€ì•ˆ ì•¡ì…˜ê³¼ ëª…ë ¹ì–´ë¥¼ ìƒì„±[1]

### ì›ë¦¬
```python
# ê¸°ì¡´ ë°ì´í„°
original_trajectory = {
    'image': [img1, img2, img3, ...],
    'instruction': "ì‚¬ë¬´ì‹¤ë¡œ ì´ë™",
    'actions': [(v1, Ï‰1), (v2, Ï‰2), ...]
}

# VLMì— ì§ˆì˜
query = "At frame 10, what alternative actions could the robot take?"
vlm_response = "Turn left to avoid obstacle" or "Stop to wait for person"

# ìƒˆ ë°ì´í„° ìƒì„±
augmented_trajectory = {
    'image': [img1, ..., img10, ...],
    'instruction': "ì¥ì• ë¬¼ í”¼í•´ ì™¼ìª½ìœ¼ë¡œ ìš°íšŒ",
    'actions': [(v1, Ï‰1), ..., (0.0, +1.5), ...]  # ì¢ŒíšŒì „ ì•¡ì…˜
}
```

### Mobile-VLA ì ìš© ë°©ì•ˆ

#### Step 1: VLM ì„ íƒ
```python
# GPT-4V ë˜ëŠ” LLaVA í™œìš©
from openai import OpenAI

client = OpenAI()

def generate_counterfactual(image, original_action, timestep):
    prompt = f"""
    This is a mobile robot's view at timestep {timestep}.
    Original action: linear_vel={original_action[0]:.2f}, angular_vel={original_action[1]:.2f}
    
    Suggest 3 alternative valid actions the robot could take and describe why:
    1. (Action, Reason)
    2. (Action, Reason)
    3. (Action, Reason)
    """
    
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image}"}}
            ]
        }]
    )
    
    return parse_alternative_actions(response.choices[0].message.content)
```

#### Step 2: ìë™ ì¦ê°• íŒŒì´í”„ë¼ì¸
```python
def cast_augmentation(dataset, samples_per_trajectory=5):
    """
    ê° ê¶¤ì ì—ì„œ ë¬´ì‘ìœ„ íƒ€ì„ìŠ¤í…ì„ ìƒ˜í”Œë§í•˜ì—¬ ëŒ€ì•ˆ ì•¡ì…˜ ìƒì„±
    
    500 trajectories Ã— 5 alternatives = 2,500 new samples
    """
    augmented_data = []
    
    for traj in dataset:
        # ë¬´ì‘ìœ„ íƒ€ì„ìŠ¤í… ì„ íƒ
        keyframes = random.sample(range(len(traj)), k=samples_per_trajectory)
        
        for t in keyframes:
            # VLMìœ¼ë¡œ ëŒ€ì•ˆ ìƒì„±
            alternatives = generate_counterfactual(
                traj['images'][t],
                traj['actions'][t],
                t
            )
            
            for alt_action, alt_instruction in alternatives:
                # ìƒˆ ê¶¤ì  í•©ì„±
                new_traj = traj.copy()
                new_traj['actions'][t:] = modify_trajectory(
                    traj['actions'][t:], 
                    alt_action
                )
                new_traj['instruction'] = alt_instruction
                augmented_data.append(new_traj)
    
    return augmented_data
```

### ì˜ˆìƒ ê²°ê³¼
- **ë°ì´í„° ìˆ˜**: 500 â†’ 500 + 2,500 = **3,000ê°œ**
- **ë‹¤ì–‘ì„±**: ì–¸ì–´ grounding í–¥ìƒ, ë‹¤ì–‘í•œ ì˜ë„ í•™ìŠµ
- **í’ˆì§ˆ**: VLMì´ ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ì•¡ì…˜ë§Œ ì œì•ˆ

---

## ğŸ“š ë°©ë²• 2: RESample - Bottleneck States Recovery

### ê°œìš”
**RESample (Recovery Exploration Sampling)**: ì„±ê³µ ê¶¤ì ì—ì„œ ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ ë†’ì€ "ë³‘ëª© ìƒíƒœ"ë¥¼ ì°¾ì•„ ë³µêµ¬ ì•¡ì…˜ì„ í•™ìŠµ[2]

### ì›ë¦¬
```
ì •ìƒ ê¶¤ì : â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â”€â”€â—â”€â”€â†’ Goal
               â†“ (bottleneck: ì¢ì€ í†µë¡œ)
ì‹¤íŒ¨ ë³µêµ¬:     â””â”€â”€â—â”€â”€â—â”€â”€â†’ Goal
                  (recovery action)
```

### Mobile-VLA ì ìš© ë°©ì•ˆ

#### Step 1: Bottleneck ê°ì§€
```python
def detect_bottlenecks(trajectory):
    """
    ë³‘ëª© ìƒíƒœ ê¸°ì¤€:
    1. ë†’ì€ ê°ì†ë„ ë³€í™” (ê¸‰íšŒì „)
    2. ì†ë„ ê¸‰ê° (ì¥ì• ë¬¼ ê·¼ì ‘)
    3. ë°˜ë³µì ì¸ ê°™ì€ íŒ¨í„´ (ë§‰íŒ ìƒíƒœ)
    """
    bottlenecks = []
    
    for t in range(len(trajectory) - 1):
        action_t = trajectory['actions'][t]
        action_t1 = trajectory['actions'][t+1]
        
        # ê¸‰íšŒì „ ê°ì§€
        angular_change = abs(action_t1[1] - action_t[1])
        if angular_change > 0.5:  # rad/s
            bottlenecks.append(('sharp_turn', t))
        
        # ê¸‰ì •ì§€ ê°ì§€
        velocity_drop = action_t[0] - action_t1[0]
        if velocity_drop > 0.3:  # m/s
            bottlenecks.append(('sudden_stop', t))
    
    return bottlenecks
```

#### Step 2: íƒìƒ‰ì  ë³µêµ¬ ì•¡ì…˜ ìƒì„±
```python
def generate_recovery_samples(trajectory, bottleneck_idx):
    """
    ë³‘ëª© ì§€ì ì—ì„œ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë³µêµ¬ ì•¡ì…˜ ìƒì„±
    """
    recovery_samples = []
    
    # ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤: ë³‘ëª©ì—ì„œ ì˜ëª»ëœ ì•¡ì…˜
    failed_actions = [
        (0.0, 0.0),     # ë©ˆì¶¤
        (0.5, +2.0),    # ê³¼ë„í•œ ì¢ŒíšŒì „
        (0.5, -2.0),    # ê³¼ë„í•œ ìš°íšŒì „
    ]
    
    for failed_action in failed_actions:
        # ì‹¤íŒ¨ ê¶¤ì  ìƒì„±
        failed_traj = trajectory.copy()
        failed_traj['actions'][bottleneck_idx] = failed_action
        
        # ë³µêµ¬ ì•¡ì…˜: ì›ë˜ ëª©í‘œë¡œ ëŒì•„ê°€ê¸°
        recovery_actions = compute_recovery_path(
            failed_position=simulate_action(trajectory['images'][bottleneck_idx], failed_action),
            target_position=trajectory['positions'][bottleneck_idx + 5]
        )
        
        failed_traj['actions'][bottleneck_idx+1:bottleneck_idx+6] = recovery_actions
        failed_traj['label'] = 'recovery'
        
        recovery_samples.append(failed_traj)
    
    return recovery_samples
```

### ì˜ˆìƒ ê²°ê³¼
- **ë°ì´í„° ìˆ˜**: ë³‘ëª© ìƒíƒœ 100ê°œ Ã— 3 ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ = **300ê°œ**
- **íš¨ê³¼**: ì¥ì• ë¬¼ íšŒí”¼, ë³µêµ¬ ëŠ¥ë ¥ í–¥ìƒ
- **ê°•ê±´ì„±**: ì‹¤íŒ¨ ìƒíƒœì—ì„œ íšŒë³µí•˜ëŠ” ë²• í•™ìŠµ

---

## ğŸ“š ë°©ë²• 3: ControlNet + Stable Diffusion (ì´ë¯¸ì§€ ì¦ê°•)

### ê°œìš”
ì‹¤ì œ ì´ë¯¸ì§€ì˜ êµ¬ì¡°(depth, edge)ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë°°ê²½/ì¡°ëª…/ìŠ¤íƒ€ì¼ë§Œ ë³€ê²½í•˜ì—¬ ì‹œê°ì  ë‹¤ì–‘ì„± í™•ë³´

### Mobile-VLA ì ìš© ë°©ì•ˆ

#### Step 1: ControlNet íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from transformers import pipeline as hf_pipeline
import torch

# ControlNet ëª¨ë¸ ë¡œë“œ
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth"
)

pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
).to("cuda")

# Depth Estimator
depth_estimator = hf_pipeline("depth-estimation", model="Intel/dpt-large")

def augment_image_with_controlnet(image, prompt):
    """
    ì´ë¯¸ì§€ì˜ ê¹Šì´ ë§µì„ ìœ ì§€í•˜ë©° ìŠ¤íƒ€ì¼ ë³€ê²½
    """
    # 1. Depth Map ì¶”ì¶œ
    depth_map = depth_estimator(image)['depth']
    
    # 2. ControlNetìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
    augmented = pipe(
        prompt=prompt,
        image=depth_map,
        num_inference_steps=20,
        controlnet_conditioning_scale=0.8
    ).images[0]
    
    return augmented
```

#### Step 2: ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
```python
augmentation_prompts = [
    # ì¡°ëª… ë³€í™”
    "bright office hallway, fluorescent lighting, professional",
    "dim corridor, evening light, warm atmosphere",
    "natural daylight from windows, sunny day",
    
    # í™˜ê²½ ë³€í™”
    "modern glass building interior, reflective surfaces",
    "industrial warehouse, concrete floors",
    "hospital corridor, clean white walls",
    
    # ë‚ ì”¨/ì‹œê°„
    "rainy day, wet floors, gloomy lighting",
    "night scene, artificial lighting, dark shadows",
    
    # ë³µì¡ë„ ë³€í™”
    "crowded hallway with people walking",
    "empty corridor, minimal furniture",
    "cluttered office space, many objects"
]

def batch_augment_dataset(dataset, prompts):
    """
    500 images Ã— 10 prompts = 5,000 augmented images
    """
    augmented = []
    
    for img_data in tqdm(dataset):
        original_image = img_data['image']
        
        for prompt in prompts:
            aug_img = augment_image_with_controlnet(original_image, prompt)
            
            # ì•¡ì…˜ ë ˆì´ë¸”ì€ ë™ì¼ (depth ìœ ì§€í–ˆìœ¼ë¯€ë¡œ ë¬¼ë¦¬ì ìœ¼ë¡œ ìœ íš¨)
            augmented.append({
                'image': aug_img,
                'action': img_data['action'],
                'instruction': img_data['instruction'],
                'augmentation_type': 'controlnet',
                'prompt': prompt
            })
    
    return augmented
```

### ì˜ˆìƒ ê²°ê³¼
- **ë°ì´í„° ìˆ˜**: 500 Ã— 10 = **5,000ê°œ**
- **ì‹œê° ë‹¤ì–‘ì„±**: ë‹¤ì–‘í•œ ì¡°ëª…/í™˜ê²½ì—ì„œ ê°•ê±´ì„± í™•ë³´
- **ë¬¼ë¦¬ì  ìœ íš¨ì„±**: Depth map ìœ ì§€ë¡œ ì•¡ì…˜ ë ˆì´ë¸” ì •í™•ì„± ë³´ì¥

---

## ğŸ“š ë°©ë²• 4: Contrastive Learning (Self-Supervised)

### ê°œìš”
**CLASP (Contrastive Language-Action-State Pre-training)**: ì–¸ì–´ì™€ ë¡œë´‡ í–‰ë™ì„ shared embeddingì— ì •ë ¬í•˜ì—¬ ì ì€ ë°ì´í„°ë¡œ íš¨ìœ¨ì  í•™ìŠµ[3]

### Mobile-VLA ì ìš© ë°©ì•ˆ

#### Step 1: Contrastive Pre-training
```python
import torch.nn.functional as F

class ContrastiveMobileVLA(nn.Module):
    def __init__(self, vision_encoder, text_encoder, temperature=0.07):
        super().__init__()
        self.vision_encoder = vision_encoder
        self.text_encoder = text_encoder
        self.temperature = temperature
    
    def contrastive_loss(self, vision_features, text_features):
        """
        InfoNCE Loss: ê°™ì€ (image, instruction) ìŒì€ ê°€ê¹ê²Œ, ë‹¤ë¥¸ ìŒì€ ë©€ê²Œ
        """
        # Normalize
        vision_features = F.normalize(vision_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)
        
        # Cosine similarity
        logits = torch.matmul(vision_features, text_features.T) / self.temperature
        
        # Cross-entropy loss
        labels = torch.arange(len(vision_features)).to(logits.device)
        loss = F.cross_entropy(logits, labels)
        
        return loss
```

#### Step 2: Data Augmentation for Contrastive Learning
```python
def create_contrastive_pairs(dataset):
    """
    í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì—ì„œ ì—¬ëŸ¬ ê¸ì •/ë¶€ì • ìŒ ìƒì„±
    
    ê¸ì • ìŒ: (image, ì˜¬ë°”ë¥¸ instruction)
    ë¶€ì • ìŒ: (image, ë‹¤ë¥¸ instruction)
    
    â†’ 500 images â†’ 2,500 pairs (1 pos + 4 neg per image)
    """
    pairs = []
    
    for i, data in enumerate(dataset):
        image = data['image']
        true_instruction = data['instruction']
        
        # Positive pair
        pairs.append({
            'image': image,
            'instruction': true_instruction,
            'label': 1  # positive
        })
        
        # Negative pairs (random other instructions)
        negative_instructions = random.sample(
            [d['instruction'] for j, d in enumerate(dataset) if j != i],
            k=4
        )
        
        for neg_inst in negative_instructions:
            pairs.append({
                'image': image,
                'instruction': neg_inst,
                'label': 0  # negative
            })
    
    return pairs  # 500 Ã— 5 = 2,500 pairs
```

### ì˜ˆìƒ ê²°ê³¼
- **ë°ì´í„° íš¨ìœ¨ì„±**: ê°™ì€ ì´ë¯¸ì§€ì—ì„œ ì—¬ëŸ¬ í•™ìŠµ ìƒ˜í”Œ ìƒì„±
- **ì–¸ì–´ ì •ë ¬**: Vision-Language alignment í–¥ìƒ
- **Few-shot ì„±ëŠ¥**: ìƒˆë¡œìš´ ëª…ë ¹ì–´ì— ë¹ ë¥´ê²Œ ì ì‘

---

## ğŸ“š ë°©ë²• 5: Trajectory Interpolation (ê¶¤ì  ë³´ê°„)

### ê°œìš”
ë‘ ì„±ê³µ ê¶¤ì  ì‚¬ì´ë¥¼ ë¶€ë“œëŸ½ê²Œ ë³´ê°„í•˜ì—¬ ìƒˆë¡œìš´ ìœ íš¨ ê¶¤ì  ìƒì„±

### Mobile-VLA ì ìš© ë°©ì•ˆ

#### Step 1: ê¶¤ì  ì„ë² ë”©
```python
def embed_trajectory(trajectory, encoder):
    """
    ê¶¤ì ì„ latent spaceë¡œ ì¸ì½”ë”©
    """
    image_features = encoder(trajectory['images'])
    action_features = encode_actions(trajectory['actions'])
    
    # Trajectory embedding (í‰ê·  pooling)
    traj_embedding = torch.cat([image_features, action_features], dim=-1).mean(dim=0)
    
    return traj_embedding
```

#### Step 2: ë³´ê°„ ë° ë””ì½”ë”©
```python
def interpolate_trajectories(traj_A, traj_B, num_samples=5):
    """
    Aì™€ B ì‚¬ì´ë¥¼ ì„ í˜• ë³´ê°„
    
    2ê°œ ê¶¤ì  â†’ 5ê°œ ìƒˆ ê¶¤ì 
    """
    embed_A = embed_trajectory(traj_A, encoder)
    embed_B = embed_trajectory(traj_B, encoder)
    
    interpolated_trajs = []
    
    for alpha in np.linspace(0.2, 0.8, num_samples):
        # Latent space ë³´ê°„
        embed_interp = alpha * embed_A + (1 - alpha) * embed_B
        
        # ë””ì½”ë”© (ìƒˆ ê¶¤ì  ìƒì„±)
        new_traj = decoder(embed_interp)
        
        # ë¬¼ë¦¬ì  íƒ€ë‹¹ì„± ê²€ì¦
        if is_physically_valid(new_traj):
            interpolated_trajs.append(new_traj)
    
    return interpolated_trajs
```

#### Step 3: Pair-wise ì¦ê°•
```python
# 500 trajectories â†’ choose 100 pairs
trajectory_pairs = random_pairs(dataset, n_pairs=100)

augmented_trajs = []
for traj_A, traj_B in trajectory_pairs:
    augmented_trajs.extend(
        interpolate_trajectories(traj_A, traj_B, num_samples=5)
    )

# 100 pairs Ã— 5 interpolations = 500 new trajectories
```

### ì˜ˆìƒ ê²°ê³¼
- **ë°ì´í„° ìˆ˜**: +500ê°œ
- **ë¶€ë“œëŸ¬ì›€**: ìì—°ìŠ¤ëŸ¬ìš´ ê¶¤ì  ìƒì„±
- **íš¨ìœ¨ì„±**: ì¸ì½”ë”/ë””ì½”ë”ë§Œ í•™ìŠµí•˜ë©´ ë¬´í•œ ìƒì„± ê°€ëŠ¥

---

## ğŸ“Š ì „ì²´ ì¦ê°• ê³„íš ìš”ì•½

| ë°©ë²• | ìƒì„± ë°ì´í„° ìˆ˜ | êµ¬ì¶• ì‹œê°„ | ë‹¤ì–‘ì„± | í’ˆì§ˆ |
|------|--------------|----------|-------|------|
| **CAST (VLM)** | +2,500 | 1ì£¼ | â­â­â­â­ | â­â­â­â­ |
| **RESample** | +300 | 3ì¼ | â­â­â­ | â­â­â­â­â­ |
| **ControlNet** | +5,000 | 1ì£¼ | â­â­â­â­â­ | â­â­â­â­ |
| **Contrastive** | +2,500 (pairs) | 3ì¼ | â­â­â­ | â­â­â­ |
| **Interpolation** | +500 | 1ì£¼ | â­â­ | â­â­â­â­ |
| **ì´í•©** | **+10,800** | **3-4ì£¼** | ë§¤ìš° ë†’ìŒ | ë†’ìŒ |

### ìµœì¢… ë°ì´í„°ì…‹
- **Original**: 500
- **Augmented**: 10,800
- **Total**: **11,300 samples**

---

## ğŸ¯ ê¶Œì¥ ì‹¤í–‰ ìš°ì„ ìˆœìœ„

### Phase 1 (Week 1): ë¹ ë¥¸ ì¦ê°•
1. âœ… **ControlNet** (êµ¬í˜„ ê°„ë‹¨ + ëŒ€ëŸ‰ ìƒì„±)
   - 500 â†’ 5,500 (10ë°°)
   
### Phase 2 (Week 2): ê³ í’ˆì§ˆ ì¦ê°•
2. âœ… **CAST (VLM)** (ì˜ë¯¸ì  ë‹¤ì–‘ì„±)
   - +2,500 (ì–¸ì–´ grounding)
   
### Phase 3 (Week 3): ê°•ê±´ì„± ì¦ê°•
3. âœ… **RESample** (ë³µêµ¬ ëŠ¥ë ¥)
   - +300 (ì‹¤íŒ¨ ë³µêµ¬ í•™ìŠµ)

### Phase 4 (Optional): ì¶”ê°€ ì¦ê°•
4. â­ **Contrastive Learning** (íš¨ìœ¨ì„±)
5. â­ **Trajectory Interpolation** (ë¶€ë“œëŸ¬ì›€)

---

## ğŸ’» í†µí•© ì½”ë“œ ì˜ˆì‹œ

```python
# augmentation_pipeline.py

class MobileVLAAugmentationPipeline:
    def __init__(self, dataset_path):
        self.dataset = load_h5_dataset(dataset_path)
        
        # ê° ì¦ê°• ì—”ì§„ ì´ˆê¸°í™”
        self.controlnet_engine = ControlNetAugmenter()
        self.cast_engine = CASTAugmenter(vlm_model="gpt-4v")
        self.resample_engine = RESampleAugmenter()
    
    def augment_all(self, output_path):
        """
        ì „ì²´ ì¦ê°• íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        print("ğŸš€ Starting augmentation pipeline...")
        
        # Phase 1: ControlNet
        print("\n[Phase 1] ControlNet Augmentation...")
        controlnet_data = self.controlnet_engine.augment(
            self.dataset, 
            prompts=AUGMENTATION_PROMPTS
        )
        print(f"âœ… Generated {len(controlnet_data)} samples")
        
        # Phase 2: CAST
        print("\n[Phase 2] CAST Augmentation...")
        cast_data = self.cast_engine.augment(
            self.dataset,
            samples_per_traj=5
        )
        print(f"âœ… Generated {len(cast_data)} samples")
        
        # Phase 3: RESample
        print("\n[Phase 3] RESample Augmentation...")
        resample_data = self.resample_engine.augment(self.dataset)
        print(f"âœ… Generated {len(resample_data)} samples")
        
        # í†µí•©
        final_dataset = (
            self.dataset + 
            controlnet_data + 
            cast_data + 
            resample_data
        )
        
        # ì €ì¥
        save_h5_dataset(final_dataset, output_path)
        print(f"\nâœ… Final dataset: {len(final_dataset)} samples")
        print(f"   Saved to: {output_path}")

# ì‹¤í–‰
if __name__ == "__main__":
    pipeline = MobileVLAAugmentationPipeline("data/mobile_vla_500.h5")
    pipeline.augment_all("data/mobile_vla_augmented_10k.h5")
```

---

## ğŸ“ˆ ê²€ì¦ ê³„íš

### Ablation Study
| ì‹¤í—˜ ì¡°ê±´ | ë°ì´í„° ìˆ˜ | Val Loss (ì˜ˆìƒ) | ìƒˆ í™˜ê²½ ì„±ê³µë¥  |
|----------|----------|----------------|--------------|
| Baseline | 500 | 0.213 | Baseline |
| +ControlNet | 5,500 | < 0.20 | +10% |
| +CAST | 8,000 | < 0.18 | +15% |
| +RESample | 8,300 | < 0.17 | +20% |
| All Methods | 11,300 | < 0.15 | +25% |

---

## ğŸ”¬ ì°¸ê³  ë…¼ë¬¸

1. **CAST**: "Counterfactual Augmentation with Synthetic Trajectories for VLA"
2. **RESample**: "Recovery Exploration for Out-of-Distribution Data in VLA"
3. **CLASP**: "Contrastive Language-Action-State Pre-training"
4. **ControlNet**: "Adding Conditional Control to Text-to-Image Diffusion Models"
5. **AugWM**: "Augmented World Models for Self-Supervised Adaptation"

---

**ì‘ì„±ì¼**: 2025-11-26  
**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 (ControlNet) êµ¬í˜„ ì‹œì‘
