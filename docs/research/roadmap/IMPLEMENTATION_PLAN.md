# Mobile-VLA Implementation Plan

> **ëª©í‘œ**: RoboVLMsë¥¼ Mobile Robotì— ì ì‘ì‹œì¼œ ì‹¤ìš©ì ì¸ VLA ì‹œìŠ¤í…œ êµ¬ì¶•  
> **í•µì‹¬ ì§ˆë¬¸**: 7DOF Manipulatorìš© VLMì´ 2DOF Mobile Robotì— ì „ì´ ê°€ëŠ¥í•œê°€?

---

## ğŸ¯ í•µì‹¬ ê²€ì¦ ì‚¬í•­

### 1. Context Vector ì˜ë¯¸ì„± ê²€ì¦ (ìµœìš°ì„ )

**ì§ˆë¬¸**: RoboVLMsê°€ mobile robot ì´ë¯¸ì§€ì—ì„œ ìœ ì˜ë¯¸í•œ contextë¥¼ ì¶”ì¶œí•˜ëŠ”ê°€?

**ì ‘ê·¼ ë°©ë²•**:
```python
# 1. Pre-trained RoboVLMs ë¡œë“œ
model = RoboPaligemma.from_pretrained("...")

# 2. Mobile-VLA ì´ë¯¸ì§€ ì…ë ¥
mobile_images = load_samples(n=50)  # ëŒ€í‘œ ìƒ˜í”Œ

# 3. Intermediate activation ì¶”ì¶œ
with model.extract_features() as extractor:
    context_vectors = extractor.forward(mobile_images)

# 4. ë¶„ì„
- t-SNE ì‹œê°í™” (Manipulator vs Mobile)
- Cosine similarity ê³„ì‚°
- Activation magnitude ë¹„êµ
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… Context vectorê°€ 0ì´ ì•„ë‹˜
- âœ… í´ëŸ¬ìŠ¤í„°ë§ì´ ì˜ë¯¸ìˆê²Œ ë‚˜ë‰¨ (Left vs Right)
- âœ… Manipulator ë°ì´í„°ì™€ ì™„ì „íˆ ë‹¤ë¥´ì§€ ì•ŠìŒ

**ì‹¤íŒ¨ ì‹œ ëŒ€ì‘**:
- Mobile robot ì´ë¯¸ì§€ë¡œ Vision encoder ì¶”ê°€ pre-training
- ë‹¤ë¥¸ VLM backbone ì‹œë„ (Flamingo, BLIP ë“±)

---

### 2. 7DOF â†’ 2DOF ì ì‘ ê°€ëŠ¥ì„±

**ì§ˆë¬¸**: Action headë§Œ êµì²´í•´ì„œ ì ì€ ë°ì´í„°ë¡œ í•™ìŠµ ê°€ëŠ¥í•œê°€?

**ì‹¤í—˜ ì„¤ê³„**:

| ì‹¤í—˜ | Action Head | í•™ìŠµ ë°ì´í„° | ì˜ˆìƒ ê²°ê³¼ |
|------|------------|----------|----------|
| **Exp 1** | Frozen VLM + New 2DOF head | 50ê°œ | Baseline |
| **Exp 2** | Frozen VLM + Adapter + 2DOF | 50ê°œ | ê°œì„ ? |
| **Exp 3** | LoRA VLM + 2DOF head | 50ê°œ | ìµœì„ ? |
| **Exp 4** | Exp 3 + 468ê°œ ì „ì²´ | 468ê°œ | Upper bound |

**êµ¬í˜„**:
```python
class Mobile2DOFHead(nn.Module):
    def __init__(self, context_dim=2048, action_dim=2):
        self.adapter = nn.Linear(context_dim, 512)  # Adapter
        self.action_proj = nn.Linear(512, action_dim * chunk_size)
    
    def forward(self, context_vector):
        # context_vector: (B, 2048) from RoboVLMs
        adapted = self.adapter(context_vector)  # (B, 512)
        actions = self.action_proj(adapted)     # (B, 20)  # 2*10
        return actions.reshape(B, 10, 2)
```

**ì„±ê³µ ê¸°ì¤€**:
- âœ… 50ê°œë¡œ ìˆ˜ë ´ ê°€ëŠ¥ (Loss < 0.5)
- âœ… 468ê°œë¡œ Val Loss < 0.2

---

## ğŸ“‹ Phaseë³„ ìƒì„¸ ê³„íš

### Phase 1: RoboVLMs ê²€ì¦ (Week 1-2)

#### 1.1 í™˜ê²½ êµ¬ì¶•
```bash
# RoboVLMs ì„¤ì¹˜
cd RoboVLMs
pip install -e .

# Pre-trained ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
python scripts/download_pretrained.py --model paligemma
```

#### 1.2 Context Vector ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `scripts/research/extract_context_vectors.py`

```python
"""
RoboVLMs context vector ì¶”ì¶œ ë° ë¶„ì„
"""

def extract_context_vectors(model, images, hook_layer='vision_tower'):
    """
    Args:
        model: RoboPaligemma
        images: (N, 3, 224, 224)
        hook_layer: 'vision_tower' or 'multi_modal_projector'
    
    Returns:
        context_vectors: (N, 2048)
    """
    contexts = []
    
    def hook_fn(module, input, output):
        contexts.append(output.detach().cpu())
    
    # Register hook
    target_layer = getattr(model, hook_layer)
    handle = target_layer.register_forward_hook(hook_fn)
    
    # Forward pass
    with torch.no_grad():
        _ = model(images)
    
    handle.remove()
    return torch.cat(contexts, dim=0)

# ì‚¬ìš©
model = load_robopaligemma()
mobile_images = sample_mobile_vla_images(n=50)
contexts = extract_context_vectors(model, mobile_images)

# ë¶„ì„
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2)
embedded = tsne.fit_transform(contexts.numpy())

# ì‹œê°í™”
plt.scatter(embedded[:, 0], embedded[:, 1], 
            c=labels,  # Left=0, Right=1
            cmap='coolwarm')
plt.savefig('context_vector_tsne.png')
```

#### 1.3 ì‹¤í—˜ ì¼ì •

| ë‚ ì§œ | ì‘ì—… | ì‚°ì¶œë¬¼ |
|------|------|--------|
| Day 1-2 | í™˜ê²½ êµ¬ì¶•, ëª¨ë¸ ë¡œë“œ | README.md |
| Day 3-4 | Context vector ì¶”ì¶œ | NPY íŒŒì¼ 50ê°œ |
| Day 5-6 | t-SNE, í´ëŸ¬ìŠ¤í„°ë§ | PNG ì‹œê°í™” |
| Day 7 | ë³´ê³ ì„œ ì‘ì„± | CONTEXT_VECTOR_REPORT.md |

---

### Phase 2: ë°ì´í„° ì¦ê°• (Week 3-6)

#### 2.1 ControlNet ì¦ê°• (Week 3-4)

**ëª©í‘œ**: 468 â†’ 4,680 (Ã—10)

**ìŠ¤í¬ë¦½íŠ¸**: `scripts/augmentation/controlnet_augment.py`

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

# í”„ë¡¬í”„íŠ¸ ì •ì˜
PROMPTS = [
    "bright office hallway, professional lighting",
    "dim corridor, evening light",
    "rainy day, wet floor",
    "crowded hallway with people",
    "industrial warehouse, concrete",
    "hospital corridor, white walls",
    "night scene, artificial lighting",
    "natural daylight from windows",
    "modern glass building interior",
    "cluttered office space, many objects"
]

# ë°°ì¹˜ í”„ë¡œì„¸ì‹±
for h5_file in tqdm(h5_files):
    with h5py.File(h5_file, 'r') as f:
        images = f['images'][:]  # (18, 720, 1280, 3)
        actions = f['actions'][:]
        instruction = f['language_instruction'][0].decode('utf-8')
    
    for i, prompt in enumerate(PROMPTS):
        # Depth map ì¶”ì¶œ
        depth_maps = estimate_depth(images)
        
        # ControlNet ìƒì„±
        augmented_images = controlnet_generate(
            images=images,
            depth_maps=depth_maps,
            prompt=prompt
        )
        
        # ì €ì¥
        save_augmented_episode(
            f"{h5_file.stem}_aug{i:02d}.h5",
            augmented_images,
            actions,  # ë™ì¼í•œ ì•¡ì…˜
            instruction
        )
```

**ì¼ì •**:
- Week 3: ControlNet í™˜ê²½ êµ¬ì¶•, íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- Week 4: 468ê°œ ì¦ê°•, í’ˆì§ˆ ê²€ì¦

#### 2.2 CAST ì¦ê°• (Week 5)

**ëª©í‘œ**: í›„ì§„ ë™ì‘ ìƒì„± (+500)

```python
# GPT-4V í™œìš©
def generate_backward_scenario(image, original_action):
    prompt = f"""
    This mobile robot is moving forward.
    Describe 3 scenarios where it needs to move BACKWARD:
    1. (Scenario, Action)
    2. (Scenario, Action)
    3. (Scenario, Action)
    """
    
    response = gpt4v(image, prompt)
    
    # Parse response
    scenarios = parse_backward_scenarios(response)
    
    return scenarios

# í›„ì§„ ë°ì´í„° ìƒì„±
backward_episodes = []
for episode in forward_only_episodes:
    scenarios = generate_backward_scenario(
        episode['images'][0],
        episode['actions'][0]
    )
    
    for scenario in scenarios:
        # ìƒˆ ì—í”¼ì†Œë“œ ìƒì„±
        new_episode = create_backward_episode(
            base_episode=episode,
            backward_action=scenario['action'],
            instruction=scenario['instruction']
        )
        backward_episodes.append(new_episode)
```

---

### Phase 3: Mobile-VLA í•™ìŠµ (Week 7-10)

#### 3.1 ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ (Week 7)

**ë³€ê²½ì‚¬í•­**:
```yaml
# config/train_mobile_vla.yaml

data:
  train_episodes: 375  # ê¸°ì¡´ 175 â†’ 375
  val_episodes: 93     # ê¸°ì¡´ 44 â†’ 93
  dataset_path: "ROS_action/mobile_vla_dataset"

model:
  backbone: "RoboPaligemma"
  lora_enable: true
  lora_r: 32
  lora_alpha: 16
  action_dim: 2  # Linear_x, Angular_zë§Œ
  window_size: 8
  action_chunk: 10

training:
  max_epochs: 30
  batch_size: 16
  learning_rate: 1e-4
  early_stopping_patience: 5
```

**ì˜ˆìƒ ê²°ê³¼**:
- Train Loss: 0.134 â†’ 0.10
- Val Loss: 0.213 â†’ 0.17

#### 3.2 ì¦ê°• ë°ì´í„° í•™ìŠµ (Week 8-9)

| ì‹¤í—˜ | ë°ì´í„° | Val Loss (ì˜ˆì¸¡) |
|------|--------|----------------|
| Baseline | 375 | 0.17 |
| +ControlNet | 4,680 | 0.14 |
| +CAST | 5,180 | 0.12 |

---

### Phase 4: ì¶”ë¡  ì‹œìŠ¤í…œ (Week 11-12)

#### 4.1 ì‹¤ì‹œê°„ ì¶”ë¡  ë£¨í”„

```python
class RealtimeVLAController:
    def __init__(self, model, camera, robot):
        self.model = model
        self.camera = camera
        self.robot = robot
        
        self.inference_rate = 0.4  # 400ms
        self.control_rate = 0.02   # 20ms
        self.chunk_size = 10
    
    def run(self, instruction: str):
        while not goal_reached():
            # 1. ì¹´ë©”ë¼ ìº¡ì²˜
            image = self.camera.capture()
            
            # 2. VLM ì¶”ë¡  (400msë§ˆë‹¤)
            start = time.time()
            action_chunk = self.model.predict(
                image=image,
                instruction=instruction,
                chunk_size=self.chunk_size
            )  # (10, 2)
            inference_time = time.time() - start
            
            # 3. Action chunk ì‹¤í–‰ (20ms ê°„ê²©)
            for action in action_chunk:
                self.robot.set_velocity(
                    linear_x=action[0],
                    angular_z=action[1]
                )
                time.sleep(self.control_rate)
            
            # Wait for next inference cycle
            time.sleep(max(0, self.inference_rate - inference_time))
```

#### 4.2 ë²¤ì¹˜ë§ˆí¬

**ì¸¡ì • ì§€í‘œ**:
- **ì¶”ë¡  ì†ë„**: VLM forward pass ì‹œê°„
- **ì œì–´ ì •í™•ë„**: ëª©í‘œê¹Œì§€ ì˜¤ì°¨ (cm)
- **ì„±ê³µë¥ **: 10íšŒ ì‹œë„ ì¤‘ ì„±ê³µ íšŸìˆ˜

---

## âš ï¸ ì£¼ìš” ë¦¬ìŠ¤í¬ ë° ì™„í™” ë°©ì•ˆ

### ë¦¬ìŠ¤í¬ 1: Context Vector ë¬´ì˜ë¯¸

**ì§•í›„**: t-SNEì—ì„œ ë¬´ì‘ìœ„ ë¶„í¬, ëª¨ë‘ 0ì— ê°€ê¹Œì›€

**ì™„í™” ë°©ì•ˆ**:
1. Vision encoder ì¶”ê°€ pre-training (Mobile robot ì´ë¯¸ì§€ 1000ì¥)
2. Intermediate layer ì‹œë„ (ë” ì•ë‹¨ feature)
3. ë‹¤ë¥¸ VLM backbone (Flamingo, BLIP-2)

### ë¦¬ìŠ¤í¬ 2: 7DOF â†’ 2DOF ë¶ˆê°€ëŠ¥

**ì§•í›„**: 468ê°œë¡œë„ Val Loss > 0.5

**ì™„í™” ë°©ì•ˆ**:
1. ì‹œë®¬ë ˆì´ì…˜ ëŒ€ëŸ‰ ì¦ê°• (10,000ê°œ)
2. Pre-training on generic mobile navigation (ImageNet ë“±)
3. Curriculum learning (ê°„ë‹¨í•œ íƒœìŠ¤í¬ë¶€í„°)

### ë¦¬ìŠ¤í¬ 3: ì¶”ë¡  ì†ë„ ëŠë¦¼

**ì§•í›„**: Inference time > 400ms

**ì™„í™” ë°©ì•ˆ**:
1. TensorRT ìµœì í™”
2. INT8 quantization
3. ì‘ì€ backbone (PaliGemma-small)
4. Action chunk í¬ê¸° ì¦ê°€ (10 â†’ 20)

---

## ğŸ“… íƒ€ì„ë¼ì¸

```
Week 1-2:  âœ… Context Vector ì¶”ì¶œ ë° ë¶„ì„
Week 3-4:  ğŸ”„ ControlNet ì¦ê°• (468 â†’ 4,680)
Week 5:    ğŸ”„ CAST í›„ì§„ ìƒì„± (+500)
Week 6:    ğŸ”„ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
Week 7:    ğŸ“š ì „ì²´ ë°ì´í„° í•™ìŠµ (468ê°œ)
Week 8-9:  ğŸ“š ì¦ê°• ë°ì´í„° í•™ìŠµ
Week 10:   ğŸ“š Ablation study
Week 11-12: ğŸ¤– ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ
Week 13-14: ğŸ“ Mobile-VLA ì„ í–‰ ì—°êµ¬ ì¡°ì‚¬
Week 15-16: ğŸ“ ë…¼ë¬¸ ì‘ì„±
```

---

**ì‘ì„±**: 2025-11-26  
**ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤**: Context Vector ì¶”ì¶œ ì™„ë£Œ (Week 2)
