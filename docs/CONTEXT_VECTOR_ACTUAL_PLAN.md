# Context Vector ì‹¤ì œ ì¶”ì¶œ ë° ë¹„êµ ì²´í¬ë¦¬ìŠ¤íŠ¸

## âŒ í˜„ì¬ ìƒíƒœ: **ë¯¸ì™„ë£Œ** (ì´ë¡ ë§Œ, ì‹¤í–‰ ì•ˆ í•¨)

### ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦

| í•­ëª© | ìƒíƒœ | ë¹„ê³  |
|:---|:---:|:---|
| RoboVLMs pretrainedë¡œ ìš°ë¦¬ ë°ì´í„° í…ŒìŠ¤íŠ¸ | âŒ | ìŠ¤í¬ë¦½íŠ¸ë§Œ ì¤€ë¹„, ì‹¤í–‰ ì•ˆ í•¨ |
| ëª¨ë¸ hook/ì½”ë“œ ìˆ˜ì • | âŒ | ë°©ë²•ë§Œ ì œì‹œ, êµ¬í˜„ ì•ˆ í•¨ |
| ì‹¤ì œ ê°’ í™•ì¸ | âŒ | ì˜ˆìƒë§Œ, ì‹¤ì œ ì¶”ì¶œ ì•ˆ í•¨ |
| Sampling ì „ëµ | âœ… | ë¬¸ì„œí™” ì™„ë£Œ (100 episodes, 5 frames) |
| HuggingFace ë¡œë“œ | âœ… | Checkpoint í™•ì¸ì™„ë£Œ |

**ê²°ë¡ **: **ì´ë¡ ì  ë¶„ì„ì€ ì™„ë£Œ, ì‹¤ì œ ì‹¤í–‰ì€ 0%**

---

## ğŸš€ ì‹¤ì œ ì‹¤í–‰ ê³„íš

### **Step 1: Checkpoint í™•ì¸** âœ…
```bash
# RoboVLMs
.vlms/RoboVLMs/checkpoints/kosmos_ph_oxe-pretrain.pt
â†’ ì‹¬ë³¼ë¦­ ë§í¬ í™•ì¸ë¨

# Mobile-VLA (trained)
RoboVLMs_upstream/runs/mobile_vla_lora_20251203/.../epoch_09...ckpt
â†’ ì¡´ì¬ í™•ì¸ë¨
```

### **Step 2: Context Vector ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±** â³

### **Step 3: ë¹„êµ ë° ì‹œê°í™”** â³

---

## ğŸ“Š ì‹¤ì œ ë¹„êµ ë°©ë²• (êµ¬ì¡°í™”)

### **ë°©ë²• 1: ì§ì ‘ ê°’ ì¶”ì¶œ**

```python
#!/usr/bin/env python3
"""
ì‹¤ì œ Context Vector ì¶”ì¶œ ë° ë¹„êµ
"""
import torch
import numpy as np
from pathlib import Path

# 1. ëª¨ë¸ ë¡œë“œ
def load_robovlms():
    """RoboVLMs pretrained ë¡œë“œ"""
    ckpt_path = ".vlms/RoboVLMs/checkpoints/kosmos_ph_oxe-pretrain.pt"
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    
    # VLMë§Œ ì¶”ì¶œ
    vlm_state = {k: v for k, v in checkpoint.items() 
                 if 'vision' in k or 'language' in k}
    
    return vlm_state

def load_mobile_vla():
    """Mobile-VLA trained ë¡œë“œ"""
    ckpt_path = "RoboVLMs_upstream/runs/mobile_vla_lora_20251203/.../epoch_09.ckpt"
    
    from robovlms.train.mobile_vla_trainer import MobileVLATrainer
    model = MobileVLATrainer.load_from_checkpoint(ckpt_path)
    model.eval()
    
    return model

# 2. Context Vector ì¶”ì¶œ
def extract_context(model, images):
    """
    ì´ë¯¸ì§€ì—ì„œ context vector ì¶”ì¶œ
    
    Args:
        model: VLM model
        images: (batch, frames, C, H, W)
    
    Returns:
        context: (batch, frames, tokens, features)
    """
    with torch.no_grad():
        context = model.encode_images(images)
    
    return context

# 3. í†µê³„ ê³„ì‚°
def compute_statistics(context):
    """
    Context vector í†µê³„
    
    Returns:
        dict: {mean, std, min, max, shape}
    """
    return {
        'shape': list(context.shape),
        'mean': float(context.mean()),
        'std': float(context.std()),
        'min': float(context.min()),
        'max': float(context.max()),
        'norm': float(torch.norm(context)),
    }

# 4. ë¹„êµ
def compare_contexts(ctx1, ctx2, name1='RoboVLMs', name2='Mobile-VLA'):
    """
    ë‘ context vector ë¹„êµ
    """
    stats1 = compute_statistics(ctx1)
    stats2 = compute_statistics(ctx2)
    
    print(f"\n{'='*60}")
    print(f"Context Vector ë¹„êµ: {name1} vs {name2}")
    print(f"{'='*60}\n")
    
    # í‘œ í˜•ì‹ ì¶œë ¥
    print(f"{'Metric':<15} | {name1:<20} | {name2:<20} | Difference")
    print("-"*80)
    
    for key in ['mean', 'std', 'min', 'max', 'norm']:
        v1 = stats1[key]
        v2 = stats2[key]
        diff = abs(v1 - v2)
        print(f"{key:<15} | {v1:<20.4f} | {v2:<20.4f} | {diff:.4f}")
    
    # Cosine similarity
    cos_sim = torch.cosine_similarity(
        ctx1.flatten(), ctx2.flatten(), dim=0
    )
    print(f"\nCosine Similarity: {cos_sim:.4f}")
    
    return {
        'stats1': stats1,
        'stats2': stats2,
        'cosine_similarity': float(cos_sim)
    }
```

### **ë°©ë²• 2: Hook ì‚¬ìš© (ë‚´ë¶€ ê°’ í™•ì¸)**

```python
def hook_context_extraction(model):
    """
    ëª¨ë¸ì— hookì„ ê±¸ì–´ì„œ ì¤‘ê°„ layer ê°’ í™•ì¸
    """
    activations = {}
    
    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()
        return hook
    
    # VLMì˜ íŠ¹ì • layerì— hook ë“±ë¡
    model.model.vision_model.register_forward_hook(
        get_activation('vision_output')
    )
    
    return activations

# ì‚¬ìš©
activations = hook_context_extraction(model)
output = model(images)
vision_context = activations['vision_output']
```

---

## ğŸ“ ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
"""
extract_and_compare_contexts.py

ì‹¤ì œ Context Vector ì¶”ì¶œ ë° ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import numpy as np
import h5py
from pathlib import Path
from PIL import Image
import torchvision.transforms as T
import json
import sys

sys.path.insert(0, "RoboVLMs_upstream")

def main():
    print("="*60)
    print("Context Vector ì‹¤ì œ ì¶”ì¶œ ë° ë¹„êµ")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§)
    print("\n[1] ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ (10 episodes)")
    h5_files = list(Path("ROS_action/mobile_vla_dataset").glob("episode*.h5"))[:10]
    
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor()
    ])
    
    sample_images = []
    for h5_file in h5_files:
        with h5py.File(h5_file, 'r') as f:
            # ì²« 8 í”„ë ˆì„
            frames = []
            for i in range(min(8, len(f['images']))):
                img = Image.fromarray(f['images'][i].astype(np.uint8))
                frames.append(transform(img))
            
            if len(frames) == 8:
                sample_images.append(torch.stack(frames))
    
    images_batch = torch.stack(sample_images).cuda()  # (N, 8, 3, 224, 224)
    print(f"  ìƒ˜í”Œ shape: {images_batch.shape}")
    
    # 2. Mobile-VLA (trained) context ì¶”ì¶œ
    print("\n[2] Mobile-VLA Context ì¶”ì¶œ")
    from robovlms.train.mobile_vla_trainer import MobileVLATrainer
    
    mobile_ckpt = "RoboVLMs_upstream/runs/mobile_vla_lora_20251203/kosmos/mobile_vla_finetune/2025-12-03/mobile_vla_lora_20251203/epoch_epoch=09-val_loss=val_loss=0.013.ckpt"
    
    mobile_model = MobileVLATrainer.load_from_checkpoint(mobile_ckpt)
    mobile_model.eval().cuda()
    
    with torch.no_grad():
        mobile_context = mobile_model.model.encode_images(images_batch)
    
    print(f"  Mobile-VLA context shape: {mobile_context.shape}")
    print(f"  Mean: {mobile_context.mean():.4f}")
    print(f"  Std: {mobile_context.std():.4f}")
    
    # 3. RoboVLMs (pretrained) context ì¶”ì¶œ (TODO: êµ¬í˜„ í•„ìš”)
    print("\n[3] RoboVLMs Context ì¶”ì¶œ (TODO)")
    print("  âš ï¸  RoboVLMs checkpoint êµ¬ì¡° ë¶„ì„ í•„ìš”")
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n[4] ê²°ê³¼ ì €ì¥")
    results = {
        'mobile_vla': {
            'shape': list(mobile_context.shape),
            'mean': float(mobile_context.mean()),
            'std': float(mobile_context.std()),
            'min': float(mobile_context.min()),
            'max': float(mobile_context.max()),
        }
    }
    
    with open('context_comparison.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("  âœ… context_comparison.json ì €ì¥ë¨")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    main()
```

---

## ğŸ¯ ë‹¤ìŒ ì•¡ì…˜

### **ì§€ê¸ˆ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥** (GPU í•„ìš”)
```bash
# 1. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python3 extract_and_compare_contexts.py

# 2. ê²°ê³¼ í™•ì¸
cat context_comparison.json
```

### **ê²°ê³¼ ì˜ˆìƒ**
```json
{
  "mobile_vla": {
    "shape": [10, 8, 64, 2048],
    "mean": -0.0234,
    "std": 1.0145,
    "min": -12.4567,
    "max": 11.2341
  },
  "robovlms": {
    "shape": [10, 8, 64, 2048],
    "mean": -0.0187,
    "std": 0.9876,
    "min": -11.8923,
    "max": 10.5634
  },
  "cosine_similarity": 0.9876
}
```

---

## ğŸ“Š ì‹œê°í™” ë°©ë²•

```python
import matplotlib.pyplot as plt

# 1. Distribution ë¹„êµ
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.hist(mobile_context.flatten().cpu(), bins=50, alpha=0.5, label='Mobile-VLA')
plt.hist(robovlms_context.flatten().cpu(), bins=50, alpha=0.5, label='RoboVLMs')
plt.legend()
plt.title('Context Distribution')

# 2. Heatmap
plt.subplot(132)
plt.imshow(mobile_context[0, 0].cpu(), cmap='viridis')
plt.title('Mobile-VLA Context')
plt.colorbar()

plt.subplot(133)
plt.imshow(robovlms_context[0, 0].cpu(), cmap='viridis')
plt.title('RoboVLMs Context')
plt.colorbar()

plt.savefig('context_comparison.png')
```

---

*ì‹¤ì œ ê°’ ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì™„ë£Œ, GPU ì„¸ì…˜ì—ì„œ ì‹¤í–‰ í•„ìš”*
