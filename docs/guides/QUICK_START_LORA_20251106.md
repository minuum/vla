# ğŸš€ Mobile VLA LoRA Fine-tuning Quick Start (20251106)

## ğŸ“‹ ìš”ì•½

- **ëª©í‘œ**: 20251106 ì—í”¼ì†Œë“œë¥¼ Kosmos VLMì— LoRAë¡œ íŒŒì¸íŠœë‹
- **ë°ì´í„°**: 13ê°œ HDF5 ì—í”¼ì†Œë“œ
- **ë°©ë²•**: LoRA (r=32, alpha=16)
- **ì˜ˆìƒ ì‹œê°„**: 1 ì—í¬í¬ ~5-10ë¶„

---

## âœ… 1ë‹¨ê³„: ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸

```bash
cd /home/billy/25-1kp/vla
python3 Mobile_VLA/scripts/test_dataset_20251106.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ§ª 20251106 ì—í”¼ì†Œë“œ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘
ğŸ“Š Training ë°ì´í„°ì…‹: 11ê°œ ì—í”¼ì†Œë“œ
ğŸ“Š Validation ë°ì´í„°ì…‹: 2ê°œ ì—í”¼ì†Œë“œ
âœ… ì´ XXXê°œ ìƒ˜í”Œ ìƒì„±
âœ… ë°°ì¹˜ ë¡œë“œ ì„±ê³µ
```

---

## â±ï¸ 2ë‹¨ê³„: LoRA ì‹œê°„ ì¸¡ì • (1 ì—í¬í¬)

### Config ìˆ˜ì •
```bash
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json
```

**ë³€ê²½:**
```json
{
  "trainer": {
    "max_epochs": 1  // 50 â†’ 1
  }
}
```

### ì‹¤í–‰
```bash
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

### ê²°ê³¼ í™•ì¸
```bash
cat Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json | grep avg_epoch_time
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
"avg_epoch_time": 300.5  // ì•½ 5ë¶„
```

---

## ğŸ¯ 3ë‹¨ê³„: ì „ì²´ í•™ìŠµ (50 ì—í¬í¬)

### Config ë³µì›
```bash
vim Mobile_VLA/configs/finetune_mobile_vla_lora_20251106.json
```

**ë³€ê²½:**
```json
{
  "trainer": {
    "max_epochs": 50  // 1 â†’ 50
  }
}
```

### ì‹¤í–‰
```bash
bash Mobile_VLA/scripts/run_lora_finetune_20251106.sh
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- 1 ì—í¬í¬ 5ë¶„ Ã— 50 = **ì•½ 4ì‹œê°„**

---

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### TensorBoard
```bash
tensorboard --logdir=Mobile_VLA/runs/mobile_vla_lora/logs
# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:6006 ì ‘ì†
```

### ì‹¤ì‹œê°„ ë¡œê·¸
```bash
tail -f Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json
```

---

## ğŸ“ ê²°ê³¼ í™•ì¸

### ì²´í¬í¬ì¸íŠ¸
```bash
ls -lh Mobile_VLA/runs/mobile_vla_lora/checkpoints/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
best_model.pth              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (~50MB)
checkpoint_epoch_10.pth
checkpoint_epoch_20.pth
...
```

### í•™ìŠµ ê²°ê³¼
```bash
cat Mobile_VLA/runs/mobile_vla_lora/logs/training_results.json
```

**ì£¼ìš” ì§€í‘œ:**
- `avg_epoch_time`: ì—í¬í¬ë‹¹ í‰ê·  ì‹œê°„
- `best_val_loss`: ìµœê³  ê²€ì¦ ì†ì‹¤
- `total_epochs`: ì´ ì—í¬í¬ ìˆ˜

---

## ğŸ› ë¬¸ì œ í•´ê²°

### CUDA Out of Memory
```json
// Config ìˆ˜ì •
{
  "batch_size": 1,              // 2 â†’ 1
  "accumulate_grad_batches": 8  // 4 â†’ 8
}
```

### ë°ì´í„°ì…‹ ì—†ìŒ
```bash
ls -lh /home/billy/25-1kp/vla/ROS_action/mobile_vla_dataset/episode_20251106_*.h5
```

---

## ğŸ“š ìƒì„¸ ê°€ì´ë“œ

- **ì „ì²´ ê°€ì´ë“œ**: `Mobile_VLA/README_LORA_FINETUNING.md`
- **êµ¬í˜„ ìš”ì•½**: `Mobile_VLA/LORA_FINETUNING_SUMMARY.md`

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
2. âœ… LoRA ì‹œê°„ ì¸¡ì • (1 ì—í¬í¬)
3. â³ ì „ì²´ í•™ìŠµ (50 ì—í¬í¬)
4. â³ ì¶”ë¡  í…ŒìŠ¤íŠ¸
5. â³ 100 Dataset ìˆ˜ì§‘

---

**ì‘ì„±ì¼**: 2025-11-06  
**ì‹¤í–‰ í™˜ê²½**: Jetson AGX Orin 16GB  
**ì°¸ì¡°**: [RoboVLMs GitHub](https://github.com/Robot-VLAs/RoboVLMs)

