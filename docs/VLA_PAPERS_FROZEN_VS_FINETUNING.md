# VLA ë…¼ë¬¸ ì¡°ì‚¬: Frozen vs Fine-tuning ë¹„êµ

**ì¡°ì‚¬ ë‚ ì§œ**: 2025-12-05  
**ëª©ì **: VLM Frozen vs Fine-tuning ì ‘ê·¼ë²• ë¹„êµ (VLA ì—°êµ¬ ì¤‘ì‹¬)

---

## ğŸ“š ì£¼ìš” VLA ë…¼ë¬¸ ë¶„ì„

### 1. RT-2 (Google DeepMind, 2023)
**Approach**: **Co-fine-tuning** (VLM + Robotic Data)

**ë°©ë²•**:
- PaLM-E, PaLI-X ê°™ì€ ëŒ€í˜• VLMì„ **robotic dataë¡œ co-fine-tune**
- Actionsë¥¼ text tokensë¡œ í‘œí˜„í•˜ì—¬ VLMì´ ì§ì ‘ ì¶œë ¥
- Web-scale data + Robot demonstration data í•¨ê»˜ í•™ìŠµ

**ê²°ê³¼**:
- âœ… Improved generalization
- âœ… Emergent capabilities (chain-of-thought)
- âœ… Zero-shot on new tasks

**ë°ì´í„°**:
- Web-scale: ìˆ˜ì–µ ê°œ
- Robot demos: ìˆ˜ë§Œ ê°œ

**ê²°ë¡ **: **Fine-tuningì´ íš¨ê³¼ì **, but ë§‰ëŒ€í•œ ë°ì´í„° í•„ìš”

---

### 2. OpenVLA (Stanford, 2024)
**Approach**: **Fine-tuning** (Pretrained VLM)

**ë°©ë²•**:
- Prismatic-7B (Llama 2 + DINOv2 + SigLIP) ì‚¬ìš©
- Open X-Embodiment dataset (~970K trajectories)ë¡œ **fine-tune**
- Consumer GPUì—ì„œë„ fine-tuning ê°€ëŠ¥í•˜ë„ë¡ ìµœì í™”

**ê²°ê³¼**:
- âœ… Outperforms RT-2-X on generalist tasks
- âœ… Fast adaptation with minimal data
- âœ… Strong performance across diverse robots

**ë°ì´í„°**:
- Pre-training: Web-scale
- Fine-tuning: ~970K robot trajectories

**ê²°ë¡ **: **Fine-tuning crucial for deployment**, íš¨ìœ¨ì  adaptation ê°€ëŠ¥

---

### 3. RoboFlamingo (UC Berkeley, 2023) â­
**Approach**: **Slightly fine-tuned policy head + Frozen VLM**

**ë°©ë²•**:
- OpenFlamingo VLM ì‚¬ìš©
- VLMì€ **vision-language comprehensionë§Œ** (ê±°ì˜ frozen)
- **Policy headë§Œ fine-tune** (imitation learning)
- Decouples VL understanding from decision-making

**ê²°ê³¼**:
- âœ… State-of-the-art with **reduced data**
- âœ… Cost-effective (no massive co-training)
- âœ… Flexible architecture

**ë°ì´í„°**:
- VLM: Pre-trained (frozen)
- Policy head: **ìˆ˜ë°±~ìˆ˜ì²œ** trajectories

**ê²°ë¡ **: **Frozen VLM + Fine-tuned policy** íš¨ê³¼ì ! â† **ìš°ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬**

---

### 4. VLM2VLA (2024)
**Approach**: **Aligned fine-tuning** (catastrophic forgetting ë°©ì§€)

**ë°©ë²•**:
- VLMì˜ **core reasoning ë³´ì¡´**í•˜ë©´ì„œ fine-tune
- Action representationì„ natural languageì™€ align
- Catastrophic forgetting ë¬¸ì œ í•´ê²°

**ê²°ê³¼**:
- âœ… VLM capabilities preserved
- âœ… No forgetting
- âœ… Better long-term performance

**ê²°ë¡ **: **Fine-tuning ì‹œ VLM ë³´ì¡´ ì¤‘ìš”**

---

## ğŸ“Š Frozen vs Fine-tuning ë¹„êµí‘œ

| Aspect | Frozen VLM | Fine-tuned VLM |
|:---|:---|:---|
| **Training** | âŒ No VLM training | âœ… VLM co-trained/fine-tuned |
| **Data Required** | ğŸŸ¢ **100s~1,000s** | ğŸ”´ **10,000s~100,000s** |
| **Computation** | ğŸŸ¢ Low (only policy) | ğŸ”´ High (VLM + policy) |
| **Generalization** | ğŸŸ¡ Good (pretrain knowledge) | ğŸŸ¢ **Excellent** (task-adapted) |
| **Performance** | ğŸŸ¡ Good (may be suboptimal) | ğŸŸ¢ **Best** (task-specific) |
| **Stability** | ğŸŸ¢ **Stable** | ğŸŸ¡ Potential drift |
| **Catastrophic Forgetting** | ğŸŸ¢ **No risk** | ğŸ”´ **High risk** |
| **Novel Scenarios** | ğŸŸ¢ Good (pretrain) | ğŸŸ¡ Needs more data |
| **Training Time** | ğŸŸ¢ **Fast** (hours) | ğŸ”´ Slow (days) |
| **Best For** | Limited data, fast iteration | Large-scale datasets |

### Examples by Approach

**Frozen VLM**:
- âœ… RoboFlamingo (policy head fine-tune)
- âœ… **ìš°ë¦¬ Case 3** (Mobile-VLA)
- Data: 100s~1,000s
- Best for: Data-limited scenarios

**Fine-tuned VLM**:
- âœ… RT-2 (co-fine-tune)
- âœ… OpenVLA (full fine-tune)
- Data: 10,000s~100,000s
- Best for: Large-scale deployment

---

## ğŸ” ì—°êµ¬ ê²°ê³¼ í•µì‹¬ Findings

### Finding 1: Frozen VLM Performance Gap
**ë¬¸ì œ**: Frozen encoderê°€ **task-specific visual-motor relationships** í¬ì°© ëª»í•¨

**ì¦ê±°**:
- Frozen policy: 42% success rate **drop** vs fine-tuned
- "Frozen encoders fail to actively contribute to decision-making"

**í•´ê²°ì±…**:
- Policy headë¥¼ ì¶©ë¶„íˆ í•™ìŠµ (RoboFlamingo ë°©ì‹)
- Adapter/PEFT ì‚¬ìš©

### Finding 2: Fine-tuning Benefits
**ì¥ì **:
- âœ… Fine-grained spatial details í¬ì°©
- âœ… Novel objects generalization
- âœ… Near 100% success (after fine-tuning)

**ë‹¨ì **:
- âŒ Representational drift
- âŒ Computational cost
- âŒ Large data requirement

### Finding 3: Hybrid Approaches
**Adapter/PEFT**:
- Small trainable parameters ì¶”ê°€
- Frozen VLM ìœ ì§€í•˜ë©´ì„œ adaptation
- ì„±ëŠ¥ gap ê°ì†Œ

**Dual-encoder**:
- One frozen (robust features)
- One trainable (task adaptation)
- Best of both worlds

---

## ğŸ’¡ ìš°ë¦¬ ì—°êµ¬ì— ëŒ€í•œ ì‹œì‚¬ì 

### í˜„ì¬ ìƒíƒœ (Case 3)
```
Approach: Frozen VLM + Fine-tuned Action Head
Data: 500 episodes
Result: val_loss = 0.027

âœ… RoboFlamingo ë°©ì‹ê³¼ ì¼ì¹˜
âœ… ë°ì´í„° íš¨ìœ¨ì 
âœ… ì•ˆì •ì  í•™ìŠµ
```

### êµìˆ˜ë‹˜ ì˜ê²¬ ê²€ì¦
**"Frozenì´ ì˜ë¯¸ ìˆì„ ê²ƒ"** â† **ë…¼ë¬¸ë“¤ì´ ì§€ì§€!**

**ê·¼ê±°**:
1. **RoboFlamingo**: Frozen VLM + Policy fine-tune = SOTA
2. **ìˆ˜ë°±~ìˆ˜ì²œ ë°ì´í„°ë¡œ ì¶©ë¶„** (ìš°ë¦¬ 500 = adequate)
3. **Catastrophic forgetting ë°©ì§€**
4. **ë¹ ë¥¸ iteration** (8ì‹œê°„ vs ìˆ˜ì¼)

### Fine-tuning (Case 4) ê³ ë ¤ ì‹œ

**í•„ìš” ì¡°ê±´**:
- ğŸ“Š Data: **1,000~3,000+ episodes** (OpenVLA ì°¸ê³ )
- â° Time: 16~24ì‹œê°„ í•™ìŠµ
- ğŸ’¾ Memory: ë” ë§ì€ GPU ë©”ëª¨ë¦¬

**ê¸°ëŒ€ íš¨ê³¼**:
- âœ… ~5-10% ì„±ëŠ¥ í–¥ìƒ (ì˜ˆìƒ)
- âœ… Novel scenarios ì¼ë°˜í™”
- âš ï¸ Catastrophic forgetting ìœ„í—˜

**ê¶Œì¥**:
- **í˜„ì¬ Frozen ê²°ê³¼ë¡œ ì¶©ë¶„** (RoboFlamingo ì‚¬ë¡€)
- Fine-tuningì€ **ì„ íƒì ** (ë” ë§ì€ ë°ì´í„° í™•ë³´ í›„)

---

## ğŸ“ˆ Frozen vs Fine-tuning: Performance vs Data

```
Performance
    â†‘
    â”‚
100%â”‚                         OpenVLA â—
    â”‚                        /
    â”‚              RT-2  â—
    â”‚                  /
 80%â”‚         RoboFlamingo â— (Frozen + Policy)
    â”‚              /
    â”‚      ìš°ë¦¬ â—  (Frozen)
    â”‚        /
 60%â”‚    /
    â”‚  /
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Data
      100  1K  10K  100K  1M

Frozen VLM: ë¹ ë¥´ê²Œ 80% ë‹¬ì„± (ì ì€ ë°ì´í„°)
Fine-tuned: ì²œì²œíˆ 100% ë„ë‹¬ (ë§ì€ ë°ì´í„°)
```

---

## âœ… ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­

### 1. Frozen VLM (Case 3) - ê¶Œì¥ âœ…

**ê·¼ê±°**:
- âœ… RoboFlamingo, VLM2VLA ë“± ë…¼ë¬¸ ê²€ì¦
- âœ… 500 episodes = ì¶©ë¶„ (ìˆ˜ë°±~ìˆ˜ì²œ ë²”ìœ„)
- âœ… No catastrophic forgetting
- âœ… Fast iteration (8ì‹œê°„)
- âœ… êµìˆ˜ë‹˜ ì˜ê²¬ê³¼ ì¼ì¹˜

**ë°œí‘œ ë©”ì‹œì§€**:
"Frozen VLM ì ‘ê·¼ì´ ë°ì´í„° íš¨ìœ¨ì ì´ë©°, RoboFlamingo ë“± ìµœì‹  ì—°êµ¬ì™€ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì„"

### 2. Fine-tuning (Case 4) - ì„ íƒì 

**ì¡°ê±´**:
- ğŸ“Š Data: 1,000+ episodes í™•ë³´
- â° Time: 1ì£¼ ì¶”ê°€ ì†Œìš”
- ğŸ¯ Goal: Publication-quality comparison

**ê¸°ëŒ€**:
- 5-10% ì„±ëŠ¥ í–¥ìƒ
- ë” robustí•œ comparison

**ê¶Œì¥**:
- ë¯¸íŒ… í›„ êµìˆ˜ë‹˜ ì˜ê²¬ ë“£ê³  ê²°ì •
- í˜„ì¬ëŠ” **Frozenë§Œìœ¼ë¡œ ì¶©ë¶„**

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. **RT-2**: Brohan et al., "RT-2: Vision-Language-Action Models Transfer Web Knowledge" (Google DeepMind, 2023)
2. **OpenVLA**: Kim et al., "OpenVLA: An Open-Source Vision-Language-Action Model" (Stanford, 2024)
3. **RoboFlamingo**: Li et al., "RoboFlamingo: Vision-Language Foundation Models as Effective Robot Policies" (UC Berkeley, 2023)
4. **VLM2VLA**: "From Vision-Language Models to Vision-Language-Action Models" (2024)

**í•µì‹¬**: VLA ì—°êµ¬ì—ì„œ **Frozen VLM + Fine-tuned Policy**ê°€ ë°ì´í„° íš¨ìœ¨ì ì´ê³  ì‹¤ìš©ì ì¸ ì ‘ê·¼ë²•ìœ¼ë¡œ ê²€ì¦ë¨!
