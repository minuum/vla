# Mobile-VLA ì¶”ë¡ (Inference) ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„

**ì‘ì„±ì¼**: 2025-12-04
**ëª©í‘œ**: ì‹¤ì œ ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„ ë° êµ¬í˜„

---

## ğŸ¯ **êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­**

### **ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤**
> ì²˜ìŒì— ê±°ë¦¬ë¥¼ ì¼, ê·¸ ìë¦¬ì—ì„œ ì¹´ë©”ë¼ë¡œ ì°ì€ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸(ê³ ì •)
> 
> **0.4ì´ˆë§ˆë‹¤ 2DOF (velocity)ë¥¼ ê°€ì ¸ì˜´**
> 
> ì•ì˜ window frameì—ì„œëŠ” ì˜¤ë˜ ê±¸ë¦¬ê³ , ë’¤ì—ì„œëŠ” ê¸°ì¡´ actionëŒ€ë¡œ ì´ë™í•˜ëŠ” í˜•íƒœ

### **Action Chunk ë°©ì‹**
> 20msë§ˆë‹¤ ì˜ˆì¸¡í•˜ë©´ ê³„ì‚°ëŸ‰ì´ ëŠë¦¬ê¸°ì—, **ì•ìœ¼ë¡œì˜ 10ê°œë¥¼ í•œêº¼ë²ˆì— ê³„ì‚°**
> 
> **200msë§ˆë‹¤ ê³„ì‚°** (10 timesteps Ã— 20ms)
>
> ë‹¤ë¥¸ íƒœìŠ¤í¬ëŠ” 20ì´ˆë§ˆë‹¤ ì˜ˆì¸¡

### **ê²€ì¦ í•„ìš”**
> íŒŒì¸íŠœë‹í•˜ê³  í•™ìŠµëœ ê°’ì„ ê°€ì§€ê³  ì¶”ë¡ í•´ì„œ **ì œëŒ€ë¡œëœ x, y ê°’ì„ ë¿Œë ¤ì£¼ëŠ”ì§€** í…ŒìŠ¤íŠ¸

---

## ğŸ“Š **í˜„ì¬ í•™ìŠµ ì„¤ì • (Training)**

### **ë°ì´í„°ì…‹ êµ¬ì¡°**
```python
window_size = 8          # ê³¼ê±° 8 í”„ë ˆì„
fwd_pred_next_n = 10     # ë¯¸ë˜ 10 í”„ë ˆì„
total_frames = 18        # ì´ 18 í”„ë ˆì„

# ì…ë ¥
images: (18, 3, 224, 224)      # 18 í”„ë ˆì„ ì´ë¯¸ì§€
actions: (18, 2)               # 18 í”„ë ˆì„ velocity

# í•™ìŠµ
for each batch:
    context = VLM(images[:8])       # ê³¼ê±° 8 í”„ë ˆì„ìœ¼ë¡œ context
    predicted = ActionHead(context)  # ë¯¸ë˜ 10 í”„ë ˆì„ ì˜ˆì¸¡
    loss = MSE(predicted, actions[8:18])
```

### **ì‹œê°„ ê°„ê²© (ìˆ˜ì§‘ ì‹œ)**
```python
# ë°ì´í„° ìˆ˜ì§‘ ì‹œ í”„ë ˆì„ ê°„ê²© í™•ì¸ í•„ìš”
frame_interval = ?  # ëª‡ ms ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘í–ˆëŠ”ì§€

# ì˜ˆìƒ: 100-200ms ê°„ê²©
# â†’ 18 í”„ë ˆì„ = 1.8~3.6ì´ˆ ì‹œí€€ìŠ¤
```

---

## ğŸš€ **ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„**

### **Scenario 1: Sliding Window (êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­)**

```python
class MobileVLAInference:
    def __init__(self):
        self.window_size = 8
        self.action_chunk_size = 10
        self.control_interval = 0.4  # 400ms (êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­)
        
        self.image_buffer = deque(maxlen=8)
        self.action_buffer = deque(maxlen=10)
        self.last_inference_time = 0
        
    def run(self):
        while not arrived:
            current_time = time.time()
            
            # Step 1: ì´ë¯¸ì§€ ìº¡ì²˜
            image = camera.capture()
            self.image_buffer.append(image)
            
            # Step 2: 0.4ì´ˆë§ˆë‹¤ ì¶”ë¡ 
            if current_time - self.last_inference_time >= 0.4:
                if len(self.image_buffer) == 8:
                    # VLM + Action Head ì¶”ë¡ 
                    context = model.vlm(self.image_buffer)
                    action_chunk = model.action_head(context)  # (10, 2)
                    
                    self.action_buffer = deque(action_chunk)
                    self.last_inference_time = current_time
            
            # Step 3: Action bufferì—ì„œ velocity ê°€ì ¸ì˜¤ê¸°
            if self.action_buffer:
                velocity = self.action_buffer.popleft()
                robot.set_velocity(velocity)
            
            time.sleep(0.02)  # 20ms control loop
```

**íŠ¹ì§•**:
- âœ… 0.4ì´ˆë§ˆë‹¤ ì¶”ë¡  (êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­)
- âœ… Action chunk í™œìš© (10ê°œ ë¯¸ë¦¬ ì˜ˆì¸¡)
- âœ… 20ms control loop
- âš ï¸ ì²˜ìŒ 8 í”„ë ˆì„ ëª¨ì„ ë•Œê¹Œì§€ ëŒ€ê¸° (0.8~1.6ì´ˆ)

---

### **Scenario 2: Action Chunk with Fast Start**

```python
class FastStartInference:
    def __init__(self):
        self.window_size = 8
        self.action_chunk_size = 10
        self.inference_interval = 0.2  # 200ms (action chunk ë°©ì‹)
        
    def run(self):
        # Step 1: ì´ˆê¸° ì´ë¯¸ì§€ ìˆ˜ì§‘ (ë¹ ë¥´ê²Œ)
        for i in range(8):
            image = camera.capture()
            self.image_buffer.append(image)
            time.sleep(0.05)  # 50ms ê°„ê²© (ë¹ ë¥´ê²Œ ì±„ì›€)
        
        # Step 2: ì¶”ë¡  ë£¨í”„
        while not arrived:
            # 200msë§ˆë‹¤ ì¶”ë¡  (êµìˆ˜ë‹˜ ì–¸ê¸‰)
            context = model.vlm(self.image_buffer)
            action_chunk = model.action_head(context)  # (10, 2)
            
            # 10ê°œ actionì„ 20msì”© ì‹¤í–‰
            for action in action_chunk:
                robot.set_velocity(action)
                time.sleep(0.02)  # 20ms
                
                # ìƒˆ ì´ë¯¸ì§€ ì¶”ê°€ (sliding)
                new_image = camera.capture()
                self.image_buffer.append(new_image)
```

**íŠ¹ì§•**:
- âœ… ë¹ ë¥¸ ì‹œì‘ (0.4ì´ˆ ë§Œì— ì‹œì‘)
- âœ… 200ms ì¶”ë¡  ê°„ê²© (10 Ã— 20ms)
- âœ… Sliding window
- âš ï¸ ê³„ì‚° ë¶€í•˜ ë†’ìŒ (200msë¡œ ì¶©ë¶„í•œì§€?)

---

## âš™ï¸ **ì„±ëŠ¥ ë¶„ì„ (Latency)**

### **ì¶”ë¡  ì‹œê°„ ì¸¡ì • í•„ìš”**

```python
# ì¸¡ì •í•´ì•¼ í•  ê²ƒë“¤
with torch.no_grad():
    # VLM forward
    t1 = time.time()
    context = model.vlm(images)
    vlm_time = time.time() - t1
    
    # Action Head forward
    t2 = time.time()
    actions = model.action_head(context)
    action_head_time = time.time() - t2
    
total_inference_time = vlm_time + action_head_time
```

**ì˜ˆìƒ (Frozen VLM)**:
```
VLM forward: ~50-100ms (Kosmos-2, frozen)
Action Head: ~5-10ms (LSTM, tiny)
Total: ~60-110ms

â†’ 200ms ê°„ê²©ì´ë©´ ì¶©ë¶„!
```

---

## ğŸ”§ **ê±°ë¦¬ ì¸¡ì • (êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­)**

> ì²˜ìŒì— ê±°ë¦¬ë¥¼ ì¼

### **ë°©ë²• 1: ì¹´ë©”ë¼ ê¸°ë°˜ (Vision)**
```python
def estimate_distance(image, bottle_detector):
    # YOLOë¡œ ë³‘ ê°ì§€
    boxes = bottle_detector(image)
    
    # Bounding box í¬ê¸°ë¡œ ê±°ë¦¬ ì¶”ì •
    if boxes:
        box_height = boxes[0].height
        # ì—­ë¹„ë¡€ ê´€ê³„ (ê°€ê¹Œìš°ë©´ í¬ê²Œ ë³´ì„)
        distance = calibration_constant / box_height
        return distance
    return None
```

### **ë°©ë²• 2: Depth Camera**
```python
# Intel RealSense ë“±
depth_image = camera.get_depth()
bottle_mask = detector(rgb_image)
average_depth = depth_image[bottle_mask].mean()
return average_depth
```

---

## ğŸ“ **ROS ë…¸ë“œ êµ¬í˜„**

```python
#!/usr/bin/env python3
import rospy
from geometry_msgs.msg import Twist
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import torch
from collections import deque

class VLAInferenceNode:
    def __init__(self):
        rospy.init_node('vla_inference')
        
        # íŒŒë¼ë¯¸í„°
        self.control_interval = rospy.get_param('~control_interval', 0.4)  # 400ms
        self.checkpoint_path = rospy.get_param('~checkpoint_path')
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model(self.checkpoint_path)
        self.model.eval()
        
        # Buffers
        self.image_buffer = deque(maxlen=8)
        self.action_buffer = deque(maxlen=10)
        
        # ROS
        self.bridge = CvBridge()
        self.image_sub = rospy.Subscriber('/camera/image_raw', Image, self.image_callback)
        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=1)
        
        self.last_inference_time = rospy.Time.now()
        
        rospy.loginfo("VLA Inference Node started")
        
    def load_model(self, checkpoint_path):
        # Checkpoint ë¡œë“œ
        checkpoint = torch.load(checkpoint_path, map_location='cuda')
        
        # ëª¨ë¸ ì¬êµ¬ì„±
        from robovlms.train.mobile_vla_trainer import MobileVLATrainer
        model = MobileVLATrainer.load_from_checkpoint(checkpoint_path)
        model.cuda()
        model.freeze()  # Ensure frozen
        
        return model
        
    def image_callback(self, msg):
        # ROS Image â†’ numpy
        cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")
        
        # ì „ì²˜ë¦¬ (224x224 resize, normalize)
        image_tensor = self.preprocess(cv_image)
        
        # Bufferì— ì¶”ê°€
        self.image_buffer.append(image_tensor)
        
    def preprocess(self, image):
        # Resize to 224x224
        from PIL import Image as PILImage
        import torchvision.transforms as T
        
        pil_img = PILImage.fromarray(image)
        pil_img = pil_img.resize((224, 224))
        
        # To tensor
        tensor = T.ToTensor()(pil_img)
        return tensor
        
    def run(self):
        rate = rospy.Rate(50)  # 20ms = 50Hz
        
        while not rospy.is_shutdown():
            current_time = rospy.Time.now()
            
            # 0.4ì´ˆë§ˆë‹¤ ì¶”ë¡ 
            if (current_time - self.last_inference_time).to_sec() >= self.control_interval:
                if len(self.image_buffer) == 8:
                    self.run_inference()
                    self.last_inference_time = current_time
            
            # Action bufferì—ì„œ velocity ê°€ì ¸ì˜¤ê¸°
            if self.action_buffer:
                velocity = self.action_buffer.popleft()
                self.publish_velocity(velocity)
            
            rate.sleep()
    
    def run_inference(self):
        # ì´ë¯¸ì§€ ìŠ¤íƒ
        images = torch.stack(list(self.image_buffer)).unsqueeze(0)  # (1, 8, 3, 224, 224)
        images = images.cuda()
        
        # ì¶”ë¡ 
        with torch.no_grad():
            # VLM forward
            context = self.model.model.encode_images(images)
            
            # Action Head forward
            actions = self.model.model.act_head(context)  # (1, 10, 2)
        
        # Bufferì— ì €ì¥
        actions = actions.squeeze(0).cpu()  # (10, 2)
        self.action_buffer = deque(actions.numpy())
        
        rospy.loginfo(f"Inference done. Predicted {len(self.action_buffer)} actions")
    
    def publish_velocity(self, velocity):
        twist = Twist()
        twist.linear.x = float(velocity[0])  # linear_x
        twist.linear.y = float(velocity[1])  # linear_y
        twist.angular.z = 0.0  # ê³ ì • (ë˜ëŠ” velocity[2] ì‚¬ìš© ì‹œ)
        
        self.cmd_vel_pub.publish(twist)

if __name__ == '__main__':
    node = VLAInferenceNode()
    node.run()
```

---

## ğŸ“Š **ê²€ì¦ ê³„íš**

### **Test 1: Velocity ê°’ ê²€ì¦**
```python
# ì˜ˆì¸¡ëœ velocityê°€ í•©ë¦¬ì ì¸ê°€?
predicted_velocities = []
for _ in range(100):
    vel = model.predict()
    predicted_velocities.append(vel)

# ë¶„ì„
mean_vel = np.mean(predicted_velocities, axis=0)
std_vel = np.std(predicted_velocities, axis=0)

print(f"Mean velocity: {mean_vel}")  # ì˜ˆìƒ: [0.1~0.3, -0.1~0.1]
print(f"Std velocity: {std_vel}")    # ì˜ˆìƒ: [0.05~0.1, 0.05~0.1]
```

### **Test 2: ì‹¤ì‹œê°„ ì„±ëŠ¥**
```python
# Latency ì¸¡ì •
latencies = []
for _ in range(100):
    t1 = time.time()
    model.predict()
    latency = time.time() - t1
    latencies.append(latency)

print(f"Mean latency: {np.mean(latencies)*1000:.1f}ms")
print(f"Max latency: {np.max(latencies)*1000:.1f}ms")

# ëª©í‘œ: < 200ms (action chunk sizeì— ë§ì¶¤)
```

### **Test 3: ì‹¤ì œ ì£¼í–‰**
```
ì‹œë‚˜ë¦¬ì˜¤:
1. ë¡œë´‡ì„ 2m ê±°ë¦¬ì— ë°°ì¹˜
2. ë°•ìŠ¤ë¥¼ ì¤‘ì•™ì— ë°°ì¹˜
3. ë³‘ì„ ë°•ìŠ¤ ë’¤ì— ë°°ì¹˜

ì¸¡ì •:
- ì„±ê³µë¥  (ë³‘ì— ë„ë‹¬)
- ì£¼í–‰ ì‹œê°„
- ê²½ë¡œ smoothness
- ì¶©ëŒ ì—¬ë¶€
```

---

## ğŸ“ **ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„**

### âœ… **ì¶”ë¡  ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„ ì™„ë£Œ**
- 0.4ì´ˆ ê°„ê²© ì¶”ë¡  (êµìˆ˜ë‹˜ ìš”êµ¬ì‚¬í•­)
- Action chunk í™œìš© (10 timesteps)
- ROS ë…¸ë“œ êµ¬í˜„ ì¤€ë¹„

### ğŸ¯ **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥**
1. ROS ë…¸ë“œ êµ¬í˜„ ì™„ë£Œ
2. Best checkpoint ë¡œë“œ
3. ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸

### â±ï¸ **ì˜ˆìƒ íƒ€ì„ë¼ì¸**
- ROS ë…¸ë“œ ì½”ë“œ ì‘ì„±: ~30ë¶„
- Latency ì¸¡ì •: ~10ë¶„
- ì‹¤ì œ ì£¼í–‰ í…ŒìŠ¤íŠ¸: ~1ì‹œê°„

---

*ë‹¤ìŒ: ROS ë…¸ë“œ êµ¬í˜„ ë° ì‹¤ì œ ì¶”ë¡  í…ŒìŠ¤íŠ¸*
