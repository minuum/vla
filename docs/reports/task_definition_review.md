# 태스크 정의 재검토

**날짜**: 2025-12-07 00:20  
**핵심 질문**: Left/Right는 같은 태스크인가, 다른 태스크인가?

---

## 📊 데이터 분석 결과

### 언어 지시문 확인

| 파일 유형 | 언어 지시문 |
|:---|:---|
| **LEFT 파일** | "Navigate around obstacles and reach the front of the beverage bottle **on the left**" |
| **RIGHT 파일** | "Navigate around obstacles and reach the front of the beverage bottle **on the right**" |

### 실제 trajectory 분석

| 구분 | linear_x (전진) | linear_y (좌우) |
|:---|:---:|:---:|
| **LEFT** | mean=1.022 | mean=**+0.319** (왼쪽으로 이동) |
| **RIGHT** | mean=1.022 | mean=**-0.383** (오른쪽으로 이동) |

---

## 🤔 태스크 해석 옵션

### Option A: 단일 태스크 (목표물 앞으로 가기)

**관점**:
- 목표: "beverage bottle 앞에 도달"
- Left/Right는 **물체의 초기 위치**일 뿐
- 최종 목표는 동일 (물체 앞에 서기)

**이 관점에서**:
- 모델은 **이미지를 보고** 물체 위치 파악
- 물체가 왼쪽에 있으면 왼쪽으로, 오른쪽에 있으면 오른쪽으로
- 언어 지시문은 **보조 정보**
- **이미지만으로 충분**해야 함

**문제점**:
- 하지만 현재 모델은 이미지를 봐도 **물체 위치를 구분하지 못함**
- Left/Right 이미지가 다른데도 같은 출력

---

### Option B: 두 개의 다른 태스크

**관점**:
- LEFT 태스크: "왼쪽에 있는 물체로 가기"
- RIGHT 태스크: "오른쪽에 있는 물체로 가기"
- 경로가 다르므로 **다른 action** 필요

**이 관점에서**:
- 언어 지시문이 태스크를 결정
- "on the left" → 왼쪽 action 패턴
- "on the right" → 오른쪽 action 패턴

**문제점**:
- 모델이 언어를 무시하면 구분 불가
- VLM이 언어 조건화를 제대로 하지 않음

---

## 🔍 핵심 분석: 왜 모델이 구분 못하는가?

### 모델 출력 분석

```
Left GT mean_y:  +0.319
Right GT mean_y: -0.383
전체 평균:       -0.032
모델 출력:       -0.130 (모든 샘플 동일!)
```

### 해석

**가능성 1: Mode Collapse (평균 수렴)**
- 모델이 Left/Right를 구분하지 않고
- 전체 데이터의 **평균값**을 학습
- MSE loss가 평균 예측을 선호

**가능성 2: 이미지 차이를 인식 못함**
- Left/Right 이미지가 유사해 보임
- VLM이 미세한 차이를 구분 못함
- Context vector 유사도 0.74 → 비슷하게 인식

**가능성 3: 언어 조건화 실패**
- VLM이 언어 지시문을 무시
- 이미지만 처리하고 "left"/"right" 단어 무시

---

## 🎯 핵심 질문과 답변

### Q1: 우리 태스크가 "목표물 앞으로 가기"라면?

**답변**: 
그래도 **이미지에서 물체 위치를 인식**해야 함.

- 물체가 왼쪽에 있으면 → 왼쪽으로 가야 함
- 물체가 오른쪽에 있으면 → 오른쪽으로 가야 함
- **이미지만으로 방향 결정 가능해야 함**

**현재 문제**:
- 모델이 이미지를 봐도 물체 위치를 구분 못함
- 또는 물체 위치를 인식해도 action에 반영 안됨

### Q2: Left/Right를 다른 태스크로 분류해야 하는가?

**답변**: 
의미적으로는 **같은 태스크**의 **다른 변형**

- 목표: 물체 앞으로 가기 (동일)
- 변형: 물체 위치가 다름 (Left vs Right)

**하지만 학습 관점에서**:
- 현재 모델은 이 변형을 **구분하지 못함**
- 따라서 **별도 처리** 필요할 수 있음

---

## 📋 결론

### 태스크 정의

| 관점 | 태스크 수 | 설명 |
|:---|:---:|:---|
| **의미적** | 1개 | "목표물 앞으로 가기" |
| **실제 구현** | 2개 | Left/Right 다른 action 패턴 필요 |

### 현재 문제의 본질

> **모델이 이미지에서 물체 위치를 인식하고,  
> 그에 맞는 action을 출력해야 하는데,  
> 현재는 모든 이미지에 대해 같은 출력 (평균값)**

### 해결 방안

**Option 1: 이미지 인식 강화**
- VLM이 물체 위치를 더 잘 인식하도록
- 이미지 증강 또는 attention 시각화

**Option 2: 언어 조건화 강화**
- "on the left"/"on the right"를 action에 직접 연결
- Cross-attention 추가

**Option 3: 별도 학습**
- Left 데이터만으로 학습 (Case 1 스타일)
- Right 데이터만으로 학습
- 성능 비교 후 통합 방법 고민

---

## 🔬 추가 검증 필요

### 1. 이미지 확인
- Left/Right 이미지에서 물체 위치가 실제로 다른가?
- VLM이 차이를 인식할 수 있는가?

### 2. Case 1 (Left only) 모델 검증
- Left 데이터만으로 학습한 모델
- 이 모델은 일관된 action을 출력하는가?

### 3. 언어 없이 학습
- 언어 지시문 없이 이미지만으로 학습
- 물체 위치 → action 직접 매핑

---

## 🔬 실험 검증: Case 1 vs Case 3

### 실험 설계
- **Case 1**: Left 데이터만 학습 (250 episodes, Loss 0.013)
- **Case 3**: Left+Right 학습 (500 episodes, Loss 0.027)

### 결과

| 모델 | Left 입력 → output | Right 입력 → output | 차이 |
|:---|:---:|:---:|:---:|
| **Case 1** (Left only) | **+0.3718** | **+0.3718** | 0.0000 |
| **Case 3** (Left+Right) | -0.1301 | -0.1303 | 0.0002 |
| **GT Left** | +0.319 | - | - |
| **GT Right** | - | -0.383 | - |

### 해석

**Case 1 분석**:
- 모델 출력: +0.372 → GT (+0.319)에 가깝게 학습됨 ✅
- **하지만**: Right 이미지에도 같은 +0.372 출력
- → **이미지를 보고 물체 위치를 구분하지 못함**

**Case 3 분석**:
- 모델 출력: -0.13 → Left/Right GT 평균에 가까움
- Left/Right 모두 같은 출력
- → **Mode collapse (평균 수렴)**

### 핵심 결론

> **VLM이 이미지에서 물체의 좌/우 위치를 인식하지 못함**  
> - Left 이미지를 봐도 "왼쪽에 물체가 있다" 인식 못함
> - Right 이미지를 봐도 "오른쪽에 물체가 있다" 인식 못함
> - 결과: 학습한 데이터의 평균 action만 출력

---

## 🎯 최종 결론

### 태스크 정의 관점

**Q: Left/Right는 같은 태스크인가?**

**A: 의미적으로는 같은 태스크**
- 목표: "목표물 앞으로 가기"
- Left/Right는 물체 위치의 변형

**하지만 현재 모델의 한계**:
- VLM이 물체 위치 인식 못함
- 따라서 조건부 action 생성 실패

### 왜 이런 일이 발생하는가?

**가능한 원인**:

1. **VLM의 한계**:
   - Kosmos-2가 "왼쪽/오른쪽" 위치 개념을 학습하지 않음
   - 또는 이미지의 물체 위치 차이가 너무 미묘함

2. **데이터 문제**:
   - Left/Right 이미지가 실제로 비슷해 보일 수 있음
   - 물체가 화면 중앙에 가까움?

3. **학습 방식 문제**:
   - MSE loss가 평균 예측 유도
   - Left/Right가 섞여서 평균 수렴

### 해결 방안

| 방안 | 설명 | 난이도 |
|:---|:---|:---:|
| **1. Visual marker** | 이미지에 목표 위치 표시 | 🟢 쉬움 |
| **2. 별도 학습** | Left only, Right only 따로 | 🟢 쉬움 |
| **3. 분류기 추가** | Left/Right classifier 추가 | 🟡 중간 |
| **4. VLM fine-tuning** | 위치 인식 학습 | 🔴 어려움 |

---

**결론**: 
> 현재 모델은 **이미지에서 물체 위치를 인식하지 못함**.  
> "목표물 앞으로 가기" 태스크 자체는 올바르나,  
> **물체 위치에 따른 조건부 action 생성**에 실패.
