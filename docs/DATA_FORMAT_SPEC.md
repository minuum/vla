# Mobile-VLA ë°ì´í„° í˜•ì‹ ëª…ì„¸ì„œ (Data Format Specification)

> **ë²„ì „**: v1.0  
> **ì‘ì„±ì¼**: 2025-11-26  
> **ê¸°ì¤€ ë°ì´í„°**: ROS_action/mobile_vla_dataset/*.h5 (468 episodes)

---

## ğŸ“‹ ê°œìš”

Mobile-VLAëŠ” **Vision-Language-Action** ëª¨ë¸ë¡œ, ë¡œë´‡ì˜ ë¹„ì „ ì…ë ¥(Video)ê³¼ ì–¸ì–´ ëª…ë ¹(Language)ì„ ë°›ì•„ ëª¨ë°”ì¼ ë¡œë´‡ì˜ ì œì–´ ì•¡ì…˜(Action)ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

### ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video   â”‚â”€â”€â”€â”€â–¶â”‚              â”‚â”€â”€â”€â”€â–¶â”‚  Action  â”‚
â”‚ (Images) â”‚     â”‚  Mobile-VLA  â”‚     â”‚ (Linear, â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    Model     â”‚     â”‚ Angular) â”‚
                 â”‚              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚              â”‚
â”‚ Language â”‚â”€â”€â”€â”€â–¶â”‚              â”‚
â”‚(Instruct)â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ Video (ì…ë ¥)

### í˜•ì‹

**H5 Key**: `images`

```python
shape: (T, H, W, C)
dtype: uint8
range: [0, 255]
```

### ì‹¤ì œ ì‚¬ì–‘ (Mobile-VLA ë°ì´í„°ì…‹)

| ì†ì„± | ê°’ | ì„¤ëª… |
|------|-----|------|
| **ì‹œê°„ ì°¨ì› (T)** | 18 steps | ì—í”¼ì†Œë“œë‹¹ í”„ë ˆì„ ìˆ˜ (ê³ ì •) |
| **ë†’ì´ (H)** | 720 pixels | ì„¸ë¡œ í•´ìƒë„ |
| **ë„ˆë¹„ (W)** | 1280 pixels | ê°€ë¡œ í•´ìƒë„ (16:9 ë¹„ìœ¨) |
| **ì±„ë„ (C)** | 3 | RGB ì»¬ëŸ¬ |
| **Dtype** | `uint8` | 0-255 ì •ìˆ˜ |
| **ì´ í¬ê¸°** | 49,766,400 elements/episode | ~50MB/ì—í”¼ì†Œë“œ |

### ì˜ˆì‹œ

```python
import h5py

with h5py.File('episode_xxx.h5', 'r') as f:
    images = f['images'][:]  # Shape: (18, 720, 1280, 3)
    
    # ì²« í”„ë ˆì„
    frame_0 = images[0]  # Shape: (720, 1280, 3), uint8
    
    # íŠ¹ì • íƒ€ì„ìŠ¤í… ë²”ìœ„
    frames = images[5:10]  # Shape: (5, 720, 1280, 3)
```

### ì „ì²˜ë¦¬ (ëª¨ë¸ ì…ë ¥ìš©)

```python
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("google/paligemma-3b-pt-224")

# ë¦¬ì‚¬ì´ì¦ˆ ë° ì •ê·œí™”
processed_image = processor(
    images=frame_0,  # (720, 1280, 3)
    return_tensors="pt"
)
# Output: (1, 3, 224, 224), float32, [-1, 1]
```

### ì£¼ìš” íŠ¹ì§•

- âœ… **ê³ í•´ìƒë„**: 720p (HD) í’ˆì§ˆ
- âœ… **RGB ìˆœì„œ**: OpenCV BGRì´ ì•„ë‹Œ RGB
- âœ… **ì¼ì • ê¸¸ì´**: ëª¨ë“  ì—í”¼ì†Œë“œ 18 í”„ë ˆì„ (ê°„ë‹¨í•œ ë°°ì¹˜ ì²˜ë¦¬)
- âš ï¸ **ëŒ€ìš©ëŸ‰**: ì—í”¼ì†Œë“œë‹¹ ~50MB

---

## ğŸ’¬ Language (ì…ë ¥)

### í˜•ì‹

**H5 Key**: `language_instruction`

```python
shape: (1,)
dtype: 'S256' (bytes, max 256 characters)
encoding: UTF-8
```

### ì‹¤ì œ ì‚¬ì–‘ (Mobile-VLA ë°ì´í„°ì…‹)

| ì†ì„± | ê°’ | ì„¤ëª… |
|------|-----|------|
| **Key** | `language_instruction` | H5 ë°ì´í„°ì…‹ í‚¤ |
| **Shape** | (1,) | ì—í”¼ì†Œë“œë‹¹ 1ê°œ ëª…ë ¹ì–´ (ê³ ì •) |
| **Dtype** | `S256` | Bytes string, UTF-8 ì¸ì½”ë”© |
| **ì›ë³¸ (í•œê¸€)** | "ì¥ì• ë¬¼ì„ í”¼í•´ ìŒë£Œìˆ˜ í˜íŠ¸ë³‘ ì•ìœ¼ë¡œ ë„ì°©í•´ë¼" | ì‹¤ì œ ìˆ˜ì§‘ íƒœìŠ¤í¬ |
| **ì˜ì–´ ë²ˆì—­** | "Navigate around obstacles and reach the front of the beverage bottle" | ê¸°ë³¸ ëª…ë ¹ì–´ |

### íƒœìŠ¤í¬ë³„ Instruction ë³€í˜•

íŒŒì¼ëª…ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë³€í˜•ëœ instructionì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

| íŒŒì¼ëª… íŒ¨í„´ | Instruction |
|-----------|-------------|
| `*_hori_left_*` | "Navigate around obstacles and reach the front of the beverage bottle **on the left**" |
| `*_hori_right_*` | "Navigate around obstacles and reach the front of the beverage bottle **on the right**" |
| ê¸°ë³¸ | "Navigate around obstacles and reach the front of the beverage bottle" |

### ì˜ˆì‹œ

```python
import h5py

with h5py.File('episode_xxx.h5', 'r') as f:
    instruction_bytes = f['language_instruction'][0]  # bytes
    instruction = instruction_bytes.decode('utf-8')   # str
    
    print(instruction)
    # Output: "Navigate around obstacles and reach the front of the beverage bottle on the left"
```

### ë¶„í¬

```
ì´ 468ê°œ ì—í”¼ì†Œë“œ:
â”œâ”€ "... on the left"  : ~224ê°œ (48%)
â”œâ”€ "... on the right" : ~244ê°œ (52%)
```

**íŠ¹ì§•**:
- âœ… **ì‹¤ì œ íƒœìŠ¤í¬ ë°˜ì˜**: ìŒë£Œìˆ˜ í˜íŠ¸ë³‘ ë„ë‹¬ íƒœìŠ¤í¬
- âœ… **ì¥ì• ë¬¼ íšŒí”¼**: "Navigate around obstacles" í¬í•¨
- âœ… **ë°©í–¥ ì •ë³´**: ì¢Œ/ìš° ëª…ì‹œ



---

## ğŸ¯ Action (ì¶œë ¥)

### í˜•ì‹

**H5 Key**: `actions`

```python
shape: (T, D)
dtype: float32
range: [-1.15, 1.15]
```

### ì‹¤ì œ ì‚¬ì–‘ (Mobile-VLA ë°ì´í„°ì…‹)

| ì†ì„± | ê°’ | ì„¤ëª… |
|------|-----|------|
| **ì‹œê°„ ì°¨ì› (T)** | 18 steps | Videoì™€ ë™ì¼ |
| **ì•¡ì…˜ ì°¨ì› (D)** | **3** | (linear_x, angular_z, gripper) |
| **Dtype** | `float32` | 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  |
| **ë²”ìœ„** | [-1.15, 1.15] | ì •ê·œí™”ëœ ì†ë„ |

### ì•¡ì…˜ ì°¨ì› ìƒì„¸

#### Dimension 0: `linear_x` (ì„ ì†ë„)

| ì†ì„± | ê°’ |
|------|-----|
| **ì˜ë¯¸** | ì „ì§„(+) / í›„ì§„(-) ì„ ì†ë„ |
| **ë‹¨ìœ„** | m/s |
| **ë²”ìœ„** | [0.0, 1.15] (ì‹¤ì œëŠ” í›„ì§„ ì—†ìŒ) |
| **í‰ê· ** | 1.02 m/s |
| **í‘œì¤€í¸ì°¨** | 0.36 m/s |

```python
# ì˜ˆì‹œ ê°’ í•´ì„
0.0   â†’ ì •ì§€
0.5   â†’ ì¤‘ê°„ ì†ë„ ì „ì§„
1.0   â†’ ë¹ ë¥¸ ì „ì§„
1.15  â†’ ìµœëŒ€ ì†ë„
```

#### Dimension 1: `angular_z` (ê°ì†ë„)

| ì†ì„± | ê°’ |
|------|-----|
| **ì˜ë¯¸** | ì¢ŒíšŒì „(+) / ìš°íšŒì „(-) ê°ì†ë„ |
| **ë‹¨ìœ„** | rad/s |
| **ë²”ìœ„** | [-1.15, 1.15] |
| **í‰ê· ** | 0.32 rad/s (ì•½ê°„ ì¢ŒíšŒì „ í¸í–¥) |
| **í‘œì¤€í¸ì°¨** | 0.75 rad/s |

```python
# ì˜ˆì‹œ ê°’ í•´ì„
 0.0   â†’ ì§ì§„
+0.5   â†’ ì™„ë§Œí•œ ì¢ŒíšŒì „
+1.15  â†’ ê¸‰ê²©í•œ ì¢ŒíšŒì „
-0.5   â†’ ì™„ë§Œí•œ ìš°íšŒì „
-1.15  â†’ ê¸‰ê²©í•œ ìš°íšŒì „
```

#### Dimension 2: `gripper` (ê·¸ë¦¬í¼)

| ì†ì„± | ê°’ |
|------|-----|
| **ì˜ë¯¸** | ê·¸ë¦¬í¼ ê°œí (ì¶”ì •) |
| **ë²”ìœ„** | **í•­ìƒ 0.0** â— |
| **í‰ê· ** | 0.0 |
| **í‘œì¤€í¸ì°¨** | 0.0 |

**ë¶„ì„**: 
- âŒ í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œ **ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ**
- ğŸ“Œ ëª¨ë°”ì¼ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜ íƒœìŠ¤í¬ì—ì„œëŠ” ë¶ˆí•„ìš”
- ğŸ”§ ì¡°ì‘ íƒœìŠ¤í¬ ì¶”ê°€ ì‹œ í™œìš© ê°€ëŠ¥

### ì•¡ì…˜ ì˜ˆì‹œ

```python
# Sample actions from dataset
actions = [
    [0.00, 0.00, 0.0],  # Step 0: ì •ì§€ (ì—í”¼ì†Œë“œ ì‹œì‘)
    [1.15, 0.00, 0.0],  # Step 1: ìµœëŒ€ ì†ë„ ì§ì§„
    [1.15, 0.00, 0.0],  # Step 2: ìµœëŒ€ ì†ë„ ì§ì§„
    [1.15, 0.00, 0.0],  # Step 3: ìµœëŒ€ ì†ë„ ì§ì§„
    [0.00, 1.15, 0.0],  # Step 4: ì œìë¦¬ ì¢ŒíšŒì „
    [1.15, 1.15, 0.0],  # Step 5: ì „ì§„í•˜ë©° ì¢ŒíšŒì „
    ...
]
```

### ì•¡ì…˜ íƒ€ì… ë³€í™˜

```python
def classify_action(linear_x, angular_z):
    """ì•¡ì…˜ì„ ì´ì‚° íƒ€ì…ìœ¼ë¡œ ë¶„ë¥˜"""
    LINEAR_THRESHOLD = 0.1
    ANGULAR_THRESHOLD = 0.2
    
    is_moving = abs(linear_x) > LINEAR_THRESHOLD
    is_turning = abs(angular_z) > ANGULAR_THRESHOLD
    
    if not is_moving and not is_turning:
        return 'STOP'
    elif is_moving and not is_turning:
        return 'FORWARD' if linear_x > 0 else 'BACKWARD'
    elif not is_moving and is_turning:
        return 'TURN_LEFT' if angular_z > 0 else 'TURN_RIGHT'
    else:
        # ë³µí•© ë™ì‘
        direction = 'FORWARD' if linear_x > 0 else 'BACKWARD'
        turn = 'LEFT' if angular_z > 0 else 'RIGHT'
        return f'{direction}_{turn}'

# ì˜ˆì‹œ
classify_action(1.15, 0.0)   â†’ 'FORWARD'
classify_action(1.15, 1.15)  â†’ 'FORWARD_LEFT'
classify_action(0.0, 0.0)    â†’ 'STOP'
```

---

## ğŸ“Š ì¶”ê°€ ë©”íƒ€ë°ì´í„°

### action_event_types

**H5 Key**: `action_event_types`

```python
shape: (T,)
dtype: object (bytes)
values: [b'episode_start', b'start_action', ...]
```

**ì˜ˆì‹œ**:
```python
[
    b'episode_start',   # Step 0
    b'start_action',    # Step 1
    b'start_action',    # Step 2
    ...
]
```

**ìš©ë„**: 
- ì—í”¼ì†Œë“œ ì‹œì‘/ì¢…ë£Œ ê°ì§€
- ì•¡ì…˜ ì‹œí€€ìŠ¤ ë¶„í• 
- ë””ë²„ê¹… ë° ë¶„ì„

---

## ğŸ”§ ë°ì´í„° ë¡œë”© ì˜ˆì‹œ

### ê¸°ë³¸ ë¡œë”©

```python
import h5py
import numpy as np

def load_episode(h5_path):
    """ë‹¨ì¼ ì—í”¼ì†Œë“œ ë¡œë“œ"""
    with h5py.File(h5_path, 'r') as f:
        data = {
            'images': f['images'][:],          # (18, 720, 1280, 3)
            'actions': f['actions'][:],        # (18, 3)
            'event_types': f['action_event_types'][:]  # (18,)
        }
    return data

episode = load_episode('episode_xxx.h5')
```

### PyTorch Dataset

```python
import torch
from torch.utils.data import Dataset
from pathlib import Path

class MobileVLADataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.h5_files = sorted(Path(data_dir).glob('*.h5'))
        self.transform = transform
    
    def __len__(self):
        return len(self.h5_files)
    
    def __getitem__(self, idx):
        with h5py.File(self.h5_files[idx], 'r') as f:
            images = f['images'][:]    # (18, 720, 1280, 3)
            actions = f['actions'][:]  # (18, 3)
        
        # ì „ì²˜ë¦¬
        if self.transform:
            images = self.transform(images)
        
        return {
            'images': torch.from_numpy(images).float(),
            'actions': torch.from_numpy(actions[:, :2]).float()  # Gripper ì œì™¸
        }
```

---

## ğŸ“ ëª¨ë¸ Input/Output ì‚¬ì–‘

### ëª¨ë¸ ì…ë ¥ (ì¶”ë¡  ì‹œ)

```python
# Vision Input
images: torch.Tensor
  shape: (batch, T, 3, H, W)  # ì „ì²˜ë¦¬ í›„
  dtype: torch.float32
  range: [0.0, 1.0] or [-1.0, 1.0]  # ì •ê·œí™” ë°©ì‹ì— ë”°ë¼
  example: (8, 8, 3, 224, 224)  # Batch=8, Window=8, 224x224

# Language Input (ì¶”ê°€ ì˜ˆì •)
instruction: str or List[str]
  example: "Move the box to the left"
  
# Tokenized
input_ids: torch.Tensor
  shape: (batch, seq_len)
  dtype: torch.long
  example: (8, 64)
```

### ëª¨ë¸ ì¶œë ¥

```python
# Predicted Actions
pred_actions: torch.Tensor
  shape: (batch, chunk_size, action_dim)
  dtype: torch.float32
  range: [-1.15, 1.15]
  example: (8, 10, 2)  # Batch=8, Chunk=10, [linear_x, angular_z]

# ì‹¤ì œ ì ìš©
action = pred_actions[0, 0, :]  # ì²« ë°°ì¹˜, ì²« ì•¡ì…˜
linear_x = action[0].item()    # m/s
angular_z = action[1].item()   # rad/s

# ROS ëª…ë ¹ìœ¼ë¡œ ë³€í™˜
from geometry_msgs.msg import Twist
cmd = Twist()
cmd.linear.x = linear_x
cmd.angular.z = angular_z
```

---

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° (LoRA í•™ìŠµ ê¸°ì¤€)

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| **window_size** | 8 | ì…ë ¥ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ê¸¸ì´ |
| **action_chunk** | 10 | ì˜ˆì¸¡ ì•¡ì…˜ ì‹œí€€ìŠ¤ ê¸¸ì´ |
| **action_dim** | 2 | ì‹¤ì œ ì‚¬ìš© ì°¨ì› (linear_x, angular_z) |
| **image_size** | 224 | ì „ì²˜ë¦¬ í›„ í¬ê¸° (ì •ì‚¬ê°í˜•) |
| **batch_size** | 8-16 | í•™ìŠµ ë°°ì¹˜ í¬ê¸° |

### Window & Chunk ì„¤ëª…

```
Episode Timeline (18 steps):
â”œâ”€ Window 1 (0-7)   â†’ Predict Actions (0-9)
â”œâ”€ Window 2 (1-8)   â†’ Predict Actions (1-10)
â”œâ”€ Window 3 (2-9)   â†’ Predict Actions (2-11)
...

Input:  [img_t, img_t+1, ..., img_t+7]  (8 frames)
Output: [act_t, act_t+1, ..., act_t+9]  (10 actions)
```

---

## ğŸ“ ë°ì´í„° í˜•ì‹ ìš”ì•½í‘œ

| í•­ëª© | Key | Shape | Dtype | Range | ë¹„ê³  |
|------|-----|-------|-------|-------|------|
| **Video** | `images` | (18, 720, 1280, 3) | uint8 | [0, 255] | RGB, 720p |
| **Language** | `language_instruction` | (1,) | S256 (bytes) | - | âœ… UTF-8, ì‹¤ì œ íƒœìŠ¤í¬ |
| **Action - Linear** | `actions[:, 0]` | (18,) | float32 | [0.0, 1.15] | m/s |
| **Action - Angular** | `actions[:, 1]` | (18,) | float32 | [-1.15, 1.15] | rad/s |
| **Action - Gripper** | `actions[:, 2]` | (18,) | float32 | í•­ìƒ 0.0 | ë¯¸ì‚¬ìš© |
| **Event Types** | `action_event_types` | (18,) | object | - | ë©”íƒ€ë°ì´í„° |

---

## ğŸš¨ ì£¼ìš” ì œì•½ì‚¬í•­ ë° ì´ìŠˆ

### 1. Language Instruction ë¶€ì¬ âœ… **í•´ê²°ë¨**

**ìƒíƒœ**: âœ… **468ê°œ íŒŒì¼ì— ì¶”ê°€ ì™„ë£Œ**  
**ë‚´ìš©**: 
- ì‹¤ì œ íƒœìŠ¤í¬: "ì¥ì• ë¬¼ì„ í”¼í•´ ìŒë£Œìˆ˜ í˜íŠ¸ë³‘ ì•ìœ¼ë¡œ ë„ì°©í•´ë¼"
- ì˜ì–´ ë²ˆì—­: "Navigate around obstacles and reach the front of the beverage bottle"
- ë°©í–¥/ì‹œê°„ëŒ€ë³„ ë³€í˜• í¬í•¨

**ì‚¬ìš©ë²•**:
```python
with h5py.File(h5_path, 'r') as f:
    instruction = f['language_instruction'][0].decode('utf-8')
```

### 2. Gripper ì°¨ì› ë¯¸ì‚¬ìš© âš ï¸

**ë¬¸ì œ**: `actions[:, 2]`ê°€ í•­ìƒ 0.0  
**ì˜í–¥**: ëª¨ë¸ì´ 3D ì•¡ì…˜ì„ ì˜ˆì¸¡í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” 2Dë§Œ ì‚¬ìš©  
**í•´ê²°ì±…**:
```python
# í•™ìŠµ ì‹œ gripper ì°¨ì› ì œê±°
actions_2d = actions[:, :2]  # (T, 2)
```

### 3. ê³ ì • ì—í”¼ì†Œë“œ ê¸¸ì´ âš ï¸

**ë¬¸ì œ**: ëª¨ë“  ì—í”¼ì†Œë“œê°€ ì •í™•íˆ 18 ìŠ¤í…  
**ì˜í–¥**: 
- âœ… ë°°ì¹˜ ì²˜ë¦¬ ê°„ë‹¨
- âŒ ë‹¤ì–‘í•œ ê¸¸ì´ íƒœìŠ¤í¬ í•™ìŠµ ë¶ˆê°€  
**ê¶Œì¥**: ê°€ë³€ ê¸¸ì´ ì§€ì› ìœ„í•´ padding/masking ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ì½”ë“œ

### ì™„ì „í•œ ë°ì´í„° ë¡œë”

```python
import h5py
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import numpy as np
from typing import Dict, Tuple

class MobileVLAH5Dataset(Dataset):
    """Mobile-VLA H5 ë°ì´í„°ì…‹ ë¡œë”"""
    
    def __init__(
        self,
        data_dir: str,
        window_size: int = 8,
        action_chunk: int = 10,
        image_size: int = 224,
        use_gripper: bool = False
    ):
        self.h5_files = sorted(Path(data_dir).glob('*.h5'))
        self.window_size = window_size
        self.action_chunk = action_chunk
        self.image_size = image_size
        self.use_gripper = use_gripper
    
    def __len__(self) -> int:
        return len(self.h5_files)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        with h5py.File(self.h5_files[idx], 'r') as f:
            images = f['images'][:]    # (18, 720, 1280, 3)
            actions = f['actions'][:]  # (18, 3)
        
        # Resize images
        # (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” transforms ì‚¬ìš©)
        images_resized = self._resize_images(images)  # (18, 224, 224, 3)
        
        # Normalize to [0, 1]
        images_norm = images_resized.astype(np.float32) / 255.0
        
        # Action dimension ì„ íƒ
        if self.use_gripper:
            actions_used = actions  # (18, 3)
        else:
            actions_used = actions[:, :2]  # (18, 2)
        
        return {
            'images': torch.from_numpy(images_norm),  # (18, 224, 224, 3)
            'actions': torch.from_numpy(actions_used).float()  # (18, 2 or 3)
        }
    
    def _resize_images(self, images):
        """ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ì‹¤ì œë¡œëŠ” cv2 ë˜ëŠ” PIL ì‚¬ìš©)"""
        import cv2
        resized = np.zeros((len(images), self.image_size, self.image_size, 3), dtype=np.uint8)
        for i, img in enumerate(images):
            resized[i] = cv2.resize(img, (self.image_size, self.image_size))
        return resized

# ì‚¬ìš© ì˜ˆì‹œ
dataset = MobileVLAH5Dataset(
    data_dir='/Users/minu/dev/vla/ROS_action/mobile_vla_dataset',
    window_size=8,
    action_chunk=10,
    use_gripper=False  # Gripper ì œì™¸
)

dataloader = DataLoader(
    dataset,
    batch_size=8,
    shuffle=True,
    num_workers=4
)

for batch in dataloader:
    images = batch['images']  # (8, 18, 224, 224, 3)
    actions = batch['actions']  # (8, 18, 2)
    break
```

---

**ì‘ì„±**: Mobile-VLA Research Team  
**ì—…ë°ì´íŠ¸**: 2025-11-26  
**ë‹¤ìŒ ë‹¨ê³„**: Language instruction ì¶”ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
