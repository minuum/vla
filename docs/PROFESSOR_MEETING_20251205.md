# κµμλ‹ λ―Έν… (2025-12-05) - VLM Frozen vs LoRA λΉ„κµ μ‹¤ν—

**λ―Έν… μΌμ‹**: 2025-12-05 (λ©)  
**λ‹¤μ λ―Έν…**: 2025-12-11 (μ) 16:00  
**μ‘μ„±μΌ**: 2025-12-05 20:00

---

## π“‹ λ―Έν… λ‚΄μ© μ”μ•½

### ν•µμ‹¬ μ‹¤ν— μ μ•: VLM ν•™μµ λ°©λ²• λΉ„κµ

**λ©μ **: VLMμ„ Freeze ν•λ” κ²ƒ vs LoRAλ΅ Fine-tuning ν•λ” κ²ƒμ ν¨κ³Ό λΉ„κµ

---

## π― λ‘ κ°€μ§€ μ‹¤ν— λ°©λ²•

### λ°©λ²• 1: VLM LoRA + Action Head (Fine-tuning)
```
κµ¬μ„±:
  - VLM: LoRAλ΅ Fine-tuning (μΌλ¶€ ν•™μµ)
  - Action Head: μ²μλ¶€ν„° ν•™μµ
  
λ°μ΄ν„° μ”κµ¬μ‚¬ν•­:
  - λ§μ€ λ°μ΄ν„° ν•„μ”: 1,000~3,000 episodes
  - μ΄μ : VLM νλΌλ―Έν„°λ„ μΌλ¶€ μ—…λ°μ΄νΈλλ―€λ΅ λ” λ§μ€ λ°μ΄ν„° ν•„μ”
  
μ¶λ ¥:
  - Context Vector (μλ―Έ λ²΅ν„°)
  - Action predictions
```

### λ°©λ²• 2: VLM Frozen + Action Head (Current approach)
```
κµ¬μ„±:
  - VLM: Frozen (ν•™μµ μ• ν•¨)
  - Action Head: μ²μλ¶€ν„° ν•™μµ
  
λ°μ΄ν„° μ”κµ¬μ‚¬ν•­:
  - μ μ€ λ°μ΄ν„°λ΅ κ°€λ¥: 500~1,000 episodes
  - μ΄μ : Action Headλ§ ν•™μµ
  
μ¶λ ¥:
  - Context Vector (μλ―Έ λ²΅ν„°)
  - Action predictions
```

---

## π”¬ λΉ„κµ λ¶„μ„ κ³„ν

### 1. Context Vector (μλ―Έ λ²΅ν„°) λΉ„κµ

**μ¶”μ¶ μ„μΉ**:
```python
# VLM μ¶λ ¥ β†’ Action Head μ…λ ¥ μ‚¬μ΄
Context Vector = VLM.encode_images(images)  # Shape: (N, T, 64, 2048)

λ°©λ²• 1 (LoRA): context_lora
λ°©λ²• 2 (Frozen): context_frozen
```

**λΉ„κµ λ©”νΈλ¦­**:
1. **Cosine Similarity (μ½”μ‚¬μΈ μ μ‚¬λ„)**
   ```python
   similarity = cosine_similarity(context_lora, context_frozen)
   # 1μ— κ°€κΉμ°λ©΄ μ μ‚¬, 0μ— κ°€κΉμ°λ©΄ λ‹¤λ¦„
   ```

2. **Euclidean Distance (μ ν΄λ¦¬λ“ κ±°λ¦¬)**
   ```python
   distance = torch.norm(context_lora - context_frozen, p=2)
   # 0μ— κ°€κΉμ°λ©΄ μ μ‚¬
   ```

3. **KL Divergence (λ¶„ν¬ μ°¨μ΄)**
   ```python
   kl_div = F.kl_div(context_lora, context_frozen)
   # 0μ— κ°€κΉμ°λ©΄ λ¶„ν¬κ°€ μ μ‚¬
   ```

4. **Feature Correlation (νΉμ§• μƒκ΄€κ΄€κ³„)**
   ```python
   correlation = np.corrcoef(
       context_lora.mean(axis=0).flatten(),
       context_frozen.mean(axis=0).flatten()
   )[0, 1]
   ```

### 2. Latent Space λ§¤μΉ­ λ¶„μ„

**λ©μ **: Action Headμ latent spaceμ—μ„ λ‘ λ°©λ²•μ κ²°κ³Ό λΉ„κµ

```python
# Action Head λ‚΄λ¶€ LSTM hidden state μ¶”μ¶
λ°©λ²• 1: latent_lora = action_head.lstm.hidden_state
λ°©λ²• 2: latent_frozen = action_head.lstm.hidden_state

λΉ„κµ:
  - Latent space distribution
  - Activation patterns
  - Feature importance
```

---

## π“ μ‹¤ν— μ„¤κ³„

### Case 4: VLM LoRA + Action Head
```
VLM: Kosmos-2 (LoRA fine-tuning)
Training: LoRA (rank=8) + Action Head
Data: 1,000 episodes (500 left + 500 right) - μ¶”κ°€ μμ§‘ ν•„μ”
Epochs: 10
Learning Rate: 
  - VLM LoRA: 1e-5
  - Action Head: 1e-4

λΉ„κµ λ€μƒ: Case 3 (Frozen + Action Head)
```

### Case 3: VLM Frozen + Action Head (κΈ°μ΅΄)
```
VLM: Kosmos-2 (Frozen)
Training: Action Head only
Data: 500 episodes (250 left + 250 right) - λ³΄μ 
Epochs: 10
Learning Rate: 1e-4

ν„μ¬ μƒνƒ: β… ν•™μµ μ™„λ£
```

---

## π” κµμλ‹ μκ²¬

### λ°©λ²• 2 (Frozen) κ°€ μλ―Έ μμ„ κ²ƒ κ°™λ‹¤

**μ΄μ  (μ¶”μ •)**:
1. **λ°μ΄ν„° ν¨μ¨μ„±**
   - μ μ€ λ°μ΄ν„°λ΅λ„ ν•™μµ κ°€λ¥
   - Mobile-VLAλ” ν„μ¬ 500 episodesλ§ λ³΄μ 

2. **VLM Pretrain ν™μ©**
   - Kosmos-2κ°€ μ΄λ―Έ μΌλ°μ μΈ vision-language μ΄ν•΄ λ¥λ ¥ λ³΄μ 
   - Mobile taskμ— νΉν™”ν•μ§€ μ•μ•„λ„ λ  κ°€λ¥μ„±

3. **μΌλ°ν™” λ¥λ ¥**
   - Frozen VLMμ€ λ‹¤μ–‘ν• taskμ— μ¬μ‚¬μ© κ°€λ¥
   - Action Headλ§ κµμ²΄ν•λ©΄ λ‹¤λ¥Έ taskλ„ κ°€λ¥

4. **μ•μ •μ„±**
   - VLMμ„ freezeν•λ©΄ ν•™μµμ΄ μ•μ •μ 
   - Catastrophic forgetting λ°©μ§€

---

## π“ λ‹¤λ¥Έ λ…Όλ¬Έ μ‚¬λ΅€ μ΅°μ‚¬

### κ΄€λ ¨ λ…Όλ¬Έλ“¤

1. **RT-2 (Google DeepMind, 2023)**
   ```
   μ ‘κ·Ό: VLM Frozen + Action tokens
   λ°©λ²•: PaLI-X VLMμ„ Frozen, actionμ„ language tokenμΌλ΅ μ¶λ ¥
   κ²°κ³Ό: Frozen VLMλ„ robotic taskμ— ν¨κ³Όμ 
   ```

2. **OpenVLA (2024)**
   ```
   μ ‘κ·Ό: VLM Fine-tuning
   λ°©λ²•: DinoV2 + Llamaλ¥Ό μ „μ²΄ Fine-tuning
   λ°μ΄ν„°: 970K episodes (Open-X)
   κ²°κ³Ό: λ§μ€ λ°μ΄ν„°λ΅ Fine-tuning μ‹ μ„±λ¥ ν–¥μƒ
   ```

3. **RoboFlamingo (2023)**
   ```
   μ ‘κ·Ό: VLM Frozen + Few-shot
   λ°©λ²•: Flamingo VLM Frozen, Action Headλ§ ν•™μµ
   κ²°κ³Ό: Few-shotμΌλ΅λ„ μΆ‹μ€ μ„±λ¥
   ```

4. **PaLM-E (2023)**
   ```
   μ ‘κ·Ό: VLM Fine-tuning
   λ°©λ²•: PaLM + ViT μ „μ²΄ Fine-tuning
   λ°μ΄ν„°: λ€κ·λ¨ (μλ°±λ§)
   κ²°κ³Ό: λ§μ€ λ°μ΄ν„° ν•„μ”ν•μ§€λ§ μ„±λ¥ μ°μ
   ```

**ν¨ν„΄**:
- **Frozen**: λ°μ΄ν„° μ μ„ λ• ν¨κ³Όμ , μ•μ •μ 
- **Fine-tuning**: λ°μ΄ν„° λ§μ„ λ• μ„±λ¥ μ°μ, λ¶μ•μ •

---

## π― μ‹¤ν— κ³„ν (ν™κ° μ—†μ΄)

### Phase 1: ν„μ¬ μƒνƒ ν™•μΈ (μ¦‰μ‹ κ°€λ¥)
```
β… Case 3 (Frozen) ν•™μµ κ²°κ³Ό λ¶„μ„
  - Checkpoint: epoch_epoch=08-val_loss=val_loss=0.027.ckpt
  - Context vector μ¶”μ¶
  - Latent space λ¶„μ„
  
μ‹¤ν–‰:
  1. Context vector μ¶”μ¶ μ¤ν¬λ¦½νΈ μ‘μ„±
  2. Latent space visualization
  3. Baseline μ„±λ¥ μ •λ¦¬
```

### Phase 2: λ°μ΄ν„° μ¶”κ°€ μμ§‘ (ν•„μ”μ‹)
```
λ©ν‘: 1,000 episodes (Case 4μ©)
ν„μ¬: 500 episodes
μ¶”κ°€: 500 episodes (250 left + 250 right)

λ‚μ΄λ„ λ‹¤μ–‘ν™”:
  - Easy: 100L + 100R
  - Medium: 150L + 150R (κΈ°μ΅΄)
  - Hard: 100L + 100R

μμƒ μ†μ”: 1μ£ΌμΌ
```

### Phase 3: Case 4 (LoRA) ν•™μµ
```
Config μ‘μ„±:
  - VLM: LoRA (rank=8, alpha=16)
  - Action Head: LSTM
  - Data: 1,000 episodes

ν•™μµ μ‹¤ν–‰:
  - Epochs: 10
  - μμƒ μ‹κ°„: 6~8μ‹κ°„

λΉ„κµ λ¶„μ„:
  - vs Case 3 (Frozen)
  - Context vector μ°¨μ΄
  - Latent space μ°¨μ΄
  - μ„±λ¥ μ°¨μ΄
```

### Phase 4: λΉ„κµ λ¶„μ„ λ° μ‹κ°ν™”
```
1. Context Vector λΉ„κµ
   - Cosine similarity
   - Distribution plot
   - t-SNE visualization

2. Latent Space λΉ„κµ
   - LSTM hidden state μ¶”μ¶
   - Activation patterns
   - Feature importance

3. μ„±λ¥ λΉ„κµ
   - Val Loss
   - RMSE
   - Generalization (left/right)

4. λ…Όλ¬Έ μμ¤€ μ‹κ°ν™” μƒμ„±
```

---

## π“… νƒ€μ„λΌμΈ (μμ”μΌ λ―Έν…κΉμ§€)

### Day 1 (λ©, 12/5): κ³„ν μλ¦½ β…
```
β… λ―Έν… λ‚΄μ© μ •λ¦¬
β… μ‹¤ν— κ³„ν μλ¦½
β¬ Context vector μ¶”μ¶ μ¤ν¬λ¦½νΈ μ‘μ„±
```

### Day 2 (κΈ, 12/6): Baseline λ¶„μ„
```
β¬ Case 3 context vector μ¶”μ¶
β¬ Latent space visualization
β¬ Baseline μ„±λ¥ μ •λ¦¬
β¬ λ…Όλ¬Έ μ‚¬λ΅€ μ΅°μ‚¬ λ° μ •λ¦¬
```

### Day 3-4 (ν† -μΌ, 12/7-8): λ°μ΄ν„° μμ§‘ (μ„ νƒ)
```
β¬ μ¶”κ°€ 500 episodes μμ§‘ μ—¬λ¶€ κ²°μ •
β¬ μμ§‘ μ‹ λ‚μ΄λ„ λ‹¤μ–‘ν™”
```

### Day 5-6 (μ›”-ν™”, 12/9-10): Case 4 μ‹¤ν—
```
β¬ LoRA config μ‘μ„±
β¬ Case 4 ν•™μµ μ‹¤ν–‰
β¬ Context vector μ¶”μ¶
β¬ λΉ„κµ λ¶„μ„
```

### Day 7 (μ, 12/11): λ―Έν… μ¤€λΉ„
```
β¬ κ²°κ³Ό μ •λ¦¬
β¬ μ‹κ°ν™” μƒμ„±
β¬ λ°ν‘ μλ£ μ¤€λΉ„
```

---

## π“ μμƒ κ²°κ³Ό (κ°€μ„¤)

### κ°€μ„¤ 1: Context Vectorκ°€ μ μ‚¬ν•  κ²ƒ
```
Frozen vs LoRA context similarity > 0.8

μ΄μ :
  - κ°™μ€ Kosmos-2 backbone
  - Mobile taskκ°€ λΉ„κµμ  λ‹¨μ
  - LoRAκ°€ μΌλ¶€ νλΌλ―Έν„°λ§ μ΅°μ •
```

### κ°€μ„¤ 2: Latent Spaceλ” λ‹¤λ¥Ό κ²ƒ
```
Frozen vs LoRA latent space difference > 0.3

μ΄μ :
  - Action Headκ°€ λ‹¤λ¥΄κ² ν•™μµλ¨
  - Input distributionμ΄ μ•½κ°„ λ‹¤λ¦„
  - LSTMμ΄ λ‹¤λ¥Έ ν¨ν„΄ ν•™μµ
```

### κ°€μ„¤ 3: μ„±λ¥μ€ λΉ„μ·ν•  κ²ƒ
```
|Loss_frozen - Loss_lora| < 0.01

μ΄μ :
  - Mobile taskκ°€ λ‹¨μ
  - 500 vs 1000 episodes μ°¨μ΄
  - VLM frozenλ„ μ¶©λ¶„ν• context μ κ³µ
```

---

## π”¬ λΉ„κµ λ©”νΈλ¦­ μ •λ¦¬

### 1. Context Vector λ λ²¨
| λ©”νΈλ¦­ | μμ‹ | μλ―Έ | λ©ν‘ |
|:---|:---|:---|:---|
| Cosine Similarity | cos(ΞΈ) = (AΒ·B)/(β€–Aβ€–β€–Bβ€–) | λ°©ν–¥ μ μ‚¬λ„ | > 0.8 |
| Euclidean Distance | d = β€–A - Bβ€–β‚‚ | μ λ€ κ±°λ¦¬ | < 0.5 |
| Correlation | r = corr(A, B) | μ„ ν• κ΄€κ³„ | > 0.7 |

### 2. Latent Space λ λ²¨
| λ©”νΈλ¦­ | μΈ΅μ • λ€μƒ | μλ―Έ |
|:---|:---|:---|
| Hidden State Similarity | LSTM h_n | μ‹κ°„μ  ν‘ν„ λΉ„κµ |
| Activation Pattern | Layer-wise | ν•™μµλ ν¨ν„΄ λΉ„κµ |
| Feature Importance | Attention weights | μ¤‘μ” feature λΉ„κµ |

### 3. Performance λ λ²¨
| λ©”νΈλ¦­ | Case 3 (Frozen) | Case 4 (LoRA) | μ°¨μ΄ |
|:---|:---:|:---:|:---:|
| Val Loss | 0.027 | ??? | ??? |
| Train Loss | 0.0123 | ??? | ??? |
| RMSE | 0.170 | ??? | ??? |

---

## π“ μ°Έκ³  λ¬Έν— (μ¶”κ°€ μμ •)

1. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
2. OpenVLA: An Open-Source Vision-Language-Action Model
3. RoboFlamingo: Vision-Language Foundation Models for Robot Manipulation
4. PaLM-E: An Embodied Multimodal Language Model
5. LoRA: Low-Rank Adaptation of Large Language Models

---

## β… Action Items (μ°μ„ μμ„)

### Immediate (μ΄λ² μ£Ό)
```
1. β… λ―Έν… λ‚΄μ© μ •λ¦¬ λ° λ¬Έμ„ν™”
2. β¬ Context vector μ¶”μ¶ μ¤ν¬λ¦½νΈ μ‘μ„±
3. β¬ Case 3 baseline λ¶„μ„ μ™„λ£
4. β¬ λ…Όλ¬Έ μ‚¬λ΅€ μ΅°μ‚¬ λ° μ •λ¦¬
```

### Short-term (λ‹¤μ μ£Ό)
```
5. β¬ λ°μ΄ν„° μ¶”κ°€ μμ§‘ μ—¬λ¶€ κ²°μ •
6. β¬ Case 4 (LoRA) config μ‘μ„±
7. β¬ Case 4 ν•™μµ μ‹¤ν–‰
8. β¬ λΉ„κµ λ¶„μ„ λ° μ‹κ°ν™”
```

### Before Meeting (μμ”μΌ μ „)
```
9. β¬ κ²°κ³Ό μ •λ¦¬
10. β¬ λ°ν‘ μλ£ μ¤€λΉ„
11. β¬ μ¶”κ°€ μ‹¤ν— κ³„ν μ μ•
```

---

## π’΅ κµμλ‹κ» μ¶”κ°€ μ§λ¬Έ μ‚¬ν•­ (λ‹¤μ λ―Έν…)

1. **λ°μ΄ν„° κ·λ¨**
   - 500 vs 1,000 episodesλ΅ μ¶©λ¶„ν•κ°€?
   - μ¶”κ°€ μμ§‘ ν•„μ”μ„±?

2. **LoRA μ„¤μ •**
   - Rankλ” 8λ΅ μ¶©λ¶„ν•κ°€?
   - μ–΄λ–¤ layerλ¥Ό tuning ν•  κ²ƒμΈκ°€?

3. **λΉ„κµ κΈ°μ¤€**
   - Context vector μ μ‚¬λ„ threshold?
   - μ–΄λ μ •λ„ μ°¨μ΄κ°€ μλ―Έ μλ”κ°€?

4. **ν›„μ† μ—°κµ¬**
   - λ‘ λ°©λ²•μ μ¥λ‹¨μ  λ¶„μ„ ν›„ λ°©ν–¥?
   - λ…Όλ¬Έ μ‘μ„± μ‹ μ–΄λ κ²°κ³Όλ¥Ό μ‚¬μ©?

---

**λ‹¤μ λ―Έν…**: 2025-12-11 (μ) 16:00  
**μ¤€λΉ„ μ‚¬ν•­**: Case 3 λ¶„μ„ μ™„λ£, Case 4 μ§„ν–‰ μƒν™©, λΉ„κµ κ²°κ³Ό (κ°€λ¥ν•λ©΄)
