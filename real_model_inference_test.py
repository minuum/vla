#!/usr/bin/env python3
"""
ì‹¤ì œ Mobile VLA ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
Jetson í™˜ê²½ì—ì„œ CUDA ì§€ì› PyTorchë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
from PIL import Image
import time
import os
import sys
from typing import Optional, Tuple, Dict, Any
import json

# Mobile VLA ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼)
class SimpleCLIPLSTMModel(nn.Module):
    """Simple CLIP + LSTM ëª¨ë¸ (ì‹¤ì œ í•™ìŠµ ì½”ë“œì™€ ë™ì¼)"""
    
    def __init__(self, 
                 vision_dim: int = 2048,
                 text_dim: int = 2048,
                 hidden_dim: int = 4096,
                 action_dim: int = 2,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.action_dim = action_dim
        self.num_layers = num_layers
        
        # Vision Encoder (CLIP features)
        self.vision_encoder = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # Text Encoder (Kosmos2 features)
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # LSTM Layer
        self.lstm = nn.LSTM(
            input_size=hidden_dim * 2,  # vision + text
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Action Predictor
        self.action_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 4, action_dim)
        )
        
    def forward(self, vision_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        batch_size = vision_features.size(0)
        
        # Vision encoding
        vision_encoded = self.vision_encoder(vision_features)  # (batch_size, hidden_dim)
        
        # Text encoding
        text_encoded = self.text_encoder(text_features)  # (batch_size, hidden_dim)
        
        # Concatenate vision and text features
        combined_features = torch.cat([vision_encoded, text_encoded], dim=-1)  # (batch_size, hidden_dim * 2)
        
        # Reshape for LSTM (sequence length = 1)
        combined_features = combined_features.unsqueeze(1)  # (batch_size, 1, hidden_dim * 2)
        
        # LSTM processing
        lstm_out, _ = self.lstm(combined_features)  # (batch_size, 1, hidden_dim)
        
        # Get the last output
        lstm_out = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
        
        # Action prediction
        actions = self.action_predictor(lstm_out)  # (batch_size, action_dim)
        
        return actions

class MobileVLAModelLoader:
    """Mobile VLA ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, model_dir: str = "./Robo+/Mobile_VLA"):
        self.model_dir = model_dir
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        if torch.cuda.is_available():
            print(f"ğŸ® CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
    def load_model(self, checkpoint_path: Optional[str] = None) -> SimpleCLIPLSTMModel:
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸš€ Mobile VLA ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.model_dir}")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ íƒì§€
        if checkpoint_path is None:
            checkpoint_path = self._find_best_checkpoint()
        
        if checkpoint_path is None:
            print("âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._list_available_checkpoints()
            return None
        
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = SimpleCLIPLSTMModel()
            self.model = self.model.to(self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            print("ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            else:
                # ì²´í¬í¬ì¸íŠ¸ê°€ ëª¨ë¸ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                self.model.load_state_dict(checkpoint)
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (ì§ì ‘ ë”•ì…”ë„ˆë¦¬)")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print_model_info(checkpoint)
            
            print("âœ… Mobile VLA ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _find_best_checkpoint(self) -> Optional[str]:
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€"""
        possible_paths = [
            f"{self.model_dir}/simple_clip_lstm_model/best_simple_clip_lstm_model.pth",
            f"{self.model_dir}/results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth",
            "./mobile-vla-omniwheel/best_simple_lstm_model.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def _list_available_checkpoints(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
        os.system('find . -name "*.pth" -type f | head -10')
    
    def _print_model_info(self, checkpoint: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
        
        if 'epoch' in checkpoint:
            print(f"ğŸ“ˆ í›ˆë ¨ ì—í¬í¬: {checkpoint['epoch']}")
        if 'loss' in checkpoint:
            print(f"ğŸ“‰ ì†ì‹¤ê°’: {checkpoint['loss']:.4f}")
        if 'val_mae' in checkpoint:
            print(f"ğŸ“Š ê²€ì¦ MAE: {checkpoint['val_mae']:.4f}")

def test_model_inference():
    """ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ§  ì‹¤ì œ Mobile VLA ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = MobileVLAModelLoader()
    
    # ëª¨ë¸ ë¡œë“œ
    model = loader.load_model()
    if model is None:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return False
    
    print("\n" + "=" * 40)
    print("ğŸ”¬ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ íŠ¹ì§• ë²¡í„° í¬ê¸°)
    batch_size = 1
    vision_dim = 2048
    text_dim = 2048
    
    # ëœë¤ íŠ¹ì§• ë²¡í„° ìƒì„± (ì‹¤ì œ CLIP/Kosmos2 ì¶œë ¥ê³¼ ìœ ì‚¬)
    vision_features = torch.randn(batch_size, vision_dim).to(loader.device)
    text_features = torch.randn(batch_size, text_dim).to(loader.device)
    
    print(f"ğŸ“¥ Vision íŠ¹ì§• í¬ê¸°: {vision_features.shape}")
    print(f"ğŸ“¥ Text íŠ¹ì§• í¬ê¸°: {text_features.shape}")
    
    # ì›Œë°ì—…
    print("ğŸ”¥ ì›Œë°ì—… ì¤‘...")
    with torch.no_grad():
        for _ in range(10):
            _ = model(vision_features, text_features)
    
    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    num_runs = 100
    times = []
    
    print(f"â±ï¸ {num_runs}íšŒ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    with torch.no_grad():
        for i in range(num_runs):
            start_time = time.time()
            output = model(vision_features, text_features)
            end_time = time.time()
            times.append(end_time - start_time)
    
    # ê²°ê³¼ ë¶„ì„
    avg_time = np.mean(times)
    min_time = np.min(times)
    max_time = np.max(times)
    fps = 1.0 / avg_time
    
    print(f"ğŸ“¤ ì¶œë ¥ í¬ê¸°: {output.shape}")
    print(f"â±ï¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.2f} ms")
    print(f"âš¡ ìµœì†Œ ì¶”ë¡  ì‹œê°„: {min_time*1000:.2f} ms")
    print(f"ğŸŒ ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {max_time*1000:.2f} ms")
    print(f"ğŸš€ ì¶”ë¡  FPS: {fps:.1f}")
    
    # ì•¡ì…˜ ê°’ ì¶œë ¥
    print(f"ğŸ¯ ì˜ˆì¸¡ ì•¡ì…˜: {output.cpu().numpy()}")
    
    return True

def test_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 40)
    print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    if torch.cuda.is_available():
        # ì´ˆê¸° ë©”ëª¨ë¦¬
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated(0)
        print(f"ğŸ”§ ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory / 1024**2:.1f} MB")
        
        # ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬
        loader = MobileVLAModelLoader()
        model = loader.load_model()
        
        if model is not None:
            model_memory = torch.cuda.memory_allocated(0)
            print(f"ğŸ§  ëª¨ë¸ ë©”ëª¨ë¦¬: {model_memory / 1024**2:.1f} MB")
            print(f"ğŸ“Š ëª¨ë¸ ë©”ëª¨ë¦¬ ì¦ê°€: {(model_memory - initial_memory) / 1024**2:.1f} MB")
            
            # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬
            vision_features = torch.randn(1, 2048).to(loader.device)
            text_features = torch.randn(1, 2048).to(loader.device)
            
            with torch.no_grad():
                _ = model(vision_features, text_features)
            
            inference_memory = torch.cuda.memory_allocated(0)
            print(f"ğŸ”¬ ì¶”ë¡  ë©”ëª¨ë¦¬: {inference_memory / 1024**2:.1f} MB")
            print(f"ğŸ“ˆ ì¶”ë¡  ë©”ëª¨ë¦¬ ì¦ê°€: {(inference_memory - model_memory) / 1024**2:.1f} MB")
            
            # ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            total_memory = torch.cuda.get_device_properties(0).total_memory
            used_memory = inference_memory
            memory_usage_percent = (used_memory / total_memory) * 100
            
            print(f"ğŸ’¾ ì´ ë©”ëª¨ë¦¬: {total_memory / 1024**2:.1f} MB")
            print(f"ğŸ“Š ì‚¬ìš©ë¥ : {memory_usage_percent:.1f}%")
            
            return True
    else:
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì‹¤ì œ Mobile VLA ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    inference_ok = test_model_inference()
    
    # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
    memory_ok = test_memory_usage()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    results = {
        "ëª¨ë¸ ì¶”ë¡ ": "âœ…" if inference_ok else "âŒ",
        "ë©”ëª¨ë¦¬ ì‚¬ìš©": "âœ…" if memory_ok else "âŒ"
    }
    
    for test_name, result in results.items():
        print(f"{test_name}: {result}")
    
    all_passed = all([inference_ok, memory_ok])
    
    if all_passed:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì‹¤ì œ Mobile VLA ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ì‹¤ì œ ì´ë¯¸ì§€ ì…ë ¥ ì²˜ë¦¬")
        print("   2. ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì—°ë™")
        print("   3. ì‹¤ì‹œê°„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬ì„±")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ëª¨ë¸ ë¡œë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
