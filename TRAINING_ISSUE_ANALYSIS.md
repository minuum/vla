# LoRA Fine-tuning λ¬Έμ  λ¶„μ„

## π”΄ λ°μƒν• μ¤λ¥

```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

## π“ ν•™μµ μ¶”μ΄

### μ„±κ³µν• λ‹¨κ³„:
1. β… λ¨λΈ λ΅λ“ μ„±κ³µ
2. β… LoRA μ μ© (57.01M trainable params)
3. β… λ°μ΄ν„°μ…‹ λ΅λ“ μ„±κ³µ
4. β… Sanity Check ν†µκ³Ό
5. β… Training μ‹μ‘

### μ‹¤ν¨ν• λ‹¨κ³„:
- β **Epoch 0, Step 0**: Backward pass μ‹¤ν¨
- β **μ›μΈ**: Gradient κ³„μ‚° λ¶κ°€

## π” λ¬Έμ  μ›μΈ λ¶„μ„

### 1. νλΌλ―Έν„° μƒνƒ
```
Trainable params: 57.01M (LoRA + LSTM)
Non-trainable params: 1.7B (Frozen Backbone)
Total params: 1.7B
```

### 2. κ°€λ¥ν• μ›μΈλ“¤

#### A. LoRA νλΌλ―Έν„°κ°€ μ‹¤μ λ΅ requires_grad=False
- LoRA μ μ©μ€ λμ—μ§€λ§, `requires_grad` ν”λκ·Έκ°€ μ λ€λ΅ μ„¤μ •λμ§€ μ•μ
- `get_peft_model` ν›„ νλΌλ―Έν„° μƒνƒ ν™•μΈ ν•„μ”

#### B. Lossκ°€ frozen νλΌλ―Έν„°λ§ μ‚¬μ©
- Loss κ³„μ‚° μ‹ LoRA νλΌλ―Έν„°λ¥Ό κ±°μΉμ§€ μ•μ
- Forward passμ—μ„ LoRA λ μ΄μ–΄κ°€ bypassλ¨

#### C. LSTM Policy Head λ¬Έμ 
- LSTM headκ°€ μ λ€λ΅ μ΄κΈ°ν™”λμ§€ μ•μ
- LSTMμ gradientκ°€ backboneμΌλ΅ μ „νλμ§€ μ•μ

#### D. Mixed Precision λ¬Έμ 
- FP16 μ‚¬μ© μ‹ gradient underflow
- Autocast μ„¤μ • λ¬Έμ 

## π”§ ν•΄κ²° λ°©μ•

### λ°©μ• 1: LoRA νλΌλ―Έν„° ν™•μΈ λ° μμ •

```python
# base_backbone.pyμ _trainable_params_setup μμ •
if self.train_setup_configs["lora_enable"]:
    from robovlms.utils.lora_utils import find_all_linear_names
    from peft import LoraConfig, get_peft_model
    
    lora_config = LoraConfig(...)
    self.model = get_peft_model(model, lora_config)
    
    # LoRA νλΌλ―Έν„° λ…μ‹μ μΌλ΅ requires_grad=True μ„¤μ •
    for name, param in self.model.named_parameters():
        if 'lora' in name.lower():
            param.requires_grad = True
            print(f"LoRA param: {name}, requires_grad={param.requires_grad}")
```

### λ°©μ• 2: LSTM Policy Head ν™•μΈ

```python
# LSTM headκ°€ ν•™μµ κ°€λ¥ν•μ§€ ν™•μΈ
for name, param in self.act_head.named_parameters():
    param.requires_grad = True
    print(f"LSTM param: {name}, requires_grad={param.requires_grad}")
```

### λ°©μ• 3: Gradient Checkpointing λΉ„ν™μ„±ν™”

```json
{
  "train_setup": {
    "gradient_checkpointing": false  // μ΄λ―Έ false
  }
}
```

### λ°©μ• 4: Precision λ³€κ²½

```json
{
  "trainer": {
    "precision": "32"  // FP16 -> FP32λ΅ λ³€κ²½
  }
}
```

## π“ λ””λ²„κΉ… λ‹¨κ³„

### 1λ‹¨κ³„: νλΌλ―Έν„° μƒνƒ ν™•μΈ
```bash
# main.pyμ— λ””λ²„κΉ… μ½”λ“ μ¶”κ°€
for name, param in model.named_parameters():
    if param.requires_grad:
        print(f"Trainable: {name}, shape={param.shape}")
```

### 2λ‹¨κ³„: Forward Pass ν™•μΈ
```python
# Lossκ°€ κ³„μ‚°λλ”μ§€ ν™•μΈ
print(f"Loss: {loss}, requires_grad={loss.requires_grad}")
```

### 3λ‹¨κ³„: Backward Pass ν™•μΈ
```python
# Gradientκ°€ κ³„μ‚°λλ”μ§€ ν™•μΈ
loss.backward()
for name, param in model.named_parameters():
    if param.requires_grad and param.grad is not None:
        print(f"Gradient: {name}, grad_norm={param.grad.norm()}")
```

## π― μ¦‰μ‹ μ‹λ„ν•  ν•΄κ²°μ±…

### μ°μ„ μμ„ 1: LoRA νλΌλ―Έν„° λ…μ‹μ  ν™μ„±ν™”
RoboVLMs upstream μ½”λ“μ—μ„ LoRA μ μ© ν›„ νλΌλ―Έν„° μƒνƒλ¥Ό λ…μ‹μ μΌλ΅ μ„¤μ •

### μ°μ„ μμ„ 2: FP32λ΅ λ³€κ²½
Mixed precision λ¬Έμ  κ°€λ¥μ„± λ°°μ 

### μ°μ„ μμ„ 3: κ°„λ‹¨ν• ν…μ¤νΈ
- λ°°μΉ ν¬κΈ° 1λ΅ μ¤„μ—¬μ„ ν…μ¤νΈ
- Window size μ¤„μ—¬μ„ ν…μ¤νΈ
- Action chunk size μ¤„μ—¬μ„ ν…μ¤νΈ

## π“ μ°Έκ³ μ‚¬ν•­

### RoboVLMs μ›λ³Έ μ„¤μ •
```json
{
  "lora_enable": true,
  "lora_r": 8,  // μ°λ¦¬λ” 32
  "lora_alpha": 16,  // λ™μΌ
  "freeze_backbone": true,  // λ™μΌ
  "train_vision": false  // λ™μΌ
}
```

### μ°¨μ΄μ 
- **lora_r**: 8 β†’ 32 (λ” ν° rank)
- **action_dim**: 7 β†’ 2 (μ°λ¦¬λ” 2D, ν¨λ”©μΌλ΅ 7D)

## π€ λ‹¤μ μ•΅μ…

1. **μ¦‰μ‹**: LoRA νλΌλ―Έν„° requires_grad ν™•μΈ
2. **λ‹¤μ**: FP32λ΅ λ³€κ²½ν•μ—¬ μ¬μ‹λ„
3. **λ§μ§€λ§‰**: RoboVLMs μ›λ³Έ μ„¤μ •μΌλ΅ λλλ ¤μ„ ν…μ¤νΈ

---

**μ‘μ„±**: 2025-11-06 16:30
**μƒνƒ**: λ¬Έμ  λ¶„μ„ μ™„λ£, ν•΄κ²° λ°©μ• μλ¦½

