# ğŸ”„ RoboVLMs â†’ Mobile VLA ë³€ê²½/ìœ ì§€/ì „ì´ ìƒì„¸ ë¶„ì„

## ğŸ¯ ë³€ê²½ í•„ìš” ë¶€ë¶„ (CHANGE)

### 1. ğŸ“Š ì•¡ì…˜ ê³µê°„ ì™„ì „ ì¬ì„¤ê³„
#### âŒ ì œê±°í•  ê²ƒ
```python
# RoboVLMs 7D ì•¡ì…˜ ê³µê°„
action_space = {
    "arm": [x, y, z, roll, pitch, yaw],  # 6DOF ë¡œë´‡ íŒ”
    "gripper": [open/close]               # ê·¸ë¦¬í¼ ì œì–´
}
```

#### âœ… ìƒˆë¡œ êµ¬í˜„í•  ê²ƒ
```python
# Mobile VLA 4D ì•¡ì…˜ ê³µê°„
action_space = {
    "linear_x": [-2.0, 2.0],     # ì „ì§„/í›„ì§„ ì†ë„ (m/s)
    "linear_y": [-1.0, 1.0],     # ì¢Œìš° ì´ë™ ì†ë„ (m/s)  
    "angular_z": [-3.14, 3.14],  # íšŒì „ ì†ë„ (rad/s)
    "action_type": [0, 1, 2, 3]  # 0:ì´ë™, 1:íšŒì „, 2:ì •ì§€, 3:íŠ¹ìˆ˜
}
```

### 2. ğŸ—ƒï¸ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬êµ¬ì„±
#### âŒ ì œê±°í•  ê²ƒ
```python
# Calvin/Bridge ë°ì´í„° ë¡œë”
class CalvinDataset:
    def __getitem__(self, idx):
        return {
            "rgb": self.episodes[idx]["rgb"],           # [T, H, W, 3]
            "action": self.episodes[idx]["action"],     # [T, 7] 
            "language": self.episodes[idx]["language"]  # "pick up the cube"
        }
```

#### âœ… ìƒˆë¡œ êµ¬í˜„í•  ê²ƒ  
```python
# Mobile VLA ë°ì´í„° ë¡œë” (mobile_vla_data_collector ê¸°ë°˜)
class MobileNavigationDataset:
    def __getitem__(self, idx):
        return {
            "images": self.episodes[idx]["images"],                    # [T, H, W, 3]
            "actions": self.episodes[idx]["actions"],                  # [T, 4]
            "action_event_types": self.episodes[idx]["action_event_types"], # [T]
            "scenario": self.episodes[idx]["scenario"],                # "1box_vert_left" 
            "language": self.korean_instructions[scenario]             # "ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”"
        }
```

### 3. ğŸ§  Policy Head ì™„ì „ ì¬ì‘ì„±
#### âŒ ì œê±°í•  ê²ƒ
```python
# RoboVLMs BasePolicyHead
class BasePolicyHead(nn.Module):
    def __init__(self, hidden_size):
        self.arm_head = MLPTanhHead(hidden_size, 6)      # 6DOF arm actions
        self.gripper_head = MLPSigmoidHead(hidden_size, 1) # gripper actions
        
    def forward(self, features):
        arm_actions = self.arm_head(features)        # [-1, 1]^6
        gripper_actions = self.gripper_head(features) # [0, 1]
        return torch.cat([arm_actions, gripper_actions], dim=-1)
```

#### âœ… ìƒˆë¡œ êµ¬í˜„í•  ê²ƒ
```python
# Mobile VLA MobilePolicyHead  
class MobilePolicyHead(nn.Module):
    def __init__(self, hidden_size):
        self.movement_head = MLPTanhHead(hidden_size, 3)    # linear_x, linear_y, angular_z
        self.action_type_head = MLPHead(hidden_size, 4)     # action type classification
        
    def forward(self, features):
        movement_actions = self.movement_head(features)     # [-1, 1]^3 â†’ scale to actual bounds
        action_types = self.action_type_head(features)      # [4] logits
        return {
            "movement": movement_actions,
            "action_type": action_types
        }
```

### 4. ğŸ“ˆ ì†ì‹¤ í•¨ìˆ˜ ì¬ì„¤ê³„
#### âŒ ì œê±°í•  ê²ƒ
```python
# RoboVLMs ì†ì‹¤ í•¨ìˆ˜
def get_loss(self, prediction):
    loss_arm_act = F.mse_loss(pred_arm, target_arm)
    loss_gripper_act = F.cross_entropy(pred_gripper, target_gripper)
    return loss_arm_act + self.arm_gripper_loss_ratio * loss_gripper_act
```

#### âœ… ìƒˆë¡œ êµ¬í˜„í•  ê²ƒ
```python
# Mobile VLA ì†ì‹¤ í•¨ìˆ˜
def get_mobile_loss(self, prediction):
    # ì—°ì† ì•¡ì…˜ ì†ì‹¤ (movement)
    movement_loss = F.mse_loss(pred_movement, target_movement)
    
    # ì•¡ì…˜ íƒ€ì… ë¶„ë¥˜ ì†ì‹¤
    type_loss = F.cross_entropy(pred_action_type, target_action_type)
    
    # ì‹œë‚˜ë¦¬ì˜¤ ì¼ê´€ì„± ì†ì‹¤ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    scenario_consistency_loss = self.compute_scenario_consistency(
        pred_actions, scenario_context
    )
    
    return movement_loss + 0.5 * type_loss + 0.1 * scenario_consistency_loss
```

---

## âœ… ìœ ì§€í•  ë¶€ë¶„ (KEEP)

### 1. ğŸ—ï¸ ì „ì²´ í•™ìŠµ í”„ë ˆì„ì›Œí¬
#### âœ… ìœ ì§€í•  ì´ìœ 
```python
# PyTorch Lightning ê¸°ë°˜ BaseTrainer êµ¬ì¡°ëŠ” ë§¤ìš° ì•ˆì •ì 
class BaseTrainer(pl.LightningModule):
    def configure_optimizers(self):      # âœ… ìœ ì§€
    def training_step(self, batch):      # âœ… ìœ ì§€  
    def validation_step(self, batch):    # âœ… ìœ ì§€
    def _get_loss(self, prediction):     # ğŸ”„ ë‚´ìš©ë§Œ ìˆ˜ì •, êµ¬ì¡° ìœ ì§€
```

### 2. ğŸ¤– VLM ë°±ë³¸ ì•„í‚¤í…ì²˜
#### âœ… ìœ ì§€í•  ì´ìœ 
```python
# PaliGemma, LLaVA, Kosmos ë“± ë°±ë³¸ì€ ê°•ë ¥í•œ ì‹œê°-ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ ë³´ìœ 
# ë‹¨ì§€ ì¶œë ¥ í—¤ë“œë§Œ mobile ìš©ìœ¼ë¡œ êµì²´
class RoboPaliGemma:
    def __init__(self):
        self.vision_encoder = ...        # âœ… ì™„ì „ ìœ ì§€
        self.language_model = ...        # âœ… ì™„ì „ ìœ ì§€  
        self.vision_resampler = ...      # âœ… ì™„ì „ ìœ ì§€
        # self.policy_head = BasePolicyHead()  âŒ êµì²´
        self.policy_head = MobilePolicyHead() # âœ… ìƒˆë¡œ ì—°ê²°
```

### 3. ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°  
#### âœ… ìœ ì§€í•  ì´ìœ 
```python
# ì´ë¯¸ì§€ ì²˜ë¦¬, ì‹œí€€ìŠ¤ íŒ¨ë”© ë“± ê¸°ë³¸ ê¸°ëŠ¥ì€ ë²”ìš©ì 
from robovlms.data.data_utils import (
    pad_sequences,           # âœ… ìœ ì§€ - ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶¤
    normalize_action,        # ğŸ”„ ìˆ˜ì • - 4D ì•¡ì…˜ìš©ìœ¼ë¡œ ì ì‘
    get_tensor_chunk,        # âœ… ìœ ì§€ - ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
    mu_law_companding       # âœ… ìœ ì§€ - ì•¡ì…˜ ì••ì¶•
)
```

### 4. ğŸ”§ í•™ìŠµ ìµœì í™” ë¡œì§
#### âœ… ìœ ì§€í•  ì´ìœ 
```python
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§, ì˜µí‹°ë§ˆì´ì € ì„¤ì • ë“±ì€ ê²€ì¦ëœ ë°©ë²•
def configure_optimizers(self):
    optimizer = torch.optim.AdamW(self.get_grouped_params(self.model), lr=eff_lr) # âœ… ìœ ì§€
    scheduler = get_cosine_schedule_with_warmup(...)  # âœ… ìœ ì§€
    return {"optimizer": optimizer, "lr_scheduler": scheduler}  # âœ… ìœ ì§€
```

---

## ğŸ”„ ì „ì´í•  ë¶€ë¶„ (TRANSFER)

### 1. ğŸ’¡ Calvin Sequential Task â†’ Mobile Navigation Scenarios
#### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ ì „ì´
```python
# Calvin: "pick and place" ì‹œí€€ì…œ íƒœìŠ¤í¬
calvin_sequence = [
    "pick up the blue block",      # Task 1
    "place it on the red plate",   # Task 2  
    "slide the drawer open"        # Task 3
]

# Mobile VLA: ì‹œë‚˜ë¦¬ì˜¤ë³„ ë„¤ë¹„ê²Œì´ì…˜ ì‹œí€€ìŠ¤
mobile_sequence = [
    "1box_vert_left",              # ì‹œë‚˜ë¦¬ì˜¤ 1: ì™¼ìª½ ìš°íšŒ ê²½ë¡œ
    "approach_obstacle",           # Task 1: ì¥ì• ë¬¼ ì ‘ê·¼
    "avoid_left",                  # Task 2: ì™¼ìª½ìœ¼ë¡œ íšŒí”¼
    "reach_target"                 # Task 3: ëª©í‘œ ë„ë‹¬
]
```

#### ğŸ”„ ì „ì´ ë°©ë²•
```python
# Calvinì˜ ì‹œí€€ì…œ ì„±ê³µë¥  í‰ê°€ â†’ Mobile Navigation ì‹œë‚˜ë¦¬ì˜¤ ì„±ê³µë¥ 
class SequentialNavigationEvaluator:
    def evaluate_scenario_sequence(self, model, scenarios):
        success_rates = {}
        for scenario in ["1box_vert_left", "1box_vert_right", ...]:
            success_rate = self.test_scenario(model, scenario)
            success_rates[scenario] = success_rate
        return success_rates
```

### 2. ğŸ§  Vision-Language Understanding â†’ Spatial-Language Navigation  
#### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ ì „ì´
```python
# RoboVLMs: ì´ë¯¸ì§€ + ì¡°ì‘ ëª…ë ¹ ì´í•´
robovlm_input = {
    "image": camera_rgb,
    "instruction": "pick up the red block on the table"
}

# Mobile VLA: ì´ë¯¸ì§€ + ë„¤ë¹„ê²Œì´ì…˜ ëª…ë ¹ ì´í•´  
mobile_vla_input = {
    "image": camera_rgb,
    "instruction": "ì™¼ìª½ ê²½ë¡œë¡œ ëŒì•„ì„œ ë¹¨ê°„ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "scenario_context": "1box_vert_left"  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
}
```

#### ğŸ”„ ì „ì´ ë°©ë²•
```python
# ê¸°ì¡´ VLMì˜ ì‹œê°-ì–¸ì–´ ìœµí•© ë©”ì»¤ë‹ˆì¦˜ í™œìš©
class SpatialLanguageEncoder:
    def __init__(self, vlm_backbone):
        self.vision_encoder = vlm_backbone.vision_encoder      # âœ… ì§ì ‘ ì „ì´
        self.language_encoder = vlm_backbone.language_encoder  # âœ… ì§ì ‘ ì „ì´
        self.spatial_fusion = MultiheadAttention(...)          # ğŸ”„ ê³µê°„ ì´í•´ ê°•í™”
        
    def encode_spatial_instruction(self, image, instruction, scenario):
        vision_features = self.vision_encoder(image)           # âœ… ê¸°ì¡´ ë°©ì‹
        language_features = self.language_encoder(instruction) # âœ… ê¸°ì¡´ ë°©ì‹
        spatial_context = self.encode_scenario(scenario)       # ğŸ†• ìƒˆë¡œìš´ ê¸°ëŠ¥
        return self.spatial_fusion(vision_features, language_features, spatial_context)
```

### 3. ğŸ“Š Action Chunking â†’ Mobile Action Sequences
#### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ ì „ì´
```python
# RoboVLMs: ì¡°ì‘ ì•¡ì…˜ ì²­í‚¹
manipulation_chunk = [
    [0.1, 0.0, -0.2, 0.0, 0.0, 0.0, 0],  # approach
    [0.0, 0.0, -0.1, 0.0, 0.0, 0.0, 1],  # grasp
    [0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 1],   # lift
]

# Mobile VLA: ë„¤ë¹„ê²Œì´ì…˜ ì•¡ì…˜ ì‹œí€€ìŠ¤  
navigation_chunk = [
    [1.0, 0.0, 0.0, 0],    # forward
    [0.0, 0.0, 1.57, 1],   # turn_left  
    [1.0, 0.0, 0.0, 0],    # forward
    [0.0, 0.0, 0.0, 2]     # stop
]
```

#### ğŸ”„ ì „ì´ ë°©ë²•
```python
# ì•¡ì…˜ ì²­í‚¹ ë¡œì§ ì¬ì‚¬ìš©, ì•¡ì…˜ ê³µê°„ë§Œ ë³€ê²½
class MobileActionChunker:
    def __init__(self, chunk_size=8):
        self.chunk_size = chunk_size  # âœ… ê¸°ì¡´ ì²­í‚¹ ì‚¬ì´ì¦ˆ ìœ ì§€
        
    def create_action_chunk(self, current_obs, target_scenario):
        # ğŸ”„ 4D ì•¡ì…˜ìœ¼ë¡œ ì²­í‚¹ ë¡œì§ ì ìš©
        action_sequence = self.predict_action_sequence(current_obs, target_scenario)
        return action_sequence[:self.chunk_size]  # âœ… ê¸°ì¡´ ì²­í‚¹ ë°©ì‹ ìœ ì§€
```

### 4. ğŸ“ˆ Multi-Task Learning â†’ Multi-Scenario Learning
#### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´ ì „ì´
```python
# RoboVLMs: ë‹¤ì¤‘ íƒœìŠ¤í¬ í•™ìŠµ (pick, place, push, etc.)
multi_task_learning = {
    "pick_task_weight": 1.0,
    "place_task_weight": 1.0, 
    "push_task_weight": 0.8,
    "slide_task_weight": 0.6
}

# Mobile VLA: ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ í•™ìŠµ (8ê°€ì§€ ì»µ ë„ë‹¬ ì‹œë‚˜ë¦¬ì˜¤)
multi_scenario_learning = {
    "1box_vert_left_weight": 1.0,
    "1box_vert_right_weight": 1.0,
    "1box_hori_left_weight": 1.2,    # ë” ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
    "1box_hori_right_weight": 1.1,
    "2box_vert_left_weight": 1.5,    # ê°€ì¥ ì–´ë ¤ìš´ ì‹œë‚˜ë¦¬ì˜¤
    "2box_vert_right_weight": 1.4,
    "2box_hori_left_weight": 1.8,
    "2box_hori_right_weight": 1.6
}
```

#### ğŸ”„ ì „ì´ ë°©ë²•
```python
# ê¸°ì¡´ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ì ìš©
class MultiScenarioTrainer(BaseTrainer):
    def __init__(self, configs):
        super().__init__(configs)
        # âœ… ê¸°ì¡´ ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ë¡œì§ ì¬ì‚¬ìš©
        self.scenario_weights = configs["scenario_weights"]
        
    def training_step(self, batch, batch_idx):
        # ğŸ”„ ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        scenario = batch["scenario"]
        loss = self.compute_loss(batch)
        weighted_loss = loss * self.scenario_weights[scenario]
        return weighted_loss
```

---

## ğŸ¯ ì•¡ì…˜ ì¢…ë¥˜ì— ë”°ë¥¸ êµ¬ì²´ì  ë³€í™”

### 1. ğŸ® ì•¡ì…˜ í‘œí˜„ ë°©ì‹ ë³€í™”
#### âŒ ê¸°ì¡´ (RoboVLMs)
```python
# 7D ì—°ì† ì•¡ì…˜ + ì´ì‚° ê·¸ë¦¬í¼
action_representation = {
    "type": "continuous + discrete",
    "arm_actions": torch.FloatTensor([x, y, z, roll, pitch, yaw]),  # 6D ì—°ì†
    "gripper_action": torch.LongTensor([0 or 1])                   # 1D ì´ì‚°
}
```

#### âœ… ìƒˆë¡œìš´ (Mobile VLA)
```python
# 3D ì—°ì† ì•¡ì…˜ + 1D ì´ì‚° íƒ€ì…
action_representation = {
    "type": "continuous + discrete",  
    "movement_actions": torch.FloatTensor([linear_x, linear_y, angular_z]),  # 3D ì—°ì†
    "action_type": torch.LongTensor([0, 1, 2, or 3])                        # 1D ì´ì‚°
}
```

### 2. ğŸ“Š ì•¡ì…˜ ì •ê·œí™” ë³€í™”
#### âŒ ê¸°ì¡´ ì •ê·œí™”
```python
# RoboVLMs ì•¡ì…˜ ì •ê·œí™” (-1 ~ 1 ë²”ìœ„)
def normalize_arm_action(action):
    # 6DOF arm: ê° ì¶•ë§ˆë‹¤ ë‹¤ë¥¸ ë²”ìœ„
    arm_bounds = [
        [-0.5, 0.5],   # x translation
        [-0.3, 0.3],   # y translation  
        [-0.4, 0.4],   # z translation
        [-Ï€, Ï€],       # roll rotation
        [-Ï€/2, Ï€/2],   # pitch rotation
        [-Ï€, Ï€]        # yaw rotation
    ]
    return normalize_to_minus_one_one(action, arm_bounds)
```

#### âœ… ìƒˆë¡œìš´ ì •ê·œí™”
```python
# Mobile VLA ì•¡ì…˜ ì •ê·œí™” (mobile_vla_data_collector ê¸°ì¤€)
def normalize_mobile_action(action):
    # mobile_vla_data_collectorì˜ WASD_TO_CONTINUOUS ê¸°ì¤€
    mobile_bounds = [
        [-2.0, 2.0],    # linear_x (ì „ì§„/í›„ì§„)
        [-1.0, 1.0],    # linear_y (ì¢Œìš° ì´ë™)
        [-3.14, 3.14]   # angular_z (íšŒì „)
    ]
    # action_typeì€ ì •ê·œí™” ì—†ì´ ì›í•« ì¸ì½”ë”©
    return normalize_to_minus_one_one(action[:3], mobile_bounds)
```

### 3. ğŸ”„ ì•¡ì…˜ ì˜ˆì¸¡ ë¡œì§ ë³€í™”
#### âŒ ê¸°ì¡´ ì˜ˆì¸¡
```python
class RoboVLMPredictor:
    def predict_action(self, observation):
        # ì´ë¯¸ì§€ + ì–¸ì–´ â†’ 7D ì•¡ì…˜ ì˜ˆì¸¡
        vlm_features = self.encode_multimodal(observation["image"], observation["text"])
        
        # ì¡°ì‘ìš© ì•¡ì…˜ ì˜ˆì¸¡
        arm_action = self.arm_head(vlm_features)      # 6D ì—°ì†
        gripper_action = self.gripper_head(vlm_features)  # 1D ì´ì‚°
        
        return torch.cat([arm_action, gripper_action])
```

#### âœ… ìƒˆë¡œìš´ ì˜ˆì¸¡  
```python
class MobileVLAPredictor:
    def predict_action(self, observation):
        # ì´ë¯¸ì§€ + ì–¸ì–´ + ì‹œë‚˜ë¦¬ì˜¤ â†’ 4D ì•¡ì…˜ ì˜ˆì¸¡
        multimodal_features = self.encode_multimodal(
            observation["image"], 
            observation["text"],
            observation["scenario"]  # ğŸ†• ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        )
        
        # ë„¤ë¹„ê²Œì´ì…˜ìš© ì•¡ì…˜ ì˜ˆì¸¡
        movement_action = self.movement_head(multimodal_features)  # 3D ì—°ì†
        action_type = self.action_type_head(multimodal_features)   # 1D ì´ì‚°
        
        return {
            "movement": movement_action,
            "action_type": torch.argmax(action_type)
        }
```

### 4. ğŸ“ˆ ì•¡ì…˜ í•™ìŠµ ì „ëµ ë³€í™”
#### âŒ ê¸°ì¡´ í•™ìŠµ
```python
# RoboVLMs: ì¡°ì‘ ì •í™•ë„ ì¤‘ì‹¬ í•™ìŠµ
def compute_manipulation_loss(pred_actions, target_actions):
    arm_loss = F.mse_loss(pred_actions[:6], target_actions[:6])
    gripper_loss = F.cross_entropy(pred_actions[6:], target_actions[6:])
    
    # ì •ë°€í•œ ì¡°ì‘ì„ ìœ„í•œ ë†’ì€ ê°€ì¤‘ì¹˜
    return arm_loss + 5.0 * gripper_loss  # ê·¸ë¦¬í¼ ì •í™•ë„ ì¤‘ìš”
```

#### âœ… ìƒˆë¡œìš´ í•™ìŠµ
```python
# Mobile VLA: ê²½ë¡œ íš¨ìœ¨ì„± + ì•ˆì „ì„± ì¤‘ì‹¬ í•™ìŠµ
def compute_navigation_loss(pred_actions, target_actions, scenario_context):
    movement_loss = F.mse_loss(pred_actions["movement"], target_actions["movement"])
    type_loss = F.cross_entropy(pred_actions["action_type"], target_actions["action_type"])
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•ˆì „ì„± ê°€ì¤‘ì¹˜
    safety_weight = get_scenario_safety_weight(scenario_context)
    
    # ê²½ë¡œ íš¨ìœ¨ì„± ì†ì‹¤ (ìƒˆë¡œìš´ ê°œë…)
    efficiency_loss = compute_path_efficiency_loss(pred_actions, scenario_context)
    
    return movement_loss + 2.0 * type_loss + safety_weight * efficiency_loss
```

---

## ğŸš€ êµ¬í˜„ ìš°ì„ ìˆœìœ„ë³„ ë³€ê²½ì‚¬í•­

### Phase 1: ë°ì´í„° ë ˆì´ì–´ ë³€ê²½ (ì¦‰ì‹œ í•„ìš”)
1. **HDF5 â†’ Calvin ë³€í™˜ê¸°**: mobile_vla_data_collector ì¶œë ¥ì„ RoboVLMs ì…ë ¥ìœ¼ë¡œ
2. **4D ì•¡ì…˜ ì •ê·œí™”**: ìƒˆë¡œìš´ ì•¡ì…˜ ê³µê°„ì— ë§ëŠ” ì •ê·œí™” í•¨ìˆ˜
3. **ì‹œë‚˜ë¦¬ì˜¤ ì¸ì½”ë”**: 8ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜

### Phase 2: ëª¨ë¸ ë ˆì´ì–´ ë³€ê²½ (1ì£¼ì°¨)
1. **MobilePolicyHead**: 4D ì•¡ì…˜ ì „ìš© ì •ì±… í—¤ë“œ
2. **ScenarioAwareEncoder**: ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸ ìœµí•©
3. **Mobile ì†ì‹¤ í•¨ìˆ˜**: ë„¤ë¹„ê²Œì´ì…˜ íŠ¹í™” ì†ì‹¤

### Phase 3: í•™ìŠµ ë ˆì´ì–´ ë³€ê²½ (2ì£¼ì°¨)  
1. **MultiScenarioTrainer**: ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì¤‘ì¹˜ í•™ìŠµ
2. **Mobile í‰ê°€ ë©”íŠ¸ë¦­**: ë„¤ë¹„ê²Œì´ì…˜ ì„±ê³µë¥ , ê²½ë¡œ íš¨ìœ¨ì„±
3. **ROS í†µí•©**: ì‹¤ì‹œê°„ ì¶”ë¡  ë° ì•¡ì…˜ ì‹¤í–‰

---

ì´ ìƒì„¸í•œ ë³€ê²½ ë¶„ì„ì„ í†µí•´ RoboVLMsì—ì„œ Mobile VLAë¡œì˜ ì „í™˜ì—ì„œ **ë¬´ì—‡ì„ ë°”ê¾¸ê³ , ë¬´ì—‡ì„ ìœ ì§€í•˜ë©°, ë¬´ì—‡ì„ ì „ì´í• ì§€** ëª…í™•í•œ ë¡œë“œë§µì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. mobile_vla_data_collector.pyì˜ ì‹¤ìš©ì  ë°ì´í„° ìˆ˜ì§‘ ë°©ì‹ê³¼ RoboVLMsì˜ ê°•ë ¥í•œ VLM í•™ìŠµ ì‹œìŠ¤í…œì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
