# π“ LoRA Fine-tuning ν•™μµ κ²°κ³Ό (2025-11-20)

## ν•™μµ μ™„λ£ μ”μ•½

### μµμΆ… κ²°κ³Ό
- **μ΄ Epoch**: 10 (λ©ν‘ λ‹¬μ„± β…)
- **μµμΆ… Train Loss**: 0.334
- **μµμΆ… Val Loss**: 0.335
- **ν•™μµ μ‹κ°„**: μ•½ 2μ‹κ°„ 40λ¶„ (Epochλ‹Ή μ•½ 16λ¶„)

### Loss μ¶”μ΄
| Epoch | Train Loss | Val Loss | λΉ„κ³  |
| :--- | :--- | :--- | :--- |
| 0 | 0.395 | 0.369 | μ΄κΈ° |
| 2 | - | 0.286 | μ²΄ν¬ν¬μΈνΈ μ €μ¥ |
| 5 | 0.105 | **0.280** | **Best Val Loss** β… |
| 8 | - | 0.294 | μ²΄ν¬ν¬μΈνΈ μ €μ¥ |
| 9 | 0.334 | 0.335 | μµμΆ… |

### κ΄€μ°°μ‚¬ν•­
1. **μ΄κΈ° μλ ΄**: Epoch 0-5μ—μ„ Train Lossκ°€ 0.395 β†’ 0.105λ΅ κΈ‰κ²©ν κ°μ†
2. **Validation Loss**: 0.280μ—μ„ μ•μ •μ μΌλ΅ μ μ§€ (κ³Όμ ν•© μ—†μ)
3. **μµμΆ… Loss**: Trainκ³Ό Val Lossκ°€ κ±°μ λ™μΌ (0.334 vs 0.335) β†’ **μΌλ°ν™” μ„±λ¥ μ–‘νΈ**

## μ²΄ν¬ν¬μΈνΈ μ •λ³΄

### μ €μ¥λ μ²΄ν¬ν¬μΈνΈ
- **Best Model**: `epoch_epoch=05-val_loss=val_loss=0.280.ckpt` (Val Loss μµμ €)
- **Top 3 Models**: 
  - Epoch 2: Val Loss 0.286
  - Epoch 5: Val Loss 0.280 (Best)
  - Epoch 8: Val Loss 0.294
- **Last Checkpoint**: `last.ckpt` (Epoch 9)
- **κ° μ²΄ν¬ν¬μΈνΈ ν¬κΈ°**: μ•½ 6.9GB

### μ²΄ν¬ν¬μΈνΈ μ„μΉ
```
RoboVLMs_upstream/runs/mobile_vla_lora_20251114/kosmos/mobile_vla_finetune/2025-11-20/mobile_vla_lora_20251114/
```

## λ‹¤μ λ‹¨κ³„

### 1. ν•™μµ κ²°κ³Ό λ¶„μ„ (μ§„ν–‰ μ¤‘)
- [ ] Loss curve μ‹κ°ν™”
- [ ] Best checkpoint μ„ μ • (val_loss κΈ°μ¤€)
- [ ] ν•™μµ μ•μ •μ„± ν‰κ°€

### 2. Inference ν…μ¤νΈ (μ¤€λΉ„)
- [ ] Inference μ¤ν¬λ¦½νΈ μ‘μ„±
- [ ] ν…μ¤νΈ λ°μ΄ν„°μ…‹ μ¤€λΉ„
- [ ] μμΈ΅ κ²°κ³Ό μ‹κ°ν™”

### 3. μ„±λ¥ ν‰κ°€
- [ ] μ •λ‰μ  μ§€ν‘ (MSE, MAE)
- [ ] μ •μ„±μ  ν‰κ°€ (κ²½λ΅ μ‹κ°ν™”)
- [ ] Baselineκ³Ό λΉ„κµ

## λ°μ΄ν„°μ…‹ μ •λ³΄
- **μ΄ μ—ν”Όμ†λ“**: 237κ°
- **μ‹λ‚λ¦¬μ¤**: 1box_left (113κ°), 1box_right (124κ°)
- **μ‹ν€€μ¤ κΈΈμ΄**: 18 ν”„λ μ„ (Window 8 + Prediction 10)

## ν•μ΄νΌνλΌλ―Έν„°
- **LoRA rank (r)**: 32
- **LoRA alpha**: 16
- **LoRA dropout**: 0.1
- **Learning Rate**: 1e-4
- **Batch Size**: 1
- **Gradient Accumulation**: 8
- **Precision**: 16-mixed (FP16)

