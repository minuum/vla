#!/usr/bin/env python3
"""
ì‹¤ì œ Mobile VLA ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì˜ ì‹¤ì œ êµ¬ì¡°ì— ë§ëŠ” ëª¨ë¸ ë¡œë”©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
from PIL import Image
import time
import os
import sys
from typing import Optional, Tuple, Dict, Any
import json

class ActualMobileVLAModel(nn.Module):
    """ì‹¤ì œ Mobile VLA ëª¨ë¸ êµ¬ì¡° (ì²´í¬í¬ì¸íŠ¸ì™€ ì¼ì¹˜)"""
    
    def __init__(self):
        super().__init__()
        
        # Kosmos2 Vision Model (24-layer, 1024 hidden size)
        self.kosmos_model = nn.ModuleDict({
            'vision_model': nn.ModuleDict({
                'model': nn.ModuleDict({
                    'embeddings': nn.ModuleDict({
                        'class_embedding': nn.Parameter(torch.randn(1024)),
                        'patch_embedding': nn.Conv2d(3, 1024, kernel_size=14, stride=14),
                        'position_embedding': nn.Embedding(257, 1024)
                    }),
                    'pre_layrnorm': nn.LayerNorm(1024),
                    'encoder': nn.ModuleDict({
                        'layers': nn.ModuleList([
                            self._create_vision_layer() for _ in range(24)
                        ])
                    }),
                    'post_layrnorm': nn.LayerNorm(1024)
                })
            }),
            'text_model': nn.ModuleDict({
                'model': nn.ModuleDict({
                    'embed_tokens': nn.Embedding(50000, 1024),
                    'layers': nn.ModuleList([
                        self._create_text_layer() for _ in range(12)
                    ])
                })
            }),
            'image_to_text_projection': nn.ModuleDict({
                'latent_query': nn.Parameter(torch.randn(64, 1024)),
                'dense': nn.Linear(1024, 1024),
                'x_attn': self._create_cross_attention_layer()
            })
        })
        
        # CLIP Model
        self.clip_model = nn.ModuleDict({
            'logit_scale': nn.Parameter(torch.ones([]) * np.log(1 / 0.07)),
            'text_model': nn.ModuleDict({
                'embeddings': nn.ModuleDict({
                    'token_embedding': nn.Embedding(49408, 512),
                    'position_embedding': nn.Embedding(77, 512)
                }),
                'encoder': nn.ModuleDict({
                    'layers': nn.ModuleList([
                        self._create_clip_text_layer() for _ in range(12)
                    ])
                }),
                'final_layer_norm': nn.LayerNorm(512)
            }),
            'vision_model': nn.ModuleDict({
                'embeddings': nn.ModuleDict({
                    'class_embedding': nn.Parameter(torch.randn(768)),
                    'patch_embedding': nn.Conv2d(3, 768, kernel_size=32, stride=32),
                    'position_embedding': nn.Embedding(197, 768)
                }),
                'pre_layrnorm': nn.LayerNorm(768),
                'encoder': nn.ModuleDict({
                    'layers': nn.ModuleList([
                        self._create_clip_vision_layer() for _ in range(12)
                    ])
                }),
                'post_layrnorm': nn.LayerNorm(768)
            }),
            'visual_projection': nn.Linear(768, 512),
            'text_projection': nn.Linear(512, 512)
        })
        
        # Feature Fusion
        self.feature_fusion = nn.Linear(2048, 2048)  # 1024 (Kosmos2) + 1024 (CLIP)
        
        # RNN (LSTM)
        self.rnn = nn.LSTM(2048, 4096, num_layers=4, batch_first=True)
        
        # Action Predictor
        self.actions = nn.ModuleDict({
            'mlp': nn.Sequential(
                nn.Linear(4096, 2048),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(2048, 1024),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.Linear(1024, 2)  # linear_x, linear_y
            )
        })
    
    def _create_vision_layer(self):
        """Kosmos2 Vision Layer ìƒì„±"""
        return nn.ModuleDict({
            'self_attn': nn.ModuleDict({
                'k_proj': nn.Linear(1024, 1024),
                'v_proj': nn.Linear(1024, 1024),
                'q_proj': nn.Linear(1024, 1024),
                'out_proj': nn.Linear(1024, 1024)
            }),
            'layer_norm1': nn.LayerNorm(1024),
            'mlp': nn.ModuleDict({
                'fc1': nn.Linear(1024, 4096),
                'fc2': nn.Linear(4096, 1024)
            }),
            'layer_norm2': nn.LayerNorm(1024)
        })
    
    def _create_text_layer(self):
        """Kosmos2 Text Layer ìƒì„±"""
        return nn.ModuleDict({
            'self_attn': nn.ModuleDict({
                'k_proj': nn.Linear(1024, 1024),
                'v_proj': nn.Linear(1024, 1024),
                'q_proj': nn.Linear(1024, 1024),
                'out_proj': nn.Linear(1024, 1024),
                'inner_attn_ln': nn.LayerNorm(1024),
                'self_attn_layer_norm': nn.LayerNorm(1024)
            }),
            'ffn': nn.ModuleDict({
                'fc1': nn.Linear(1024, 4096),
                'fc2': nn.Linear(4096, 1024),
                'ffn_layernorm': nn.LayerNorm(1024)
            }),
            'final_layer_norm': nn.LayerNorm(1024)
        })
    
    def _create_clip_text_layer(self):
        """CLIP Text Layer ìƒì„±"""
        return nn.ModuleDict({
            'self_attn': nn.ModuleDict({
                'k_proj': nn.Linear(512, 512),
                'v_proj': nn.Linear(512, 512),
                'q_proj': nn.Linear(512, 512),
                'out_proj': nn.Linear(512, 512)
            }),
            'layer_norm1': nn.LayerNorm(512),
            'mlp': nn.ModuleDict({
                'fc1': nn.Linear(512, 2048),
                'fc2': nn.Linear(2048, 512)
            }),
            'layer_norm2': nn.LayerNorm(512)
        })
    
    def _create_clip_vision_layer(self):
        """CLIP Vision Layer ìƒì„±"""
        return nn.ModuleDict({
            'self_attn': nn.ModuleDict({
                'k_proj': nn.Linear(768, 768),
                'v_proj': nn.Linear(768, 768),
                'q_proj': nn.Linear(768, 768),
                'out_proj': nn.Linear(768, 768)
            }),
            'layer_norm1': nn.LayerNorm(768),
            'mlp': nn.ModuleDict({
                'fc1': nn.Linear(768, 3072),
                'fc2': nn.Linear(3072, 768)
            }),
            'layer_norm2': nn.LayerNorm(768)
        })
    
    def _create_cross_attention_layer(self):
        """Cross Attention Layer ìƒì„±"""
        return nn.ModuleDict({
            'k_proj': nn.Linear(1024, 1024),
            'v_proj': nn.Linear(1024, 1024),
            'q_proj': nn.Linear(1024, 1024),
            'out_proj': nn.Linear(1024, 1024)
        })
    
    def forward(self, images: torch.Tensor, texts: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)"""
        # ì‹¤ì œ êµ¬í˜„ì€ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
        batch_size = images.size(0)
        
        # ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” Kosmos2 + CLIP ì‚¬ìš©)
        vision_features = torch.randn(batch_size, 1024).to(images.device)
        text_features = torch.randn(batch_size, 1024).to(images.device)
        
        # Feature fusion
        combined_features = torch.cat([vision_features, text_features], dim=-1)
        fused_features = self.feature_fusion(combined_features)
        
        # RNN processing
        fused_features = fused_features.unsqueeze(1)  # (batch_size, 1, 2048)
        rnn_out, _ = self.rnn(fused_features)
        rnn_out = rnn_out[:, -1, :]  # (batch_size, 4096)
        
        # Action prediction
        actions = self.actions.mlp(rnn_out)
        
        return actions

class ActualMobileVLAModelLoader:
    """ì‹¤ì œ Mobile VLA ëª¨ë¸ ë¡œë”"""
    
    def __init__(self, model_dir: str = "./Robo+/Mobile_VLA"):
        self.model_dir = model_dir
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {self.device}")
        if torch.cuda.is_available():
            print(f"ğŸ® CUDA ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}")
        
    def load_model(self, checkpoint_path: Optional[str] = None) -> ActualMobileVLAModel:
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸš€ ì‹¤ì œ Mobile VLA ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.model_dir}")
        
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ìë™ íƒì§€
        if checkpoint_path is None:
            checkpoint_path = self._find_best_checkpoint()
        
        if checkpoint_path is None:
            print("âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self._list_available_checkpoints()
            return None
        
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {checkpoint_path}")
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = ActualMobileVLAModel()
            self.model = self.model.to(self.device)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            print("ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ (strict=Falseë¡œ ì¼ë¶€ë§Œ ë¡œë“œ)
            if 'model_state_dict' in checkpoint:
                missing_keys, unexpected_keys = self.model.load_state_dict(
                    checkpoint['model_state_dict'], strict=False
                )
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (ì¼ë¶€ë§Œ ë¡œë“œ)")
                print(f"   ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                print(f"   ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
            else:
                print("âŒ ì²´í¬í¬ì¸íŠ¸ì— model_state_dictê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            self._print_model_info(checkpoint)
            
            print("âœ… ì‹¤ì œ Mobile VLA ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return self.model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _find_best_checkpoint(self) -> Optional[str]:
        """ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ìë™ íƒì§€"""
        possible_paths = [
            f"{self.model_dir}/simple_clip_lstm_model/best_simple_clip_lstm_model.pth",
            f"{self.model_dir}/results/simple_clip_lstm_results_extended/best_simple_clip_lstm_model.pth",
            "./mobile-vla-omniwheel/best_simple_lstm_model.pth"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def _list_available_checkpoints(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¶œë ¥"""
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
        os.system('find . -name "*.pth" -type f | head -10')
    
    def _print_model_info(self, checkpoint: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
        
        if 'epoch' in checkpoint:
            print(f"ğŸ“ˆ í›ˆë ¨ ì—í¬í¬: {checkpoint['epoch']}")
        if 'loss' in checkpoint:
            print(f"ğŸ“‰ ì†ì‹¤ê°’: {checkpoint['loss']:.4f}")
        if 'val_mae' in checkpoint:
            print(f"ğŸ“Š ê²€ì¦ MAE: {checkpoint['val_mae']:.4f}")

def test_actual_model_inference():
    """ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ğŸ§  ì‹¤ì œ Mobile VLA ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë” ìƒì„±
    loader = ActualMobileVLAModelLoader()
    
    # ëª¨ë¸ ë¡œë“œ
    model = loader.load_model()
    if model is None:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        return False
    
    print("\n" + "=" * 40)
    print("ğŸ”¬ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 1
    image_size = (3, 224, 224)
    text_length = 77
    
    # ëœë¤ ì…ë ¥ ë°ì´í„° ìƒì„±
    images = torch.randn(batch_size, *image_size).to(loader.device)
    texts = torch.randint(0, 1000, (batch_size, text_length)).to(loader.device)
    
    print(f"ğŸ“¥ ì´ë¯¸ì§€ í¬ê¸°: {images.shape}")
    print(f"ğŸ“¥ í…ìŠ¤íŠ¸ í¬ê¸°: {texts.shape}")
    
    # ì›Œë°ì—…
    print("ğŸ”¥ ì›Œë°ì—… ì¤‘...")
    with torch.no_grad():
        for _ in range(5):
            _ = model(images, texts)
    
    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    num_runs = 50
    times = []
    
    print(f"â±ï¸ {num_runs}íšŒ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¤‘...")
    with torch.no_grad():
        for i in range(num_runs):
            start_time = time.time()
            output = model(images, texts)
            end_time = time.time()
            times.append(end_time - start_time)
    
    # ê²°ê³¼ ë¶„ì„
    avg_time = np.mean(times)
    min_time = np.min(times)
    max_time = np.max(times)
    fps = 1.0 / avg_time
    
    print(f"ğŸ“¤ ì¶œë ¥ í¬ê¸°: {output.shape}")
    print(f"â±ï¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.2f} ms")
    print(f"âš¡ ìµœì†Œ ì¶”ë¡  ì‹œê°„: {min_time*1000:.2f} ms")
    print(f"ğŸŒ ìµœëŒ€ ì¶”ë¡  ì‹œê°„: {max_time*1000:.2f} ms")
    print(f"ğŸš€ ì¶”ë¡  FPS: {fps:.1f}")
    
    # ì•¡ì…˜ ê°’ ì¶œë ¥
    print(f"ğŸ¯ ì˜ˆì¸¡ ì•¡ì…˜: {output.cpu().numpy()}")
    
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì‹¤ì œ Mobile VLA ëª¨ë¸ êµ¬ì¡° ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    inference_ok = test_actual_model_inference()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    results = {
        "ì‹¤ì œ ëª¨ë¸ ì¶”ë¡ ": "âœ…" if inference_ok else "âŒ"
    }
    
    for test_name, result in results.items():
        print(f"{test_name}: {result}")
    
    if inference_ok:
        print("\nğŸ‰ ì‹¤ì œ ëª¨ë¸ êµ¬ì¡° í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ì‹¤ì œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±")
        print("   2. ì¹´ë©”ë¼ ì…ë ¥ ì—°ë™")
        print("   3. ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ ì™„ì„±")
    else:
        print("\nâš ï¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    return inference_ok

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
