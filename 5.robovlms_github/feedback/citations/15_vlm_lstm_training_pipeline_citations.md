# 15. VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìƒì„¸ ë¶„ì„

## ðŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” RoboVLMsì—ì„œ VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ ê³¼ì •ì„ ì¼ë°˜ì ì¸ AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ìžì„¸ížˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ðŸ” 1. Real-World ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •

### 1.1 CALVIN ë°ì´í„°ì…‹ì˜ Real-World íŠ¹ì„±

**ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½**
```python
# CALVIN ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ë¡œë´‡ í™˜ê²½
obs_config = DictConfig({
    "rgb_obs": ["rgb_static", "rgb_gripper"],    # ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼
    "depth_obs": [],                             # ê¹Šì´ ì •ë³´ (ì‚¬ìš© ì•ˆí•¨)
    "state_obs": ["robot_obs"],                  # ë¡œë´‡ ìƒíƒœ ì •ë³´
    "actions": ["rel_actions"],                    # ìƒëŒ€ì  ì•¡ì…˜
    "language": ["language"],                     # ì–¸ì–´ ëª…ë ¹
})
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:63-71`

**Real-World ë°ì´í„° êµ¬ì„±**
- **Franka Emika Panda 7-DOF ë¡œë´‡íŒ”**: ì‹¤ì œ ë¡œë´‡ í•˜ë“œì›¨ì–´
- **ë‹¤ì¤‘ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ**: ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼
- **ì‹¤ì œ ë¬¼ë¦¬ í™˜ê²½**: í…Œì´ë¸”, ë¬¼ì²´, ì¡°ìž‘ ê³µê°„
- **ë‹¤ì–‘í•œ íƒœìŠ¤í¬**: pick-and-place, navigation, manipulation
- **ì‹¤ì œ ë¡œë´‡ ì¡°ìž‘**: ì „ë¬¸ê°€ê°€ ì§ì ‘ ì¡°ìž‘í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘

### 1.2 ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •

**ì´ë¯¸ì§€ ì „ì²˜ë¦¬**
```python
# CALVIN ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ ì²˜ë¦¬
def process_rgb(self, episode, observation_space, transforms, seq_idx=0, window_size=0):
    # RGB ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
    rgb_static = episode["rgb_static"]      # ì •ì  ì¹´ë©”ë¼ ì´ë¯¸ì§€
    rgb_gripper = episode["rgb_gripper"]    # ê·¸ë¦¬í¼ ì¹´ë©”ë¼ ì´ë¯¸ì§€
    
    # ì´ë¯¸ì§€ ì •ê·œí™” ë° ë¦¬ì‚¬ì´ì§•
    transforms = [
        Resize((224, 224)),                 # 224x224ë¡œ ë¦¬ì‚¬ì´ì§•
        RandomHorizontalFlip(p=0.1),        # ì œí•œì  ì¦ê°•
        ColorJitter(brightness=0.1, contrast=0.1),
        Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                 std=[0.26862954, 0.26130258, 0.27577711])
    ]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:236-243`

**ì•¡ì…˜ ì •ê·œí™”**
```python
# ì•¡ì…˜ ì •ê·œí™” ê³¼ì •
def collater(self, sample):
    if self.norm_action:
        for s in sample:
            s["actions"] = normalize_action(
                s["actions"], 
                self.norm_min,    # -1
                self.norm_max,    # 1
                maintain_last=True
            )
    
    # ê·¸ë¦¬í¼ ì•¡ì…˜ ì´ì§„í™”
    action_tensors[..., -1] = ((action_tensors[..., -1] + 1) // 2).float()
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:823-868`

## ðŸŽ¯ 2. VLM Fine-tuning ê³¼ì •

### 2.1 VLM ì•„í‚¤í…ì²˜ ì„ íƒ

**ì§€ì›ë˜ëŠ” VLM ëª¨ë¸ë“¤**
```python
# ë‹¤ì–‘í•œ VLM ë°±ë³¸ ì§€ì›
vlm_configs = {
    "PaliGemmaForConditionalGeneration": "paligemma-3b-pt-224",
    "RoboFlamingo": "flamingo-3b",
    "RoboKosmos": "kosmos-2",
    "RoboUform": "uform-vl-14b",
    "RoboPaligemma": "paligemma-3b-pt-224"
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:280-284`

### 2.2 Fine-tuning ì„¤ì •

**Full Fine-tuning (F-FT) ì„¤ì •**
```python
# Full Fine-tuning ì„¤ì •
train_setup = {
    "lora_enable": False,           # LoRA ë¹„í™œì„±í™”
    "freeze_backbone": False,       # ë°±ë³¸ ëª¨ë¸ ë™ê²° í•´ì œ
    "freeze_mm_mlp_adapter": False, # ë©€í‹°ëª¨ë‹¬ ì–´ëŒ‘í„° ë™ê²° í•´ì œ
    "train_vision": True,           # ë¹„ì „ ëª¨ë¸ í•™ìŠµ
    "train_text_embedding": True,   # í…ìŠ¤íŠ¸ ìž„ë² ë”© í•™ìŠµ
    "gradient_checkpointing": False # ê·¸ëž˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë¹„í™œì„±í™”
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:228-250`

**LoRA ì„¤ì • (ì„ íƒì )**
```python
# LoRA ì„¤ì • (ì¼ë¶€ ëª¨ë¸ì—ì„œ ì‚¬ìš©)
lora_config = {
    "lora_enable": True,           # LoRA í™œì„±í™”
    "lora_r": 64,                  # LoRA rank
    "lora_alpha": 16,              # LoRA alpha
    "lora_dropout": 0.05,         # LoRA ë“œë¡­ì•„ì›ƒ
    "lora_bias": "none"            # LoRA bias ì„¤ì •
}
```

### 2.3 VLM í•™ìŠµ ê³¼ì •

**ë©€í‹°ëª¨ë‹¬ ìž…ë ¥ ì²˜ë¦¬**
```python
# BaseRoboVLM.forward()ì—ì„œ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
def forward(self, vision_x, lang_x, attention_mask=None, **kwargs):
    # 1ë‹¨ê³„: ë¹„ì „ ì¸ì½”ë”©
    vision_features = self.encode_images(vision_x)
    
    # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¸ì½”ë”©
    text_features = self.encode_text(lang_x)
    
    # 3ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ìœµí•©
    multimodal_features = self.merge_multi_modal_input(
        vision_features, text_features
    )
    
    # 4ë‹¨ê³„: VLM Forward Pass
    output = self.model(
        input_ids=multimodal_features,
        attention_mask=attention_mask,
        output_hidden_states=True
    )
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1261-1284`

## ðŸ§  3. LSTM Layer í•™ìŠµ ê³¼ì •

### 3.1 LSTM Decoder ì•„í‚¤í…ì²˜

**LSTM Decoder êµ¬ì¡°**
```python
class LSTMDecoder(BasePolicyHead):
    def __init__(self, in_features, action_dim, down_sample, latent, 
                 fwd_pred_next_n, window_size, hidden_size=1024, num_layers=4):
        
        # LSTM ë ˆì´ì–´ ì´ˆê¸°í™”
        self.rnn = lstm_decoder(
            in_features * latent,      # ìž…ë ¥ ì°¨ì›
            hidden_size * latent,      # ížˆë“  ì°¨ì›
            num_layers,                # LSTM ë ˆì´ì–´ ìˆ˜
            policy_rnn_dropout_p=0.0   # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        )
        
        # ì•¡ì…˜ í—¤ë“œ (íŒ” ì•¡ì…˜ìš©)
        self.actions = MLPTanhHead(
            self.hidden_size * latent, 
            fwd_pred_next_n * (action_dim - 1)  # 6-DOF íŒ” ì•¡ì…˜
        )
        
        # ê·¸ë¦¬í¼ í—¤ë“œ (ê·¸ë¦¬í¼ ì•¡ì…˜ìš©)
        self.gripper = MLPSigmoidHead(
            self.hidden_size * latent, 
            fwd_pred_next_n  # 1-DOF ê·¸ë¦¬í¼ ì•¡ì…˜
        )
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:173-204`

### 3.2 LSTM í•™ìŠµ ê³¼ì •

**1ë‹¨ê³„: VLM íŠ¹ì§• ì¶”ì¶œ**
```python
# VLMì—ì„œ ì¶”ì¶œëœ íŠ¹ì§•ì„ LSTM ìž…ë ¥ìœ¼ë¡œ ì‚¬ìš©
def forward(self, tok_seq, h_0=None, **kwargs):
    # VLM ì¶œë ¥ íŠ¹ì§•: [batch_size, window_size, latent, feature_dim]
    # LSTM ìž…ë ¥ìœ¼ë¡œ ë³€í™˜: [batch_size, window_size, latent * feature_dim]
    
    if self.down_sample == "none":
        tok_seq = rearrange(tok_seq, "b l n d-> b l (n d)")
    
    # LSTM Forward Pass
    x, h_n = self.rnn(tok_seq, self.hidden_state)
    self.hidden_state = h_n
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:223-224`

**2ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡**
```python
# LSTM ì¶œë ¥ì„ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
def forward(self, tok_seq, **kwargs):
    # LSTM ì²˜ë¦¬
    x, h_n = self.rnn(tok_seq, self.hidden_state)
    
    # ì•¡ì…˜ ì˜ˆì¸¡
    actions = self.actions(x)      # íŒ” ì•¡ì…˜ (6-DOF)
    gripper = self.gripper(x)      # ê·¸ë¦¬í¼ ì•¡ì…˜ (1-DOF)
    
    # ì¶œë ¥ í˜•íƒœ ì¡°ì •
    actions = rearrange(actions, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    gripper = rearrange(gripper, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    
    return actions, gripper
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:223-224`

### 3.3 Loss ê³„ì‚° ë° í•™ìŠµ

**Loss ê³„ì‚° ê³¼ì •**
```python
def loss(self, pred_action_logits, labels, attention_mask=None):
    # 1ë‹¨ê³„: ì‹œí€€ìŠ¤ ì‹œí”„íŠ¸ (autoregressive í•™ìŠµ)
    shift_logits = pred_action_logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # 2ë‹¨ê³„: CrossEntropyLoss ê³„ì‚°
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(
        shift_logits.view(-1, shift_logits.size(-1)), 
        shift_labels.view(-1)
    )
    
    # 3ë‹¨ê³„: ì•¡ì…˜ ë§ˆìŠ¤í‚¹
    mask = torch.logical_and(
        labels > self.action_tokenizer.action_token_begin_idx,
        labels < self.action_tokenizer.action_token_end_idx
    )
    
    # 4ë‹¨ê³„: ì •í™•ë„ ê³„ì‚°
    pred_action = pred_action_logits.argmax(dim=-1)
    correct_preds = torch.logical_and((pred_action == labels), mask)
    
    return {
        "loss_arm": loss,
        "acc_arm": arm_acc,
        "acc_gripper": gripper_acc
    }
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:226-281`

## ðŸ”„ 4. ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸

### 4.1 í•™ìŠµ ë‹¨ê³„ë³„ ê³¼ì •

**1ë‹¨ê³„: ë°ì´í„° ë¡œë”©**
```python
# BaseTrainer.training_step()ì—ì„œ ë°°ì¹˜ ì²˜ë¦¬
def training_step(self, batch, batch_idx):
    # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
    (rgb, hand_rgb, attention_mask, language, text_mask,
     arm_action, gripper_action, instr_and_action_ids,
     instr_and_action_labels, instr_and_action_mask) = self._process_batch(batch)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:565-591`

**2ë‹¨ê³„: ëª¨ë¸ Forward Pass**
```python
# ëª¨ë¸ Forward Pass
prediction = self.model.forward(
    rgb,                    # ë¹„ì „ ìž…ë ¥
    language,               # ì–¸ì–´ ìž…ë ¥
    attention_mask=text_mask,
    action_labels=(arm_action, gripper_action),
    action_mask=chunk_mask,
    instr_and_action_ids=instr_and_action_ids,
    instr_and_action_labels=instr_and_action_labels,
    instr_and_action_mask=instr_and_action_mask
)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:593-619`

**3ë‹¨ê³„: Loss ê³„ì‚°**
```python
# Loss ê³„ì‚° ë° ìµœì í™”
def _get_loss(self, prediction):
    loss_arm_act = prediction.get("loss_arm_act", None)
    loss_gripper_act = prediction.get("loss_gripper_act", None)
    
    # ì•¡ì…˜ Loss ê³„ì‚°
    loss_act = (loss_arm_act if loss_arm_act is not None else 0) + (
        loss_gripper_act * self.arm_gripper_loss_ratio
        if loss_gripper_act is not None else 0
    )
    
    return {"loss": loss_act, "loss_act": loss_act}
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:269-315`

### 4.2 í•™ìŠµ ì„¤ì •

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```python
# í•™ìŠµ ì„¤ì •
training_config = {
    "learning_rate": 1e-4,           # í•™ìŠµë¥ 
    "weight_decay": 0.0,             # ê°€ì¤‘ì¹˜ ê°ì‡ 
    "batch_size": 4,                 # ë°°ì¹˜ í¬ê¸°
    "max_epochs": 5,                 # ìµœëŒ€ ì—í¬í¬
    "gradient_clip_val": 1.0,        # ê·¸ëž˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    "precision": "bf16"               # í˜¼í•© ì •ë°€ë„
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:221-223`

**Loss ê°€ì¤‘ì¹˜**
```python
# Loss ê°€ì¤‘ì¹˜ ì„¤ì •
loss_weights = {
    "arm_gripper_loss_ratio": 0.01,   # íŒ”-ê·¸ë¦¬í¼ Loss ë¹„ìœ¨
    "cap_loss_ratio": 0.05,           # ìº¡ì…˜ Loss ë¹„ìœ¨
    "fwd_loss_ratio": 0               # ë¯¸ëž˜ ì˜ˆì¸¡ Loss ë¹„ìœ¨
}
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:288-292`

## ðŸ”§ 5. RoboVLMsë§Œì˜ ê³ ìœ í•œ ë””í…Œì¼

### 5.1 ë©€í‹°ëª¨ë‹¬ ìœµí•© ë©”ì»¤ë‹ˆì¦˜

**Vision + Language + Action í†µí•© ì²˜ë¦¬**
```python
# BaseRoboVLM.merge_multi_modal_input()ì—ì„œ ë©€í‹°ëª¨ë‹¬ ìœµí•©
def merge_multi_modal_input(self, input_embeds, vision_x, attention_mask=None):
    # 1ë‹¨ê³„: ë¹„ì „ íŠ¹ì§• ì¸ì½”ë”©
    vision_features = self.encode_images(vision_x)
    
    # 2ë‹¨ê³„: ì‹œìž‘/ë ì´ë¯¸ì§€ í† í° ì‚½ìž…
    start_img_token = self.start_img_token.unsqueeze(0).unsqueeze(0)
    end_img_token = self.end_img_token.unsqueeze(0).unsqueeze(0)
    
    # 3ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ìž„ë² ë”© ê²°í•©
    multimodal_embeds = torch.cat([
        input_embeds[:, :start_idx], 
        start_img_token, 
        vision_features, 
        end_img_token, 
        input_embeds[:, start_idx:]
    ], dim=1)
    
    return multimodal_embeds, attention_mask
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1390-1410`

### 5.2 ì´ì‚° ì•¡ì…˜ ìƒì„± ê³¼ì •

**Autoregressive ì•¡ì…˜ í† í° ìƒì„±**
```python
# BaseRoboVLM.pred_action_discrete()ì—ì„œ ì´ì‚° ì•¡ì…˜ ìƒì„±
def pred_action_discrete(self, instr_and_action_ids, vision_x, vision_gripper=None):
    # 1ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ìœµí•©
    multimodal_embeds = self.merge_multi_modal_input(
        input_embeds, vision_x, attention_mask=attention_mask
    )
    
    # 2ë‹¨ê³„: ê·¸ë¦¬í¼ ë¹„ì „ ì¶”ê°€ (ì„ íƒì )
    if vision_gripper is not None:
        multimodal_embeds = self.merge_multi_modal_input(
            multimodal_embeds, vision_gripper, attention_mask=multimodal_attention_mask
        )
    
    # 3ë‹¨ê³„: Autoregressive ì•¡ì…˜ ìƒì„±
    generated_ids = []
    kv_cache = None
    for i in range(action_dim * self.fwd_pred_next_n):
        output_hs = self.model(
            inputs_embeds=multimodal_embeds,
            past_key_values=kv_cache,
            use_cache=True
        )
        kv_cache = output_hs.past_key_values
        cur_id = output_hs.logits[:, -1].argmax(dim=-1)
        generated_ids.append(cur_id)
        
        # ë‹¤ìŒ í† í°ì„ ìœ„í•œ ìž„ë² ë”© ì¶”ê°€
        cur_embed = self.word_embedding(cur_id)
        multimodal_embeds = torch.cat([multimodal_embeds, cur_embed.unsqueeze(1)], dim=1)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1384-1436`

### 5.3 Vision Resampler (ì„ íƒì )

**PerceiverResamplerë¥¼ í†µí•œ ë¹„ì „ í† í° ì••ì¶•**
```python
# Vision Resampler ì„¤ì •
vision_resampler_configs = {
    "use_vision_resampler": True,    # Vision Resampler ì‚¬ìš© ì—¬ë¶€
    "vision_resampler_configs": {
        "depth": 8,                  # PerceiverResampler ê¹Šì´
        "heads": 8,                  # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
        "dim_head": 64,              # í—¤ë“œ ì°¨ì›
        "num_latents": 64            # ì••ì¶•ëœ í† í° ìˆ˜ (196 â†’ 64)
    }
}
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/README.md:51-52`

### 5.4 ì•¡ì…˜ ížˆìŠ¤í† ë¦¬ ê´€ë¦¬

**Window Size ê¸°ë°˜ ížˆìŠ¤í† ë¦¬ ê´€ë¦¬**
```python
# LSTMDecoderì—ì„œ ížˆìŠ¤í† ë¦¬ ê´€ë¦¬
def forward(self, tok_seq, h_0=None, **kwargs):
    if tok_seq.shape[1] == 1:
        self.history_memory.append(tok_seq)
        if len(self.history_memory) <= self.history_len:
            # ížˆìŠ¤í† ë¦¬ ê¸¸ì´ ë‚´ì—ì„œ LSTM ì²˜ë¦¬
            x, h_n = self.rnn(tok_seq, self.hidden_state)
            self.hidden_state = h_n
        else:
            # ížˆìŠ¤í† ë¦¬ ê¸¸ì´ ì´ˆê³¼ ì‹œ ìœˆë„ìš° ìŠ¬ë¼ì´ë”©
            cur_len = len(self.history_memory)
            for _ in range(cur_len - self.history_len):
                self.history_memory.pop(0)
            
            # ìœˆë„ìš° í¬ê¸°ë§Œí¼ ížˆìŠ¤í† ë¦¬ ìž¬êµ¬ì„±
            hist_feature = torch.cat(self.history_memory, dim=1)
            self.hidden_state = None
            x, h_n = self.rnn(hist_feature, self.hidden_state)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:223-224`

### 5.5 ì•¡ì…˜ ìž„ë² ë”© ì‹œìŠ¤í…œ

**LinearActionEncoderë¥¼ í†µí•œ ì•¡ì…˜ ì¸ì½”ë”©**
```python
# LinearActionEncoderì—ì„œ ì•¡ì…˜ ìž„ë² ë”©
class LinearActionEncoder(nn.Module):
    def __init__(self, action_dim, hidden_size):
        self.arm_mlp = nn.Sequential(
            nn.Linear(action_dim - 1, hidden_size),  # íŒ” ì•¡ì…˜ (6-DOF)
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size)
        )
        self.gripper_mlp = nn.Sequential(
            nn.Linear(1, hidden_size),               # ê·¸ë¦¬í¼ ì•¡ì…˜ (1-DOF)
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size)
        )
    
    def forward(self, action):
        arm_action = action[:, :, :6]      # íŒ” ì•¡ì…˜
        gripper_action = action[:, :, 6:7] # ê·¸ë¦¬í¼ ì•¡ì…˜
        
        arm_embed = self.arm_mlp(arm_action)
        gripper_embed = self.gripper_mlp(gripper_action)
        
        action_embed = torch.cat([arm_embed, gripper_embed], dim=-1)
        return action_embed
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/action_encoder/linear_encoder.py:4-41`

### 5.6 ë‹¤ì¤‘ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ

**ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼ ì²˜ë¦¬**
```python
# BaseRoboVLMì—ì„œ ë‹¤ì¤‘ ì¹´ë©”ë¼ ì²˜ë¦¬
def forward(self, vision_x, lang_x, vision_gripper=None, **kwargs):
    # 1ë‹¨ê³„: ì •ì  ì¹´ë©”ë¼ ì²˜ë¦¬
    vision_features = self.encode_images(vision_x)
    
    # 2ë‹¨ê³„: ê·¸ë¦¬í¼ ì¹´ë©”ë¼ ì²˜ë¦¬ (ì„ íƒì )
    if vision_gripper is not None:
        gripper_features = self.encode_images(vision_gripper)
        # ê·¸ë¦¬í¼ íŠ¹ì§•ì„ ì •ì  ì¹´ë©”ë¼ íŠ¹ì§•ê³¼ ê²°í•©
        vision_features = torch.cat([vision_features, gripper_features], dim=1)
    
    # 3ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ìœµí•©
    multimodal_features = self.merge_multi_modal_input(
        vision_features, lang_x
    )
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1399-1409`

### 5.7 ì•¡ì…˜ í† í° ì‚½ìž… ë©”ì»¤ë‹ˆì¦˜

**âš ï¸ ì¤‘ìš”: í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©ë˜ëŠ” ë©”ì»¤ë‹ˆì¦˜**

**í•™ìŠµ ì‹œ: ì•¡ì…˜ í† í°ì„ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ì— ì‚½ìž…**
```python
# í•™ìŠµ ì‹œ ì•¡ì…˜ í† í° ì‚½ìž… ë¡œì§ (ActionPredictionBatchTransform)
def cat_input_ids_and_action_ids(self, input_ids, action_ids, eos_token_id, right_pad_len):
    # 1ë‹¨ê³„: ì•¡ì…˜ í† í°ì„ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ì— ì‚½ìž…
    input_ids = input_ids + action_ids
    
    # 2ë‹¨ê³„: ë¼ë²¨ ìƒì„± (ì•¡ì…˜ í† í° ë¶€ë¶„ë§Œ í•™ìŠµ ëŒ€ìƒ)
    labels = [-100] * len(input_ids[:-len(action_ids)]) + action_ids
    
    # 3ë‹¨ê³„: ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
    attention_masks = [1] * len(input_ids)
    
    return input_ids, labels, attention_masks
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:141-172`

**ì¶”ë¡  ì‹œ: ì•¡ì…˜ í† í° ì‚½ìž… ì—†ì´ ì§ì ‘ ìƒì„±**
```python
# ì¶”ë¡  ì‹œ ì•¡ì…˜ ìƒì„± (BaseRoboVLM.pred_action_discrete)
def pred_action_discrete(self, instr_and_action_ids, vision_x, vision_gripper=None):
    # 1ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ìœµí•© (ì•¡ì…˜ í† í° ì‚½ìž… ì—†ìŒ)
    multimodal_embeds = self.merge_multi_modal_input(
        input_embeds, vision_x, attention_mask=attention_mask
    )
    
    # 2ë‹¨ê³„: Autoregressive ì•¡ì…˜ ìƒì„±
    generated_ids = []
    for i in range(action_dim * self.fwd_pred_next_n):
        output_hs = self.model(
            inputs_embeds=multimodal_embeds,
            past_key_values=kv_cache,
            use_cache=True
        )
        cur_id = output_hs.logits[:, -1].argmax(dim=-1)
        generated_ids.append(cur_id)
        
        # ìƒì„±ëœ í† í°ì„ ë‹¤ìŒ ìž…ë ¥ìœ¼ë¡œ ì‚¬ìš©
        cur_embed = self.word_embedding(cur_id)
        multimodal_embeds = torch.cat([multimodal_embeds, cur_embed.unsqueeze(1)], dim=1)
    
    # 3ë‹¨ê³„: ì•¡ì…˜ í† í°ì„ ì—°ì† ì•¡ì…˜ìœ¼ë¡œ ë””ì½”ë”©
    discretized_actions = self.action_tokenizer.decode_token_ids_to_actions(
        predicted_action_ids
    )
    
    return discretized_actions
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1384-1452`

**í•™ìŠµ vs ì¶”ë¡  ì°¨ì´ì **

| êµ¬ë¶„ | í•™ìŠµ ì‹œ | ì¶”ë¡  ì‹œ |
|------|---------|---------|
| **ì•¡ì…˜ í† í° ì‚½ìž…** | âœ… í…ìŠ¤íŠ¸ì— ì•¡ì…˜ í† í° ì‚½ìž… | âŒ ì•¡ì…˜ í† í° ì‚½ìž… ì—†ìŒ |
| **ëª©ì ** | ì•¡ì…˜ í† í° ì˜ˆì¸¡ í•™ìŠµ | ì•¡ì…˜ í† í° ìžë™ ìƒì„± |
| **ìž…ë ¥** | í…ìŠ¤íŠ¸ + ì•¡ì…˜ í† í° | í…ìŠ¤íŠ¸ë§Œ |
| **ì¶œë ¥** | ì•¡ì…˜ í† í° ì˜ˆì¸¡ | ì•¡ì…˜ í† í° ìƒì„± |
| **ë°©ì‹** | Teacher Forcing | Autoregressive Generation |

## ðŸ”§ 6. ì‹¤ì œ FT ì½”ë“œì™€ LSTM Layer í•™ìŠµ ì½”ë“œ

### 6.1 VLM Fine-tuning ì½”ë“œ

**BaseRoboVLM._trainable_params_setup() - íŒŒë¼ë¯¸í„° ë™ê²° ì„¤ì •**
```python
def _trainable_params_setup(self):
    model = self.model
    
    # 1ë‹¨ê³„: ë°±ë³¸ ëª¨ë¸ ë™ê²° ì„¤ì •
    if self.train_setup_configs["freeze_backbone"]:
        model.requires_grad_(False)  # ì „ì²´ ëª¨ë¸ ë™ê²°
    else:
        if self.train_setup_configs.get("train_decoder_layers", -1) == -1:
            model.requires_grad_(True)  # ì „ì²´ ëª¨ë¸ í•™ìŠµ
        else:
            # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
            model.requires_grad_(False)
            for layer in self.text_tower.layers[-self.train_setup_configs["train_decoder_layers"]:]:
                layer.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë¹„ì „ íƒ€ì›Œ ë™ê²° ì„¤ì •
    if self.train_setup_configs.get("train_vision", False):
        self.vision_tower.requires_grad_(True)
    else:
        self.vision_tower.requires_grad_(False)
    
    # 3ë‹¨ê³„: LoRA ì„¤ì •
    if self.train_setup_configs["lora_enable"]:
        # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        pass
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:470-512`

**BaseTrainer.training_step() - í•™ìŠµ ìŠ¤í…**
```python
def training_step(self, batch, batch_idx):
    # 1ë‹¨ê³„: ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
    (rgb, hand_rgb, attention_mask, language, text_mask,
     arm_action, gripper_action, instr_and_action_ids,
     instr_and_action_labels, instr_and_action_mask) = self._process_batch(batch)
    
    # 2ë‹¨ê³„: ëª¨ë¸ Forward Pass
    prediction = self.model.forward(
        rgb,                    # ë¹„ì „ ìž…ë ¥
        language,               # ì–¸ì–´ ìž…ë ¥
        attention_mask=text_mask,
        action_labels=(arm_action, gripper_action),
        action_mask=chunk_mask,
        instr_and_action_ids=instr_and_action_ids,
        instr_and_action_labels=instr_and_action_labels,
        instr_and_action_mask=instr_and_action_mask
    )
    
    # 3ë‹¨ê³„: Loss ê³„ì‚°
    output = self._get_loss(prediction)
    
    return output
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:565-621`

### 6.2 LSTM Layer í•™ìŠµ ì½”ë“œ

**LSTMDecoder.forward() - LSTM Forward Pass**
```python
def forward(self, tok_seq, h_0=None, **kwargs):
    # 1ë‹¨ê³„: ë‹¤ìš´ìƒ˜í”Œë§ ì²˜ë¦¬
    if self.down_sample == "none":
        tok_seq = rearrange(tok_seq, "b l n d-> b l (n d)")
    elif self.down_sample == "pooling":
        tok_seq = self.global_1d_pool(tok_seq.permute(0, 2, 1))
    
    # 2ë‹¨ê³„: ížˆìŠ¤í† ë¦¬ ê´€ë¦¬
    if tok_seq.shape[1] == 1:
        self.history_memory.append(tok_seq)
        if len(self.history_memory) <= self.history_len:
            # ížˆìŠ¤í† ë¦¬ ê¸¸ì´ ë‚´ì—ì„œ LSTM ì²˜ë¦¬
            x, h_n = self.rnn(tok_seq, self.hidden_state)
            self.hidden_state = h_n
        else:
            # ìœˆë„ìš° ìŠ¬ë¼ì´ë”©
            for _ in range(len(self.history_memory) - self.history_len):
                self.history_memory.pop(0)
            hist_feature = torch.cat(self.history_memory, dim=1)
            self.hidden_state = None
            x, h_n = self.rnn(hist_feature, self.hidden_state)
    else:
        # ë°°ì¹˜ ì²˜ë¦¬
        x, h_n = self.rnn(tok_seq, h_0)
        self.hidden_state = h_n
    
    # 3ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡
    actions = self.actions(x)      # íŒ” ì•¡ì…˜ (6-DOF)
    gripper = self.gripper(x)      # ê·¸ë¦¬í¼ ì•¡ì…˜ (1-DOF)
    
    # 4ë‹¨ê³„: ì¶œë ¥ í˜•íƒœ ì¡°ì •
    actions = rearrange(actions, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    gripper = rearrange(gripper, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    
    return actions, gripper
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:223-224`

**LSTMDecoder.loss() - Loss ê³„ì‚°**
```python
def loss(self, pred_action_logits, labels, attention_mask=None):
    # 1ë‹¨ê³„: ì‹œí€€ìŠ¤ ì‹œí”„íŠ¸ (autoregressive í•™ìŠµ)
    shift_logits = pred_action_logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # 2ë‹¨ê³„: CrossEntropyLoss ê³„ì‚°
    loss_fct = nn.CrossEntropyLoss()
    loss = loss_fct(
        shift_logits.view(-1, shift_logits.size(-1)), 
        shift_labels.view(-1)
    )
    
    # 3ë‹¨ê³„: ì•¡ì…˜ ë§ˆìŠ¤í‚¹
    mask = torch.logical_and(
        labels > self.action_tokenizer.action_token_begin_idx,
        labels < self.action_tokenizer.action_token_end_idx
    )
    
    # 4ë‹¨ê³„: ì •í™•ë„ ê³„ì‚°
    pred_action = pred_action_logits.argmax(dim=-1)
    correct_preds = torch.logical_and((pred_action == labels), mask)
    
    # 5ë‹¨ê³„: íŒ”/ê·¸ë¦¬í¼ ì •í™•ë„ ë¶„ë¦¬ ê³„ì‚°
    arm_acc = correct_preds_cut[:, :6].sum().float() / correct_preds_cut[:, :6].numel()
    gripper_acc = correct_preds_cut[:, -1].sum().float() / correct_preds_cut[:, -1].numel()
    
    return {
        "loss_arm": loss,
        "acc_arm": arm_acc,
        "acc_gripper": gripper_acc
    }
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:226-281`

### 6.3 ì‹¤ì œ í•™ìŠµ ë£¨í”„ ì½”ë“œ

**LSTM í•™ìŠµ ë£¨í”„ ì˜ˆì‹œ**
```python
# LSTM í•™ìŠµ ë£¨í”„ (base_policy.py:625-642)
net = LSTMDecoder(
    in_features=1024,
    action_dim=7,
    down_sample="pooling",
    latent=1,
    fwd_pred_next_n=2,
    window_size=12,
)

optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)
bs = 5
window_size = 12
text_len = 8
tokens = torch.randn(bs, window_size, text_len, 1024)
labels = (torch.randn(bs, window_size, 2, 6), torch.ones(bs, window_size, 2))
att_mask = torch.ones(bs, window_size, 2)

for i in range(10000):
    # Forward Pass
    actions, gripper = net(tokens)
    pred_action_logitss = torch.cat([actions, gripper.unsqueeze(-1)], dim=-1)
    
    # Loss ê³„ì‚°
    optimizer.zero_grad()
    loss = net.loss(pred_action_logitss, labels, att_mask)
    
    # Backward Pass
    loss_arm = loss["loss_arm"]
    loss_gripper = loss["loss_gripper"]
    acc_gripper = loss["acc_gripper"]
    loss_act = loss_arm + 0.01 * loss_gripper
    loss_act.backward()
    optimizer.step()
    
    print("iter: {}, loss: {} gripper: {} acc: {}".format(
        i, loss_act.item(), loss_gripper.item(), acc_gripper
    ))
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:625-642`

### 6.4 Loss ê³„ì‚° í•¨ìˆ˜

**calculate_vl_cross_entropy() - Vision-Language Cross Entropy**
```python
def calculate_vl_cross_entropy(logits, labels, mask=None):
    # 1ë‹¨ê³„: ì‹œí€€ìŠ¤ ì‹œí”„íŠ¸
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # 2ë‹¨ê³„: Loss ê³„ì‚°
    if mask is None:
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(
            shift_logits.view(-1, logits.shape[-1]),
            shift_labels.view(-1),
        )
    else:
        # ë§ˆìŠ¤í‚¹ëœ Loss ê³„ì‚°
        loss_fct = nn.CrossEntropyLoss(reduction="none")
        loss = loss_fct(
            shift_logits.view(-1, logits.shape[-1]),
            shift_labels.view(-1),
        )
        # ë§ˆìŠ¤í¬ ì ìš©
        mask = mask[..., 1:].contiguous()
        loss = loss * mask.reshape(-1)
        loss = loss.mean()
    
    return loss
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/loss.py:5-28`

### 6.5 ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

**CALVIN Fine-tuning ì„¤ì •**
```json
{
  "train_setup": {
    "precision": "bf16",
    "predict_action": true,
    "predict_forward": false,
    "predict_caption": false,
    "train_vision": true,
    "freeze_backbone": false,
    "freeze_mm_mlp_adapter": false,
    "lora_enable": false,
    "train_text_embedding": true
  },
  "act_head": {
    "type": "LSTMDecoder",
    "hidden_size": 1024,
    "action_dim": 7,
    "down_sample": "none",
    "latent": 1,
    "fwd_pred_next_n": 1,
    "window_size": 1,
    "action_space": "continuous"
  }
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:228-267`

## ðŸ”§ 7. í•™ìŠµ ë³€ìˆ˜ì™€ ì¶”ë¡  ë³€ìˆ˜ ìƒì„¸ ë¶„ì„

### 7.1 í•™ìŠµ ë³€ìˆ˜ (Training Variables)

**BaseRoboVLM._trainable_params_setup() - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì„¤ì •**
```python
def _trainable_params_setup(self):
    model = self.model
    
    # 1ë‹¨ê³„: ë°±ë³¸ ëª¨ë¸ ë™ê²° ì„¤ì •
    if self.train_setup_configs["freeze_backbone"]:
        model.requires_grad_(False)  # ì „ì²´ ëª¨ë¸ ë™ê²°
    else:
        if self.train_setup_configs.get("train_decoder_layers", -1) == -1:
            model.requires_grad_(True)  # ì „ì²´ ëª¨ë¸ í•™ìŠµ
        else:
            # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
            model.requires_grad_(False)
            for layer in self.text_tower.layers[-self.train_setup_configs["train_decoder_layers"]:]:
                layer.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë¹„ì „ íƒ€ì›Œ ë™ê²° ì„¤ì •
    if self.train_setup_configs.get("train_vision", False):
        self.vision_tower.requires_grad_(True)
    else:
        self.vision_tower.requires_grad_(False)
    
    # 3ë‹¨ê³„: LoRA ì„¤ì •
    if self.train_setup_configs["lora_enable"]:
        # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        pass
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:470-512`

**BaseTrainer.get_grouped_params() - í•™ìŠµ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”**
```python
def get_grouped_params(self, model):
    return [
        {
            "params": [p for n, p in model.named_parameters() if p.requires_grad],
            "weight_decay": self.configs["weight_decay"],
        }
    ]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:716-722`

**RoboFlamingo._trainable_params_setup() - Flamingo ëª¨ë¸ í•™ìŠµ ì„¤ì •**
```python
def _trainable_params_setup(self):
    self.requires_grad_(False)
    
    # 1ë‹¨ê³„: ë¹„ì „ ì¸ì½”ë” í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_vision"]:
        self.vision_encoder.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë””ì½”ë” ë ˆì´ì–´ í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_decoder_layers"] == -1:
        self.model.gated_cross_attn_layers.requires_grad_(True)
    else:
        # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
        ix = self.train_setup_configs["train_decoder_layers"]
        for layer in self.model.gated_cross_attn_layers[-ix:]:
            layer.requires_grad_(True)
    
    # 3ë‹¨ê³„: ì „ì²´ ë””ì½”ë” í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_full_decoder"]:
        self.model.requires_grad_(True)
    
    # 4ë‹¨ê³„: ë¦¬ìƒ˜í”ŒëŸ¬ í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_resampler"]:
        self.perceiver.requires_grad_(True)
    else:
        self.perceiver.requires_grad_(False)
    
    # 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ìž„ë² ë”© í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_text_embedding"]:
        self.model.get_input_embeddings().requires_grad_(True)
    else:
        self.model.get_input_embeddings().requires_grad_(False)
    
    # 6ë‹¨ê³„: ì•¡ì…˜ í—¤ë“œ í•™ìŠµ ì„¤ì •
    self.act_head.requires_grad_(True)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/roboflamingo.py:131-156`

### 7.2 ì¶”ë¡  ë³€ìˆ˜ (Inference Variables)

**BaseRoboVLM.inference() - ì¶”ë¡  ëª¨ë“œ ì„¤ì •**
```python
def inference(
    self,
    vision_x: torch.Tensor,
    lang_x: torch.Tensor,
    attention_mask: torch.Tensor = None,
    position_ids: torch.LongTensor = None,
    use_cached_vision_x: bool = False,
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,
    action_mask: torch.Tensor = None,
    caption_labels: torch.Tensor = None,
    caption_mask: torch.Tensor = None,
    past_key_values=None,
    use_cache: bool = False,
    vision_gripper=None,
    **kwargs,
):
    prediction = {}
    
    # 1ë‹¨ê³„: ìž…ë ¥ ê²€ì¦
    assert vision_x is not None
    bs, seq_len = vision_x.shape[:2]
    action_space = self.act_head_configs.get("action_space", "continuous")
    
    # 2ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡
    if self.train_setup_configs["predict_action"]:
        if action_space == "discrete":
            action = self.pred_action_discrete(
                lang_x, vision_x, vision_gripper, attention_mask
            )
            prediction["action"] = action
        else:
            prediction["action"] = self.forward_continuous(
                vision_x,
                lang_x,
                attention_mask,
                vision_gripper=vision_gripper,
                mode="inference",
            )
    
    return prediction
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1454-1491`

**BaseModelInference.__init__() - ì¶”ë¡  ëª¨ë¸ ì´ˆê¸°í™”**
```python
def __init__(
    self,
    ckpt_path,
    configs,
    device,
    save_dir=None,
    unnorm_key: Optional[str] = None,
    policy_setup: str = "widowx_bridge",
    exec_horizon=1,
):
    self.configs = configs
    self.dataset_stat = self.load_dataset_stat()
    self.model = BaseTrainer(configs=configs)
    self.policy = self.model
    
    # 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # 2ë‹¨ê³„: ì •ì±… ì„¤ì •
    if policy_setup == "widowx_bridge":
        unnorm_key = "bridge_orig" if unnorm_key is None else unnorm_key
    elif policy_setup == "google_robot":
        unnorm_key = "fractal20220817_data" if unnorm_key is None else unnorm_key
    
    # 3ë‹¨ê³„: ê·¸ë¦¬í¼ ì•¡ì…˜ ì„¤ì •
    self.sticky_gripper_num_repeat = 2
    self.policy_setup = policy_setup
    self.unnorm_key = unnorm_key
    
    if self.policy_setup == "google_robot":
        self.close_gripper_act = -1
    elif self.policy_setup == "widowx_bridge":
        self.close_gripper_act = 1
    
    # 4ë‹¨ê³„: ì´ë¯¸ì§€ ë° ì•¡ì…˜ ì„¤ì •
    self.image_size = self.configs.get("image_size", 224)
    self.action_scale = self.configs.get("action_scale", 1.0)
    self.horizon = self.configs["window_size"]
    self.window_size = self.horizon
    self.pred_action_horizon = exec_horizon
```

**ì¶œì²˜**: `RoboVLMs/eval/simpler/model_wrapper.py:15-58`

**StandaloneVLAInference.load_model() - ì¶”ë¡  ëª¨ë¸ ë¡œë“œ**
```python
def load_model(self):
    """VLA ëª¨ë¸ ë¡œë“œ"""
    try:
        print(f"ðŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_id}")
        
        model_save_path = Path(self.model_cache_dir) / self.model_id.split('/')[-1]
        model_save_path.mkdir(parents=True, exist_ok=True)

        # 1ë‹¨ê³„: í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(
            self.model_id, 
            cache_dir=model_save_path
        )

        # 2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "cache_dir": model_save_path,
            "low_cpu_mem_usage": True
        }
        
        if self.device.type == "cuda":
            model_kwargs["torch_dtype"] = torch.bfloat16
            model_kwargs["device_map"] = "auto"
        else:
            model_kwargs["torch_dtype"] = torch.float32

        self.model = PaliGemmaForConditionalGeneration.from_pretrained(
            self.model_id, 
            **model_kwargs
        )
        
        if self.device.type != "cuda":
            self.model.to(self.device)
        
        # 3ë‹¨ê³„: ì¶”ë¡  ëª¨ë“œ ì„¤ì •
        self.model.eval()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise
```

**ì¶œì²˜**: `RoboVLMs/vla_test/standalone_vla_test.py:46-85`

### 7.3 í•™ìŠµ vs ì¶”ë¡  ë³€ìˆ˜ ë¹„êµ

| êµ¬ë¶„ | í•™ìŠµ ë³€ìˆ˜ | ì¶”ë¡  ë³€ìˆ˜ |
|------|-----------|-----------|
| **ëª¨ë“œ** | `model.train()` | `model.eval()` |
| **ê·¸ëž˜ë””ì–¸íŠ¸** | `requires_grad=True` | `requires_grad=False` |
| **ìºì‹œ** | `use_cache=False` | `use_cache=True` |
| **ë“œë¡­ì•„ì›ƒ** | í™œì„±í™” | ë¹„í™œì„±í™” |
| **ë°°ì¹˜ ì •ê·œí™”** | í•™ìŠµ ëª¨ë“œ | í‰ê°€ ëª¨ë“œ |
| **ë©”ëª¨ë¦¬** | ë†’ìŒ (ê·¸ëž˜ë””ì–¸íŠ¸) | ë‚®ìŒ (ê·¸ëž˜ë””ì–¸íŠ¸ ì—†ìŒ) |
| **ìž…ë ¥** | `action_labels` í¬í•¨ | `action_labels` ì—†ìŒ |
| **ì¶œë ¥** | Loss ê³„ì‚° | ì•¡ì…˜ ì˜ˆì¸¡ë§Œ |
| **í† í° ì‚½ìž…** | Teacher Forcing | Autoregressive |

### 7.4 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**Docker í™˜ê²½ ë³€ìˆ˜ (docker-compose.yml)**
```yaml
environment:
  - DISPLAY=${DISPLAY:-:0}
  - ROS_DOMAIN_ID=42
  - CUDA_VISIBLE_DEVICES=0
  - TORCH_DTYPE=bfloat16
  - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
  - TRANSFORMERS_CACHE=/workspace/.vlms
  - HF_HOME=/workspace/.vlms
  - PYTHONPATH=/workspace:/workspace/robovlms
  - VLA_MODEL=paligemma-3b-mix-224
  - ACTION_MODE=automotive
  - ACTION_DIM=4
  - WINDOW_SIZE=8
  - INFERENCE_LATENCY_TARGET=100
  - PROJECT_NAME=k_project_event_vla
```

**ì¶œì²˜**: `RoboVLMs/docker-compose.yml:25-39`

### 7.5 í•™ìŠµ ë³€ìˆ˜ ìƒì„¸ ì„¤ì •

**CALVIN Fine-tuning ì„¤ì •**
```json
{
  "train_setup": {
    "precision": "bf16",
    "predict_action": true,
    "predict_forward": false,
    "predict_caption": false,
    "train_vision": true,
    "freeze_backbone": false,
    "freeze_mm_mlp_adapter": false,
    "lora_enable": false,
    "train_text_embedding": true
  },
  "act_head": {
    "type": "LSTMDecoder",
    "hidden_size": 1024,
    "action_dim": 7,
    "down_sample": "none",
    "latent": 1,
    "fwd_pred_next_n": 1,
    "window_size": 1,
    "action_space": "continuous"
  }
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:228-267`

## ðŸ¤– 8. ì‹¤ì œ ë¡œë´‡ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ìƒì„¸ ë¶„ì„

### 8.1 Real-World Experiments ë²¤ì¹˜ë§ˆí¬

**ë²¤ì¹˜ë§ˆí¬ ê°œìš”**
- **ì´ ìž‘ì—… ìˆ˜**: 105ê°œì˜ ì¡°ìž‘ ìž‘ì—…
- **ë°ì´í„° ê·œëª¨**: 70,000ê°œ ì´ìƒì˜ ì›ê²© ì¡°ìž‘ ì¸ê°„ ê¶¤ì 
- **í‰ê°€ ì„¤ì •**: 1ê°œ ë‹¨ìˆœ ì„¤ì • + 4ê°œ ë„ì „ì  ë¯¸ì§€ ì„¤ì •
- **ì´ í‰ê°€ ìž‘ì—…**: 20ê°œ ìž‘ì—…
- **ë¡¤ì•„ì›ƒ**: ê° ì„¤ì •ë‹¹ 3íšŒ ë¡¤ì•„ì›ƒ (ìž‘ì—…ë‹¹ 5ê°œ ì„¤ì •)

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Appendix K, Appendix D, Figure 15-17

**ë¡œë´‡ ì‚¬ì–‘**
- **ìžìœ ë„**: 7-DOF (6ì°¨ì› ìžì„¸ + 1ì°¨ì› ê·¸ë¦¬í¼)
- **ê´€ì¸¡ ì •ë³´**: ê³ ìœ  ê°ê° ì •ë³´ + ì‹œê° ê´€ì¸¡ + ì–¸ì–´ ìž…ë ¥

### 8.2 CALVIN ë²¤ì¹˜ë§ˆí¬ ìƒì„¸

**CALVIN [32] - Simulated Benchmark**

**ë°ì´í„°ì…‹ êµ¬ì„±**
```python
# CALVIN ë°ì´í„°ì…‹ êµ¬ì¡°
calvin_dataset = {
    "demonstrations": 24000,                    # 24k ì¸ê°„ ì›ê²© ì¡°ìž‘ ë°ëª¨
    "trajectory_length": "< 64 timesteps",      # ê° ê¶¤ì  64 íƒ€ìž„ìŠ¤í… ì´í•˜
    "language_annotations": True,               # ì–¸ì–´ ëª…ë ¹ í¬í•¨
    "basic_skills": 34,                         # 34ê°œ ì‚¬ì „ ì •ì˜ ê¸°ë³¸ ìŠ¤í‚¬
    "splits": ["scene_A", "scene_B", "scene_C", "scene_D"]
}
```

**34ê°œ ê¸°ë³¸ ìŠ¤í‚¬ ëª©ë¡**
1. rotate blue block right
2. move slider right
3. lift red block slider
4. place slider
5. turn off light bulb
6. turn off led light
7. push in drawer
8. lift blue block drawer
9. close drawer
10. lift pink block slider
11. lift pink block table
12. move slider left
13. open drawer
14. turn on light bulb
15. rotate blue block left
16. push blue block left
17. rotate red block right
18. turn on led light
19. push pink block right
20. push red block left
21. lift blue block table
22. place in drawer
23. rotate red block left
24. push pink block left
25. lift stacked blocks
26. lift blue block slider
27. push red block right

**í‰ê°€ ë©”íŠ¸ë¦­**
- **Sequential Task Success Rate**: 5ê°œ ì—°ì† ìž‘ì—… ì™„ë£Œ ì„±ê³µë¥ 
- **Average Length**: ë‹¬ì„±í•œ ìž‘ì—…ì˜ í‰ê·  ê¸¸ì´
- **í‰ê°€ ê·œëª¨**: D splitì—ì„œ 1000 ë¡¤ì•„ì›ƒ, ê° ë¡¤ì•„ì›ƒë‹¹ 5ê°œ ì—°ì† ì„œë¸ŒíƒœìŠ¤í¬

**ì¶œì²˜**: CALVIN ë…¼ë¬¸ [32], RoboVLMs ë…¼ë¬¸

### 8.3 SimplerEnv ë²¤ì¹˜ë§ˆí¬

**SimplerEnv [25] - Real-to-Sim Evaluation**

**ë²¤ì¹˜ë§ˆí¬ ëª©ì **
- ì‹¤ì œ ë¡œë´‡ ì •ì±…ì„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í‰ê°€
- Google Robot, BridgeData V2ì™€ ë¹„êµ ê°€ëŠ¥í•œ ì•„ë ˆë‚˜ ì œê³µ
- íš¨ìœ¨ì ì´ê³  í™•ìž¥ ê°€ëŠ¥í•œ ì‹¤ì œ ì„¸ê³„ í‰ê°€ ëŒ€ì•ˆ

#### 8.3.1 Google Robot ì„¤ì • ìž‘ì—…

**1) pick coke can**
```python
# pick coke can ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ë¹ˆ ì½”í¬ ìº”ì„ í…Œì´ë¸”ì—ì„œ ì§‘ì–´ ë“¤ê¸°",
    "positions": ["horizontal", "vertical", "upright"],  # 3ê°€ì§€ ìœ„ì¹˜
    "grid_points": 25,                                   # ì§ì‚¬ê°í˜• ì˜ì—­ ë‚´ 25ê°œ ê·¸ë¦¬ë“œ
    "total_trials": 75,                                  # 25 Ã— 3 = 75 ì‹œí—˜
    "distractors": False                                 # í‘œì¤€ ì„¤ì •ì—ì„œëŠ” ë°©í•´ ìš”ì†Œ ì—†ìŒ
}
```

**2) move {obj1} near {obj2}**
```python
# move near ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "obj1ì„ obj2 ê·¼ì²˜ë¡œ ì´ë™",
    "objects": ["blue plastic bottle", "Pepsi can", "orange", 
                "7up can", "apple", "sponge", "Coke can", "Redbull can"],  # 8ê°œ ë¬¼ì²´
    "formation": "triangular",                           # ì‚¼ê°í˜• ë°°ì¹˜
    "triplets": 5,                                       # 5ê°œ triplet (ëžœë¤ ì„ íƒ)
    "patterns": ["upright", "inverted"],                 # 2ê°€ì§€ ì‚¼ê°í˜• íŒ¨í„´
    "trials_per_triplet": 6,                            # tripletë‹¹ 6íšŒ ì‹œí—˜
    "total_trials": 60                                   # 5 Ã— 6 Ã— 2 = 60 ì‹œí—˜
}
```

**3) (open/close) (top/middle/bottom) drawer**
```python
# drawer ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "íŠ¹ì • ì„œëž ì—´ê¸°/ë‹«ê¸°",
    "drawers": 3,                                        # top, middle, bottom
    "actions": ["open", "close"],                        # 2ê°€ì§€ ì•¡ì…˜
    "robot_positions": 9,                                # 9ê°œ ê·¸ë¦¬ë“œ ìœ„ì¹˜
    "total_trials": 54,                                  # 3 Ã— 2 Ã— 9 = 54 ì‹œí—˜
    "evaluation_type": "articulated_objects"             # ê´€ì ˆ ë¬¼ì²´ ì²˜ë¦¬ ëŠ¥ë ¥ í‰ê°€
}
```

**4) open top drawer; place apple into top drawer**
```python
# multi-step ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ì„œëž ì—´ê³  ì‚¬ê³¼ë¥¼ ì„œëžì— ë„£ê¸°",
    "steps": [
        "open top drawer",
        "place apple into top drawer"
    ],
    "robot_positions": 3,                                # ë¡œë´‡ ìœ„ì¹˜ 3ê°œ
    "apple_positions": 9,                                # ì‚¬ê³¼ ê·¸ë¦¬ë“œ ìœ„ì¹˜ 9ê°œ
    "total_trials": 27,                                  # 3 Ã— 9 = 27 ì‹œí—˜
    "instruction_switch": "midpoint or terminate token", # ëª…ë ¹ ì „í™˜ ì‹œì 
    "evaluation_type": "sequential_multi-action"         # ìˆœì°¨ì  ë‹¤ì¤‘ ì•¡ì…˜ í‰ê°€
}
```

#### 8.3.2 WidowX + Bridge ì„¤ì • ìž‘ì—…

**1) put the spoon on the towel**
```python
# spoon on towel ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ìˆ˜ì €ë¥¼ íƒ€ì›” ìœ„ì— ë†“ê¸°",
    "square_size": "15 cm",                              # ì •ì‚¬ê°í˜• í¬ê¸°
    "spoon_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "towel_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "spoon_orientations": ["horizontal", "vertical"],    # 2ê°€ì§€ ë°©í–¥
    "total_trials": 24,                                  # 4 Ã— 4 Ã— 2 / 2 = 24 ì‹œí—˜
    "gripper_adjustment": True                           # ê·¸ë¦¬í¼ ë°©í–¥ ì¡°ì • í•„ìš”
}
```

**2) put carrot on plate**
```python
# carrot on plate ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ë‹¹ê·¼ì„ ì ‘ì‹œ ìœ„ì— ë†“ê¸°",
    "square_size": "15 cm",
    "carrot_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "plate_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "total_trials": 24,
    "similar_to": "put the spoon on the towel"
}
```

**3) stack the green block on the yellow block**
```python
# block stacking ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ì´ˆë¡ ë¸”ë¡ì„ ë…¸ëž€ ë¸”ë¡ ìœ„ì— ìŒ“ê¸°",
    "block_size": "3 cm",                                # ë¸”ë¡ í¬ê¸°
    "square_configs": [
        {"size": "10 cm", "trials": 12},                 # 10cm ì •ì‚¬ê°í˜•
        {"size": "20 cm", "trials": 12}                  # 20cm ì •ì‚¬ê°í˜•
    ],
    "green_block_positions": 4,                          # 4ê°œ ì½”ë„ˆ
    "yellow_block_positions": 4,                         # 4ê°œ ì½”ë„ˆ
    "total_trials": 24                                   # (4 Ã— 4 / 2) Ã— 2 = 24 ì‹œí—˜
}
```

**4) put eggplant into yellow basket**
```python
# eggplant into basket ìž‘ì—… ì„¤ì •
task_config = {
    "objective": "ê°€ì§€ë¥¼ ë…¸ëž€ ë°”êµ¬ë‹ˆì— ë„£ê¸°",
    "environment": "sink with two basins",               # 2ê°œ ì„¸ë©´ëŒ€
    "eggplant_location": "right basin (random)",         # ì˜¤ë¥¸ìª½ ì„¸ë©´ëŒ€ (ëžœë¤ ìœ„ì¹˜)
    "basket_location": "left basin",                     # ì™¼ìª½ ì„¸ë©´ëŒ€
    "eggplant_variations": {
        "position": "random",
        "orientation": "random",
        "constraint": "easily graspable, away from edges"
    },
    "total_trials": 24
}
```

### 8.4 ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ìš”ì•½

| ë²¤ì¹˜ë§ˆí¬ | ìœ í˜• | ìž‘ì—… ìˆ˜ | ë°ì´í„° ê·œëª¨ | í‰ê°€ ë©”íŠ¸ë¦­ |
|---------|------|---------|-------------|-------------|
| **Real-World Experiments** | ì‹¤ì œ ë¡œë´‡ | 20ê°œ (105ê°œ ì¤‘) | 70,000+ ê¶¤ì  | ì„¤ì •ë³„ í‰ê·  ì„±ê³µë¥  |
| **CALVIN** | ì‹œë®¬ë ˆì´ì…˜ | 34ê°œ ê¸°ë³¸ ìŠ¤í‚¬ | 24,000 ë°ëª¨ | Sequential Success Rate, Avg Length |
| **SimplerEnv (Google)** | Real-to-Sim | 4ê°œ ìž‘ì—… | - | ì‹œí—˜ë³„ ì„±ê³µë¥  (75-54íšŒ) |
| **SimplerEnv (Bridge)** | Real-to-Sim | 4ê°œ ìž‘ì—… | - | ì‹œí—˜ë³„ ì„±ê³µë¥  (24íšŒ) |

### 8.5 ì½”ë“œ êµ¬í˜„ ì˜ˆì‹œ

**DiskCalvinDataset - CALVIN ë°ì´í„° ë¡œë”©**
```python
class DiskCalvinDataset(BaseCalvinDataset):
    """ë””ìŠ¤í¬ì—ì„œ ê°œë³„ íŒŒì¼ë¡œ ì—í”¼ì†Œë“œë¥¼ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹"""
    def __init__(
        self,
        image_fn: Callable,
        tokenizer: Callable,
        *args: Any,
        skip_frames: int = 1,
        seq_len: int = 1,
        **kwargs: Any,
    ):
        super().__init__(*args, **kwargs)
        # ... (ì´ˆê¸°í™” ì½”ë“œ)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:428-447`

**SimplerEnv í‰ê°€ í•¨ìˆ˜**
```python
def evaluate_simpler_env(model, env, task_config):
    """SimplerEnvì—ì„œ ëª¨ë¸ í‰ê°€"""
    success_count = 0
    total_trials = task_config["total_trials"]
    
    for trial in range(total_trials):
        # í™˜ê²½ ì´ˆê¸°í™”
        obs = env.reset()
        
        # ëª¨ë¸ ì¶”ë¡ 
        action = model.inference(
            vision_x=obs["rgb"],
            lang_x=task_config["instruction"]
        )
        
        # ì•¡ì…˜ ì‹¤í–‰ ë° í‰ê°€
        success = env.step(action)
        success_count += int(success)
    
    success_rate = success_count / total_trials
    return success_rate
```

## ðŸŽ¯ 9. í•µì‹¬ í•™ìŠµ ì•„ì´ë””ì–´

### 9.1 VLMì˜ ì—­í• 

**1) ë©€í‹°ëª¨ë‹¬ ì´í•´**
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì´í•´
- ë¡œë´‡ í™˜ê²½ì˜ ì‹œê°ì  ìƒí™© íŒŒì•…
- ì–¸ì–´ ëª…ë ¹ì˜ ì˜ë¯¸ í•´ì„

**2) íŠ¹ì§• ì¶”ì¶œ**
- ì´ë¯¸ì§€ì—ì„œ ë¡œë´‡ ìƒíƒœ ì •ë³´ ì¶”ì¶œ
- í…ìŠ¤íŠ¸ì—ì„œ ì•¡ì…˜ ì˜ë„ íŒŒì•…
- ë©€í‹°ëª¨ë‹¬ ìœµí•© íŠ¹ì§• ìƒì„±

### 5.2 LSTMì˜ ì—­í• 

**1) ì‹œí€€ìŠ¤ ëª¨ë¸ë§**
- ì‹œê°„ì  ì˜ì¡´ì„± í•™ìŠµ
- ì´ì „ ì•¡ì…˜ì— ê¸°ë°˜í•œ ë‹¤ìŒ ì•¡ì…˜ ì˜ˆì¸¡
- ë¡œë´‡ ê¶¤ì ì˜ ì—°ì†ì„± ë³´ìž¥

**2) ì•¡ì…˜ ì˜ˆì¸¡**
- VLM íŠ¹ì§•ì„ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
- 7-DOF ë¡œë´‡ ì•¡ì…˜ ìƒì„±
- íŒ”ê³¼ ê·¸ë¦¬í¼ì˜ ì¡°í™”ë¡œìš´ ì œì–´

### 5.3 í•™ìŠµ ì „ëžµ

**1) End-to-End í•™ìŠµ**
- VLMê³¼ LSTMì„ ë™ì‹œì— í•™ìŠµ
- ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ìµœì í™”
- ë©€í‹°ëª¨ë‹¬ ì´í•´ì™€ ì•¡ì…˜ ì˜ˆì¸¡ì˜ í†µí•©

**2) ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**
- ì•¡ì…˜ ì˜ˆì¸¡ (ì£¼ íƒœìŠ¤í¬)
- ìº¡ì…˜ ìƒì„± (ë³´ì¡° íƒœìŠ¤í¬)
- ë¯¸ëž˜ ì˜ˆì¸¡ (ì„ íƒì )

## ðŸ“Š 6. í•™ìŠµ íš¨ê³¼ ë¶„ì„

### 6.1 VLM Fine-tuning íš¨ê³¼

**Before Fine-tuning**
- ì¼ë°˜ì ì¸ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì´í•´
- ë¡œë´‡ í™˜ê²½ì— íŠ¹í™”ë˜ì§€ ì•ŠìŒ
- ì•¡ì…˜ ì˜ˆì¸¡ ëŠ¥ë ¥ ë¶€ì¡±

**After Fine-tuning**
- ë¡œë´‡ í™˜ê²½ì— íŠ¹í™”ëœ ì´í•´
- ì•¡ì…˜-ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ê´€ì„± í•™ìŠµ
- ë©€í‹°ëª¨ë‹¬ ìœµí•© ëŠ¥ë ¥ í–¥ìƒ

### 6.2 LSTM í•™ìŠµ íš¨ê³¼

**Before LSTM í•™ìŠµ**
- ë‹¨ìˆœí•œ ì•¡ì…˜ ë§¤í•‘
- ì‹œê°„ì  ì˜ì¡´ì„± ë¶€ì¡±
- ê¶¤ì  ì—°ì†ì„± ë¬¸ì œ

**After LSTM í•™ìŠµ**
- ì‹œí€€ìŠ¤ ê¸°ë°˜ ì•¡ì…˜ ì˜ˆì¸¡
- ì‹œê°„ì  ì˜ì¡´ì„± í•™ìŠµ
- ë¶€ë“œëŸ¬ìš´ ë¡œë´‡ ê¶¤ì  ìƒì„±

## ðŸ“Š 8. RoboVLMs ë…¼ë¬¸ ë°ì´í„° ìˆ˜ì§‘ ì •ë³´

### 8.1 ë…¼ë¬¸ ì •ë³´

**ë…¼ë¬¸ ì œëª©**: "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models"
**ë°œí‘œì¼**: 2024ë…„ 12ì›” 18ì¼
**arXiv ë§í¬**: https://arxiv.org/abs/2412.14058
**ì €ìž**: Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu

### 8.2 CALVIN ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì •ë³´

**CALVIN ë…¼ë¬¸ ì •ë³´**
- **ë…¼ë¬¸ ì œëª©**: "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks"
- **ë°œí‘œ ì—°ë„**: 2022ë…„
- **IEEE ë…¼ë¬¸ ë§í¬**: https://ieeexplore.ieee.org/document/9788026
- **GitHub ì €ìž¥ì†Œ**: https://github.com/mees/calvin

**ë°ì´í„° ìˆ˜ì§‘ ê·œëª¨**
- **ì´ ë°ëª¨ ìˆ˜**: 25,000ê°œ demonstrations
- **íƒœìŠ¤í¬ ìˆ˜**: 34ê°œ ê¸°ë³¸ íƒœìŠ¤í¬
- **ë¡œë´‡ í•˜ë“œì›¨ì–´**: Franka Emika Panda 7-DOF ë¡œë´‡íŒ”
- **ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½**: ì‹¤ì œ ë¬¼ë¦¬ í™˜ê²½ (í…Œì´ë¸”, ë¬¼ì²´, ì¡°ìž‘ ê³µê°„)

**ì¶œì²˜**: 
- arXiv ë…¼ë¬¸: https://arxiv.org/abs/2112.03227 (2021ë…„ 12ì›”)
- GitHub ì €ìž¥ì†Œ: https://github.com/mees/calvin
- Hugging Face ë°ì´í„°ì…‹: https://huggingface.co/datasets/nhop/calvin

**ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•**
- **ì „ë¬¸ê°€ ì¡°ìž‘**: ìˆ™ë ¨ëœ ì¡°ìž‘ìžê°€ ì§ì ‘ ë¡œë´‡ì„ ì œì–´
- **ë‹¤ì¤‘ ì¹´ë©”ë¼**: ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ
- **ì–¸ì–´ ì£¼ì„**: ê° ë°ëª¨ì— ëŒ€í•œ ìžì—°ì–´ ì„¤ëª… ì¶”ê°€
- **ë‹¤ì–‘í•œ íƒœìŠ¤í¬**: pick-and-place, navigation, manipulation ë“±

**ì¶œì²˜**: 
- arXiv ë…¼ë¬¸: https://arxiv.org/abs/2112.03227
- GitHub ì €ìž¥ì†Œ: https://github.com/mees/calvin
- Hugging Face ë°ì´í„°ì…‹: https://huggingface.co/datasets/nhop/calvin

**ë°ì´í„° êµ¬ì„±**
- **ì´ë¯¸ì§€ ë°ì´í„°**: RGB ì´ë¯¸ì§€ (224x224)
- **ì•¡ì…˜ ë°ì´í„°**: 7-DOF ì—°ì† ì•¡ì…˜ (íŒ” 6-DOF + ê·¸ë¦¬í¼ 1-DOF)
- **ì–¸ì–´ ë°ì´í„°**: ìžì—°ì–´ ëª…ë ¹ ë° ì„¤ëª…
- **ìƒíƒœ ë°ì´í„°**: ë¡œë´‡ ê´€ì ˆ ìƒíƒœ ë° ì„¼ì„œ ì •ë³´

**CALVIN ë°ì´í„°ì…‹ì˜ íŠ¹ì§•**
- **ìž¥ê¸°ì  ìž‘ì—… ì‹œí€€ìŠ¤**: "ì„œëžì„ ì—´ì–´ë¼", "íŒŒëž€ ë¸”ë¡ì„ ì„œëžì— ë°€ì–´ ë„£ì–´ë¼", "ì„œëžì„ ë‹«ì•„ë¼"ì™€ ê°™ì€ ì¼ë ¨ì˜ ì–¸ì–´ ì§€ì‹œ
- **ì—°ì†ì  ì œì–´**: 30Hzë¡œ ì—°ì†ì ì¸ ë™ìž‘ ìˆ˜í–‰
- **ìœ ì—°í•œ ì„¼ì„œ êµ¬ì„±**: ë‹¤ì–‘í•œ ì„¼ì„œ ìž…ë ¥ ì‹¤í—˜ ì§€ì›
- **ì˜¤í”ˆ ì†ŒìŠ¤**: ì—°êµ¬ìžë“¤ì´ ìžìœ ë¡­ê²Œ ì‚¬ìš©í•˜ê³  í™•ìž¥ ê°€ëŠ¥

### 8.3 ë°ì´í„° ìˆ˜ì§‘ì˜ íŠ¹ì§•

**1) Real-World íŠ¹ì„±**
- ì‹¤ì œ ë¬¼ë¦¬ í™˜ê²½ì—ì„œ ìˆ˜ì§‘
- ë¬¼ë¦¬ ë²•ì¹™ì„ ë”°ë¥´ëŠ” ë¡œë´‡ ë™ìž‘
- ì‹¤ì œ ë¬¼ì²´ì™€ì˜ ìƒí˜¸ìž‘ìš©

**2) ë‹¤ì–‘ì„±**
- 34ê°œ ì„œë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬
- ë‹¤ì–‘í•œ ë¬¼ì²´ì™€ í™˜ê²½
- ë‹¤ì–‘í•œ ì¡°ìž‘ íŒ¨í„´

**3) í’ˆì§ˆ**
- ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì¡°ìž‘
- ì¼ê´€ëœ ë°ì´í„° í’ˆì§ˆ
- ì •í™•í•œ ì–¸ì–´ ì£¼ì„

### 8.4 ë…¼ë¬¸ì˜ ì‹¤í—˜ ê·œëª¨

**ì‹¤í—˜ ê·œëª¨**
- **VLM ë°±ë³¸**: 8ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ VLM ëª¨ë¸
- **ì •ì±… ì•„í‚¤í…ì²˜**: 4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜
- **ì´ ì‹¤í—˜ ìˆ˜**: 600ê°œ ì´ìƒì˜ ì‹¤í—˜
- **í‰ê°€ í™˜ê²½**: ì‹œë®¬ë ˆì´ì…˜ + ì‹¤ì œ í™˜ê²½

**ì‹¤í—˜ ë²”ìœ„**
- ë‹¤ì–‘í•œ VLM ë°±ë³¸ ë¹„êµ
- ì •ì±… ì•„í‚¤í…ì²˜ ë¹„êµ
- ë°ì´í„° ë¶„í¬ ì˜í–¥ ë¶„ì„
- í•™ìŠµ ë°©ë²• ë¹„êµ

## ðŸŽ¯ 9. í•µì‹¬ ìš”ì•½

### 9.1 ì „ì²´ íŒŒì´í”„ë¼ì¸

1. **Real-World ë°ì´í„° ìˆ˜ì§‘**: CALVIN ë°ì´í„°ì…‹ (25k demonstrations, 34 tasks)
2. **VLM Fine-tuning**: ë©€í‹°ëª¨ë‹¬ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ
3. **LSTM í•™ìŠµ**: ì‹œí€€ìŠ¤ ê¸°ë°˜ ì•¡ì…˜ ì˜ˆì¸¡
4. **End-to-End ìµœì í™”**: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í•™ìŠµ

### 9.2 í•µì‹¬ ì•„ì´ë””ì–´

**VLMì˜ ì—­í• **: ë©€í‹°ëª¨ë‹¬ ì´í•´ + íŠ¹ì§• ì¶”ì¶œ
**LSTMì˜ ì—­í• **: ì‹œí€€ìŠ¤ ëª¨ë¸ë§ + ì•¡ì…˜ ì˜ˆì¸¡
**í•™ìŠµ ì „ëžµ**: End-to-End + ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ

### 9.3 í•™ìŠµ ìˆœì„œ

1. **ë°ì´í„° ì „ì²˜ë¦¬**: ì´ë¯¸ì§€ ì •ê·œí™” + ì•¡ì…˜ ì •ê·œí™”
2. **VLM Fine-tuning**: ë©€í‹°ëª¨ë‹¬ ì´í•´ ëŠ¥ë ¥ í•™ìŠµ
3. **LSTM í•™ìŠµ**: ì‹œí€€ìŠ¤ ê¸°ë°˜ ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ
4. **í†µí•© ìµœì í™”**: ì „ì²´ íŒŒì´í”„ë¼ì¸ End-to-End í•™ìŠµ

### 9.4 ë…¼ë¬¸ ì°¸ê³  ì •ë³´

**ë…¼ë¬¸ ë§í¬**: https://arxiv.org/abs/2412.14058
**ê³µì‹ ì›¹ì‚¬ì´íŠ¸**: https://robovlms.github.io/
**GitHub ì €ìž¥ì†Œ**: RoboVLMs í”„ë¡œì íŠ¸

ì´ ë¶„ì„ì„ í†µí•´ RoboVLMsì˜ VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ ê³¼ì •ì„ ëª…í™•ížˆ ì´í•´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
