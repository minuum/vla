# 07_4 RoboVLMs Complete Sequence Diagram - GitHub Citations

## ğŸ“Š **RoboVLMs Complete Training & Inference Sequence Diagram**

### **7.4.1 Training Pipeline Sequence**
- **Source**: `RoboVLMs/robovlms/train/base_trainer.py:565-625`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ í›ˆë ¨ íŒŒì´í”„ë¼ì¸
- **Training Step Process**:  # í›ˆë ¨ ë‹¨ê³„ ê³¼ì •
  ```python
  def training_step(self, batch, batch_idx):
      """í›ˆë ¨ ë‹¨ê³„ (ë°°ì¹˜ ì²˜ë¦¬)"""
      # ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
      (rgb, hand_rgb, attention_mask, language, text_mask, 
       fwd_rgb_chunck, fwd_hand_rgb_chunck, arm_action, gripper_action,
       arm_action_chunck, gripper_action_chunck, chunck_mask, fwd_mask,
       instr_and_action_ids, instr_and_action_labels, instr_and_action_mask,
       raw_text, rel_state, data_source) = self._process_batch(batch)
      
      # ëª¨ë¸ ìˆœì „íŒŒ (18í”„ë ˆì„ ë°°ì¹˜ ì²˜ë¦¬)
      prediction = self.model.forward(
          rgb, language, attention_mask=text_mask,
          action_labels=(arm_action_chunck, gripper_action_chunck),
          action_mask=chunck_mask, vision_gripper=hand_rgb,
          fwd_rgb_labels=fwd_rgb_chunck, fwd_hand_rgb_labels=fwd_hand_rgb_chunck,
          fwd_mask=fwd_mask, instr_and_action_ids=instr_and_action_ids,
          instr_and_action_labels=instr_and_action_labels,
          instr_and_action_mask=instr_and_action_mask,
          raw_text=raw_text, data_source=data_source, rel_state=rel_state
      )
      
      # ì†ì‹¤ ê³„ì‚°
      output = self._get_loss(prediction)
  ```

### **7.4.2 Inference Pipeline Sequence**
- **Source**: `RoboVLMs/vla_test/standalone_vla_test.py:87-124`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- **Inference Step Process**:  # ì¶”ë¡  ë‹¨ê³„ ê³¼ì •
  ```python
  def infer_from_image_and_text(self, image: np.ndarray, text_prompt: str) -> str:
      """ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì¶”ë¡ """
      # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
      if len(image.shape) == 3 and image.shape[2] == 3:
          rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB ë³€í™˜
      else:
          rgb_image = image
      pil_image = PilImage.fromarray(rgb_image)              # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
      
      # 2. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ (ë‹¨ì¼ ì´ë¯¸ì§€)
      inputs = self.processor(
          images=pil_image, text=text_prompt, return_tensors="pt"
      ).to(self.device)
      
      # 3. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì²˜ë¦¬)
      with torch.no_grad():
          outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens, do_sample=False)
          result = self.processor.decode(outputs[0], skip_special_tokens=True)
      
      return result
  ```

### **7.4.3 CALVIN Step Function Sequence**
- **Source**: `RoboVLMs/eval/calvin/model_wrapper.py:318-378`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ CALVIN Step í•¨ìˆ˜
- **Step Function Process**:  # Step í•¨ìˆ˜ ê³¼ì •
  ```python
  def step(self, obs, goal):
      """CALVIN Step í•¨ìˆ˜ (ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì²˜ë¦¬)"""
      # 1. ê´€ì°° ì „ì²˜ë¦¬
      image_x, gripper_x, text_x, mask = self.preprocess(obs, goal, self.action_space)
      
      # 2. ì…ë ¥ ë”•ì…”ë„ˆë¦¬ êµ¬ì„± (ë‹¨ì¼ ì´ë¯¸ì§€)
      input_dict = {
          "rgb": image_x,        # ë‹¨ì¼ ì´ë¯¸ì§€
          "hand_rgb": gripper_x, # ë‹¨ì¼ ê·¸ë¦¬í¼ ì´ë¯¸ì§€
          "text": text_x,        # í…ìŠ¤íŠ¸
          "text_mask": mask      # í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬
      }
      
      # 3. ëª¨ë¸ ì¶”ë¡  (ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì²˜ë¦¬)
      with torch.no_grad():
          action = self.policy.inference_step(input_dict)["action"]
      
      # 4. ì•¡ì…˜ í›„ì²˜ë¦¬
      if self.action_space != "discrete":
          action = torch.cat([action[0], (torch.nn.functional.sigmoid(action[1]) > 0.5).float()], dim=-1)
      
      # 5. ì•¡ì…˜ ì•™ìƒë¸” ì ìš©
      action = self.ensemble_action(action)
      
      # 6. ìµœì¢… ì•¡ì…˜ ë°˜í™˜
      if isinstance(action, torch.Tensor):
          action = action.squeeze()
          if action.ndim == 2:
              action = action[0]
      
      return action
  ```

## ğŸ¯ **Complete Sequence Diagram**

### **7.4.4 Training Sequence Diagram**
```mermaid
sequenceDiagram
    participant DataLoader as DataLoader
    participant Trainer as BaseTrainer
    participant Model as BaseRoboVLM
    participant Vision as Vision Encoder
    participant Text as Text Encoder
    participant Action as Action Encoder
    participant Fusion as Multimodal Fusion
    participant Policy as Policy Head
    participant Loss as Loss Function

    DataLoader->>Trainer: Batch Data (RGB, Text, Actions)
    Trainer->>Trainer: _process_batch()
    Note over Trainer: Process 18 frames (window_size + fwd_pred_next_n)
    
    Trainer->>Model: forward()
    Model->>Vision: encode_images(rgb)
    Vision-->>Model: Vision Features
    Model->>Text: encode_text(language)
    Text-->>Model: Text Features
    Model->>Action: encode_actions(actions)
    Action-->>Model: Action Features
    
    Model->>Fusion: merge_multi_modal_input()
    Fusion-->>Model: Fused Features
    Model->>Policy: forward_action()
    Policy-->>Model: Predicted Actions
    Model-->>Trainer: Prediction
    
    Trainer->>Loss: _get_loss()
    Loss-->>Trainer: Loss Values
    Trainer->>Trainer: Backward Pass
    Trainer->>Trainer: Update Parameters
```

### **7.4.5 Inference Sequence Diagram**
```mermaid
sequenceDiagram
    participant Camera as Camera
    participant Preprocess as Image Preprocessing
    participant Processor as Model Processor
    participant Model as RoboVLM Model
    participant Vision as Vision Encoder
    participant Text as Text Encoder
    participant Fusion as Multimodal Fusion
    participant Policy as Policy Head
    participant Robot as Robot Controller

    Camera->>Preprocess: Single Image
    Preprocess->>Preprocess: BGR to RGB Conversion
    Preprocess->>Processor: PIL Image
    Processor->>Processor: Tokenize Text
    Processor->>Model: Input Tensors
    
    Model->>Vision: encode_images(single_image)
    Vision-->>Model: Vision Features
    Model->>Text: encode_text(text_prompt)
    Text-->>Model: Text Features
    
    Model->>Fusion: merge_multi_modal_input()
    Fusion-->>Model: Fused Features
    Model->>Policy: forward_action()
    Policy-->>Model: Predicted Action
    
    Model-->>Processor: Action Output
    Processor->>Robot: Execute Action
    Robot->>Camera: Next Image
```

### **7.4.6 CALVIN Evaluation Sequence Diagram**
```mermaid
sequenceDiagram
    participant Env as CALVIN Environment
    participant Wrapper as CustomModel
    participant Preprocess as Preprocessing
    participant Model as RoboVLM Model
    participant Ensemble as Action Ensemble
    participant Robot as Robot Controller

    Env->>Wrapper: Observation (obs, goal)
    Wrapper->>Preprocess: preprocess(obs, goal)
    Preprocess-->>Wrapper: (image_x, gripper_x, text_x, mask)
    
    Wrapper->>Model: inference_step(input_dict)
    Model->>Model: Vision + Text + Action Processing
    Model-->>Wrapper: Raw Action
    
    Wrapper->>Ensemble: ensemble_action(action)
    Ensemble-->>Wrapper: Smoothed Action
    
    Wrapper->>Robot: Final Action
    Robot->>Env: Execute Action
    Env->>Wrapper: Next Observation
```

## ğŸ“Š **Technical Implementation Details**

### **7.4.7 Training vs Inference Key Differences**
- **Source**: `RoboVLMs/robovlms/train/base_trainer.py:345-395` vs `RoboVLMs/vla_test/standalone_vla_test.py:87-124`
- **Training**:  # í›ˆë ¨
  - **Batch Processing**: Multiple sequences (18 frames)  # ì—¬ëŸ¬ ì‹œí€€ìŠ¤ (18í”„ë ˆì„)
  - **Data Chunking**: Sliding window approach  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ‘ê·¼ë²•
  - **Loss Calculation**: Backward pass with gradients  # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•œ ì—­ì „íŒŒ
- **Inference**:  # ì¶”ë¡ 
  - **Single Image**: One image at a time  # í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€
  - **Sequential Processing**: Step-by-step execution  # ë‹¨ê³„ë³„ ì‹¤í–‰
  - **Real-time**: No gradient computation  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì—†ìŒ

### **7.4.8 Data Flow Architecture**
- **Source**: `RoboVLMs/robovlms/model/README.md:58-104`  # GitHub READMEì—ì„œ í™•ì¸ëœ ë°ì´í„° í”Œë¡œìš°
- **Input Processing**:  # ì…ë ¥ ì²˜ë¦¬
  - **Vision**: Camera images â†’ Vision Encoder â†’ Vision Features  # ì¹´ë©”ë¼ ì´ë¯¸ì§€ â†’ ë¹„ì „ ì¸ì½”ë” â†’ ë¹„ì „ íŠ¹ì§•
  - **Language**: Text prompt â†’ Text Encoder â†’ Text Features  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ â†’ í…ìŠ¤íŠ¸ ì¸ì½”ë” â†’ í…ìŠ¤íŠ¸ íŠ¹ì§•
  - **Action**: Previous actions â†’ Action Encoder â†’ Action Features  # ì´ì „ ì•¡ì…˜ â†’ ì•¡ì…˜ ì¸ì½”ë” â†’ ì•¡ì…˜ íŠ¹ì§•
- **Fusion**: Multimodal features â†’ Cross-attention â†’ Fused representation  # ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• â†’ í¬ë¡œìŠ¤ ì–´í…ì…˜ â†’ ìœµí•©ëœ í‘œí˜„
- **Output**: Policy Head â†’ Predicted actions â†’ Robot execution  # ì •ì±… í—¤ë“œ â†’ ì˜ˆì¸¡ëœ ì•¡ì…˜ â†’ ë¡œë´‡ ì‹¤í–‰

### **7.4.9 Memory Management**
- **Source**: `RoboVLMs/robovlms/data/data_utils.py:249-270`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ë©”ëª¨ë¦¬ ê´€ë¦¬
- **Training**:  # í›ˆë ¨
  - **Window Size**: Controls historical context length  # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œì–´
  - **Chunking**: Efficient processing of long sequences  # ê¸´ ì‹œí€€ìŠ¤ì˜ íš¨ìœ¨ì  ì²˜ë¦¬
  - **Batch Processing**: Multiple sequences simultaneously  # ì—¬ëŸ¬ ì‹œí€€ìŠ¤ ë™ì‹œ ì²˜ë¦¬
- **Inference**:  # ì¶”ë¡ 
  - **Single Image**: Minimal memory footprint  # ìµœì†Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
  - **Sequential**: One image at a time  # í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€
  - **Real-time**: Optimized for speed  # ì†ë„ ìµœì í™”

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/train/base_trainer.py` (L565-625)  # í›ˆë ¨ íŒŒì´í”„ë¼ì¸
- `RoboVLMs/vla_test/standalone_vla_test.py` (L87-124)  # ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- `RoboVLMs/eval/calvin/model_wrapper.py` (L318-378)  # CALVIN Step í•¨ìˆ˜
- `RoboVLMs/robovlms/model/README.md` (L58-104)  # ëª¨ë¸ ì•„í‚¤í…ì²˜
- `RoboVLMs/robovlms/data/data_utils.py` (L249-270)  # ë°ì´í„° ìœ í‹¸ë¦¬í‹°
- `RoboVLMs/robovlms/model/backbone/base_backbone.py` (L34-1495)  # ê¸°ë³¸ ë°±ë³¸ ëª¨ë¸
