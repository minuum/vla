# 15-2. VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ: í•™ìŠµ/ì¶”ë¡  ë³€ìˆ˜ ë° ë²¤ì¹˜ë§ˆí¬

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” RoboVLMsì—ì„œ í•™ìŠµ ë³€ìˆ˜ì™€ ì¶”ë¡  ë³€ìˆ˜ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ê³ , ì‹¤ì œ ë¡œë´‡ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ”§ 1. í•™ìŠµ ë³€ìˆ˜ì™€ ì¶”ë¡  ë³€ìˆ˜ ìƒì„¸ ë¶„ì„

### 1.1 í•™ìŠµ ë³€ìˆ˜ (Training Variables)

**BaseRoboVLM._trainable_params_setup() - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì„¤ì •**
```python
def _trainable_params_setup(self):
    model = self.model  # ë°±ë³¸ VLM ëª¨ë¸ (PaliGemma, Kosmos, LLaVA ë“±)
    
    # 1ë‹¨ê³„: ë°±ë³¸ ëª¨ë¸ ë™ê²° ì„¤ì •
    if self.train_setup_configs["freeze_backbone"]:
        model.requires_grad_(False)  # ì „ì²´ ëª¨ë¸ ë™ê²°
    else:
        if self.train_setup_configs.get("train_decoder_layers", -1) == -1:
            model.requires_grad_(True)  # ì „ì²´ ëª¨ë¸ í•™ìŠµ
        else:
            # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
            model.requires_grad_(False)
            for layer in self.text_tower.layers[-self.train_setup_configs["train_decoder_layers"]:]:
                layer.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë¹„ì „ ì¸ì½”ë” ë™ê²° ì„¤ì •
    # vision_tower: VLMì˜ ë¹„ì „ ì¸ì½”ë” (CLIP, SigLIP ë“±)
    if self.train_setup_configs.get("train_vision", False):
        self.vision_tower.requires_grad_(True)
    else:
        self.vision_tower.requires_grad_(False)
    
    # 3ë‹¨ê³„: LoRA ì„¤ì •
    if self.train_setup_configs["lora_enable"]:
        # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        pass
```

**vision_towerì™€ text_tower ì„¤ëª…**:
- **`vision_tower`**: VLMì˜ ë¹„ì „ ì¸ì½”ë” ë¶€ë¶„ (ì´ë¯¸ì§€ â†’ íŠ¹ì§• ë²¡í„°)
  - PaliGemma: `model.vision_tower` (SigLIP ê¸°ë°˜)
  - Kosmos: `model.vision_model` (CLIP ê¸°ë°˜)
  - LLaVA: `model.get_vision_tower()` (CLIP ê¸°ë°˜)
  - Flamingo: `self.vision_encoder` (CLIP ê¸°ë°˜)

- **`text_tower`**: VLMì˜ í…ìŠ¤íŠ¸/ì–¸ì–´ ëª¨ë¸ ë¶€ë¶„ (í…ìŠ¤íŠ¸ â†’ íŠ¹ì§• ë²¡í„°)
  - PaliGemma: `model.language_model.model` (Gemma Decoder)
  - Kosmos: `model.text_model.model` (Decoder-only Transformer)
  - LLaVA: `model.transformer` (GPT-style Transformer)
  - Flamingo: `self.model` (ì–¸ì–´ ëª¨ë¸ ì „ì²´)

**ë°±ë³¸ë³„ êµ¬í˜„ ì˜ˆì‹œ**:
```python
# RoboPaligemma (robopaligemma.py:19-24)
@property
def text_tower(self):
    return self.model.language_model.model  # Gemma Decoder

@property
def vision_tower(self):
    return self.model.vision_tower  # SigLIP

# RoboKosMos (robokosmos.py:16-21)
@property
def text_tower(self):
    return self.model.text_model.model  # Transformer Decoder

@property
def vision_tower(self):
    return self.model.vision_model  # CLIP Vision

# RoboLLaVA (robollava.py:19-24)
@property
def text_tower(self):
    return self.model.transformer  # GPT Transformer

@property
def vision_tower(self):
    return self.model.get_vision_tower()  # CLIP Vision
```

**Kosmos2Processor ê³µì‹ ë¬¸ì„œ ê·¼ê±°**:

Hugging Face ê³µì‹ ë¬¸ì„œì— ë”°ë¥´ë©´, Kosmos-2ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤:

```python
class transformers.Kosmos2Processor(
    image_processor,  # CLIPImageProcessor
    tokenizer,        # XLMRobertaTokenizerFast
    num_patch_index_tokens = 1024,
    **kwargs
)
```

**Parameters**:
- **image_processor** (`CLIPImageProcessor`) â€” An instance of `CLIPImageProcessor`. The image processor is a required input.
- **tokenizer** (`XLMRobertaTokenizerFast`) â€” An instance of `['XLMRobertaTokenizerFast']`. The tokenizer is a required input.

> "Constructs an KOSMOS-2 processor which wraps a KOSMOS-2 image processor and a KOSMOS-2 tokenizer into a single processor."

> "Kosmos2Processor offers all the functionalities of **CLIPImageProcessor** and some functionalities of **XLMRobertaTokenizerFast**."

ì´ê²ƒì´ Kosmos-2ì˜ `vision_tower`ê°€ CLIP ê¸°ë°˜ì´ê³ , `text_tower`ê°€ XLM-Roberta ê¸°ë°˜ Transformerì¸ ì´ìœ ì…ë‹ˆë‹¤.

**ì¶œì²˜**: 
- [Hugging Face KOSMOS-2 Documentation](https://huggingface.co/docs/transformers/en/model_doc/kosmos-2)
- `RoboVLMs/robovlms/model/backbone/base_backbone.py:470-512`
- `RoboVLMs/robovlms/model/backbone/robopaligemma.py:19-24`
- `RoboVLMs/robovlms/model/backbone/robokosmos.py:16-21`
- `RoboVLMs/robovlms/model/backbone/robollava.py:19-24`
- `RoboVLMs/robovlms/model/backbone/roboflamingo.py:35-40`

**BaseTrainer.get_grouped_params() - í•™ìŠµ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”**
```python
def get_grouped_params(self, model):
    return [
        {
            "params": [p for n, p in model.named_parameters() if p.requires_grad],
            "weight_decay": self.configs["weight_decay"],
        }
    ]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:716-722`

**RoboFlamingo._trainable_params_setup() - Flamingo ëª¨ë¸ í•™ìŠµ ì„¤ì •**
```python
def _trainable_params_setup(self):
    self.requires_grad_(False)
    
    # 1ë‹¨ê³„: ë¹„ì „ ì¸ì½”ë” í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_vision"]:
        self.vision_encoder.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë””ì½”ë” ë ˆì´ì–´ í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_decoder_layers"] == -1:
        self.model.gated_cross_attn_layers.requires_grad_(True)
    else:
        # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
        ix = self.train_setup_configs["train_decoder_layers"]
        for layer in self.model.gated_cross_attn_layers[-ix:]:
            layer.requires_grad_(True)
    
    # 3ë‹¨ê³„: ì „ì²´ ë””ì½”ë” í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_full_decoder"]:
        self.model.requires_grad_(True)
    
    # 4ë‹¨ê³„: ë¦¬ìƒ˜í”ŒëŸ¬ í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_resampler"]:
        self.perceiver.requires_grad_(True)
    else:
        self.perceiver.requires_grad_(False)
    
    # 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì„ë² ë”© í•™ìŠµ ì„¤ì •
    if self.train_setup_configs["train_text_embedding"]:
        self.model.get_input_embeddings().requires_grad_(True)
    else:
        self.model.get_input_embeddings().requires_grad_(False)
    
    # 6ë‹¨ê³„: ì•¡ì…˜ í—¤ë“œ í•™ìŠµ ì„¤ì •
    self.act_head.requires_grad_(True)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/roboflamingo.py:131-156`

### 1.2 ì¶”ë¡  ë³€ìˆ˜ (Inference Variables)

**BaseRoboVLM.inference() - ì¶”ë¡  ëª¨ë“œ ì„¤ì •**
```python
def inference(
    self,
    vision_x: torch.Tensor,
    lang_x: torch.Tensor,
    attention_mask: torch.Tensor = None,
    position_ids: torch.LongTensor = None,
    use_cached_vision_x: bool = False,
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,
    action_mask: torch.Tensor = None,
    caption_labels: torch.Tensor = None,
    caption_mask: torch.Tensor = None,
    past_key_values=None,
    use_cache: bool = False,
    vision_gripper=None,
    **kwargs,
):
    prediction = {}
    
    # 1ë‹¨ê³„: ì…ë ¥ ê²€ì¦
    assert vision_x is not None
    bs, seq_len = vision_x.shape[:2]
    action_space = self.act_head_configs.get("action_space", "continuous")
    
    # 2ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡
    if self.train_setup_configs["predict_action"]:
        if action_space == "discrete":
            action = self.pred_action_discrete(
                lang_x, vision_x, vision_gripper, attention_mask
            )
            prediction["action"] = action
        else:
            prediction["action"] = self.forward_continuous(
                vision_x,
                lang_x,
                attention_mask,
                vision_gripper=vision_gripper,
                mode="inference",
            )
    
    return prediction
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1454-1491`

**BaseModelInference.__init__() - ì¶”ë¡  ëª¨ë¸ ì´ˆê¸°í™”**
```python
def __init__(
    self,
    ckpt_path,
    configs,
    device,
    save_dir=None,
    unnorm_key: Optional[str] = None,
    policy_setup: str = "widowx_bridge",
    exec_horizon=1,
):
    self.configs = configs
    self.dataset_stat = self.load_dataset_stat()
    self.model = BaseTrainer(configs=configs)
    self.policy = self.model
    
    # 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # 2ë‹¨ê³„: ì •ì±… ì„¤ì •
    if policy_setup == "widowx_bridge":
        unnorm_key = "bridge_orig" if unnorm_key is None else unnorm_key
    elif policy_setup == "google_robot":
        unnorm_key = "fractal20220817_data" if unnorm_key is None else unnorm_key
    
    # 3ë‹¨ê³„: ê·¸ë¦¬í¼ ì•¡ì…˜ ì„¤ì •
    self.sticky_gripper_num_repeat = 2
    self.policy_setup = policy_setup
    self.unnorm_key = unnorm_key
    
    if self.policy_setup == "google_robot":
        self.close_gripper_act = -1
    elif self.policy_setup == "widowx_bridge":
        self.close_gripper_act = 1
    
    # 4ë‹¨ê³„: ì´ë¯¸ì§€ ë° ì•¡ì…˜ ì„¤ì •
    self.image_size = self.configs.get("image_size", 224)
    self.action_scale = self.configs.get("action_scale", 1.0)
    self.horizon = self.configs["window_size"]
    self.window_size = self.horizon
    self.pred_action_horizon = exec_horizon
```

**ì¶œì²˜**: `RoboVLMs/eval/simpler/model_wrapper.py:15-58`

**StandaloneVLAInference.load_model() - ì¶”ë¡  ëª¨ë¸ ë¡œë“œ**
```python
def load_model(self):
    """VLA ëª¨ë¸ ë¡œë“œ"""
    try:
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_id}")
        
        model_save_path = Path(self.model_cache_dir) / self.model_id.split('/')[-1]
        model_save_path.mkdir(parents=True, exist_ok=True)

        # 1ë‹¨ê³„: í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(
            self.model_id, 
            cache_dir=model_save_path
        )

        # 2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ
        model_kwargs = {
            "cache_dir": model_save_path,
            "low_cpu_mem_usage": True
        }
        
        if self.device.type == "cuda":
            model_kwargs["torch_dtype"] = torch.bfloat16
            model_kwargs["device_map"] = "auto"
        else:
            model_kwargs["torch_dtype"] = torch.float32

        self.model = PaliGemmaForConditionalGeneration.from_pretrained(
            self.model_id, 
            **model_kwargs
        )
        
        if self.device.type != "cuda":
            self.model.to(self.device)
        
        # 3ë‹¨ê³„: ì¶”ë¡  ëª¨ë“œ ì„¤ì •
        self.model.eval()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise
```

**ì¶œì²˜**: `RoboVLMs/vla_test/standalone_vla_test.py:46-85`

### 1.3 í•™ìŠµ vs ì¶”ë¡  ë³€ìˆ˜ ë¹„êµ

| êµ¬ë¶„ | í•™ìŠµ ë³€ìˆ˜ | ì¶”ë¡  ë³€ìˆ˜ |
|------|-----------|-----------|
| **ëª¨ë“œ** | `model.train()` | `model.eval()` |
| **ê·¸ë˜ë””ì–¸íŠ¸** | `requires_grad=True` | `requires_grad=False` |
| **ìºì‹œ** | `use_cache=False` | `use_cache=True` |
| **ë“œë¡­ì•„ì›ƒ** | í™œì„±í™” | ë¹„í™œì„±í™” |
| **ë°°ì¹˜ ì •ê·œí™”** | í•™ìŠµ ëª¨ë“œ | í‰ê°€ ëª¨ë“œ |
| **ë©”ëª¨ë¦¬** | ë†’ìŒ (ê·¸ë˜ë””ì–¸íŠ¸) | ë‚®ìŒ (ê·¸ë˜ë””ì–¸íŠ¸ ì—†ìŒ) |
| **ì…ë ¥** | `action_labels` í¬í•¨ | `action_labels` ì—†ìŒ |
| **ì¶œë ¥** | Loss ê³„ì‚° | ì•¡ì…˜ ì˜ˆì¸¡ë§Œ |
| **í† í° ì‚½ì…** | Teacher Forcing | Autoregressive |

### 1.4 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**Docker í™˜ê²½ ë³€ìˆ˜ (docker-compose.yml)**
```yaml
environment:
  - DISPLAY=${DISPLAY:-:0}
  - ROS_DOMAIN_ID=42
  - CUDA_VISIBLE_DEVICES=0
  - TORCH_DTYPE=bfloat16
  - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
  - TRANSFORMERS_CACHE=/workspace/.vlms
  - HF_HOME=/workspace/.vlms
  - PYTHONPATH=/workspace:/workspace/robovlms
  - VLA_MODEL=paligemma-3b-mix-224
  - ACTION_MODE=automotive
  - ACTION_DIM=4
  - WINDOW_SIZE=8
  - INFERENCE_LATENCY_TARGET=100
  - PROJECT_NAME=k_project_event_vla
```

**ì¶œì²˜**: `RoboVLMs/docker-compose.yml:25-39`

### 1.5 í•™ìŠµ ë³€ìˆ˜ ìƒì„¸ ì„¤ì •

**CALVIN Fine-tuning ì„¤ì •**
```json
{
  "train_setup": {
    "precision": "bf16",
    "predict_action": true,
    "predict_forward": false,
    "predict_caption": false,
    "train_vision": true,
    "freeze_backbone": false,
    "freeze_mm_mlp_adapter": false,
    "lora_enable": false,
    "train_text_embedding": true
  },
  "act_head": {
    "type": "LSTMDecoder",
    "hidden_size": 1024,
    "action_dim": 7,
    "down_sample": "none",
    "latent": 1,
    "fwd_pred_next_n": 1,
    "window_size": 1,
    "action_space": "continuous"
  }
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:228-267`

## ğŸ¤– 2. ì‹¤ì œ ë¡œë´‡ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ìƒì„¸ ë¶„ì„

### 2.1 Real-World Experiments ë²¤ì¹˜ë§ˆí¬

**ë²¤ì¹˜ë§ˆí¬ ê°œìš”**
- **ì´ ì‘ì—… ìˆ˜**: 105ê°œì˜ ì¡°ì‘ ì‘ì—…
- **ë°ì´í„° ê·œëª¨**: 70,000ê°œ ì´ìƒì˜ ì›ê²© ì¡°ì‘ ì¸ê°„ ê¶¤ì 
- **í‰ê°€ ì„¤ì •**: 1ê°œ ë‹¨ìˆœ ì„¤ì • + 4ê°œ ë„ì „ì  ë¯¸ì§€ ì„¤ì •
- **ì´ í‰ê°€ ì‘ì—…**: 20ê°œ ì‘ì—…
- **ë¡¤ì•„ì›ƒ**: ê° ì„¤ì •ë‹¹ 3íšŒ ë¡¤ì•„ì›ƒ (ì‘ì—…ë‹¹ 5ê°œ ì„¤ì •)

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Appendix K, Appendix D, Figure 15-17

**ë¡œë´‡ ì‚¬ì–‘**
- **ììœ ë„**: 7-DOF (6ì°¨ì› ìì„¸ + 1ì°¨ì› ê·¸ë¦¬í¼)
- **ê´€ì¸¡ ì •ë³´**: ê³ ìœ  ê°ê° ì •ë³´ + ì‹œê° ê´€ì¸¡ + ì–¸ì–´ ì…ë ¥

### 2.2 CALVIN ë²¤ì¹˜ë§ˆí¬ ìƒì„¸

**CALVIN [32] - Simulated Benchmark**

**ë°ì´í„°ì…‹ êµ¬ì„±**
```python
# CALVIN ë°ì´í„°ì…‹ êµ¬ì¡°
calvin_dataset = {
    "demonstrations": 24000,                    # 24k ì¸ê°„ ì›ê²© ì¡°ì‘ ë°ëª¨
    "trajectory_length": "< 64 timesteps",      # ê° ê¶¤ì  64 íƒ€ì„ìŠ¤í… ì´í•˜
    "language_annotations": True,               # ì–¸ì–´ ëª…ë ¹ í¬í•¨
    "basic_skills": 34,                         # 34ê°œ ì‚¬ì „ ì •ì˜ ê¸°ë³¸ ìŠ¤í‚¬
    "splits": ["scene_A", "scene_B", "scene_C", "scene_D"]
}
```

**34ê°œ ê¸°ë³¸ ìŠ¤í‚¬ ëª©ë¡** (15ê°œ íƒœìŠ¤í¬ ìœ í˜• Ã— ìƒ‰ìƒ/ë°©í–¥ ì¡°í•©)

**1-6. Rotate ë¸”ë¡ (6ê°œ)**
- Rotate red/blue/pink block right: zì¶• ê¸°ì¤€ ì‹œê³„ë°©í–¥ 60ë„ ì´ìƒ íšŒì „ (x/yì¶• 30ë„ ì´ë‚´)
- Rotate red/blue/pink block left: zì¶• ê¸°ì¤€ ë°˜ì‹œê³„ë°©í–¥ 60ë„ ì´ìƒ íšŒì „ (x/yì¶• 30ë„ ì´ë‚´)

**7-12. Push ë¸”ë¡ (6ê°œ)**
- Push red/blue/pink block right: ë¸”ë¡ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ 10cm ì´ìƒ ì´ë™ (ì–‘ìª½ í”„ë ˆì„ì—ì„œ í‘œë©´ ì ‘ì´‰ ìœ ì§€)
- Push red/blue/pink block left: ë¸”ë¡ì„ ì™¼ìª½ìœ¼ë¡œ 10cm ì´ìƒ ì´ë™ (ì–‘ìª½ í”„ë ˆì„ì—ì„œ í‘œë©´ ì ‘ì´‰ ìœ ì§€)

**13-14. Move slider (2ê°œ)**
- Move slider left/right: ìŠ¬ë¼ì´ë”© ë„ì–´ë¥¼ ìµœì†Œ 12cm ë°€ê¸°

**15-16. Drawer ì¡°ì‘ (2ê°œ)**
- Open/close drawer: ì„œëì„ ìµœì†Œ 10cm ë°€ì–´ë„£ê¸°/ë‹¹ê¸°ê¸°

**17-19. Lift block table (3ê°œ)**
- Lift red/blue/pink block table: í…Œì´ë¸” í‘œë©´ì—ì„œ ë¸”ë¡ì„ ì¡ì•„ ìµœì†Œ 5cm ë“¤ì–´ì˜¬ë¦¬ê¸°
  (ì²« í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ëŠ” ë¬¼ì²´ë¥¼ í„°ì¹˜í•˜ì§€ ì•ŠìŒ)

**20-22. Lift block slider (3ê°œ)**
- Lift red/blue/pink block slider: ìŠ¬ë¼ì´ë”© ìºë¹„ë‹› í‘œë©´ì—ì„œ ë¸”ë¡ì„ ì¡ì•„ ìµœì†Œ 3cm ë“¤ì–´ì˜¬ë¦¬ê¸°
  (ì²« í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ëŠ” ë¬¼ì²´ë¥¼ í„°ì¹˜í•˜ì§€ ì•ŠìŒ)

**23-25. Lift block drawer (3ê°œ)**
- Lift red/blue/pink block drawer: ì„œë í‘œë©´ì—ì„œ ë¸”ë¡ì„ ì¡ì•„ ìµœì†Œ 5cm ë“¤ì–´ì˜¬ë¦¬ê¸°
  (ì²« í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ëŠ” ë¬¼ì²´ë¥¼ í„°ì¹˜í•˜ì§€ ì•ŠìŒ)

**26. Place in slider/drawer (1ê°œ)**
- Place in slider/drawer: ìŠ¬ë¼ì´ë”© ìºë¹„ë‹›/ì„œëì— ë¬¼ì²´ë¥¼ ë„£ê¸°
  (ì²« í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ê°€ ë¬¼ì²´ë¥¼ ë“¤ê³  ìˆì–´ì•¼ í•¨)

**27. Push into drawer (1ê°œ)**
- Push into drawer: ì„œëì— ë¬¼ì²´ë¥¼ ë°€ì–´ë„£ê¸°
  (ì²« í”„ë ˆì„ì—ì„œ í…Œì´ë¸” í‘œë©´ì˜ ë¬¼ì²´ë¥¼ í„°ì¹˜í•´ì•¼ í•¨)

**28. Stack blocks (1ê°œ)**
- Stack blocks: í•œ ë¸”ë¡ì„ ë‹¤ë¥¸ ë¸”ë¡ ìœ„ì— ìŒ“ê¸°
  (ìµœì¢… í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ê°€ ë¸”ë¡ê³¼ ì ‘ì´‰í•˜ì§€ ì•ŠìŒ)

**29. Unstack blocks (1ê°œ)**
- Unstack blocks: ë‹¤ë¥¸ ë¸”ë¡ ìœ„ì—ì„œ ë¸”ë¡ì„ ì œê±°
  (ìµœì¢… í”„ë ˆì„ì—ì„œ ê·¸ë¦¬í¼ê°€ ë¸”ë¡ê³¼ ì ‘ì´‰í•˜ì§€ ì•ŠìŒ)

**30-31. Light bulb (2ê°œ)**
- Turn on/off light bulb: ë…¸ë€ìƒ‰ ì „êµ¬ë¥¼ ì¼œê¸°/ë„ê¸° ìœ„í•´ ìŠ¤ìœ„ì¹˜ë¥¼ ìœ„/ì•„ë˜ë¡œ ëˆ„ë¥´ê¸°

**32-33. LED (2ê°œ)**
- Turn on/off LED: ì´ˆë¡ìƒ‰ LED ë¼ì´íŠ¸ë¥¼ ì¼œê¸°/ë„ê¸° ìœ„í•´ ë²„íŠ¼ì„ ëˆ„ë¥´ê¸°

**ì´ 34ê°œ ìŠ¤í‚¬** = Rotate(6) + Push(6) + Slider(2) + Drawer(2) + Lift table(3) + Lift slider(3) + Lift drawer(3) + Place(1) + Push into(1) + Stack(1) + Unstack(1) + Light(2) + LED(2) + Open oven(1) = 34ê°œ

**í‰ê°€ ë©”íŠ¸ë¦­**
- **Sequential Task Success Rate**: 5ê°œ ì—°ì† ì‘ì—… ì™„ë£Œ ì„±ê³µë¥ 
- **Average Length**: ë‹¬ì„±í•œ ì‘ì—…ì˜ í‰ê·  ê¸¸ì´
- **í‰ê°€ ê·œëª¨**: D splitì—ì„œ 1000 ë¡¤ì•„ì›ƒ, ê° ë¡¤ì•„ì›ƒë‹¹ 5ê°œ ì—°ì† ì„œë¸ŒíƒœìŠ¤í¬

**ì¶œì²˜**: CALVIN ë…¼ë¬¸ [32], RoboVLMs ë…¼ë¬¸

### 2.3 SimplerEnv ë²¤ì¹˜ë§ˆí¬

**SimplerEnv [25] - Real-to-Sim Evaluation**

**ë²¤ì¹˜ë§ˆí¬ ëª©ì **
- ì‹¤ì œ ë¡œë´‡ ì •ì±…ì„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í‰ê°€
- Google Robot, BridgeData V2ì™€ ë¹„êµ ê°€ëŠ¥í•œ ì•„ë ˆë‚˜ ì œê³µ
- íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì œ ì„¸ê³„ í‰ê°€ ëŒ€ì•ˆ

#### 2.3.1 Google Robot ì„¤ì • ì‘ì—…

**1) pick coke can**
```python
# pick coke can ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ë¹ˆ ì½”í¬ ìº”ì„ í…Œì´ë¸”ì—ì„œ ì§‘ì–´ ë“¤ê¸°",
    "positions": ["horizontal", "vertical", "upright"],  # 3ê°€ì§€ ìœ„ì¹˜
    "grid_points": 25,                                   # ì§ì‚¬ê°í˜• ì˜ì—­ ë‚´ 25ê°œ ê·¸ë¦¬ë“œ
    "total_trials": 75,                                  # 25 Ã— 3 = 75 ì‹œí—˜
    "distractors": False                                 # í‘œì¤€ ì„¤ì •ì—ì„œëŠ” ë°©í•´ ìš”ì†Œ ì—†ìŒ
}
```

**2) move {obj1} near {obj2}**
```python
# move near ì‘ì—… ì„¤ì •
task_config = {
    "objective": "obj1ì„ obj2 ê·¼ì²˜ë¡œ ì´ë™",
    "objects": ["blue plastic bottle", "Pepsi can", "orange", 
                "7up can", "apple", "sponge", "Coke can", "Redbull can"],  # 8ê°œ ë¬¼ì²´
    "formation": "triangular",                           # ì‚¼ê°í˜• ë°°ì¹˜
    "triplets": 5,                                       # 5ê°œ triplet (ëœë¤ ì„ íƒ)
    "patterns": ["upright", "inverted"],                 # 2ê°€ì§€ ì‚¼ê°í˜• íŒ¨í„´
    "trials_per_triplet": 6,                            # tripletë‹¹ 6íšŒ ì‹œí—˜
    "total_trials": 60                                   # 5 Ã— 6 Ã— 2 = 60 ì‹œí—˜
}
```

**3) (open/close) (top/middle/bottom) drawer**
```python
# drawer ì‘ì—… ì„¤ì •
task_config = {
    "objective": "íŠ¹ì • ì„œë ì—´ê¸°/ë‹«ê¸°",
    "drawers": 3,                                        # top, middle, bottom
    "actions": ["open", "close"],                        # 2ê°€ì§€ ì•¡ì…˜
    "robot_positions": 9,                                # 9ê°œ ê·¸ë¦¬ë“œ ìœ„ì¹˜
    "total_trials": 54,                                  # 3 Ã— 2 Ã— 9 = 54 ì‹œí—˜
    "evaluation_type": "articulated_objects"             # ê´€ì ˆ ë¬¼ì²´ ì²˜ë¦¬ ëŠ¥ë ¥ í‰ê°€
}
```

**4) open top drawer; place apple into top drawer**
```python
# multi-step ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ì„œë ì—´ê³  ì‚¬ê³¼ë¥¼ ì„œëì— ë„£ê¸°",
    "steps": [
        "open top drawer",
        "place apple into top drawer"
    ],
    "robot_positions": 3,                                # ë¡œë´‡ ìœ„ì¹˜ 3ê°œ
    "apple_positions": 9,                                # ì‚¬ê³¼ ê·¸ë¦¬ë“œ ìœ„ì¹˜ 9ê°œ
    "total_trials": 27,                                  # 3 Ã— 9 = 27 ì‹œí—˜
    "instruction_switch": "midpoint or terminate token", # ëª…ë ¹ ì „í™˜ ì‹œì 
    "evaluation_type": "sequential_multi-action"         # ìˆœì°¨ì  ë‹¤ì¤‘ ì•¡ì…˜ í‰ê°€
}
```

#### 2.3.2 WidowX + Bridge ì„¤ì • ì‘ì—…

**1) put the spoon on the towel**
```python
# spoon on towel ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ìˆ˜ì €ë¥¼ íƒ€ì›” ìœ„ì— ë†“ê¸°",
    "square_size": "15 cm",                              # ì •ì‚¬ê°í˜• í¬ê¸°
    "spoon_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "towel_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "spoon_orientations": ["horizontal", "vertical"],    # 2ê°€ì§€ ë°©í–¥
    "total_trials": 24,                                  # 4 Ã— 4 Ã— 2 / 2 = 24 ì‹œí—˜
    "gripper_adjustment": True                           # ê·¸ë¦¬í¼ ë°©í–¥ ì¡°ì • í•„ìš”
}
```

**2) put carrot on plate**
```python
# carrot on plate ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ë‹¹ê·¼ì„ ì ‘ì‹œ ìœ„ì— ë†“ê¸°",
    "square_size": "15 cm",
    "carrot_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "plate_positions": ["corner_1", "corner_2", "corner_3", "corner_4"],
    "total_trials": 24,
    "similar_to": "put the spoon on the towel"
}
```

**3) stack the green block on the yellow block**
```python
# block stacking ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ì´ˆë¡ ë¸”ë¡ì„ ë…¸ë€ ë¸”ë¡ ìœ„ì— ìŒ“ê¸°",
    "block_size": "3 cm",                                # ë¸”ë¡ í¬ê¸°
    "square_configs": [
        {"size": "10 cm", "trials": 12},                 # 10cm ì •ì‚¬ê°í˜•
        {"size": "20 cm", "trials": 12}                  # 20cm ì •ì‚¬ê°í˜•
    ],
    "green_block_positions": 4,                          # 4ê°œ ì½”ë„ˆ
    "yellow_block_positions": 4,                         # 4ê°œ ì½”ë„ˆ
    "total_trials": 24                                   # (4 Ã— 4 / 2) Ã— 2 = 24 ì‹œí—˜
}
```

**4) put eggplant into yellow basket**
```python
# eggplant into basket ì‘ì—… ì„¤ì •
task_config = {
    "objective": "ê°€ì§€ë¥¼ ë…¸ë€ ë°”êµ¬ë‹ˆì— ë„£ê¸°",
    "environment": "sink with two basins",               # 2ê°œ ì„¸ë©´ëŒ€
    "eggplant_location": "right basin (random)",         # ì˜¤ë¥¸ìª½ ì„¸ë©´ëŒ€ (ëœë¤ ìœ„ì¹˜)
    "basket_location": "left basin",                     # ì™¼ìª½ ì„¸ë©´ëŒ€
    "eggplant_variations": {
        "position": "random",
        "orientation": "random",
        "constraint": "easily graspable, away from edges"
    },
    "total_trials": 24
}
```

### 2.4 ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ìš”ì•½

| ë²¤ì¹˜ë§ˆí¬ | ìœ í˜• | ì‘ì—… ìˆ˜ | ë°ì´í„° ê·œëª¨ | í‰ê°€ ë©”íŠ¸ë¦­ |
|---------|------|---------|-------------|-------------|
| **Real-World Experiments** | ì‹¤ì œ ë¡œë´‡ | 20ê°œ (105ê°œ ì¤‘) | 70,000+ ê¶¤ì  | ì„¤ì •ë³„ í‰ê·  ì„±ê³µë¥  |
| **CALVIN** | ì‹œë®¬ë ˆì´ì…˜ | 34ê°œ ê¸°ë³¸ ìŠ¤í‚¬ | 24,000 ë°ëª¨ | Sequential Success Rate, Avg Length |
| **SimplerEnv (Google)** | Real-to-Sim | 4ê°œ ì‘ì—… | - | ì‹œí—˜ë³„ ì„±ê³µë¥  (75-54íšŒ) |
| **SimplerEnv (Bridge)** | Real-to-Sim | 4ê°œ ì‘ì—… | - | ì‹œí—˜ë³„ ì„±ê³µë¥  (24íšŒ) |

### 2.5 ì½”ë“œ êµ¬í˜„ ì˜ˆì‹œ

**DiskCalvinDataset - CALVIN ë°ì´í„° ë¡œë”©**
```python
class DiskCalvinDataset(BaseCalvinDataset):
    """ë””ìŠ¤í¬ì—ì„œ ê°œë³„ íŒŒì¼ë¡œ ì—í”¼ì†Œë“œë¥¼ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹"""
    def __init__(
        self,
        image_fn: Callable,
        tokenizer: Callable,
        *args: Any,
        skip_frames: int = 1,
        seq_len: int = 1,
        **kwargs: Any,
    ):
        super().__init__(*args, **kwargs)
        # ... (ì´ˆê¸°í™” ì½”ë“œ)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:428-447`

**SimplerEnv í‰ê°€ í•¨ìˆ˜**
```python
def evaluate_simpler_env(model, env, task_config):
    """SimplerEnvì—ì„œ ëª¨ë¸ í‰ê°€"""
    success_count = 0
    total_trials = task_config["total_trials"]
    
    for trial in range(total_trials):
        # í™˜ê²½ ì´ˆê¸°í™”
        obs = env.reset()
        
        # ëª¨ë¸ ì¶”ë¡ 
        action = model.inference(
            vision_x=obs["rgb"],
            lang_x=task_config["instruction"]
        )
        
        # ì•¡ì…˜ ì‹¤í–‰ ë° í‰ê°€
        success = env.step(action)
        success_count += int(success)
    
    success_rate = success_count / total_trials
    return success_rate
```

## ğŸ¯ 3. í•µì‹¬ í•™ìŠµ ì•„ì´ë””ì–´

### 3.1 VLMì˜ ì—­í• 

**1) ë©€í‹°ëª¨ë‹¬ ì´í•´**
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì´í•´
- ë¡œë´‡ í™˜ê²½ì˜ ì‹œê°ì  ìƒí™© íŒŒì•…
- ì–¸ì–´ ëª…ë ¹ì˜ ì˜ë¯¸ í•´ì„

**2) íŠ¹ì§• ì¶”ì¶œ**
- Vision Encoder: ì´ë¯¸ì§€ì—ì„œ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ
- Language Encoder: í…ìŠ¤íŠ¸ì—ì„œ ì–¸ì–´ì  íŠ¹ì§• ì¶”ì¶œ
- Cross-modal Fusion: ë¹„ì „-ì–¸ì–´ íŠ¹ì§• ìœµí•©

**3) Fine-tuning ëª©ì **
- ë¡œë´‡ ë„ë©”ì¸ì— íŠ¹í™”ëœ í‘œí˜„ í•™ìŠµ
- ì•¡ì…˜ê³¼ ê´€ë ¨ëœ ì‹œê°ì /ì–¸ì–´ì  íŠ¹ì§• ê°•í™”
- Policy Headë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ íŠ¹ì§• ì œê³µ

### 3.2 LSTM Layerì˜ ì—­í• 

**1) ì‹œí€€ìŠ¤ ì²˜ë¦¬**
- ì‹œê°„ì  ì—°ì†ì„± ëª¨ë¸ë§
- íˆìŠ¤í† ë¦¬ ì •ë³´ í™œìš©
- ë™ì  ìƒíƒœ ì¶”ì 

**2) ì•¡ì…˜ ì˜ˆì¸¡**
- VLM íŠ¹ì§•ì„ ì•¡ì…˜ ê³µê°„ìœ¼ë¡œ ë§¤í•‘
- 6-DOF íŒ” ì•¡ì…˜ + 1-DOF ê·¸ë¦¬í¼ ì˜ˆì¸¡
- ì—°ì†ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ì•¡ì…˜ ìƒì„±

**3) í•™ìŠµ ëª©ì **
- VLM íŠ¹ì§•ê³¼ ì•¡ì…˜ ê°„ì˜ ê´€ê³„ í•™ìŠµ
- ìµœì ì˜ ì•¡ì…˜ ì •ì±… í•™ìŠµ
- ë¡œë´‡ ì œì–´ì— íŠ¹í™”ëœ í‘œí˜„ í•™ìŠµ

### 3.3 ë™ì‹œ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜

**End-to-End í•™ìŠµ**
```python
# VLM + LSTM ë™ì‹œ í•™ìŠµ
loss_total = loss_vlm + loss_action

# VLM Loss: ë©€í‹°ëª¨ë‹¬ í‘œí˜„ í•™ìŠµ
loss_vlm = calculate_vl_cross_entropy(vlm_logits, text_labels)

# Action Loss: ì•¡ì…˜ ì˜ˆì¸¡ í•™ìŠµ
loss_action = loss_arm + 0.01 * loss_gripper
```

**ì¥ì **
1. **í†µí•© ìµœì í™”**: VLMê³¼ LSTMì´ í•¨ê»˜ ìµœì í™”
2. **íŠ¹ì§• í’ˆì§ˆ**: ì•¡ì…˜ ì˜ˆì¸¡ì— ìœ ìš©í•œ íŠ¹ì§• í•™ìŠµ
3. **íš¨ìœ¨ì„±**: ë³„ë„ í•™ìŠµ ëŒ€ë¹„ ì‹œê°„/ìì› ì ˆì•½

### 3.4 í•™ìŠµ vs ì¶”ë¡  ì°¨ì´

**í•™ìŠµ ì‹œ (Training)**
- Action Labels ì œê³µ
- Loss ê³„ì‚° ë° Backpropagation
- Gradient ì—…ë°ì´íŠ¸
- Teacher Forcing (Discrete Action)

**ì¶”ë¡  ì‹œ (Inference)**
- Action Labels ì—†ìŒ
- Action ì˜ˆì¸¡ë§Œ ìˆ˜í–‰
- Gradient ê³„ì‚° ì—†ìŒ
- Autoregressive Generation (Discrete Action)

## ğŸ“Š 4. ì „ì²´ ì‹œìŠ¤í…œ ìš”ì•½

### 4.1 í•™ìŠµ íŒŒì´í”„ë¼ì¸

```
[Real-World Data]
    â†“
[CALVIN/Bridge Dataset]
    â†“
[Data Preprocessing]
    â†“
[VLM Fine-tuning] â† Full-FT or LoRA
    â†“
[LSTM Training] â† Action Prediction
    â†“
[Loss Calculation] â† VL Loss + Action Loss
    â†“
[Optimization] â† Adam/AdamW
    â†“
[Trained Model]
```

### 4.2 ì¶”ë¡  íŒŒì´í”„ë¼ì¸

```
[Robot Camera] â†’ [Image] â†’ [VLM Encoder]
                                â†“
[Language Command] â†’ [Text] â†’ [VLM Encoder]
                                â†“
                        [Multimodal Fusion]
                                â†“
                        [LSTM Decoder]
                                â†“
                        [Action Prediction]
                                â†“
                        [Robot Control]
```

### 4.3 í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ

| ë²¤ì¹˜ë§ˆí¬ | í‰ê°€ ë©”íŠ¸ë¦­ | ëª©í‘œ ì„±ëŠ¥ |
|---------|-------------|-----------|
| **CALVIN** | Sequential Success Rate | 5ê°œ ì—°ì† ì‘ì—… ì™„ë£Œìœ¨ |
| **SimplerEnv** | Task Success Rate | ê°œë³„ ì‘ì—… ì„±ê³µë¥  |
| **Real-World** | Rollout Success Rate | ì‹¤ì œ í™˜ê²½ ì„±ê³µë¥  |

### 4.4 RoboVLMs íŠ¹ì§•

**1) ë‹¤ì–‘í•œ VLM ë°±ë³¸ ì§€ì›**
- PaliGemma, Flamingo, Kosmos, Qwen-VL ë“±

**2) ìœ ì—°í•œ Policy Head**
- LSTMDecoder, FCDecoder, GPTDecoder, DiscreteDecoder

**3) íš¨ìœ¨ì ì¸ í•™ìŠµ**
- Full Fine-tuningê³¼ LoRA ëª¨ë‘ ì§€ì›
- BFloat16 ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- Gradient Checkpointingìœ¼ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ í•™ìŠµ

**4) ì‹¤ìš©ì ì¸ í‰ê°€**
- CALVIN, SimplerEnv, Real-World ë²¤ì¹˜ë§ˆí¬
- ë‹¤ì–‘í•œ ë‚œì´ë„ì™€ ì„¤ì •ì—ì„œ í‰ê°€
- ì²´ê³„ì ì¸ ì„±ëŠ¥ ì¸¡ì •

## ğŸ“š ì°¸ê³  ìë£Œ

**ì¶œì²˜ ë…¼ë¬¸**
- RoboVLMs: "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models"
- CALVIN [32]: "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks"
- SimplerEnv [25]: Real-to-Sim Evaluation Framework

**GitHub ì €ì¥ì†Œ**
- RoboVLMs: https://github.com/RoboVLMs/RoboVLMs
- CALVIN Dataset: https://github.com/mees/calvin

