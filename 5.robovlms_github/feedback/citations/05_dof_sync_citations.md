# 5. 7 DOF Synchronization Method and Learning - Citation Evidence

## ğŸ” **GitHub Code Implementation (100% Confirmed from @RoboVLMs)**

### **5.1 Action Space Configuration**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1344-1382` (Updated from @RoboVLMs)
- **Implementation**: Action space configuration and routing
- **Code**:
```python
def forward_action(self, vision_x, lang_x, attention_mask=None, ...):
    """ì•¡ì…˜ ê³µê°„ì— ë”°ë¥¸ í¬ì›Œë“œ ë¼ìš°íŒ…"""
    # ì•¡ì…˜ ê³µê°„ ì„¤ì • í™•ì¸ (ì—°ì†/ì´ì‚°)
    action_space = self.act_head_configs.get("action_space", "continuous")
    
    if action_space == "discrete":
        # ì´ì‚° ì•¡ì…˜ ê³µê°„ ì²˜ë¦¬
        return self.forward_discrete(
            vision_x=vision_x,           # ë¹„ì „ ì…ë ¥
            lang_x=lang_x,               # ì–¸ì–´ ì…ë ¥
            attention_mask=attention_mask, # ì–´í…ì…˜ ë§ˆìŠ¤í¬
            action_labels=action_labels,  # ì•¡ì…˜ ë ˆì´ë¸”
            action_mask=action_mask,      # ì•¡ì…˜ ë§ˆìŠ¤í¬
            # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
        )
    else:
        # ì—°ì† ì•¡ì…˜ ê³µê°„ ì²˜ë¦¬
        return self.forward_continuous(
            vision_x=vision_x,           # ë¹„ì „ ì…ë ¥
            lang_x=lang_x,               # ì–¸ì–´ ì…ë ¥
            attention_mask=attention_mask, # ì–´í…ì…˜ ë§ˆìŠ¤í¬
            action_labels=action_labels,  # ì•¡ì…˜ ë ˆì´ë¸”
            action_mask=action_mask,      # ì•¡ì…˜ ë§ˆìŠ¤í¬
            # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë“¤
        )
```

### **5.2 Action Parser Implementation**
- **File**: `RoboVLMs/vla_test/robovlm_action_parser.py:15-102` (Updated from @RoboVLMs)
- **Implementation**: Action space enum and parser configuration
- **Code**:
```python
class ActionSpace(Enum):
    """ì•¡ì…˜ ê³µê°„ íƒ€ì…"""
    CONTINUOUS = "continuous"  # ì—°ì† ì•¡ì…˜ ê³µê°„
    DISCRETE = "discrete"      # ì´ì‚° ì•¡ì…˜ ê³µê°„

class RoboVLMActionParser:
    """RoboVLMs ì•¡ì…˜ íŒŒì„œ (7 DOF ì§€ì›)"""
    def __init__(self, 
                 action_space: ActionSpace = ActionSpace.CONTINUOUS,
                 action_dim: int = 6,  # 6 DOF + 1 gripper = 7 DOF
                 bins: int = 256,
                 min_action: float = -1.0,
                 max_action: float = 1.0,
                 prediction_horizon: int = 1):
        
        self.action_space = action_space    # ì•¡ì…˜ ê³µê°„ íƒ€ì…
        self.action_dim = action_dim        # 7 DOF ì„¤ì • (6 DOF íŒ” + 1 DOF ê·¸ë¦¬í¼)
        self.bins = bins                    # ì´ì‚°í™” ì‹œ ì‚¬ìš©í•  ë¹ˆ ìˆ˜
        self.min_action = min_action        # ì•¡ì…˜ ìµœì†Œê°’ (-1.0)
        self.max_action = max_action        # ì•¡ì…˜ ìµœëŒ€ê°’ (1.0)
        
        # ì´ì‚° ì•¡ì…˜ ê³µê°„ì„ ìœ„í•œ ë¹ˆ ìƒì„±
        if action_space == ActionSpace.DISCRETE:
            # ì•¡ì…˜ ë²”ìœ„ë¥¼ ë¹ˆìœ¼ë¡œ ë¶„í• 
            self.action_bins = np.linspace(min_action, max_action, bins)
            # ê° ë¹ˆì˜ ì¤‘ì‹¬ê°’ ê³„ì‚°
            self.bin_centers = (self.action_bins[:-1] + self.action_bins[1:]) / 2.0
```

### **5.3 Discrete Action Decoder**
- **File**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:173-227` (Updated from @RoboVLMs)
- **Implementation**: `DiscreteDecoder` class for discrete action processing
- **Code**:
```python
class DiscreteDecoder(BasePolicyHead):
    """ì´ì‚° ì•¡ì…˜ ë””ì½”ë” (7 DOF)"""
    def __init__(
        self,
        hidden_size,              # íˆë“  ìƒíƒœ í¬ê¸°
        action_dim,               # ì•¡ì…˜ ì°¨ì› (7 DOF)
        action_space="continuous", # ì•¡ì…˜ ê³µê°„ íƒ€ì…
        down_sample="pooling",     # ë‹¤ìš´ìƒ˜í”Œë§ ë°©ë²•
        latent=1,                 # ì ì¬ ì°¨ì›
        cont_token_nun=1,         # ì—°ì† í† í° ìˆ˜
        n_bin=256,                # ì´ì‚°í™” ë¹ˆ ìˆ˜
        min_action=-1,            # ì•¡ì…˜ ìµœì†Œê°’
        max_action=1,             # ì•¡ì…˜ ìµœëŒ€ê°’
        tokenizer=None,           # í† í¬ë‚˜ì´ì €
        **kwargs,
    ):
        super().__init__(
            hidden_size, action_dim, action_space, down_sample, latent, **kwargs
        )
        self.cont_token_num = cont_token_nun  # ì—°ì† í† í° ìˆ˜
        self.n_bin = n_bin                    # ì´ì‚°í™” ë¹ˆ ìˆ˜
        self.min_action = min_action          # ì•¡ì…˜ ìµœì†Œê°’
        self.max_action = max_action          # ì•¡ì…˜ ìµœëŒ€ê°’

        # ì•¡ì…˜ í† í¬ë‚˜ì´ì € import ë° ì´ˆê¸°í™”
        from robovlms.model.policy_head.action_tokenizer import ActionTokenizer

        self.action_tokenizer = ActionTokenizer(
            tokenizer,                    # í† í¬ë‚˜ì´ì €
            bins=self.n_bin,              # ë¹ˆ ìˆ˜
            min_action=self.min_action,   # ìµœì†Œ ì•¡ì…˜ê°’
            max_action=self.max_action,   # ìµœëŒ€ ì•¡ì…˜ê°’
        )
```

### **5.2.1 Discrete vs Continuous Action Space ì°¨ì´ì **

#### **Continuous Action Space (ì—°ì† ì•¡ì…˜ ê³µê°„)**
- **ì •ì˜**: ì‹¤ìˆ˜ ê°’ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ì—°ì†ì ì¸ ì•¡ì…˜
- **ì˜ˆì‹œ**: `[0.5, -0.3, 0.8, 0.2, -0.1, 0.7, 0.0]` (7 DOF)
- **íŠ¹ì§•**: 
  - ì •ë°€í•œ ì œì–´ ê°€ëŠ¥
  - ì§ì ‘ì ì¸ ë¡œë´‡ ì œì–´
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **ì‚¬ìš© ì‚¬ë¡€**: ì •ë°€í•œ ë¡œë´‡ ì¡°ì‘, ì—°ì†ì ì¸ ì›€ì§ì„

#### **Discrete Action Space (ì´ì‚° ì•¡ì…˜ ê³µê°„)**
- **ì •ì˜**: í† í°ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ì´ì‚°ì ì¸ ì•¡ì…˜
- **ì˜ˆì‹œ**: `[45, 23, 67, 12, 89, 34, 0]` (í† í° ID)
- **íŠ¹ì§•**:
  - í† í° ê¸°ë°˜ í‘œí˜„
  - ì–¸ì–´ ëª¨ë¸ê³¼ í˜¸í™˜
  - ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ìš©ì´
- **ì‚¬ìš© ì‚¬ë¡€**: ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì œì–´, ì‹œí€€ìŠ¤ ì˜ˆì¸¡

#### **ë³€í™˜ ê³¼ì •**
```python
# Continuous â†’ Discrete ë³€í™˜
continuous_action = [0.5, -0.3, 0.8]  # ì—°ì† ê°’
discrete_tokens = tokenizer.encode(continuous_action)  # [45, 23, 67]

# Discrete â†’ Continuous ë³€í™˜  
discrete_tokens = [45, 23, 67]  # í† í° ID
continuous_action = tokenizer.decode(discrete_tokens)  # [0.5, -0.3, 0.8]
```

#### **7 DOF ë™ê¸°í™” ê³¼ì •**
1. **Continuous**: 7ì°¨ì› ì‹¤ìˆ˜ ë²¡í„° ì§ì ‘ ì‚¬ìš©
2. **Discrete**: 7ì°¨ì› ì‹¤ìˆ˜ ë²¡í„° â†’ 7ê°œ í† í° â†’ ì‹œí€€ìŠ¤ ì˜ˆì¸¡
3. **ë™ê¸°í™”**: ì‹œê°„ì  ì¼ê´€ì„± ìœ ì§€
4. **í•™ìŠµ**: End-to-end íŒŒì¸íŠœë‹

### **5.2.2 Continuous vs Discrete ì‚¬ìš© ì‚¬ë¡€**

#### **Continuous Action Space ì‚¬ìš© ì‚¬ë¡€**

##### **1. ì •ë°€í•œ ë¡œë´‡ ì¡°ì‘**
- **ìƒí™©**: ë¯¸ì„¸í•œ ì›€ì§ì„ì´ í•„ìš”í•œ ì‘ì—…
- **ì˜ˆì‹œ**: ìˆ˜ìˆ ìš© ë¡œë´‡, ì •ë°€ ì¡°ë¦½, ë¯¸ì„¸ ì¡°ì‘
- **ì¥ì **: ì—°ì†ì ì¸ ì œì–´, ì •ë°€ë„ ë†’ìŒ
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì •ë°€í•œ ê·¸ë¦¬í¼ ì œì–´
gripper_action = 0.75  # 75% ë‹«í˜ (ì—°ì†ê°’)
arm_position = [0.123, -0.456, 0.789]  # ì •ë°€í•œ ìœ„ì¹˜
```

##### **2. ì‹¤ì‹œê°„ ì œì–´**
- **ìƒí™©**: ì‹¤ì‹œê°„ í”¼ë“œë°±ì´ í•„ìš”í•œ ì‘ì—…
- **ì˜ˆì‹œ**: ë™ì  í™˜ê²½ì—ì„œì˜ ì¡°ì‘, ì‹¤ì‹œê°„ ì¶”ì 
- **ì¥ì **: ì§ì ‘ì ì¸ ì œì–´, ì§€ì—° ì‹œê°„ ìµœì†Œí™”
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì‹¤ì‹œê°„ ì œì–´
current_action = [x, y, z, rx, ry, rz, gripper]  # ì§ì ‘ ì œì–´
robot.execute_action(current_action)  # ì¦‰ì‹œ ì‹¤í–‰
```

##### **3. ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜**
- **ìƒí™©**: ë¬¼ë¦¬ ì—”ì§„ê³¼ì˜ ì—°ë™
- **ì˜ˆì‹œ**: MuJoCo, PyBullet ì‹œë®¬ë ˆì´ì…˜
- **ì¥ì **: ë¬¼ë¦¬ ë²•ì¹™ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ë™
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½
action = env.action_space.sample()  # ì—°ì† ì•¡ì…˜ ìƒ˜í”Œë§
obs, reward, done, info = env.step(action)  # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
```

#### **Discrete Action Space ì‚¬ìš© ì‚¬ë¡€**

##### **1. ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì œì–´**
- **ìƒí™©**: VLMê³¼ì˜ í†µí•©ì´ í•„ìš”í•œ ê²½ìš°
- **ì˜ˆì‹œ**: RoboVLMs, RT-2, PaLM-E
- **ì¥ì **: ì–¸ì–´ ëª¨ë¸ê³¼ ìì—°ìŠ¤ëŸ¬ìš´ í†µí•©
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì–¸ì–´ ëª¨ë¸ê³¼ í†µí•©
text_prompt = "Pick up the red block"
action_tokens = model.generate(text_prompt)  # [45, 23, 67, 12, 89, 34, 0]
action = tokenizer.decode(action_tokens)  # [0.5, -0.3, 0.8, 0.2, -0.1, 0.7, 0.0]
```

##### **2. ì‹œí€€ìŠ¤ ëª¨ë¸ë§**
- **ìƒí™©**: ì‹œê°„ì  ì˜ì¡´ì„±ì´ ì¤‘ìš”í•œ ì‘ì—…
- **ì˜ˆì‹œ**: ì¥ê¸° ê³„íš, ë³µì¡í•œ íƒœìŠ¤í¬ ì‹œí€€ìŠ¤
- **ì¥ì **: ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ ê°•ë ¥í•œ í‘œí˜„ë ¥ í™œìš©
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì‹œí€€ìŠ¤ ì˜ˆì¸¡
action_sequence = model.predict_sequence(obs_history)  # [token1, token2, ..., token7]
for token in action_sequence:
    action = tokenizer.decode(token)
    robot.execute_action(action)
```

##### **3. ì´ì‚°í™”ëœ ì œì–´**
- **ìƒí™©**: ì œí•œëœ ì•¡ì…˜ ê³µê°„ì´ í•„ìš”í•œ ê²½ìš°
- **ì˜ˆì‹œ**: ê²Œì„ AI, ì œí•œëœ í™˜ê²½ì—ì„œì˜ í•™ìŠµ
- **ì¥ì **: íƒìƒ‰ ê³µê°„ ì¶•ì†Œ, í•™ìŠµ ì•ˆì •ì„±
- **ì½”ë“œ ì˜ˆì‹œ**:
```python
# ì´ì‚°í™”ëœ ì•¡ì…˜ ê³µê°„
discrete_actions = [0, 1, 2, 3, 4, 5, 6]  # 7ê°œ ì´ì‚° ì•¡ì…˜
selected_action = discrete_actions[3]  # 4ë²ˆì§¸ ì•¡ì…˜ ì„ íƒ
```

#### **5.2.3 ì„ íƒ ê¸°ì¤€**

##### **Continuousë¥¼ ì„ íƒí•˜ëŠ” ê²½ìš°**
- âœ… **ì •ë°€í•œ ì œì–´**ê°€ í•„ìš”í•œ ê²½ìš°
- âœ… **ì‹¤ì‹œê°„ ì œì–´**ê°€ ì¤‘ìš”í•œ ê²½ìš°
- âœ… **ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜**ê³¼ ì—°ë™í•˜ëŠ” ê²½ìš°
- âœ… **ì§ì ‘ì ì¸ ë¡œë´‡ ì œì–´**ê°€ í•„ìš”í•œ ê²½ìš°

##### **Discreteë¥¼ ì„ íƒí•˜ëŠ” ê²½ìš°**
- âœ… **ì–¸ì–´ ëª¨ë¸ê³¼ í†µí•©**í•˜ëŠ” ê²½ìš°
- âœ… **ì‹œí€€ìŠ¤ ëª¨ë¸ë§**ì´ ì¤‘ìš”í•œ ê²½ìš°
- âœ… **ì´ì‚°í™”ëœ ì œì–´**ê°€ í•„ìš”í•œ ê²½ìš°
- âœ… **í† í° ê¸°ë°˜ í‘œí˜„**ì´ ìœ ë¦¬í•œ ê²½ìš°

##### **RoboVLMsì—ì„œì˜ ì„ íƒ**
- **ê¸°ë³¸ ì„¤ì •**: `"action_space": "continuous"` (ëŒ€ë¶€ë¶„ì˜ ì„¤ì • íŒŒì¼)
- **Discrete ì‚¬ìš©**: íŠ¹ì • ì‹¤í—˜ì—ì„œë§Œ ì‚¬ìš©
- **ì´ìœ **: ì •ë°€í•œ ë¡œë´‡ ì œì–´ê°€ ì£¼ ëª©ì ì´ê¸° ë•Œë¬¸

### **5.3 Action Head Initialization**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:425-468`
- **Implementation**: `_init_heads()` function
- **Code**:
```python
def _init_heads(self):
    action_head = None
    if self.act_head_configs is not None:
        import robovlms.model.policy_head as action_heads
        
        _kwargs = copy.deepcopy(self.act_head_configs)
        _kwargs.update(
            dict(
                hidden_size=self.hidden_size,
                fwd_pred_next_n=self.fwd_pred_next_n,
                window_size=self.window_size,
                n_bin=self.act_head_configs.get("n_bin", 256),
                min_action=self.act_head_configs.get("min_action", -1),
                max_action=self.act_head_configs.get("max_action", 1),
            )
        )
        _cls = getattr(action_heads, _kwargs.pop("type"))
        self.latent_num = self.act_head_configs.get("latent", 1)
        action_head = _cls(**_kwargs)
    
    return action_head, fwd_decoder, clip_norm_head
```

## ğŸ“Š **Learning Process Evidence**

### **5.4 Action Sequence Learning**
- **Sequence Length**: Configurable window size (8, 16, 32)
- **Action Chunking**: Multi-step action prediction
- **Temporal Modeling**: History-aware action generation

### **5.5 7 DOF Synchronization**
- **Position (3 DOF)**: X, Y, Z coordinates
- **Orientation (3 DOF)**: Euler angles (X, Y, Z)
- **Gripper (1 DOF)**: Binary or continuous control
- **Total**: 7 DOF synchronized action space

## ğŸ¯ **Key Findings**

1. **Discrete Action Space**: Tokenized action representation
2. **Sequence Prediction**: Multi-step action generation
3. **7 DOF Support**: Complete robot arm control
4. **Configurable**: Flexible action space configuration

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/model/backbone/base_backbone.py`
- `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py`
- `RoboVLMs/configs/calvin_finetune/*.json` (9 files)
- `RoboVLMs/configs/oxe_training/*.json` (4 files)
