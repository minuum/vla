# 4. Multimodal Synchronization - Citation Evidence

## ğŸ” **GitHub Code Implementation (100% Confirmed from @RoboVLMs)**

### **4.1 Multimodal Fusion Function**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:323-392` (Updated from @RoboVLMs)
- **Implementation**: `merge_multi_modal_input()` function for vision-language-action fusion
- **Core Code**:
```python
def merge_multi_modal_input(
    self,
    input_embeds: torch.Tensor,
    multimodal_feats: torch.Tensor = None,
    labels: torch.Tensor = None,
    attention_mask: torch.Tensor = None,
    is_image=True,
    insert_idx=1,
    fill_zero=False,
):
    """ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ìœµí•© (Vision-Language-Action)"""
    bs = input_embeds.shape[0]  # ë°°ì¹˜ í¬ê¸°
    
    if is_image:
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        rgb_feats = self.encode_images(multimodal_feats)
        
        # ì´ë¯¸ì§€ í† í° ì‹œì‘/ë ë§ˆì»¤ ì¶”ê°€
        if self.start_image_token_id is not None:
            # ì´ë¯¸ì§€ ì‹œì‘ í† í° ì„ë² ë”©
            image_start_embed = (
                self.word_embedding(self.start_image_token_id.to(self.model.device))
                .unsqueeze(0)
                .unsqueeze(0)
                .repeat(*rgb_feats.shape[:2], 1, 1)
            )
            
            # ì´ë¯¸ì§€ ë í† í° ID ì„¤ì •
            if self.end_image_token_id is None:
                end_image_token_id = self.start_image_token_id + 1
            else:
                end_image_token_id = self.end_image_token_id
            # ì´ë¯¸ì§€ ë í† í° ì„ë² ë”©
            image_end_embed = (
                self.word_embedding(end_image_token_id.to(self.model.device))
                .unsqueeze(0)
                .unsqueeze(0)
                .repeat(*rgb_feats.shape[:2], 1, 1)
            )
            
            # ì‹œì‘-ì´ë¯¸ì§€-ë í† í° ê²°í•©
            rgb_feats = torch.cat([image_start_embed, rgb_feats, image_end_embed], dim=2)
        
        # ì‹œí€€ìŠ¤ ì°¨ì› í‰íƒ„í™”
        rgb_feats = rearrange(rgb_feats, "b l n d -> b (l n) d")
    else:
        rgb_feats = multimodal_feats
    
    added_seq_len = rgb_feats.shape[1]  # ì¶”ê°€ëœ ì‹œí€€ìŠ¤ ê¸¸ì´
    
    # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© ê²°í•©
    multimodal_embeds = torch.cat(
        [input_embeds[:, :insert_idx], rgb_feats, input_embeds[:, insert_idx:]],
        dim=1,
    )
```

### **4.2 BaseRoboVLM Class Structure**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:34-57` (Updated from @RoboVLMs)
- **Implementation**: `BaseRoboVLM` class for multimodal VLA architecture
- **Code**:
```python
class BaseRoboVLM(nn.Module):
    """ë©€í‹°ëª¨ë‹¬ VLA ì•„í‚¤í…ì²˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    def __init__(
        self,
        configs,                    # ëª¨ë¸ ì„¤ì •
        train_setup_configs,        # í•™ìŠµ ì„¤ì •
        act_encoder_configs=None,   # ì•¡ì…˜ ì¸ì½”ë” ì„¤ì •
        act_head_configs=None,      # ì•¡ì…˜ í—¤ë“œ ì„¤ì •
        fwd_head_configs=None,      # ìˆœë°©í–¥ í—¤ë“œ ì„¤ì •
        window_size=None,          # ìœˆë„ìš° í¬ê¸°
        use_obs_queries=True,       # ê´€ì°° ì¿¼ë¦¬ ì‚¬ìš© ì—¬ë¶€
        use_act_queries=True,      # ì•¡ì…˜ ì¿¼ë¦¬ ì‚¬ìš© ì—¬ë¶€
        use_hand_rgb=False,        # ì† RGB ì‚¬ìš© ì—¬ë¶€
        use_pixel_loss=True,       # í”½ì…€ ì†ì‹¤ ì‚¬ìš© ì—¬ë¶€
        use_mim_obs_loss=False,    # MIM ê´€ì°° ì†ì‹¤ ì‚¬ìš© ì—¬ë¶€
        use_time_causal_attn=True, # ì‹œê°„ ì¸ê³¼ì  ì–´í…ì…˜ ì‚¬ìš© ì—¬ë¶€
        vision_masked_ratio=0.9,   # ë¹„ì „ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨
        use_tube_mask=False,       # íŠœë¸Œ ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€
        fwd_pred_next_n=1,         # ìˆœë°©í–¥ ì˜ˆì¸¡ ìŠ¤í… ìˆ˜
        use_vision_resampler=False, # ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬ ì‚¬ìš© ì—¬ë¶€
        vision_resampler_configs=None, # ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬ ì„¤ì •
        use_clip_norm=False,       # CLIP ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        use_state=False,           # ìƒíƒœ ì‚¬ìš© ì—¬ë¶€
        **kwargs,
    ):
```

### **4.3 RoboFlamingo Multimodal Integration**
- **File**: `RoboVLMs/robovlms/model/backbone/roboflamingo.py:200-254` (Updated from @RoboVLMs)
- **Implementation**: `cat_multi_input_ids()` function for multimodal input concatenation
- **Code**:
```python
def cat_multi_input_ids(
    self,
    input_ids: torch.Tensor,
    multimodal_ids: torch.Tensor = None,
    insert_idx: int = 0,
    attention_masks: torch.Tensor = None,
):
    """ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ID ê²°í•©"""
    bs, seq_len = input_ids.shape[:2]  # ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´
    device = input_ids.device
    
    if insert_idx >= 0:
        # í…ìŠ¤íŠ¸ì™€ ë©€í‹°ëª¨ë‹¬ ID ê²°í•©
        return_ids = torch.cat(
            (input_ids[:, :insert_idx], multimodal_ids, input_ids[:, insert_idx:]),
            dim=1,
        )
        # ì‚½ì… ë§ˆìŠ¤í¬ ìƒì„± (ë©€í‹°ëª¨ë‹¬ ë¶€ë¶„ë§Œ 1)
        insert_masks = torch.cat(
            (
                torch.zeros(bs, insert_idx),           # í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ (0)
                torch.ones(multimodal_ids.shape),      # ë©€í‹°ëª¨ë‹¬ ë¶€ë¶„ (1)
                torch.zeros(bs, seq_len - insert_idx), # í…ìŠ¤íŠ¸ ë’·ë¶€ë¶„ (0)
            ),
            dim=1,
        )
    return return_ids, insert_masks
```

### **4.3 Multimodal Feature Processing**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1384-1442`
- **Implementation**: `pred_action_discrete()` function
- **Code**:
```python
def pred_action_discrete(self, instr_and_action_ids, vision_x, ...):
    """ì´ì‚° ì•¡ì…˜ ì˜ˆì¸¡ (ë©€í‹°ëª¨ë‹¬ ìœµí•©)"""
    action_dim = self.act_head_configs["action_dim"]  # 7DOF ì•¡ì…˜ ì°¨ì› (x,y,z,rx,ry,rz,gripper)
    generated_ids = []                                # ìƒì„±ëœ ì•¡ì…˜ í† í° ID ë¦¬ìŠ¤íŠ¸
    kv_cache = None                                   # Key-Value ìºì‹œ (ì–´í…ì…˜ ìµœì í™”)
    self.fwd_pred_next_n = 1                         # ìˆœë°©í–¥ ì˜ˆì¸¡ ìŠ¤í… ìˆ˜
    
    for i in range(action_dim * self.fwd_pred_next_n):  # 7ê°œ ì•¡ì…˜ ì°¨ì› Ã— 1ìŠ¤í…
        output_hs = self.model(                        # VLM ëª¨ë¸ ìˆœì „íŒŒ
            inputs_embeds=multimodal_embeds,           # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸+ì•¡ì…˜)
            past_key_values=kv_cache,                  # ì´ì „ ì–´í…ì…˜ ìƒíƒœ ì¬ì‚¬ìš©
            use_cache=True,                            # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
        )
        kv_cache = output_hs.past_key_values           # ì–´í…ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        cur_id = output_hs.logits[:, -1].argmax(dim=-1) # í˜„ì¬ ì•¡ì…˜ í† í° ì˜ˆì¸¡
        generated_ids.append(cur_id)                   # ì˜ˆì¸¡ëœ í† í° ì €ì¥
```

## ğŸ“Š **Synchronization Method Evidence**

### **4.4 Vision-Language Fusion**
- **Image Encoding**: ë‹¤ì–‘í•œ ë¹„ì „ ì¸ì½”ë” ì§€ì› (ì´ë¯¸ì§€ â†’ ì‹œê°ì  íŠ¹ì§•)
  - **RoboFlamingo**: CLIP-based vision encoder (`clip_vision_encoder`)
  - **RoboKosMos**: Kosmos-2 vision model (`vision_model`)
  - **RoboUform**: Uform image encoder (`image_encoder`)
  - **RoboPaligemma**: Paligemma vision tower (`vision_tower`)
- **Text Encoding**: Language model tokenizer (í…ìŠ¤íŠ¸ â†’ ì–¸ì–´ì  íŠ¹ì§•)
- **Fusion**: Attention-based multimodal fusion (ì–´í…ì…˜ ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ìœµí•©)
- **Output**: Unified multimodal representation (í†µí•©ëœ ë©€í‹°ëª¨ë‹¬ í‘œí˜„)

### **4.5 Action Integration**
- **Action Head**: Dedicated action prediction head (ì „ìš© ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ)
- **History Modeling**: Temporal action sequence processing (ì‹œê°„ì  ì•¡ì…˜ ì‹œí€€ìŠ¤ ì²˜ë¦¬)
- **End-to-End**: Joint vision-language-action learning (í†µí•©ëœ ì‹œê°-ì–¸ì–´-ì•¡ì…˜ í•™ìŠµ)

### **4.6 Multimodal Synchronization Features**
- **Token-based Fusion**: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸/ì•¡ì…˜ì„ í† í°ìœ¼ë¡œ í†µì¼
- **Causal Attention**: ì‹œê°„ì  ì¸ê³¼ì  ì–´í…ì…˜ (ê³¼ê±° â†’ ë¯¸ë˜)
- **Cache Optimization**: Key-Value ìºì‹œë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- **Sequential Generation**: ìˆœì°¨ì  ì•¡ì…˜ í† í° ìƒì„±

## ğŸ¯ **Key Findings**

1. **Unified Architecture**: Single model for vision, language, and action (í†µí•© ì•„í‚¤í…ì²˜)
2. **Attention-based Fusion**: Advanced multimodal attention mechanisms (ì–´í…ì…˜ ê¸°ë°˜ ìœµí•©)
3. **Temporal Modeling**: History-aware action prediction (ì‹œê°„ì  ëª¨ë¸ë§)
4. **End-to-End Learning**: Joint optimization of all modalities (í†µí•© ìµœì í™”)
5. **Token-based Synchronization**: ëª¨ë“  ëª¨ë‹¬ë¦¬í‹°ë¥¼ í† í°ìœ¼ë¡œ í†µì¼ (í† í° ê¸°ë°˜ ë™ê¸°í™”)
6. **Causal Generation**: ìˆœì°¨ì  ì•¡ì…˜ í† í° ìƒì„±ìœ¼ë¡œ ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/model/backbone/base_backbone.py` (ê¸°ë³¸ ë©€í‹°ëª¨ë‹¬ ìœµí•©)
- `RoboVLMs/robovlms/model/backbone/robokosmos.py` (Kosmos-2 ë¹„ì „ ëª¨ë¸)
- `RoboVLMs/robovlms/model/backbone/robouform.py` (Uform ì´ë¯¸ì§€ ì¸ì½”ë”)
- `RoboVLMs/robovlms/model/backbone/robopaligemma.py` (Paligemma ë¹„ì „ íƒ€ì›Œ)
- `RoboVLMs/robovlms/model/backbone/roboflamingo.py` (CLIP ë¹„ì „ ì¸ì½”ë”)
- `RoboVLMs/robovlms/model/vision_encoder/vision_transformer.py` (CLIP ë¹„ì „ ì¸ì½”ë” êµ¬í˜„)
