# 10. Input Data Format for Finetuning - Citation Evidence

## ğŸ” **GitHub Code Implementation (100% Confirmed)**

### **10.1 Action Prediction Batch Transform**
- **File**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:24-416`
- **Implementation**: `ActionPredictionBatchTransform` class for data transformation
- **Code**:
```python
@dataclass
class ActionPredictionBatchTransform:
    """
    ë°ì´í„°ì…‹ì˜ í•œ í•­ëª©ì„ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
    """

    model_name: str                        # ëª¨ë¸ ì´ë¦„
    tokenizer: PreTrainedTokenizerBase     # í† í¬ë‚˜ì´ì €
    text_fn: Callable                      # í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
    image_fn: Callable[[List[Image.Image]], torch.Tensor]  # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜

    window_size: int                       # ìœˆë„ìš° í¬ê¸°
    fwd_pred_next_n: int                   # ìˆœë°©í–¥ ì˜ˆì¸¡ ìŠ¤í… ìˆ˜
    predict_stop_token: bool               # ì •ì§€ í† í° ì˜ˆì¸¡ ì—¬ë¶€

    organize_type: str                     # ì¡°ì§í™” íƒ€ì… (interleave/segment)
    image_history: bool                    # ì´ë¯¸ì§€ íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
    action_history: bool                   # ì•¡ì…˜ íˆìŠ¤í† ë¦¬ ì‚¬ìš© ì—¬ë¶€
    discrete: bool                         # ì´ì‚° ì•¡ì…˜ ì‚¬ìš© ì—¬ë¶€
    action_tokenizer: Optional[ActionTokenizer]  # ì•¡ì…˜ í† í¬ë‚˜ì´ì €
    special_history_id: int                # íŠ¹ë³„ íˆìŠ¤í† ë¦¬ ID
    mode: str                              # ëª¨ë“œ

    norm_action: bool                      # ì•¡ì…˜ ì •ê·œí™” ì—¬ë¶€
    norm_min: float                        # ì •ê·œí™” ìµœì†Œê°’
    norm_max: float                        # ì •ê·œí™” ìµœëŒ€ê°’
    x_mean: float                          # X í‰ê· ê°’
    x_std: float                           # X í‘œì¤€í¸ì°¨
    regular_action: bool                   # ì •ê·œ ì•¡ì…˜ ì‚¬ìš© ì—¬ë¶€
    use_mu_law: bool                       # Î¼-law ì‚¬ìš© ì—¬ë¶€
    min_action: float                      # ì•¡ì…˜ ìµœì†Œê°’
    max_action: float                      # ì•¡ì…˜ ìµœëŒ€ê°’

    def __call__(
        self,
        task_description: str,              # íƒœìŠ¤í¬ ì„¤ëª…
        action: np.ndarray,               # ì•¡ì…˜ ë°°ì—´
        episode_mask: np.ndarray,         # ì—í”¼ì†Œë“œ ë§ˆìŠ¤í¬
        images: np.ndarray,               # ì´ë¯¸ì§€ ë°°ì—´
        gripper_images: Optional[np.ndarray] = None,  # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ ë°°ì—´
    ) -> Dict[str, Any]:
        """í•­ëª©ì„ collator/modelsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        episode_mask = torch.tensor(episode_mask)  # ì—í”¼ì†Œë“œ ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜

        # ì´ë¯¸ì§€ì™€ ì•¡ì…˜ í…ì„œ íŒ¨ë”©
        image_tensors, image_chunk, image_chunk_mask = self.convert_image(
            images, episode_mask
        )
        gripper_image_tensors, gripper_image_chunk, _ = self.convert_image(
            gripper_images, episode_mask, static=False
        )

        # ì•¡ì…˜ í…ì„œ ì²˜ë¦¬
        action, action_mask, action_chunk, action_chunk_mask = self.convert_action(
            action, episode_mask
        )

        # ì…ë ¥ ID ìƒì„± (ì´ì‚° ì•¡ì…˜ ID í¬í•¨)
        if self.organize_type == "interleave":
            # ì¸í„°ë¦¬ë¸Œ ë°©ì‹: ì§€ì‹œì‚¬í•­ê³¼ ì•¡ì…˜ì„ êµëŒ€ë¡œ ë°°ì¹˜
            (
                input_ids,
                labels,
                attention_mask,
            ) = self.wrap_instruction_and_action_interleave(
                task_description, action, action_mask
            )
        elif self.organize_type == "segment":
            # ì„¸ê·¸ë¨¼íŠ¸ ë°©ì‹: ì§€ì‹œì‚¬í•­ê³¼ ì•¡ì…˜ì„ êµ¬ê°„ë³„ë¡œ ë°°ì¹˜
            (
                input_ids,
                labels,
                attention_mask,
            ) = self.wrap_instruction_and_action_segment(
                task_description, action, action_mask
            )
        else:
            raise TypeError("The organize type must be interleave or segment")

        # ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        return dict(
            image_tensors=image_tensors,           # ì´ë¯¸ì§€ í…ì„œ
            image_chunk=image_chunk,               # ì´ë¯¸ì§€ ì²­í¬
            image_chunk_mask=image_chunk_mask,     # ì´ë¯¸ì§€ ì²­í¬ ë§ˆìŠ¤í¬
            gripper_image_tensors=gripper_image_tensors,  # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ í…ì„œ
            gripper_image_chunk=gripper_image_chunk,       # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ ì²­í¬
            input_ids=input_ids,                   # ì…ë ¥ ID
            labels=labels,                         # ë ˆì´ë¸”
            attention_mask=attention_mask,         # ì–´í…ì…˜ ë§ˆìŠ¤í¬
            action_tensors=action,                 # ì•¡ì…˜ í…ì„œ
            action_mask=action_mask,               # ì•¡ì…˜ ë§ˆìŠ¤í¬
            action_chunk=action_chunk,             # ì•¡ì…˜ ì²­í¬
            action_chunk_mask=action_chunk_mask,   # ì•¡ì…˜ ì²­í¬ ë§ˆìŠ¤í¬
        )
```

### **10.2 Data Collation Implementation**
- **File**: `RoboVLMs/robovlms/data/concat_dataset.py:93-142`
- **Implementation**: `collater` function for batch collation
- **Code**:
```python
def collater(self, data):
    # action_tensors = torch.from_numpy(np.array([np.stack(s["action"]) for s in data]))
    # print(data)
    # return self.datasets[0].collater(data)
    action_tensors = (
        torch.stack([s["action"] for s in data], dim=0)
        if data[0]["action"] is not None
        else None
    )
    image_tensors = torch.stack([s["rgb"] for s in data])
    image_mask = torch.stack([s["attention_mask"] for s in data])
    gripper_tensors = (
        torch.stack([s["hand_rgb"] for s in data])
        if data[0]["hand_rgb"] is not None
        else None
    )

    fwd_rgb_chunck = generate_chunck_data(
        image_tensors, self.window_size, self.fwd_pred_next_n
    )
    fwd_hand_rgb_chunck = generate_chunck_data(
        gripper_tensors, self.window_size, self.fwd_pred_next_n
    )
    chunck_mask = generate_chunck_data(
        image_mask, self.window_size, self.fwd_pred_next_n
    )

    action_chunck = generate_chunck_data(
        action_tensors, self.window_size, self.fwd_pred_next_n
    )

    stacked_language = [s["raw_text"] for s in data]
    text_tensors, text_mask = self.text_fn(stacked_language)

    res = {
        "rgb": image_tensors,
        "attention_mask": image_mask,
        "hand_rgb": gripper_tensors,
        "action": action_tensors,
        "text": text_tensors,
        "text_mask": text_mask,
        "fwd_rgb_chunck": fwd_rgb_chunck,
        "fwd_hand_rgb_chunck": fwd_hand_rgb_chunck,
        "action_chunck": action_chunck,
        "chunck_mask": chunck_mask,
    }

    # return image_tensors, (text_tensors, text_mask), action_tensors, gripper_tensors, image_mask,\
    #     fwd_rgb_chunck, fwd_hand_rgb_chunck, action_chunk
    return res
```

### **10.3 Text Processing Functions**
- **File**: `RoboVLMs/robovlms/data/data_utils.py:273-433`
- **Implementation**: `get_text_function` for different tokenizer types
- **Code**:
```python
def get_text_function(tokenizer, tokenizer_type, max_length=256):
    import functools

    if tokenizer_type == "flamingo":

        def preprocess_text_flamingo(sample, tokenizer):
            tokenizer.padding_side = "right"
            sample = [
                (f"<image>{s.strip()}<|endofchunk|>{tokenizer.eos_token}")
                for s in sample
            ]
            text = tokenizer(
                sample,
                max_length=max_length,
                padding="longest",
                truncation="only_first",
                return_tensors="pt",
            )
            return text["input_ids"], text["attention_mask"]

        return functools.partial(preprocess_text_flamingo, tokenizer=tokenizer)
    elif tokenizer_type == "llava":
        DEFAULT_IMAGE_TOKEN = "<image>"

        def preprocess_text_llava(sample, tokenizer):
            # tokenizer.padding_side = "right"
            sample = [
                (f"{tokenizer.eos_token}{s.strip()}")
                for s in sample
            ]
            text = tokenizer(
                sample,
                max_length=max_length,
                padding="longest",
                truncation="only_first",
                return_tensors="pt",
            )
            return text["input_ids"], text["attention_mask"]

        return functools.partial(preprocess_text_llava, tokenizer=tokenizer)
    elif tokenizer_type == "paligemma":

        def preprocess_text_paligemma(sample, tokenizer):
            tokenizer.padding_side = "right"
            sample = [(f"{tokenizer.eos_token}{s.strip()}\n") for s in sample]
            text = tokenizer(
                sample,
                truncation="only_first",
                return_tensors="pt",
                padding="longest",
                max_length=512,
                add_special_tokens=False,
            )
            return text["input_ids"], text["attention_mask"]

        return functools.partial(preprocess_text_paligemma, tokenizer=tokenizer)
    else:

        def preprocess_text_default(sample, tokenizer):
            tokenizer.padding_side = "right"
            sample = [(f"<|endoftext|>{s.strip()}") for s in sample]
            text = tokenizer(
                sample,
                truncation="only_first",
                return_tensors="pt",
                padding="longest",
                max_length=512,
                add_special_tokens=True,
            )
            return text["input_ids"], text["attention_mask"]

        return functools.partial(preprocess_text_default, tokenizer=tokenizer)
```

### **10.4 Image Processing Implementation**
- **File**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:77-108`
- **Implementation**: `convert_image` method for image processing
- **Code**:
```python
def convert_image(
    self,
    images: Optional[np.ndarray],
    image_mask: torch.Tensor,
    static: bool = True,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    if images is None:
        return None, None, None

    if not self.image_history:
        image_tensors = self.image_fn(
            [Image.fromarray(images[self.window_size - 1])], static=static
        )
        return image_tensors, None, None

    image_tensors = self.image_fn(
        [Image.fromarray(each_image) for each_image in images], static=static
    )

    # you can't get chunk image in the segment dataset because segment dataset will padding in the left side
    if self.organize_type == "segment":
        return image_tensors, None, None

    left_pad_index = self.window_size - image_mask[: self.window_size].sum()
    image_tensors[:left_pad_index] = image_tensors[left_pad_index]

    # this chunk is to predict next fwd_pred_next_n images, it is based on one image, so we need to skip the first one which including image0
    image_chunk = get_tensor_chunk(image_tensors, self.fwd_pred_next_n)[1:]
    image_chunk_mask = get_tensor_chunk(image_mask, self.fwd_pred_next_n)[1:]

    image_tensors = image_tensors[: self.window_size]
    return image_tensors, image_chunk, image_chunk_mask
```

## ğŸ“Š **Data Format Evidence**

### **10.5 Image Data Format**
- **RGB Images**: [Batch, Time, Channel, Height, Width]
- **Resolution**: 224x224 or 336x336 pixels
- **Normalization**: [0, 1] range
- **Augmentation**: Random cropping, flipping, color jittering

### **10.6 Action Data Format**
- **7-DOF Actions**: [Batch, Time, 7] (position + orientation + gripper)
- **Normalization**: Scaled to (-1, 1) range
- **Chunking**: Multi-step action sequences
- **Masking**: Valid action chunk masking

### **10.7 Text Data Format**
- **Language Instructions**: Natural language task descriptions
- **Tokenization**: BPE or WordPiece tokenization
- **Max Length**: 512 tokens
- **Padding**: Dynamic padding to max length in batch

## ğŸ¯ **Key Findings**

1. **Unified Format**: Consistent data format across all modalities
2. **Temporal Sequences**: Time-series data with windowing
3. **Multimodal Integration**: RGB, action, and text in single batch
4. **Efficient Processing**: Optimized data loading and preprocessing

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/data/base_action_prediction_dataset.py`
- `RoboVLMs/robovlms/data/concat_dataset.py`
- `RoboVLMs/robovlms/data/data_utils.py`
- `RoboVLMs/robovlms/data/calvin_dataset.py`
