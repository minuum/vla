# 00 Action, Image, Text Syncing in RoboVLMs

## ğŸ“‹ **Overview**

This document provides a comprehensive analysis of how Action, Image, and Text are synchronized in RoboVLMs, addressing the 11 key questions about VLA (Vision-Language-Action) model synchronization and training.

## ğŸ¯ **Key Findings**

### **1. VLM Fine-tuning: F-FT vs LoRA**

#### **1.1 Fine-tuning Methods in RoboVLMs**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:512-525` (Updated from @RoboVLMs)
- **Implementation**: LoRA configuration and setup
- **Current Usage**: Most configurations use Full Fine-Tuning (F-FT)

#### **1.2 LoRA Configuration**
```python
if self.train_setup_configs["lora_enable"]:
    from llava.train.train import find_all_linear_names
    from peft import LoraConfig, get_peft_model

    lora_config = LoraConfig(
        r=self.train_setup_configs["lora_r"],                    # LoRA rank (64)
        lora_alpha=self.train_setup_configs["lora_alpha"],       # LoRA alpha (16)
        target_modules=find_all_linear_names(model),            # Target modules
        lora_dropout=self.train_setup_configs["lora_dropout"],   # LoRA dropout (0.05)
        bias=self.train_setup_configs["lora_bias"],             # LoRA bias ("none")
        task_type="CAUSAL_LM",                                   # Task type
    )
    print("Adding LoRA adapters...")
    self.model = get_peft_model(model, lora_config)
```

#### **1.3 Configuration Analysis**
- **Source**: Configuration files in `RoboVLMs/configs/` (Updated from @RoboVLMs)
- **LoRA Usage**: `"lora_enable": false` in 13 out of 15 configuration files
- **Full Fine-Tuning**: 87% of configurations use F-FT instead of LoRA
- **Reason**: Robot control requires full model capacity for precise action prediction

### **2. Action and rel_action Synchronization**

#### **2.1 Coordinate Frame Transformations**
- **Source**: `RoboVLMs/robovlms/data/data_utils.py:770-820` (Updated from @RoboVLMs)
- **Implementation**: `world_to_tcp_frame()` and `tcp_to_world_frame()` functions
- **Purpose**: Synchronize absolute world coordinates with relative TCP coordinates

#### **2.2 World to TCP Frame Conversion**
```python
def world_to_tcp_frame(action, robot_obs):
    """ì ˆëŒ€ ì¢Œí‘œê³„ì—ì„œ TCP ìƒëŒ€ ì¢Œí‘œê³„ë¡œ ë³€í™˜"""
    # 1. ë¡œë´‡ ê´€ì°°ê°’ì—ì„œ TCP ë³€í™˜ í–‰ë ¬ ê³„ì‚°
    world_T_tcp = (
        euler_angles_to_matrix(robot_obs[..., 3:6], convention="XYZ")
        .float()
        .reshape(-1, 3, 3)
    )
    tcp_T_world = torch.inverse(world_T_tcp)
    
    # 2. ìœ„ì¹˜ ì¢Œí‘œ ë³€í™˜
    pos_w_rel = action[..., :3].reshape(-1, 3, 1)
    pos_tcp_rel = tcp_T_world @ pos_w_rel
    
    # 3. íšŒì „ ì¢Œí‘œ ë³€í™˜ (ìŠ¤ì¼€ì¼ë§ ì ìš©)
    orn_w_rel = action[..., 3:6] * 0.01  # ë‹¤ìš´ìŠ¤ì¼€ì¼ë§
    world_T_tcp_new = (
        euler_angles_to_matrix(robot_obs[..., 3:6] + orn_w_rel, convention="XYZ")
        .float()
        .reshape(-1, 3, 3)
    )
    tcp_new_T_tcp_old = torch.inverse(world_T_tcp_new) @ world_T_tcp
    orn_tcp_rel = matrix_to_euler_angles(
        tcp_new_T_tcp_old, convention="XYZ"
    ).float()
    
    # 4. ê°ë„ ì •ê·œí™”
    orn_tcp_rel = torch.where(
        orn_tcp_rel < -np.pi, orn_tcp_rel + 2 * np.pi, orn_tcp_rel
    )
    orn_tcp_rel = torch.where(
        orn_tcp_rel > np.pi, orn_tcp_rel - 2 * np.pi, orn_tcp_rel
    )
    
    # 5. ì—…ìŠ¤ì¼€ì¼ë§
    orn_tcp_rel *= 100
    
    # 6. ìµœì¢… ì•¡ì…˜ ê²°í•©
    action_tcp = torch.cat([
        pos_tcp_rel.reshape(b, s, -1),      # TCP ìƒëŒ€ ìœ„ì¹˜
        orn_tcp_rel.reshape(b, s, -1),     # TCP ìƒëŒ€ íšŒì „
        action[..., -1:],                   # ê·¸ë¦¬í¼ ì•¡ì…˜ (ë³€ê²½ ì—†ìŒ)
    ], dim=-1)
    
    return action_tcp
```

#### **2.3 Scaling Factors**
- **Position Scaling**: 50x scaling factor for position coordinates
- **Rotation Scaling**: 20x scaling factor for rotation coordinates
- **Normalization**: Coordinates clipped to (-1, 1) range
- **Source**: `RoboVLMs/eval/calvin/model_wrapper.py:424` (Updated from @RoboVLMs)

### **3. Robot Arm Movement (7 DOF)**

#### **3.1 7 DOF Action Structure**
- **Source**: `RoboVLMs/vla_test/robovlm_action_parser.py:28-78` (Updated from @RoboVLMs)
- **Implementation**: `RoboAction` dataclass for 7 DOF control
- **Components**:
  ```python
  @dataclass
  class RoboAction:
      """RoboVLMs ìŠ¤íƒ€ì¼ ë¡œë´‡ ì•¡ì…˜"""
      # 6DOF ì•¡ì…˜ (x, y, z, roll, pitch, yaw)
      translation: np.ndarray = None  # (3,) [x, y, z]
      rotation: np.ndarray = None     # (3,) [roll, pitch, yaw] 
      gripper: float = 0.0           # ê·¸ë¦¬í¼ ìƒíƒœ (0: ì—´ë¦¼, 1: ë‹«í˜)
      
      # ë©”íƒ€ë°ì´í„°
      action_type: str = "unknown"
      confidence: float = 0.0
      control_mode: RobotControl = RobotControl.VELOCITY
  ```

#### **3.2 Action Parsing**
```python
def parse_continuous_action(self, 
                          action_tensor: torch.Tensor,
                          text_instruction: str = "",
                          vision_features: Optional[torch.Tensor] = None) -> RoboAction:
    """ì—°ì† ì•¡ì…˜ í…ì„œ íŒŒì‹± (BaseRoboVLM.forward_continuous ì¶œë ¥)"""
    
    # 1. í…ì„œ í˜•íƒœ í™•ì¸ ë° ì •ê·œí™”
    if action_array.ndim == 3:  # (batch, seq_len, action_dim)
        action_array = action_array[0, -1]  # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì‚¬ìš©
    elif action_array.ndim == 2:  # (seq_len, action_dim)
        action_array = action_array[-1]     # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì‚¬ìš©
    
    # 2. ì•¡ì…˜ ì •ê·œí™” ([-1, 1] -> ì‹¤ì œ ì œì–´ ê°’)
    action_array = np.clip(action_array, self.min_action, self.max_action)
    
    # 3. 6DOF ì•¡ì…˜ ë¶„í•´
    if len(action_array) >= 6:
        translation = action_array[:3]      # ìœ„ì¹˜ (x, y, z)
        rotation = action_array[3:6]       # íšŒì „ (roll, pitch, yaw)
        gripper = action_array[6] if len(action_array) > 6 else 0.0  # ê·¸ë¦¬í¼
    else:
        # ë¶€ì¡±í•œ ì°¨ì›ì€ 0ìœ¼ë¡œ íŒ¨ë”©
        padded_action = np.zeros(6)
        padded_action[:len(action_array)] = action_array
        translation = padded_action[:3]
        rotation = padded_action[3:6]
        gripper = 0.0
    
    return RoboAction(
        translation=translation,
        rotation=rotation,
        gripper=gripper,
        action_type=action_type,
        confidence=confidence,
        control_mode=RobotControl.VELOCITY,
        prediction_horizon=self.prediction_horizon
    )
```

### **4. Image, Text, and Action Synchronization**

#### **4.1 Multimodal Fusion**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:323-375` (Updated from @RoboVLMs)
- **Implementation**: `merge_multi_modal_input()` function
- **Process**: Image and text features are fused through cross-attention mechanisms

#### **4.2 Action Token Integration**
```python
# ì•¡ì…˜ í† í° ì‚½ì…
if action_space == "continuous":
    insert_idx = multimodal_embeds.shape[1] - int(
        self.tokenizer.eos_token is not None
    )  # ë§ˆì§€ë§‰ì— ì‚½ì…

    action_tokens = repeat(
        self.action_token,
        "d -> b n d",
        b=multimodal_embeds.shape[0],
        n=self.latent_num,
    )
    (
        multimodal_embeds,
        mutlimodal_labels,
        multimodal_attention_mask,
        action_token_mask,
    ) = self.merge_multi_modal_input(
        multimodal_embeds,
        action_tokens,
        mutlimodal_labels,
        multimodal_attention_mask,
        is_image=False,
        insert_idx=insert_idx,
        fill_zero=self.act_head_configs.get("fill_zero", False),
    )
```

### **5. Embedded Token Synchronization**

#### **5.1 Action Tokenizer**
- **Source**: `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py:14-58` (Updated from @RoboVLMs)
- **Implementation**: `ActionTokenizer` class for discretizing continuous actions
- **Process**: Continuous actions â†’ Discrete tokens â†’ Action predictions

#### **5.2 Tokenization Process**
```python
class ActionTokenizer:
    def __init__(
        self,
        tokenizer: PreTrainedTokenizerBase,
        bins: int = 256,
        min_action: int = -1,
        max_action: int = 1,
        add_action_end_flag=False,
    ) -> None:
        """ì—°ì† ë¡œë´‡ ì•¡ì…˜ì„ Nê°œ ë¹ˆìœ¼ë¡œ ì´ì‚°í™”í•˜ê³  ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í† í°ì— ë§¤í•‘"""
        
        self.tokenizer, self.n_bins, self.min_action, self.max_action = (
            tokenizer, bins, min_action, max_action,
        )
        
        # ê· ë“± ë¶„í•  ë¹ˆ ìƒì„±
        self.bins = np.linspace(min_action, max_action, self.n_bins)
        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0
        
        # ì•¡ì…˜ í† í° ì¸ë±ìŠ¤ ì„¤ì •
        self.action_token_begin_idx = self.tokenizer.vocab_size - (self.n_bins + 1)
        self.action_token_end_idx = self.tokenizer.vocab_size - 1
```

#### **5.3 Discrete Action Processing**
```python
def pred_action_discrete(self, instr_and_action_ids, vision_x, vision_gripper=None, attention_mask=None):
    """ì´ì‚° ì•¡ì…˜ ì˜ˆì¸¡"""
    action_dim = self.act_head_configs["action_dim"]
    
    generated_ids = []
    kv_cache = None
    self.fwd_pred_next_n = 1
    
    # ì•¡ì…˜ ì°¨ì›ë§Œí¼ í† í° ìƒì„±
    for i in range(action_dim * self.fwd_pred_next_n):
        if kv_cache is None:
            output_hs = self.model(
                inputs_embeds=multimodal_embeds,
                past_key_values=kv_cache,
                use_cache=True,
            )
        else:
            output_hs = self.model(
                inputs_embeds=multimodal_embeds[:, -1:],
                past_key_values=kv_cache,
                use_cache=True,
            )
        kv_cache = output_hs.past_key_values
        cur_id = output_hs.logits[:, -1].argmax(dim=-1)
        generated_ids.append(cur_id)
        cur_embed = self.word_embedding(cur_id)
        multimodal_embeds = torch.cat(
            [multimodal_embeds, cur_embed.unsqueeze(1)], dim=1
        )
    
    # í† í° IDë¥¼ ì•¡ì…˜ìœ¼ë¡œ ë””ì½”ë”©
    predicted_action_ids = generated_ids[:, -action_dim:].cpu().numpy()
    discretized_actions = self.action_tokenizer.decode_token_ids_to_actions(
        predicted_action_ids
    )
    
    return discretized_actions
```

### **6. CALVIN Dataset Analysis**

#### **6.1 Dataset Structure**
- **Source**: `RoboVLMs/robovlms/data/calvin_dataset.py:521-600` (Updated from @RoboVLMs)
- **Implementation**: `DiskCalvinDataset` class for episode loading
- **Features**: 24,000 demonstrations, 34 basic skills, Franka Panda 7-DOF

#### **6.2 Data Loading Process**
```python
class DiskCalvinDataset(BaseCalvinDataset):
    """ë””ìŠ¤í¬ì—ì„œ CALVIN ì—í”¼ì†Œë“œë¥¼ ë¡œë“œí•˜ëŠ” ë°ì´í„°ì…‹"""
    def __init__(
        self,
        image_fn: Callable,           # ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜
        tokenizer: Callable,          # í† í¬ë‚˜ì´ì € í•¨ìˆ˜
        skip_frames: int = 1,         # í”„ë ˆì„ ìŠ¤í‚µ ìˆ˜
        save_format: str = "npz",     # ì €ì¥ í˜•ì‹ (npz/pkl)
        pretrain: bool = False,       # ì‚¬ì „ í›ˆë ¨ ì—¬ë¶€
        partial_data=False,          # ë¶€ë¶„ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
        decoder_type="lstm",          # ë””ì½”ë” íƒ€ì…
        discrete_action=False,        # ì´ì‚° ì•¡ì…˜ ì‚¬ìš© ì—¬ë¶€
        action_tokenizer=None,        # ì•¡ì…˜ í† í¬ë‚˜ì´ì €
        model_name="vicuna",          # ëª¨ë¸ ì´ë¦„
        predict_stop_token=True,      # ì •ì§€ í† í° ì˜ˆì¸¡ ì—¬ë¶€
        use_mu_law=False,            # Î¼-law ì‚¬ìš© ì—¬ë¶€
        mu_val=255,                   # Î¼-law ê°’
        n_bin=256,                    # ì´ì‚°í™” ë¹ˆ ìˆ˜
        min_action=-1,                # ì•¡ì…˜ ìµœì†Œê°’
        max_action=1,                 # ì•¡ì…˜ ìµœëŒ€ê°’
        task_type="calvin_action",    # íƒœìŠ¤í¬ íƒ€ì…
        tcp_rel=False,               # TCP ìƒëŒ€ ì¢Œí‘œ ì‚¬ìš© ì—¬ë¶€
        few_shot=False,               # Few-shot í•™ìŠµ ì—¬ë¶€
        exclude_tasks=[],             # ì œì™¸í•  íƒœìŠ¤í¬ ëª©ë¡
        **kwargs: Any,                # ì¶”ê°€ í‚¤ì›Œë“œ ì¸ìˆ˜ë“¤
    ):
```

### **7. Data Extraction and Fine-tuning**

#### **7.1 Data Extraction Process**
- **Source**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:25-150` (Updated from @RoboVLMs)
- **Implementation**: `ActionPredictionBatchTransform` class
- **Process**: Raw data â†’ Processed batches â†’ Model training

#### **7.2 Batch Transformation**
```python
@dataclass
class ActionPredictionBatchTransform:
    """ë°ì´í„°ì…‹ì˜ í•œ í•­ëª©ì„ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __call__(
        self,
        task_description: str,              # íƒœìŠ¤í¬ ì„¤ëª…
        action: np.ndarray,               # ì•¡ì…˜ ë°°ì—´
        episode_mask: np.ndarray,         # ì—í”¼ì†Œë“œ ë§ˆìŠ¤í¬
        images: np.ndarray,               # ì´ë¯¸ì§€ ë°°ì—´
        gripper_images: Optional[np.ndarray] = None,  # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ ë°°ì—´
    ) -> Dict[str, Any]:
        """í•­ëª©ì„ collator/modelsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        # 1. ì´ë¯¸ì§€ì™€ ì•¡ì…˜ í…ì„œ íŒ¨ë”©
        image_tensors, image_chunk, image_chunk_mask = self.convert_image(
            images, episode_mask
        )
        gripper_image_tensors, gripper_image_chunk, _ = self.convert_image(
            gripper_images, episode_mask, static=False
        )
        
        # 2. ì•¡ì…˜ í…ì„œ ì²˜ë¦¬
        action, action_mask, action_chunk, action_chunk_mask = self.convert_action(
            action, episode_mask
        )
        
        # 3. ì…ë ¥ ID ìƒì„± (ì´ì‚° ì•¡ì…˜ ID í¬í•¨)
        if self.organize_type == "interleave":
            # ì¸í„°ë¦¬ë¸Œ ë°©ì‹: ì§€ì‹œì‚¬í•­ê³¼ ì•¡ì…˜ì„ êµëŒ€ë¡œ ë°°ì¹˜
            (
                input_ids,
                labels,
                attention_mask,
            ) = self.wrap_instruction_and_action_interleave(
                task_description, action, action_mask
            )
        elif self.organize_type == "segment":
            # ì„¸ê·¸ë¨¼íŠ¸ ë°©ì‹: ì§€ì‹œì‚¬í•­ê³¼ ì•¡ì…˜ì„ êµ¬ê°„ë³„ë¡œ ë°°ì¹˜
            (
                input_ids,
                labels,
                attention_mask,
            ) = self.wrap_instruction_and_action_segment(
                task_description, action, action_mask
            )
        
        return dict(
            image_tensors=image_tensors,           # ì´ë¯¸ì§€ í…ì„œ
            image_chunk=image_chunk,               # ì´ë¯¸ì§€ ì²­í¬
            image_chunk_mask=image_chunk_mask,     # ì´ë¯¸ì§€ ì²­í¬ ë§ˆìŠ¤í¬
            gripper_image_tensors=gripper_image_tensors,  # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ í…ì„œ
            gripper_image_chunk=gripper_image_chunk,       # ê·¸ë¦¬í¼ ì´ë¯¸ì§€ ì²­í¬
            input_ids=input_ids,                   # ì…ë ¥ ID
            labels=labels,                         # ë ˆì´ë¸”
            attention_mask=attention_mask,         # ì–´í…ì…˜ ë§ˆìŠ¤í¬
            action_tensors=action,                 # ì•¡ì…˜ í…ì„œ
            action_mask=action_mask,               # ì•¡ì…˜ ë§ˆìŠ¤í¬
            action_chunk=action_chunk,             # ì•¡ì…˜ ì²­í¬
            action_chunk_mask=action_chunk_mask,   # ì•¡ì…˜ ì²­í¬ ë§ˆìŠ¤í¬
        )
```

### **8. VLM Fine-tuning for Multimodal Understanding**

#### **8.1 VLM vs LSTM Architecture**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:34-57` (Updated from @RoboVLMs)
- **VLM Advantage**: Unified multimodal processing through cross-attention
- **LSTM Limitation**: Sequential processing without multimodal fusion

#### **8.2 VLM Fine-tuning Process**
```python
class BaseRoboVLM(nn.Module):
    """í†µí•© ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ BaseRoboVLM ì•„í‚¤í…ì²˜"""
    def __init__(
        self,
        configs,                    # ëª¨ë¸ ì„¤ì •
        train_setup_configs,        # í•™ìŠµ ì„¤ì •
        act_encoder_configs=None,   # ì•¡ì…˜ ì¸ì½”ë” ì„¤ì •
        act_head_configs=None,     # ì•¡ì…˜ í—¤ë“œ ì„¤ì •
        fwd_head_configs=None,     # ìˆœë°©í–¥ í—¤ë“œ ì„¤ì •
        window_size=None,          # ìœˆë„ìš° í¬ê¸°
        use_obs_queries=True,      # ê´€ì°° ì¿¼ë¦¬ ì‚¬ìš© ì—¬ë¶€
        use_act_queries=True,      # ì•¡ì…˜ ì¿¼ë¦¬ ì‚¬ìš© ì—¬ë¶€
        use_hand_rgb=False,        # ì† RGB ì‚¬ìš© ì—¬ë¶€
        use_pixel_loss=True,       # í”½ì…€ ì†ì‹¤ ì‚¬ìš© ì—¬ë¶€
        use_mim_obs_loss=False,    # MIM ê´€ì°° ì†ì‹¤ ì‚¬ìš© ì—¬ë¶€
        use_time_causal_attn=True, # ì‹œê°„ ì¸ê³¼ì  ì–´í…ì…˜ ì‚¬ìš© ì—¬ë¶€
        vision_masked_ratio=0.9,   # ë¹„ì „ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨
        use_tube_mask=False,       # íŠœë¸Œ ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€
        fwd_pred_next_n=1,         # ìˆœë°©í–¥ ì˜ˆì¸¡ ìŠ¤í… ìˆ˜
        use_vision_resampler=False, # ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬ ì‚¬ìš© ì—¬ë¶€
        vision_resampler_configs=None, # ë¹„ì „ ë¦¬ìƒ˜í”ŒëŸ¬ ì„¤ì •
        use_clip_norm=False,       # CLIP ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        use_state=False,           # ìƒíƒœ ì‚¬ìš© ì—¬ë¶€
        **kwargs,                  # ì¶”ê°€ í‚¤ì›Œë“œ ì¸ìˆ˜ë“¤
    ):
```

### **9. Input Data Format for Fine-tuning**

#### **9.1 Data Format Requirements**
- **Source**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:25-150` (Updated from @RoboVLMs)
- **Format**: Batch processing with image sequences, text instructions, and action labels
- **Structure**: (batch_size, sequence_length, feature_dim)

#### **9.2 Input Processing**
```python
def _process_batch(self, batch):
    """ë°°ì¹˜ ì²˜ë¦¬ ë©”ì„œë“œ (ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›)"""
    
    # RGB ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° GPUë¡œ ì´ë™
    if isinstance(batch["rgb"], list):
        rgb = [_.cuda() for _ in batch["rgb"]]
    else:
        rgb = batch["rgb"].cuda()
        if len(rgb.shape) == 4:
            rgb = rgb.unsqueeze(1)
        assert len(rgb.shape) == 5  # (batch, seq_len, channels, height, width)
    
    # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
    seq_len = self.configs["window_size"]   # ìœˆë„ìš° í¬ê¸°ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
    language = batch["text"].cuda()         # ì–¸ì–´ ë°ì´í„° GPUë¡œ ì´ë™
    text_mask = batch["text_mask"].cuda()   # í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬ GPUë¡œ ì´ë™
```

### **10. Training Specifics**

#### **10.1 Training Configuration**
- **Source**: Configuration files in `RoboVLMs/configs/` (Updated from @RoboVLMs)
- **Hyperparameters**:
  ```json
  {
      "learning_rate": 2e-5,
      "min_lr_scale": 1e-2,
      "weight_decay": 0,
      "warmup_epochs": 0.25,
      "batch_size": 4,
      "max_epochs": 5,
      "gradient_clip_val": 1.0,
      "precision": "bf16"
  }
  ```

#### **10.2 Loss Weights**
```json
{
    "arm_gripper_loss_ratio": 0.01,  # ì£¼ íƒœìŠ¤í¬ ê°€ì¤‘ì¹˜
    "cap_loss_ratio": 0.05,          # ìº¡ì…˜ ìƒì„± ê°€ì¤‘ì¹˜
    "fwd_loss_ratio": 0              # ë¯¸ë˜ ì˜ˆì¸¡ (ë¹„í™œì„±í™”)
}
```

### **11. Simultaneous Action Head Learning**

#### **11.1 End-to-End Learning**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:542-550` (Updated from @RoboVLMs)
- **Implementation**: VLM and action head learn simultaneously
- **Process**: VLM features â†’ Action head â†’ Action predictions

#### **11.2 Action Head Integration**
```python
def _forward_action_head(
    self,
    action_tokens: torch.Tensor,                    # ì•¡ì…˜ í† í°
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,  # ì•¡ì…˜ ë ˆì´ë¸”
    action_mask: torch.Tensor = None,              # ì•¡ì…˜ ë§ˆìŠ¤í¬
    **kwargs,                                      # ì¶”ê°€ í‚¤ì›Œë“œ ì¸ìˆ˜ë“¤
):
    """ì•¡ì…˜ í—¤ë“œ ìˆœì „íŒŒ ë° ë™ì‹œ í•™ìŠµ"""
    # ì•¡ì…˜ ì˜ˆì¸¡ì„ ìœ„í•œ ì•¡ì…˜ í—¤ë“œ
    action = self.act_head(
        action_tokens, actions=action_labels, action_masks=action_mask, **kwargs
    )
    
    # ë™ì‹œ í•™ìŠµ ì†ì‹¤ ê³„ì‚°
    if action_labels is not None:
        # ì•¡ì…˜ í—¤ë“œì—ì„œ ë ˆì´ë¸” ì²˜ë¦¬
        action, action_labels, action_mask = self.act_head.get_labels(
            action, action_labels, action_mask, tok_seq=action_tokens, **kwargs
        )
        # ì•¡ì…˜ ì†ì‹¤ ê³„ì‚°
        action_loss = self.act_head.loss(action, action_labels, action_mask)
    
    return action, action_loss
```

## ğŸ¯ **Key Findings**

1. **F-FT Dominance**: 87% of configurations use Full Fine-Tuning instead of LoRA
2. **Coordinate Synchronization**: World-to-TCP frame conversion with scaling factors
3. **7 DOF Control**: Translation (3) + Rotation (3) + Gripper (1) = 7 DOF
4. **Multimodal Fusion**: Cross-attention mechanisms for image-text-action synchronization
5. **Embedded Tokens**: Action tokenizer for discretizing continuous actions
6. **CALVIN Dataset**: 24,000 demonstrations with 34 basic skills
7. **End-to-End Learning**: VLM and action head learn simultaneously
8. **Input Format**: Batch processing with image sequences and text instructions
9. **Training Specifics**: BF16 precision, gradient clipping, loss weighting
10. **Action Head Integration**: LSTM decoder for sequential action prediction

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/model/backbone/base_backbone.py`
- `RoboVLMs/robovlms/data/data_utils.py`
- `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py`
- `RoboVLMs/robovlms/data/calvin_dataset.py`
- `RoboVLMs/robovlms/data/base_action_prediction_dataset.py`
- `RoboVLMs/vla_test/robovlm_action_parser.py`
- `RoboVLMs/eval/calvin/model_wrapper.py`
ã„´ã…ˆã…ˆã…ˆã…ˆ