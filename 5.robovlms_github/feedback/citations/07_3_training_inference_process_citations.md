# 07_3 Training & Inference Process Analysis - RoboVLMs GitHub Citations

## ğŸ“Š **Training & Inference Process Technical Analysis**

### **7.3.1 Training Process - Batch Processing**
- **Source**: `RoboVLMs/robovlms/train/base_trainer.py:345-395`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ í›ˆë ¨ ê³¼ì •
- **Batch Structure**:  # ë°°ì¹˜ êµ¬ì¡°
  ```python
  def _process_batch(self, batch):
      """
      ì•¡ì…˜ ì˜ˆì¸¡ ë°°ì¹˜ ì²˜ë¦¬
      args: rgb, language, attention_mask, hand_rgb, action
      reformat: action to input and target (seq_len = window size + chunck size)
      """
      # RGB ë°ì´í„° ì²˜ë¦¬
      if len(rgb.shape) == 4:
          rgb = rgb.unsqueeze(1)              # 4ì°¨ì› â†’ 5ì°¨ì›ìœ¼ë¡œ í™•ì¥
      assert len(rgb.shape) == 5              # (batch, seq_len, channels, height, width)
      
      # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
      seq_len = self.configs["window_size"]   # ìœˆë„ìš° í¬ê¸°ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
      language = batch["text"].cuda()         # ì–¸ì–´ ë°ì´í„° GPUë¡œ ì´ë™
      text_mask = batch["text_mask"].cuda()   # í…ìŠ¤íŠ¸ ë§ˆìŠ¤í¬ GPUë¡œ ì´ë™
  ```

### **7.3.2 Training Process - Sequence Length**
- **Source**: `RoboVLMs/robovlms/train/base_trainer.py:349`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ì‹œí€€ìŠ¤ ê¸¸ì´
- **Sequence Length Formula**: `seq_len = window_size + chunk_size`  # ì‹œí€€ìŠ¤ ê¸¸ì´ = ìœˆë„ìš° í¬ê¸° + ì²­í¬ í¬ê¸°
- **Example**: `window_size=16, fwd_pred_next_n=2` â†’ `seq_len=18`  # ì˜ˆì‹œ: ìœˆë„ìš° 16, ìˆœë°©í–¥ ì˜ˆì¸¡ 2 â†’ ì‹œí€€ìŠ¤ ê¸¸ì´ 18

### **7.3.3 Training Process - Data Chunking**
- **Source**: `RoboVLMs/robovlms/data/data_utils.py:249-270`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ë°ì´í„° ì²­í‚¹
- **Chunk Generation**:  # ì²­í¬ ìƒì„±
  ```python
  def generate_chunck_data(data, window_size, chunk_size):
      """ë°ì´í„° ì²­í‚¹ ìƒì„± í•¨ìˆ˜"""
      bs, seq_len = data.shape[:2]                    # ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´
      assert seq_len == window_size + chunk_size      # ì‹œí€€ìŠ¤ ê¸¸ì´ = ìœˆë„ìš° í¬ê¸° + ì²­í¬ í¬ê¸°
      data_flatten = repeat(data_flatten, "b s d -> b w s d", w=window_size)  # ìœˆë„ìš° í¬ê¸°ë§Œí¼ ë°˜ë³µ
      mask = claw_matrix(seq_len, chunk_size - 1, data_flatten.device)        # í´ë¡œ ë§¤íŠ¸ë¦­ìŠ¤ ë§ˆìŠ¤í¬
      mask = mask[:window_size].bool()                 # ìœˆë„ìš° í¬ê¸°ë§Œí¼ ë§ˆìŠ¤í¬ ìë¥´ê¸°
      data_flatten = data_flatten.view(bs, window_size, chunk_size, *raw_data_shape)  # ìµœì¢… ë°ì´í„° í˜•íƒœ
      return data_flatten
  ```

### **7.3.4 Training Process - Mobile VLA Example**
- **Source**: `RoboVLMs/robovlms/data/mobile_vla_action_dataset.py:223-238`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ Mobile VLA ì˜ˆì‹œ
- **Length Consistency**:  # ê¸¸ì´ ì¼ê´€ì„±
  ```python
  # ê¸¸ì´ ì •í•©ì„± ë³´ì¥: images=18(window+fwd), actions=17(window+fwd-1) í•„ìš”
  target_img_len = self.window_size + self.fwd_pred_next_n  # 16 + 2 = 18
  if images.shape[0] > target_img_len:
      images = images[:target_img_len]                      # ì´ë¯¸ì§€ ê¸¸ì´ ìë¥´ê¸°
  elif images.shape[0] < target_img_len:
      pad = target_img_len - images.shape[0]               # íŒ¨ë”© ê³„ì‚°
      last = images[-1:]                                    # ë§ˆì§€ë§‰ ì´ë¯¸ì§€
      images = np.concatenate([images, np.repeat(last, pad, axis=0)], axis=0)  # íŒ¨ë”© ì¶”ê°€
  
  # actionsê°€ 18ì´ë©´ ë§ˆì§€ë§‰ 1ê°œë¥¼ ì œê±°í•´ ìœˆë„ìš° ê·œì¹™(window=16, fwd=2)ì— ë§ì¶¤
  if actions.shape[0] > 17:
      actions = actions[:17]                               # ì•¡ì…˜ ê¸¸ì´ ìë¥´ê¸°
  ```

### **7.3.5 Training Process - Forward Pass**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:910-928`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ìˆœì „íŒŒ
- **Forward Pass**:  # ìˆœì „íŒŒ
  ```python
  def forward_discrete(self, vision_x, lang_x, ...):
      """ì´ì‚° ì•¡ì…˜ ìˆœì „íŒŒ"""
      assert vision_x is not None
      bs, window_size = vision_x.shape[:2]  # ë°°ì¹˜ í¬ê¸°, ìœˆë„ìš° í¬ê¸°
      
      # 2ì°¨ì›ì¸ ê²½ìš° ìœˆë„ìš° í¬ê¸°ë§Œí¼ ë°˜ë³µ
      if instr_and_action_ids.ndim == 2:
          instr_and_action_ids = instr_and_action_ids.unsqueeze(1).repeat(1, window_size, 1)
          instr_and_action_labels = instr_and_action_labels.unsqueeze(1).repeat(1, window_size, 1)
          instr_and_action_mask = instr_and_action_mask.unsqueeze(1).repeat(1, window_size, 1)
      
      # ì°¨ì› í‰íƒ„í™”
      instr_and_action_ids = instr_and_action_ids.flatten(0, 1)
      vision_x = vision_x.flatten(0, 1)  # (bs * window_size, ...)
  ```

### **7.3.6 Inference Process - Single Image Sequential Processing**
- **Source**: `RoboVLMs/vla_test/standalone_vla_test.py:87-124`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì²˜ë¦¬
- **Sequential Single Image Inference**:  # ìˆœì°¨ì  ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ 
  ```python
  def infer_from_image_and_text(self, image: np.ndarray, text_prompt: str) -> str:
      """ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¡œë¶€í„° VLA ì¶”ë¡  ìˆ˜í–‰ (ë‹¨ì¼ ì´ë¯¸ì§€ ìˆœì°¨ ì²˜ë¦¬)"""
      # ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (í•œ ë²ˆì— í•˜ë‚˜ì”©)
      if len(image.shape) == 3 and image.shape[2] == 3:
          rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB ë³€í™˜
      else:
          rgb_image = image
      
      pil_image = PilImage.fromarray(rgb_image)  # ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
      
      # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ (ë‹¨ì¼ ì´ë¯¸ì§€)
      inputs = self.processor(
          images=pil_image,      # ë‹¨ì¼ ì´ë¯¸ì§€
          text=text_prompt,      # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
          return_tensors="pt"    # PyTorch í…ì„œë¡œ ë°˜í™˜
      ).to(self.device)
      
      # ì¶”ë¡  ì‹¤í–‰ (ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•´)
      with torch.no_grad():
          outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens, do_sample=False)
          result = self.processor.decode(outputs[0], skip_special_tokens=True)
  ```

### **7.3.6.1 Inference Process - Step Function Sequential Processing**
- **Source**: `RoboVLMs/eval/calvin/model_wrapper.py:318-378`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ Step í•¨ìˆ˜ ìˆœì°¨ ì²˜ë¦¬
- **Sequential Step Processing**:  # ìˆœì°¨ì  Step ì²˜ë¦¬
  ```python
  def step(self, obs, goal):
      """Step function - í•œ ë²ˆì— í•˜ë‚˜ì˜ ê´€ì°° ì²˜ë¦¬"""
      input_dict = dict()
      image_x, gripper_x, text_x, mask = self.preprocess(obs, goal, self.action_space)
      
      input_dict["rgb"] = image_x  # ë‹¨ì¼ ì´ë¯¸ì§€
      input_dict["hand_rgb"] = gripper_x  # ë‹¨ì¼ ê·¸ë¦¬í¼ ì´ë¯¸ì§€
      input_dict["text"] = text_x  # ë‹¨ì¼ í…ìŠ¤íŠ¸
      input_dict["text_mask"] = mask  # ë‹¨ì¼ ë§ˆìŠ¤í¬
      
      # ë‹¨ì¼ ê´€ì°°ì— ëŒ€í•œ ì¶”ë¡ 
      with torch.no_grad():
          action = self.policy.inference_step(input_dict)["action"]
      
      # ì•¡ì…˜ í›„ì²˜ë¦¬ (ë‹¨ì¼ ì•¡ì…˜)
      if self.action_space != "discrete":
          if action[0].ndim == action[1].ndim + 1:
              action = (action[0], action[1].unsqueeze(2))
          action = torch.cat([action[0], (torch.nn.functional.sigmoid(action[1]) > 0.5).float()], dim=-1)
      
      # ì•¡ì…˜ ì•™ìƒë¸” ì ìš© (ë‹¨ì¼ ì•¡ì…˜ì— ëŒ€í•´)
      action = self.ensemble_action(action)
      
      if isinstance(action, torch.Tensor):
          action = action.squeeze()
          if action.ndim == 2:
              action = action[0]  # ë‹¨ì¼ ì•¡ì…˜ ë°˜í™˜
  ```

### **7.3.6.2 Inference Process - VLA Node Sequential Processing**
- **Source**: `RoboVLMs/vla_node.py:258-281`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ VLA ë…¸ë“œ ìˆœì°¨ ì²˜ë¦¬
- **Sequential VLA Processing**:  # ìˆœì°¨ì  VLA ì²˜ë¦¬
  ```python
  def infer_and_parse(current_raw_image, current_prompt):
      """ë‹¨ì¼ ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ìˆœì°¨ ì²˜ë¦¬"""
      img_width, img_height = current_raw_image.size
      print(f"Input image size: ({img_width}, {img_height}) for prompt: '{current_prompt}'")
      
      # ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬
      inputs_data = processor(text=current_prompt, images=current_raw_image, return_tensors="pt").to(device)
      
      print("Performing inference...")
      with torch.inference_mode():  # ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ 
          try:
              output_ids = model.generate(**inputs_data, max_new_tokens=max_new_tokens, do_sample=False)
              generated_text_output = processor.decode(output_ids[0], skip_special_tokens=True)
              
              # ë‹¨ì¼ ê²°ê³¼ íŒŒì‹±
              parsed_detections = parse_segmentation_output(generated_text_output, img_width, img_height, current_prompt)
  ```

### **7.3.6.3 Inference Process - Test Script Sequential Processing**
- **Source**: `RoboVLMs/test.py:144-177`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìˆœì°¨ ì²˜ë¦¬
- **Sequential Test Processing**:  # ìˆœì°¨ì  í…ŒìŠ¤íŠ¸ ì²˜ë¦¬
  ```python
  def inference(model, image, instruction, device="cpu"):
      """ë‹¨ì¼ ì´ë¯¸ì§€ì™€ ì§€ì‹œë¬¸ì— ëŒ€í•œ ìˆœì°¨ ì¶”ë¡ """
      logger.info("ì¸í¼ëŸ°ìŠ¤ ì‹œì‘...")
      start_time = time.time()
      
      try:
          # ë‹¨ì¼ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
          logger.info("ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘...")
          if isinstance(image, str):
              image = load_image(image)  # ë‹¨ì¼ ì´ë¯¸ì§€ ë¡œë“œ
          
          preprocessed_image = preprocess_image(image, model.configs["image_size"])
          preprocessed_image = preprocessed_image.to(device)
          
          # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
          logger.info("í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì¤‘...")
          encoded_text = model.encode_text(instruction)
          
          # ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ëª¨ë¸ ì¶”ë¡ 
          logger.info("ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰ ì¤‘...")
          with torch.no_grad():
              output = model.generate(
                  preprocessed_image,  # ë‹¨ì¼ ì´ë¯¸ì§€
                  encoded_text,  # ë‹¨ì¼ í…ìŠ¤íŠ¸
                  max_new_tokens=128, 
                  temperature=0.7
              )
  ```

### **7.3.7 Inference Process - Model Wrapper**
- **Source**: `RoboVLMs/eval/calvin/model_wrapper.py:28-39`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ëª¨ë¸ ë˜í¼
- **Model Wrapper**:  # ëª¨ë¸ ë˜í¼
  ```python
  class CustomModel:
      def __init__(
          self,
          ckpt_path,
          configs,
          device,
          save_dir=None,
          raw_calvin=True,
          debug=False,
          action_ensemble=False,
      ):
          self.model = BaseTrainer(configs=configs)
          self.init_config(ckpt_path, configs, device, save_dir, raw_calvin, debug)
  ```

### **7.3.8 Training vs Inference Process Comparison**
- **Source**: `RoboVLMs/robovlms/data/base_action_prediction_dataset.py:177-181`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ í›ˆë ¨ vs ì¶”ë¡  ë¹„êµ
- **Training Mode**:  # í›ˆë ¨ ëª¨ë“œ
  ```python
  if self.mode == "train":
      assert action.shape[0] == self.window_size + self.fwd_pred_next_n - 1
      window_size = self.window_size
  else:
      window_size = action.shape[0] + 1
  ```

### **7.3.9 Image Processing in Model**
- **Source**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:188-221`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ëª¨ë¸ì˜ ì´ë¯¸ì§€ ì²˜ë¦¬
- **Image Encoding**:  # ì´ë¯¸ì§€ ì¸ì½”ë”©
  ```python
  def encode_images(self, images, image_sizes=None):
      # input: images: list of b,c,h,w or b,t,c,h,w
      # output: image_features: b, t, n, d
      
      if images.ndim == 4:
          images = images.unsqueeze(1)  # (b, c, h, w) -> (b, 1, c, h, w)
      
      bs, seq_len = images.shape[:2]  # ë°°ì¹˜ í¬ê¸°, ì‹œí€€ìŠ¤ ê¸¸ì´
      
      if type(images) is list or images.ndim == 5:
          if type(images) is list:
              images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]
          concat_images = torch.cat([image for image in images], dim=0)
          image_features = self.model_encode_images(concat_images)
      else:
          image_features = self.model_encode_images(images)
      
      image_features = torch.stack(image_features, dim=0).view(
          bs, seq_len, -1, image_features[0].shape[-1]
      )
  ```

### **7.3.10 Data Collation Process**
- **Source**: `RoboVLMs/robovlms/data/concat_dataset.py:110-122`  # GitHub ì½”ë“œì—ì„œ í™•ì¸ëœ ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •
- **Data Collation**:  # ë°ì´í„° ìˆ˜ì§‘
  ```python
  fwd_rgb_chunck = generate_chunck_data(
      image_tensors, self.window_size, self.fwd_pred_next_n
  )
  fwd_hand_rgb_chunck = generate_chunck_data(
      gripper_tensors, self.window_size, self.fwd_pred_next_n
  )
  chunck_mask = generate_chunck_data(
      image_mask, self.window_size, self.fwd_pred_next_n
  )
  action_chunck = generate_chunck_data(
      action_tensors, self.window_size, self.fwd_pred_next_n
  )
  ```

## ğŸ¯ **Key Findings**

### **7.3.11 Training vs Inference Process Summary**
1. **Training Process**:  # í›ˆë ¨ ê³¼ì •
   - **Batch Processing**: Processes multiple sequences simultaneously  # ì—¬ëŸ¬ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬
   - **Sequence Length**: `window_size + fwd_pred_next_n` (e.g., 16 + 2 = 18)  # ì‹œí€€ìŠ¤ ê¸¸ì´: ìœˆë„ìš° í¬ê¸° + ìˆœë°©í–¥ ì˜ˆì¸¡ ìŠ¤í…
   - **Data Chunking**: Uses sliding window approach for temporal context  # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ‘ê·¼ë²• ì‚¬ìš©

2. **Inference Process**:  # ì¶”ë¡  ê³¼ì •
   - **Single Image Sequential Processing**: Processes one image at a time sequentially  # í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
   - **Step-by-Step**: Each step processes single observation  # ê° ë‹¨ê³„ë§ˆë‹¤ ë‹¨ì¼ ê´€ì°° ì²˜ë¦¬
   - **Real-time**: Suitable for real-time robot control  # ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ì— ì í•©
   - **Sequential**: Processes images one by one in sequence  # ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬

### **7.3.12 Sequential Processing Evidence Summary**
- **Source**: `RoboVLMs/vla_test/standalone_vla_test.py:87-124`  # Standalone VLA ìˆœì°¨ ì²˜ë¦¬
- **Source**: `RoboVLMs/eval/calvin/model_wrapper.py:318-378`  # CALVIN Step í•¨ìˆ˜ ìˆœì°¨ ì²˜ë¦¬
- **Source**: `RoboVLMs/vla_node.py:258-281`  # VLA ë…¸ë“œ ìˆœì°¨ ì²˜ë¦¬
- **Source**: `RoboVLMs/test.py:144-177`  # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìˆœì°¨ ì²˜ë¦¬

### **7.3.13 Sequential Processing Technical Details**
1. **Single Image Input**: All inference functions take single image as input  # ëª¨ë“  ì¶”ë¡  í•¨ìˆ˜ëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
2. **One-by-One Processing**: Each function processes one image at a time  # ê° í•¨ìˆ˜ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€ ì²˜ë¦¬
3. **Sequential Execution**: Images are processed in sequence, not in batches  # ì´ë¯¸ì§€ëŠ” ë°°ì¹˜ê°€ ì•„ë‹Œ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬
4. **Real-time Capability**: Designed for real-time robot control  # ì‹¤ì‹œê°„ ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•´ ì„¤ê³„ë¨

### **7.3.12 Technical Implementation Details**
- **Window Size**: Controls historical context length  # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œì–´
- **Forward Prediction**: Predicts multiple future actions  # ì—¬ëŸ¬ ë¯¸ë˜ ì•¡ì…˜ ì˜ˆì¸¡
- **Data Structure**: `(batch_size, window_size, channels, height, width)`  # ë°ì´í„° êµ¬ì¡°
- **Memory Management**: Efficient processing of long sequences  # ê¸´ ì‹œí€€ìŠ¤ì˜ íš¨ìœ¨ì  ì²˜ë¦¬

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/train/base_trainer.py` (L345-395)  # ê¸°ë³¸ íŠ¸ë ˆì´ë„ˆ
- `RoboVLMs/robovlms/data/data_utils.py` (L249-270)  # ë°ì´í„° ìœ í‹¸ë¦¬í‹°
- `RoboVLMs/robovlms/data/mobile_vla_action_dataset.py` (L223-238)  # Mobile VLA ì•¡ì…˜ ë°ì´í„°ì…‹
- `RoboVLMs/robovlms/model/backbone/base_backbone.py` (L188-221, 910-928)  # ê¸°ë³¸ ë°±ë³¸ ëª¨ë¸
- `RoboVLMs/vla_test/standalone_vla_test.py` (L87-124)  # ë…ë¦½ VLA í…ŒìŠ¤íŠ¸ - ìˆœì°¨ ì²˜ë¦¬
- `RoboVLMs/eval/calvin/model_wrapper.py` (L28-39, 318-378)  # CALVIN ëª¨ë¸ ë˜í¼ - Step í•¨ìˆ˜ ìˆœì°¨ ì²˜ë¦¬
- `RoboVLMs/vla_node.py` (L258-281)  # VLA ë…¸ë“œ - ìˆœì°¨ ì²˜ë¦¬
- `RoboVLMs/test.py` (L144-177)  # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ìˆœì°¨ ì²˜ë¦¬
- `RoboVLMs/robovlms/data/concat_dataset.py` (L110-122)  # ì—°ê²° ë°ì´í„°ì…‹
