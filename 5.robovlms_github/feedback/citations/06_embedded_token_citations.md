# 6. Embedded Token Synchronization - Citation Evidence

## ğŸ” **GitHub Code Implementation (100% Confirmed from @RoboVLMs)**

### **6.1 Action Tokenizer Implementation**
- **File**: `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py:14-95` (Updated from @RoboVLMs)
- **Implementation**: `ActionTokenizer` class for discrete action tokenization
- **Code**:
```python
class ActionTokenizer:
    """ì—°ì† ì•¡ì…˜ì„ ì´ì‚° í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í† í¬ë‚˜ì´ì €"""
    def __init__(
        self,
        tokenizer: PreTrainedTokenizerBase,  # ê¸°ë³¸ í† í¬ë‚˜ì´ì €
        bins: int = 256,                     # ì´ì‚°í™” ë¹ˆ ìˆ˜
        min_action: int = -1,               # ì•¡ì…˜ ìµœì†Œê°’
        max_action: int = 1,                # ì•¡ì…˜ ìµœëŒ€ê°’
        add_action_end_flag=False,          # ì•¡ì…˜ ë í”Œë˜ê·¸ ì¶”ê°€ ì—¬ë¶€
    ) -> None:
        """
        ì—°ì† ë¡œë´‡ ì•¡ì…˜ì„ ì°¨ì›ë‹¹ Nê°œ ë¹ˆìœ¼ë¡œ ì´ì‚°í™”í•˜ê³  ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í† í°ì— ë§¤í•‘
        """
        # ê¸°ë³¸ ì„¤ì • ì €ì¥
        self.tokenizer, self.n_bins, self.min_action, self.max_action = (
            tokenizer, bins, min_action, max_action,
        )
        
        # ê· ë“± ë¹ˆ ìƒì„± ë° ë¹ˆ ì¤‘ì‹¬ê°’ ê³„ì‚°
        self.bins = np.linspace(min_action, max_action, self.n_bins)
        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0
        
        # ì•¡ì…˜ í† í° ì¸ë±ìŠ¤ ì„¤ì •
        self.action_token_begin_idx: int = int(
            self.tokenizer_orig_size - (self.n_bins + 1)  # ì•¡ì…˜ í† í° ì‹œì‘ ì¸ë±ìŠ¤
        )
        self.action_token_end_idx = self.tokenizer_orig_size  # ì•¡ì…˜ í† í° ë ì¸ë±ìŠ¤
```

### **6.2 Action Token Encoding**
- **File**: `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py:82-95` (Updated from @RoboVLMs)
- **Implementation**: `encode_actions_to_token_ids()` and `decode_token_ids_to_actions()` functions
- **Code**:
```python
def encode_actions_to_token_ids(self, action: np.ndarray) -> np.ndarray:
    """ì—°ì† ì•¡ì…˜ì„ í† í° IDë¡œ ì¸ì½”ë”©"""
    # ì•¡ì…˜ì„ ì§€ì •ëœ ë²”ìœ„ë¡œ í´ë¦¬í•‘
    action = np.clip(
        action, a_min=float(self.min_action), a_max=float(self.max_action)
    )
    # ì•¡ì…˜ì„ ë¹ˆìœ¼ë¡œ ì´ì‚°í™”
    discretized_action = np.digitize(action, self.bins)
    
    # 1ì°¨ì›ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if len(discretized_action.shape) == 1:
        return list(self.tokenizer_orig_size - discretized_action)
    else:
        # ë‹¤ì°¨ì›ì¸ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜
        return np.array(self.tokenizer_orig_size - discretized_action).tolist()

def decode_token_ids_to_actions(self, action_token_ids: np.ndarray) -> np.ndarray:
    """í† í° IDë¥¼ ì—°ì† ì•¡ì…˜ìœ¼ë¡œ ë””ì½”ë”©"""
    # í† í° IDë¥¼ ì—°ì† ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” êµ¬í˜„ ì„¸ë¶€ì‚¬í•­
```

### **6.3 Discrete Action Decoder**
- **File**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:173-227` (Updated from @RoboVLMs)
- **Implementation**: `DiscreteDecoder` class with ActionTokenizer integration
- **Code**:
```python
class DiscreteDecoder(BasePolicyHead):
    """ì´ì‚° ì•¡ì…˜ ë””ì½”ë” (ActionTokenizer í†µí•©)"""
    def __init__(
        self,
        hidden_size,              # íˆë“  ìƒíƒœ í¬ê¸°
        action_dim,               # ì•¡ì…˜ ì°¨ì› (7 DOF)
        action_space="continuous", # ì•¡ì…˜ ê³µê°„ íƒ€ì…
        n_bin=256,                # ì´ì‚°í™” ë¹ˆ ìˆ˜
        min_action=-1,            # ì•¡ì…˜ ìµœì†Œê°’
        max_action=1,             # ì•¡ì…˜ ìµœëŒ€ê°’
        tokenizer=None,           # í† í¬ë‚˜ì´ì €
        **kwargs,
    ):
        super().__init__(hidden_size, action_dim, action_space, **kwargs)
        self.n_bin = n_bin                    # ì´ì‚°í™” ë¹ˆ ìˆ˜
        self.min_action = min_action          # ì•¡ì…˜ ìµœì†Œê°’
        self.max_action = max_action          # ì•¡ì…˜ ìµœëŒ€ê°’

        # ActionTokenizer import ë° ì´ˆê¸°í™”
        from robovlms.model.policy_head.action_tokenizer import ActionTokenizer

        self.action_tokenizer = ActionTokenizer(
            tokenizer,                    # í† í¬ë‚˜ì´ì €
            bins=self.n_bin,              # ë¹ˆ ìˆ˜
            min_action=self.min_action,   # ìµœì†Œ ì•¡ì…˜ê°’
            max_action=self.max_action,   # ìµœëŒ€ ì•¡ì…˜ê°’
        )
```

### **6.3 Action Token Integration in Multimodal Input**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1097-1121`
- **Implementation**: Action token insertion into multimodal embeddings
- **Code**:
```python
if action_space == "continuous":
    # ì—°ì† ì•¡ì…˜ ê³µê°„: EOS í† í° ì§ì „ì— action token ì‚½ì…
    insert_idx = multimodal_embeds.shape[1] - int(
        self.tokenizer.eos_token is not None  # EOS í† í° ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ì¡°ì •
    )
    
    # Learnable action tokenì„ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
    action_tokens = repeat(
        self.action_token,
        "d -> b n d",
        b=multimodal_embeds.shape[0],
        n=self.latent_num,
    )
    
    # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì— action token í†µí•©
    (
        multimodal_embeds,
        mutlimodal_labels,
        multimodal_attention_mask,
        action_token_mask,
    ) = self.merge_multi_modal_input(
        multimodal_embeds,
        action_tokens,
        mutlimodal_labels,
        multimodal_attention_mask,
        is_image=False,
        insert_idx=insert_idx,
        fill_zero=self.act_head_configs.get("fill_zero", False),
    )
```

### **6.4 Multimodal Input Merging Function**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:323-375`
- **Implementation**: `merge_multi_modal_input` function for token integration
- **Code**:
```python
def merge_multi_modal_input(
    self,
    input_embeds: torch.Tensor,
    multimodal_feats: torch.Tensor = None,
    labels: torch.Tensor = None,
    attention_mask: torch.Tensor = None,
    is_image=True,
    insert_idx=1,
    fill_zero=False,
):
    # Action tokenì˜ ê²½ìš° is_image=Falseë¡œ ì²˜ë¦¬
    if is_image:
        rgb_feats = self.encode_images(multimodal_feats)
        # ì´ë¯¸ì§€ í† í° ì²˜ë¦¬ ë¡œì§
    else:
        rgb_feats = multimodal_feats  # Action token ì§ì ‘ ì‚¬ìš©
    
    added_seq_len = rgb_feats.shape[1]
    
    # ì…ë ¥ ì„ë² ë”©ì— ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• í†µí•©
    multimodal_embeds = torch.cat(
        [input_embeds[:, :insert_idx], rgb_feats, input_embeds[:, insert_idx:]],
        dim=1,
    )
```

### **6.5 Discrete Action Prediction**
- **File**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1384-1452`
- **Implementation**: Token-based action prediction for discrete actions
- **Code**:
```python
def pred_action_discrete(
    self, instr_and_action_ids, vision_x, vision_gripper=None, attention_mask=None
):
    action_dim = self.act_head_configs["action_dim"]
    generated_ids = []
    kv_cache = None
    
    # ì•¡ì…˜ ì°¨ì› Ã— ë¯¸ë˜ ì˜ˆì¸¡ ìŠ¤í…ë§Œí¼ í† í° ìƒì„±
    for i in range(action_dim * self.fwd_pred_next_n):
        output_hs = self.model(
            inputs_embeds=multimodal_embeds,
            past_key_values=kv_cache,
            use_cache=True,
        )
        kv_cache = output_hs.past_key_values
        cur_id = output_hs.logits[:, -1].argmax(dim=-1)
        generated_ids.append(cur_id)
    
    # ìƒì„±ëœ í† í° IDë¥¼ ì—°ì† ì•¡ì…˜ìœ¼ë¡œ ë””ì½”ë”©
    predicted_action_ids = generated_ids[:, -action_dim:].cpu().numpy()
    discretized_actions = self.action_tokenizer.decode_token_ids_to_actions(
        predicted_action_ids
    )
    
    # ê·¸ë¦¬í¼ ì•¡ì…˜ ì´ì§„í™” ì²˜ë¦¬
    discretized_actions[:, -1] = np.where(discretized_actions[:, -1] > 0, 1, -1)
    
    return discretized_actions
```

### **6.6 Action Token Processing in Data Pipeline**
- **File**: `RoboVLMs/robovlms/data/calvin_dataset.py:750-780`
- **Implementation**: Action token encoding in dataset
- **Code**:
```python
def wrap_instruction_and_action(self, lang, action, action_mask):
    # ì•¡ì…˜ì„ í† í° IDë¡œ ì¸ì½”ë”©
    action_ids = self.action_tokenizer.encode_actions_to_token_ids(action)
    
    # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    conversation = [
        {
            "from": "human",
            "value": f"What action should the robot take to {lang}?",
        },
        {"from": "gpt", "value": ""},
    ]
    
    # ì…ë ¥ IDì™€ ì•¡ì…˜ ID ê²°í•©
    input_ids = self.tokenizer(
        prompt_builder.get_prompt(), add_special_tokens=True
    ).input_ids
```

## ğŸ“Š **Token Synchronization Evidence**

### **6.7 Continuous vs Discrete Action Token Processing**

#### **Continuous Action Space (ì—°ì† ì•¡ì…˜ ê³µê°„)**
- **Learnable Token**: `nn.Parameter(torch.zeros(self.hidden_size))` ìƒì„±
- **Token Integration**: `merge_multi_modal_input()` í•¨ìˆ˜ë¡œ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì— í†µí•©
- **Processing**: ì§ì ‘ì ì¸ ì—°ì† ê°’ ì²˜ë¦¬, í† í°í™” ì—†ìŒ
- **Usage**: ì •ë°€í•œ ë¡œë´‡ ì œì–´, ì‹¤ì‹œê°„ ì œì–´

#### **Discrete Action Space (ì´ì‚° ì•¡ì…˜ ê³µê°„)**
- **Action Tokenizer**: 256ê°œ ë¹ˆìœ¼ë¡œ ì—°ì† ì•¡ì…˜ì„ ì´ì‚°í™”
- **Token Encoding**: `encode_actions_to_token_ids()` í•¨ìˆ˜ë¡œ í† í° ID ë³€í™˜
- **Token Decoding**: `decode_token_ids_to_actions()` í•¨ìˆ˜ë¡œ ì—°ì† ì•¡ì…˜ ë³µì›
- **Usage**: ì–¸ì–´ ëª¨ë¸ í˜¸í™˜ì„±, ì‹œí€€ìŠ¤ ëª¨ë¸ë§

### **6.8 Multimodal Token Fusion Process**

#### **Token Integration Pipeline**
1. **Vision Tokens**: ì´ë¯¸ì§€ íŠ¹ì§•ì„ í† í°ìœ¼ë¡œ ë³€í™˜
2. **Language Tokens**: í…ìŠ¤íŠ¸ ëª…ë ¹ì„ í† í°ìœ¼ë¡œ ë³€í™˜
3. **Action Tokens**: ì•¡ì…˜ì„ í† í°ìœ¼ë¡œ ë³€í™˜ (ì—°ì†/ì´ì‚°)
4. **Fusion**: `merge_multi_modal_input()` í•¨ìˆ˜ë¡œ í†µí•©

#### **Token Synchronization Mechanism**
```python
# 1. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„±
multimodal_embeds = self.merge_multi_modal_input(
    vision_embeds, language_embeds, action_embeds
)

# 2. í†µí•©ëœ ì„ë² ë”©ìœ¼ë¡œ ëª¨ë¸ ì‹¤í–‰
output = self.model(
    inputs_embeds=multimodal_embeds,
    attention_mask=multimodal_attention_mask
)

# 3. ì•¡ì…˜ ì˜ˆì¸¡ ë° ë””ì½”ë”©
predicted_actions = self.decode_action_tokens(output.logits)
```

### **6.9 End-to-End Learning Process**

#### **Training Phase**
- **Joint Optimization**: Vision, Language, Action í† í° ë™ì‹œ í•™ìŠµ
- **Loss Calculation**: ë©€í‹°ëª¨ë‹¬ ì†ì‹¤ í•¨ìˆ˜ë¡œ í†µí•© í•™ìŠµ
- **Gradient Flow**: ëª¨ë“  í† í° íƒ€ì…ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì „íŒŒ

#### **Inference Phase**
- **Token Generation**: ì‹œí€€ìŠ¤ ëª¨ë¸ë¡œ ì•¡ì…˜ í† í° ìƒì„±
- **Action Decoding**: í† í°ì„ ì—°ì† ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
- **Robot Control**: ë³€í™˜ëœ ì•¡ì…˜ìœ¼ë¡œ ë¡œë´‡ ì œì–´

## ğŸ¯ **Key Findings**

### **6.10 Technical Innovations**

1. **Learnable Action Tokens**: 
   - ì—°ì† ì•¡ì…˜ ê³µê°„ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ ì•¡ì…˜ í† í° ìƒì„±
   - `nn.Parameter`ë¡œ êµ¬í˜„ëœ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

2. **Discrete Tokenization**: 
   - 256ê°œ ë¹ˆìœ¼ë¡œ ì—°ì† ì•¡ì…˜ì„ ì´ì‚°í™”
   - ì–¸ì–´ ëª¨ë¸ê³¼ì˜ í˜¸í™˜ì„± í™•ë³´

3. **Multimodal Fusion**: 
   - Vision, Language, Action í† í°ì˜ í†µí•© ì²˜ë¦¬
   - `merge_multi_modal_input()` í•¨ìˆ˜ë¡œ êµ¬í˜„

4. **End-to-End Learning**: 
   - ëª¨ë“  í† í° íƒ€ì…ì˜ ë™ì‹œ ìµœì í™”
   - ë©€í‹°ëª¨ë‹¬ ì†ì‹¤ í•¨ìˆ˜ í™œìš©

### **6.11 Implementation Details**

#### **Token Creation Process**
```python
# Continuous: Learnable parameter ìƒì„±
self.action_token = nn.Parameter(torch.zeros(self.hidden_size))

# Discrete: ActionTokenizerë¡œ í† í°í™”
action_tokenizer = ActionTokenizer(tokenizer, bins=256)
```

#### **Token Integration Process**
```python
# ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì— ì•¡ì…˜ í† í° í†µí•©
multimodal_embeds = self.merge_multi_modal_input(
    input_embeds, action_tokens, labels, attention_mask,
    is_image=False, insert_idx=insert_idx
)
```

#### **Token Processing Process**
```python
# ì´ì‚° ì•¡ì…˜ ì˜ˆì¸¡
predicted_actions = self.pred_action_discrete(
    instr_and_action_ids, vision_x, vision_gripper
)

# ì—°ì† ì•¡ì…˜ ì˜ˆì¸¡  
predicted_actions = self.forward_continuous(
    vision_x, lang_x, attention_mask
)
```

## ğŸ“ **Supporting Files**
- `RoboVLMs/robovlms/model/backbone/base_backbone.py` (L124-126, L323-375, L1097-1121, L1384-1452)
- `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py` (L14-58)
- `RoboVLMs/robovlms/data/calvin_dataset.py` (L750-780)
- `RoboVLMs/robovlms/model/policy_head/base_policy.py` (L173-227)
- `RoboVLMs/robovlms/data/base_action_prediction_dataset.py` (L141-226)
