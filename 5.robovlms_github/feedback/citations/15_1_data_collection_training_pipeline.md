# 15-1. VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ: ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” RoboVLMsì—ì„œ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° VLM Fine-tuningê³¼ LSTM Layer í•™ìŠµ ê³¼ì •ê¹Œì§€ë¥¼ ì¼ë°˜ì ì¸ AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë°©ì‹ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## âš™ï¸ 0. VLA êµ¬ì¡°ì™€ Action Space ì„¤ì •

### 0.1 VLA (Vision-Language-Action) ëª¨ë¸ ê°œìš”

**VLA ì •ì˜**:
```
at:t+Lâˆ’1 = VLA(otâˆ’H+1:t, lprompt)
```
- `at:t+Lâˆ’1`: ì˜ˆì¸¡ëœ 7-DOF ì•¡ì…˜ ì‹œí€€ìŠ¤ (Translation 3 + Rotation 3 + Gripper 1)
- `L`: ì•¡ì…˜ ì‹œí€€ìŠ¤ ê¸¸ì´ (action chunk size)
- `H`: íˆìŠ¤í† ë¦¬ ê´€ì¸¡ ê¸¸ì´ (window size)
- `ot`: í˜„ì¬ ì‹œê°„ tì˜ ê´€ì¸¡ê°’ (ì‹œê° ì •ë³´ + proprioceptive state)
- `lprompt`: ì–¸ì–´ í”„ë¡¬í”„íŠ¸

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section B, Equation (4)

---

### 0.2 VLA êµ¬ì¡° ë¶„ë¥˜ (4ê°€ì§€)

RoboVLMs ë…¼ë¬¸ì—ì„œëŠ” VLAë¥¼ **íˆìŠ¤í† ë¦¬ ì •ë³´ ëª¨ë¸ë§ ë°©ì‹**ê³¼ **ì•¡ì…˜ ê³µê°„**ì— ë”°ë¼ 4ê°€ì§€ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.

#### **ë¶„ë¥˜ ê¸°ì¤€**:
1. **íˆìŠ¤í† ë¦¬ ì •ë³´ ì²˜ë¦¬**: One-step vs Interleaved vs Policy-Head
2. **ì•¡ì…˜ ê³µê°„**: Continuous vs Discrete

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section C, Fig. 12

---

### 0.3 Action Space ê°œë…

**`action_space`ëŠ” ì„¤ì • íŒŒë¼ë¯¸í„°**ì…ë‹ˆë‹¤. `continuous`ì™€ `discrete`ëŠ” **ê°™ì€ ê³„ì¸µ(ì„ íƒì§€)ì— ìˆìœ¼ë©°**, ì‹¤ì œ ë¡œë´‡ íƒœìŠ¤í¬ì˜ íŠ¹ì„±ì— ë”°ë¼ ì„ íƒí•˜ëŠ” **ë‘ ê°€ì§€ ì•¡ì…˜ í‘œí˜„ ë°©ì‹**ì…ë‹ˆë‹¤.

```python
# Config íŒŒì¼ì—ì„œ action_space ì„¤ì •
{
    "act_head": {
        "type": "LSTMDecoder",          # Policy Head íƒ€ì…
        "action_space": "continuous",   # ì„ íƒ: "continuous" ë˜ëŠ” "discrete"
        "hidden_size": 1024,
        "action_dim": 7,
        "down_sample": "none"
    }
}
```

**ì¶œì²˜**: 
- `RoboVLMs/paligemma_config.json:33-44`
- `RoboVLMs/configs/k_project/ros2_automotive.json:40-61`

---

### 0.4 Action ì „ì²˜ë¦¬ ê³¼ì •

#### **0.4.1 Action Normalization (ëª¨ë“  ì•¡ì…˜ ê³µê°„ ê³µí†µ)**

**1ë‹¨ê³„: Quantile ê¸°ë°˜ Clipping**
```python
# 1stì™€ 99th percentile ê¸°ë°˜ clipping
aiâ€² = min(ai_99th, max(ai_1st, ai))
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (5)

**2ë‹¨ê³„: [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”**
```python
# ê° ì°¨ì›ì„ [-1, 1]ë¡œ ì •ê·œí™”
Ã£i = 2 Ã— (aiâ€² âˆ’ ai_1st) / (ai_99th âˆ’ ai_1st) âˆ’ 1

# ì •ê·œí™”ëœ ì•¡ì…˜
Ã£ = [Ã£1, Ã£2, ..., Ã£7]  # ê° ì°¨ì› âˆˆ [-1, 1]
# ë§ˆì§€ë§‰ ì°¨ì› (gripper): âˆˆ {-1, 1}
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (6)

**ì½”ë“œ êµ¬í˜„**:
```python
# CALVIN ë°ì´í„°ì…‹ ì •ê·œí™”
{
    "norm_action": true,
    "norm_min": -0.65,  # 1st percentile
    "norm_max": 0.65    # 99th percentile
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json:126-128`

---

#### **0.4.2 Action Discretization (Discrete ì•¡ì…˜ ê³µê°„ ì „ìš©)**

**256ê°œ Binìœ¼ë¡œ ê· ë“± ë¶„í• **:
```python
# ê° ì°¨ì›ì„ ë…ë¦½ì ìœ¼ë¡œ 256ê°œ binìœ¼ë¡œ ì´ì‚°í™”
# bin width = (ai_99th - ai_1st) / 256

# ì´ì‚°í™”ëœ ì•¡ì…˜: 7ê°œ ì •ìˆ˜
a_discrete = [bin_idx1, bin_idx2, ..., bin_idx7]  # ê°ê° âˆˆ [0...255]

# í† í° ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ offset ì¶”ê°€ (ê¸°ë³¸ 10)
token_id = vocab_size - offset - bin_idx
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ "Action Discretization" ì„¹ì…˜

---

### 0.5 ë‘ ë°©ì‹ì˜ ì—­í• ê³¼ ì°¨ì´

#### **Continuous Action Space** (ì—°ì† ì•¡ì…˜ ê³µê°„)
**ì—­í• **: ë¡œë´‡ì˜ ì•¡ì…˜ì„ **ì—°ì†ì ì¸ ì‹¤ìˆ˜ ê°’**ìœ¼ë¡œ ì§ì ‘ ì˜ˆì¸¡

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
[LRN] = VLM(ot, lprompt)
Ã¢t:t+Lâˆ’1 = MLP([LRN])  ë˜ëŠ”  h([LRN]tâˆ’H+1, ..., [LRN]t)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (10), (14)

```python
# BaseRoboVLM ì´ˆê¸°í™”
def __init__(self, ...):
    self.action_space = self.act_head_configs.get("action_space", "continuous")
    
    if self.action_space == "continuous":
        # í•™ìŠµ ê°€ëŠ¥í•œ ì•¡ì…˜ í† í° ìƒì„± (VLMì´ ì´ í† í°ì„ í†µí•´ ì•¡ì…˜ ì •ë³´ë¥¼ ìœµí•©)
        self.action_token = nn.Parameter(torch.zeros(self.hidden_size))
        self.action_token.requires_grad_(True)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:115-127`

**ì²˜ë¦¬ íë¦„**:
```python
# forward_action() - ì•¡ì…˜ ê³µê°„ì— ë”°ë¼ ë¶„ê¸°
def forward_action(self, vision_x, lang_x, ...):
    action_space = self.act_head_configs.get("action_space", "continuous")
    
    if action_space == "discrete":
        return self.forward_discrete(...)  # ì´ì‚° ì•¡ì…˜ ì²˜ë¦¬
    else:
        return self.forward_continuous(...)  # ì—°ì† ì•¡ì…˜ ì²˜ë¦¬ (ê¸°ë³¸ê°’)
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1344-1382`

**ì†ì‹¤ í•¨ìˆ˜ (ë…¼ë¬¸)**:
```python
# MSE Loss (ì²˜ìŒ 6ê°œ ì°¨ì›) + BCE Loss (gripper ì°¨ì›)
lVLA = Î£(MSE(Ã¢i,pose, Ã£i,pose) + Î» * BCE(Ã¢i,gripper, Ã£i,gripper))
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (7)

**íŠ¹ì§•**:
- **ì¶œë ¥ í˜•íƒœ**: `(batch_size, seq_len, action_dim)` - ì˜ˆ: `[0.5, -0.3, 0.1, ..., 0.8]`
- **ì†ì‹¤ í•¨ìˆ˜**: MSE Loss (pose 6ì°¨ì›) + BCE Loss (gripper 1ì°¨ì›)
- **ì¥ì **: ì •ë°€í•œ ì œì–´, ë¶€ë“œëŸ¬ìš´ ë™ì‘
- **ëŒ€í‘œ ëª¨ë¸**: ACT, BC-Z, MVP, R3M, VIMA, 3D Diffuser, RoboMamba, Ï€0
- **ì‚¬ìš© ì‚¬ë¡€**: ë¡œë´‡íŒ” ì •ë°€ ì¡°ì‘, ì—°ì† ê¶¤ì  ì œì–´

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ "One-step continuous-action models"

---

#### **Discrete Action Space** (ì´ì‚° ì•¡ì…˜ ê³µê°„)
**ì—­í• **: ì—°ì† ì•¡ì…˜ì„ **Nê°œ binìœ¼ë¡œ ì´ì‚°í™”**í•˜ì—¬ **í† í° ID**ë¡œ ì˜ˆì¸¡ (VLMì˜ next-token prediction ë°©ì‹ í™œìš©)

```python
# BaseRoboVLM ì´ˆê¸°í™”
def __init__(self, ...):
    self.action_space = self.act_head_configs.get("action_space", "continuous")
    
    if self.action_space == "discrete":
        # ActionTokenizer ìƒì„± (ì—°ì† ê°’ â†’ í† í° ID ë³€í™˜)
        self.action_tokenizer = ActionTokenizer(
            self.tokenizer,
            bins=self.act_head_configs["n_bin"],       # ê¸°ë³¸ 256ê°œ
            min_action=self.act_head_configs["min_action"],  # -1
            max_action=self.act_head_configs["max_action"],  # 1
        )
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:115-122`

**ActionTokenizerì˜ ì—­í• **:
```python
class ActionTokenizer:
    def __init__(self, tokenizer, bins=256, min_action=-1, max_action=1):
        """ì—°ì† ë¡œë´‡ ì•¡ì…˜ì„ Nê°œ binìœ¼ë¡œ ì´ì‚°í™”í•˜ê³  í† í° IDë¡œ ë§¤í•‘"""
        self.n_bins = bins
        self.bins = np.linspace(min_action, max_action, self.n_bins)  # ê· ë“± ë¶„í• 
        self.bin_centers = (self.bins[:-1] + self.bins[1:]) / 2.0
        
        # í† í° ID ë²”ìœ„ ì„¤ì • (vocabì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ì‚¬ìš©)
        self.tokenizer_orig_size = self.tokenizer.vocab_size - special_tokens - offset
    
    def encode_actions_to_token_ids(self, action: np.ndarray) -> np.ndarray:
        """ì—°ì† ì•¡ì…˜ â†’ í† í° ID ë³€í™˜"""
        discretized_action = np.digitize(action, self.bins)
        return self.tokenizer_orig_size - discretized_action
    
    def decode_token_ids_to_actions(self, action_token_ids: np.ndarray) -> np.ndarray:
        """í† í° ID â†’ ì—°ì† ì•¡ì…˜ ë³µì›"""
        discretized_actions = self.tokenizer_orig_size - action_token_ids
        discretized_actions = np.clip(discretized_actions - 1, 0, len(self.bin_centers) - 1)
        return self.bin_centers[discretized_actions]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/action_tokenizer.py:14-115`

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
[ACT]^1:7_t:t+Lâˆ’1 = VLM(ot, lprompt)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (11)

**ì†ì‹¤ í•¨ìˆ˜ (ë…¼ë¬¸)**:
```python
# Cross Entropy Loss (ê° ì°¨ì›ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°)
lVLA = Î£ Î£ CE([ACT]^j_i, Ã£^j_i)
      i  j
# i: ì‹œê°„ ì¸ë±ìŠ¤ (t:t+L-1)
# j: ì•¡ì…˜ ì°¨ì› ì¸ë±ìŠ¤ (1:7)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (8)

**ì¶”ë¡  ì‹œ De-tokenization**:
```python
# í† í° ID â†’ bin index â†’ ì—°ì† ê°’ (bin center)
predicted_action = bin_centers[token_id_to_bin_idx]
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ "Discrete Actions" ì„¹ì…˜

**íŠ¹ì§•**:
- **ì¶œë ¥ í˜•íƒœ**: í† í° ID ì‹œí€€ìŠ¤ - ì˜ˆ: `[32145, 32089, 32178, ..., 32200]`
- **ì†ì‹¤ í•¨ìˆ˜**: CrossEntropyLoss (ê° ì°¨ì›ë³„ í† í° ë¶„ë¥˜)
- **ì¥ì **: VLMì˜ ì–¸ì–´ ëª¨ë¸ë§ ëŠ¥ë ¥ í™œìš©, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **ëŒ€í‘œ ëª¨ë¸**: RT-1, RT-2, 3D-VLA, LAPA, OpenVLA, EmbodiedCOT
- **ì‚¬ìš© ì‚¬ë¡€**: ë³µì¡í•œ multi-modal ìœµí•©, ì–¸ì–´-ë¹„ì „-ì•¡ì…˜ í†µí•© í•™ìŠµ

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ "One-step discrete-action models"

---

### 0.6 VLA êµ¬ì¡°ë³„ ìƒì„¸ ë¶„ë¥˜

RoboVLMs ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ 4ê°€ì§€ VLA êµ¬ì¡°ë¥¼ **RoboVLMs ì½”ë“œë² ì´ìŠ¤ì™€ ëŒ€ì‘**í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.

---

#### **0.6.1 One-Step-Continuous-Action Models**

**íŠ¹ì§•**: 
- íˆìŠ¤í† ë¦¬ ê¸¸ì´ H = 1 (í˜„ì¬ ê´€ì¸¡ê°’ë§Œ ì‚¬ìš©)
- MLPë¡œ ì—°ì† ì•¡ì…˜ ì§ì ‘ ì˜ˆì¸¡

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
Ã¢t:t+Lâˆ’1 = VLA(ot, lprompt)
[LRN] = VLM(ot, lprompt)
Ã¢t:t+Lâˆ’1 = MLP([LRN])
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (9), (10)

**ëŒ€í‘œ ëª¨ë¸**: ACT, BC-Z, MVP, R3M, VIMA, 3D Diffuser, RoboMamba, Ï€0

**RoboVLMs êµ¬í˜„**: ì§€ì›í•˜ì§€ë§Œ ê¸°ë³¸ ì„¤ì • ì•„ë‹˜

---

#### **0.6.2 One-Step-Discrete-Action Models**

**íŠ¹ì§•**:
- íˆìŠ¤í† ë¦¬ ê¸¸ì´ H = 1
- VLMì˜ next-token predictionìœ¼ë¡œ ì•¡ì…˜ í† í° ìƒì„±

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
[ACT]^1:7_t:t+Lâˆ’1 = VLM(ot, lprompt)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (11)

**ëŒ€í‘œ ëª¨ë¸**: RT-1, RT-2, 3D-VLA, LAPA, **OpenVLA**, EmbodiedCOT

**RoboVLMs êµ¬í˜„**: `action_space: "discrete"` (ì½”ë“œì— ì¡´ì¬í•˜ì§€ë§Œ ì‹¤ì œ ì‚¬ìš© ì•ˆ í•¨)

---

#### **0.6.3 Interleaved-Continuous-Action Models**

**íŠ¹ì§•**:
- VLM ë°±ë³¸ **ë‚´ë¶€**ì—ì„œ íˆìŠ¤í† ë¦¬ ìœµí•©
- Decoder-only êµ¬ì¡°ì—ì„œë§Œ ê°€ëŠ¥
- ê´€ì¸¡-ì•¡ì…˜ í† í° interleaved í˜•ì‹

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
Ot = ([OBS]tâˆ’H+1, [LRN]), ..., ([OBS]t, [LRN])
[LRN]tâˆ’H+1:t = VLM(Ot)
Ã¢t:t+Lâˆ’1 = MLP([LRN]t)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (12)

**ëŒ€í‘œ ëª¨ë¸**: GR-1, OCTO, GR-2

**RoboVLMs êµ¬í˜„**: ì§€ì›í•˜ì§€ ì•ŠìŒ (Policy-Head ë°©ì‹ ì„ í˜¸)

---

#### **0.6.4 Policy-Head-Continuous-Action Models** â­ **RoboVLMsì˜ ì„ íƒ**

**íŠ¹ì§•**:
- VLMì€ ë‹¨ì¼ ì‹œê°„ ë‹¨ê³„ì˜ multi-modal representationë§Œ ì œê³µ
- **Policy Head (LSTM/RNN/Transformer/Diffusion)ê°€ íˆìŠ¤í† ë¦¬ ëª¨ë¸ë§ ë‹´ë‹¹**
- Encoder-Decoderì™€ Decoder-only êµ¬ì¡° ëª¨ë‘ ê°€ëŠ¥

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
ot = ([OBS]t, [LRN])
[LRN]t = VLM(ot, lprompt)
at:t+Lâˆ’1 = h([LRN]tâˆ’H+1, ..., [LRN]t)
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Equation (13), (14)

**ëŒ€í‘œ ëª¨ë¸**: **RoboFlamingo**, RoboUniview, DeeRVLA, **RoboVLMs (Kosmos, PaliGemma, LLaVA ë“±)**

**RoboVLMs êµ¬í˜„**:
```json
{
    "act_head": {
        "type": "LSTMDecoder",           // Policy Head = LSTM
        "action_space": "continuous",     // Continuous ì•¡ì…˜
        "with_history": true,             // íˆìŠ¤í† ë¦¬ ì‚¬ìš©
        "history_type": "post",           // LSTMì´ íˆìŠ¤í† ë¦¬ ëª¨ë¸ë§
        "window_size": 1                  // VLM ì…ë ¥ì€ ë‹¨ì¼ í”„ë ˆì„
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json:74-84`

**ì¥ì **:
1. **ëª¨ë“ˆì„±**: VLMê³¼ Policy Head ë…ë¦½ì  í•™ìŠµ
2. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ VLM ë°±ë³¸ (Encoder-Decoder/Decoder-only) ì‚¬ìš© ê°€ëŠ¥
3. **íš¨ìœ¨ì„±**: VLMì€ ë‹¨ì¼ í”„ë ˆì„ë§Œ ì²˜ë¦¬, LSTMì´ temporal reasoning ë‹´ë‹¹
4. **ì„±ëŠ¥**: CALVIN Avg. Len. **4.49** (ì „ì²´ 1ìœ„)

---

#### **0.6.5 VLA êµ¬ì¡° ë¹„êµí‘œ**

| **êµ¬ì¡°** | **íˆìŠ¤í† ë¦¬ ìœ„ì¹˜** | **ì•¡ì…˜ ê³µê°„** | **VLM êµ¬ì¡°** | **ëŒ€í‘œ ëª¨ë¸** | **RoboVLMs ì‚¬ìš©** |
|---------|-----------------|--------------|------------|-------------|-----------------|
| One-Step Continuous | ì—†ìŒ (H=1) | Continuous | Any | ACT, MVP | âŒ |
| One-Step Discrete | ì—†ìŒ (H=1) | Discrete | Any | RT-2, OpenVLA | âŒ |
| Interleaved Continuous | VLM ë‚´ë¶€ | Continuous | Decoder-only | GR-1, OCTO | âŒ |
| **Policy-Head Continuous** | **Policy Head** | **Continuous** | **Any** | **RoboVLMs, RoboFlamingo** | **âœ… (í‘œì¤€)** |

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Fig. 12, Section C

---

### 0.7 Policy Headì—ì„œì˜ ì²˜ë¦¬

```python
# DiscreteDecoder - action_spaceì— ë”°ë¼ í† í° ì‹œí€€ìŠ¤ ì²˜ë¦¬
class DiscreteDecoder(BasePolicyHead):
    def __init__(self, ..., action_space="continuous", ...):
        self.action_space = action_space  # continuous/discrete ë‘˜ ë‹¤ ì²˜ë¦¬ ê°€ëŠ¥
        
        # ActionTokenizerëŠ” discreteì¼ ë•Œë§Œ ì‚¬ìš©
        if action_space == "discrete":
            self.action_tokenizer = ActionTokenizer(tokenizer, bins=n_bin, ...)
    
    def process_token_sequence(self, tok_seq):
        """í† í° ì‹œí€€ìŠ¤ ì²˜ë¦¬ - action_spaceì— ë”°ë¼ ë¶„ê¸°"""
        if self.action_space == "continuous":
            # ì—°ì† ì•¡ì…˜: flatten dimension
            tok_seq = tok_seq.reshape(bs, seq_len, -1)
        
        elif self.action_space == "discrete":
            # ì´ì‚° ì•¡ì…˜: ê·¸ëŒ€ë¡œ pass (ì´ë¯¸ í† í° ID í˜•íƒœ)
            pass
        
        return tok_seq
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:173-221`

---

### 0.4 ì¶”ë¡  ì‹œ ë¶„ê¸° ì²˜ë¦¬

```python
# inference() - ì¶”ë¡  ì‹œì—ë„ action_spaceì— ë”°ë¼ ë¶„ê¸°
def inference(self, vision_x, lang_x, ...):
    prediction = {}
    action_space = self.act_head_configs.get("action_space", "continuous")
    
    if self.train_setup_configs["predict_action"]:
        if action_space == "discrete":
            # ì´ì‚° ì•¡ì…˜: autoregressive generationìœ¼ë¡œ í† í° ID ìƒì„±
            action = self.pred_action_discrete(lang_x, vision_x, ...)
            prediction["action"] = action
        else:
            # ì—°ì† ì•¡ì…˜: forward_continuousë¡œ ì§ì ‘ ì•¡ì…˜ ê°’ ì˜ˆì¸¡
            prediction["action"] = self.forward_continuous(
                vision_x, lang_x, ..., mode="inference"
            )
    
    return prediction
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1454-1491`

---

### 0.8 ë¹„êµ ìš”ì•½

#### **0.8.1 Continuous vs Discrete Action Space**

| **êµ¬ë¶„** | **Continuous** | **Discrete** |
|---------|----------------|--------------|
| **ê³„ì¸µ** | ê°™ì€ ê³„ì¸µ (ì„ íƒì§€) | ê°™ì€ ê³„ì¸µ (ì„ íƒì§€) |
| **ì„¤ì • ìœ„ì¹˜** | `act_head.action_space` | `act_head.action_space` |
| **ì¶œë ¥ í˜•íƒœ** | ì—°ì† ì‹¤ìˆ˜ ê°’ `[0.5, -0.3, ...]` | í† í° ID `[32145, 32089, ...]` |
| **ì†ì‹¤ í•¨ìˆ˜** | MSE + BCE (ë…¼ë¬¸ Eq. 7) | CrossEntropyLoss (ë…¼ë¬¸ Eq. 8) |
| **VLM ì—­í• ** | íŠ¹ì§• ì¶”ì¶œ + ì•¡ì…˜ í† í° ìœµí•© | íŠ¹ì§• ì¶”ì¶œ + next-token prediction |
| **Policy Head** | MLP, LSTM (íšŒê·€) | DiscreteDecoder (ë¶„ë¥˜) |
| **ì •ë°€ë„** | ì—°ì† ê°’ (ë§¤ìš° ë†’ìŒ) | bin í¬ê¸°ì— ë”°ë¦„ (256 bin â†’ 0.0078 ê°„ê²©) |
| **ë©”ëª¨ë¦¬** | ë§ìŒ | ì ìŒ (í† í° IDë§Œ ì €ì¥) |
| **ëŒ€í‘œ ëª¨ë¸** | ACT, RoboVLMs, OCTO | RT-2, OpenVLA |
| **ì‚¬ìš© ì‚¬ë¡€** | ì •ë°€ ì¡°ì‘, ì—°ì† ê¶¤ì  | VLM next-token í™œìš© |

---

#### **0.8.2 VLA êµ¬ì¡°ë³„ ë¹„êµ (ë…¼ë¬¸ ê¸°ì¤€)**

| **êµ¬ì¡°** | **íˆìŠ¤í† ë¦¬** | **ì•¡ì…˜** | **VLM êµ¬ì¡°** | **ì„±ëŠ¥** | **RoboVLMs** |
|---------|------------|---------|------------|---------|-------------|
| One-Step Cont. | H=1 | Continuous | Any | ë‚®ìŒ | âŒ |
| One-Step Disc. | H=1 | Discrete | Any | ì¤‘ê°„ | âŒ |
| Interleaved Cont. | VLM ë‚´ë¶€ | Continuous | Decoder-only | ì¤‘ìƒ | âŒ |
| **Policy-Head Cont.** | **Policy Head** | **Continuous** | **Any** | **ìµœê³  (4.49)** | **âœ…** |

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section C, README ì„±ëŠ¥ í‘œ

---

### 0.9 ì‹¤ì œ Config ì˜ˆì‹œ

**Continuous ì„¤ì •**:
```json
{
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",
        "action_dim": 7,
        "hidden_size": 1024,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/paligemma_config.json:33-44`

**Discrete ì„¤ì •** (ì‚¬ìš© ì‹œ):
```json
{
    "act_head": {
        "type": "DiscreteDecoder",
        "action_space": "discrete",
        "action_dim": 7,
        "n_bin": 256,
        "min_action": -1,
        "max_action": 1
    }
}
```
**ì¶œì²˜**: `RoboVLMs/README.md:253-268`

---

### 0.10 RoboVLMsì—ì„œ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” Action Space

#### **ê²°ë¡ : RoboVLMsëŠ” Policy-Head-Continuous-Action êµ¬ì¡°ë¥¼ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤**

**ì„ íƒ ì´ìœ **:
1. **VLM ìœ ì—°ì„±**: Encoder-Decoder (Kosmos, PaliGemma) / Decoder-only (LLaVA) ëª¨ë‘ ê°€ëŠ¥
2. **ëª¨ë“ˆ ë¶„ë¦¬**: VLM (íŠ¹ì§• ì¶”ì¶œ) + LSTM (íˆìŠ¤í† ë¦¬ + ì•¡ì…˜ ì˜ˆì¸¡) ë…ë¦½ í•™ìŠµ
3. **íš¨ìœ¨ì„±**: VLMì€ ë‹¨ì¼ í”„ë ˆì„ë§Œ ì²˜ë¦¬, LSTMì´ temporal reasoning
4. **ì„±ëŠ¥**: CALVIN Avg. Len. **4.49** (ì „ì²´ 1ìœ„)

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section C.3 "Policy-Head-Continuous-Action Models"

**ì „ì²´ Config íŒŒì¼ ë¶„ì„ ê²°ê³¼** (ì´ 13ê°œ ì„¤ì •):
- **`continuous`**: 11ê°œ (84.6%)
- **`down_sample`**: 2ê°œ (15.4%)
- **`discrete`**: 0ê°œ (0%)

**ì¶œì²˜**: `RoboVLMs/configs/` ì „ì²´ ê²€ìƒ‰ ê²°ê³¼

---

#### **0.7.1 Kosmos ëª¨ë¸ Config (CALVIN ìµœê³  ì„±ëŠ¥ ëª¨ë¸)**

**CALVIN Benchmark ì„±ëŠ¥**:
- **ABCD â†’ D**: 5-task Avg. Len. **4.49** (ì „ì²´ 1ìœ„)
- **ABC â†’ D**: 5-task Avg. Len. **4.25** (ì „ì²´ 1ìœ„)

**ì¶œì²˜**: `RoboVLMs/README.md:113-136`

**ëª¨ë“  Kosmos Configì—ì„œ `continuous` ì‚¬ìš©**:

```json
// 1. CALVIN Fine-tuning (ê¸°ë³¸)
{
    "robovlm_name": "RoboKosmos",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none",
        "with_history": true,
        "history_type": "post"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json:74-84`

```json
// 2. CALVIN Fine-tuning (Hand RGB ì‚¬ìš©)
{
    "robovlm_name": "RoboKosmos",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none",
        "window_size": 1
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_use-hand_ws-16_act-10.json:74-84`

```json
// 3. OXE Pretrain (Real-World ë°ì´í„°)
{
    "robovlm_name": "RoboKosmos",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/oxe_training/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_use-hand_ws-16_act-10_oxe_pretrain.json:74-84`

```json
// 4. RT/Bridge Fine-tuning (Real-World)
{
    "robovlm_name": "RoboKosmos",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7
    }
}
```
**ì¶œì²˜**: 
- `RoboVLMs/configs/oxe_training/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_use-hand_ws-16_act-10_rt_finetune.json:74-84`
- `RoboVLMs/configs/oxe_training/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_use-hand_ws-16_act-10_bridge_finetune.json:74-84`

```json
// 5. Mobile VLA (ì‹¤ì œ ë¡œë´‡ ë„¤ë¹„ê²Œì´ì…˜)
{
    "robovlm_name": "RoboKosmos",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none",
        "window_size": 16
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/oxe_training/finetune_kosmos_mobile_vla.json:66-77`

---

#### **0.7.2 ë‹¤ë¥¸ VLM ëª¨ë¸ë“¤ë„ ëª¨ë‘ `continuous` ì‚¬ìš©**

**PaliGemma**:
```json
{
    "robovlm_name": "RoboPaligemma",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_paligemma_cont-lstm-post_full-ft_text_vision_wd=0_ws-8_act-10.json:74-84`

**LLaVA**:
```json
{
    "robovlm_name": "RoboLLaVA",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_llava-mpt-7b_cont-lstm-post_ful_ft_wd=0_hist-8_act-10.json:78-85`

**Qwen-VL**:
```json
{
    "robovlm_name": "RoboQwen",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_qwen-vl-7b_cont-lstm-post_full_ft_text_vision_wd=0_ws-8_act-10.json:74-84`

**Moondream**:
```json
{
    "robovlm_name": "RoboMoondream",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "continuous",  // âœ… continuous ì‚¬ìš©
        "action_dim": 7,
        "down_sample": "none"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_moondream_cont-all-lstm-post_full-ft_text_vision_wd=0_ws-8_act-10.json:74-84`

---

#### **0.7.3 ì˜ˆì™¸: `down_sample` ì‚¬ìš© ì‚¬ë¡€ (2ê°œ)**

**Uform** (ê²½ëŸ‰ ëª¨ë¸):
```json
{
    "robovlm_name": "RoboUform",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "down_sample",  // âš ï¸ down_sample ì‚¬ìš© (ì˜ˆì™¸)
        "action_dim": 7,
        "down_sample": "pooling",       // pooling ì ìš©
        "token_source": "all"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_uform_cont-lstm-post_full-ft_text_vision_wd=0_ws-8_act-10.json:74-85`

**Flamingo**:
```json
{
    "robovlm_name": "RoboFlamingo",
    "act_head": {
        "type": "LSTMDecoder",
        "action_space": "down_sample",  // âš ï¸ down_sample ì‚¬ìš© (ì˜ˆì™¸)
        "action_dim": 7,
        "down_sample": "pooling"
    }
}
```
**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_flamingo_mpt_3b_ws-8_act-10_lstm_calvin.json:72-82`

---

#### **0.10.4 ì™œ RoboVLMsëŠ” Policy-Head-Continuousë¥¼ ì„ í˜¸í•˜ëŠ”ê°€?**

**1. êµ¬ì¡°ì  ì¥ì  (ë…¼ë¬¸ ê¸°ì¤€)**

**Policy-Head vs Interleaved**:
```
Policy-Head:
- VLM ì…ë ¥: ë‹¨ì¼ í”„ë ˆì„ ([OBS]t, [LRN])
- íˆìŠ¤í† ë¦¬ ìœµí•©: LSTMì—ì„œ ì²˜ë¦¬
- ì¥ì : VLMê³¼ Policy ë…ë¦½ì  ìµœì í™”

Interleaved:
- VLM ì…ë ¥: ì „ì²´ íˆìŠ¤í† ë¦¬ ([OBS]tâˆ’H+1, [LRN]), ..., ([OBS]t, [LRN])
- íˆìŠ¤í† ë¦¬ ìœµí•©: VLM ë‚´ë¶€ self-attention
- ë‹¨ì : Decoder-onlyë§Œ ê°€ëŠ¥, VLM ê³„ì‚°ëŸ‰ ì¦ê°€
```
**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section C.2, C.3

**2. VLM ë°±ë³¸ í˜¸í™˜ì„±**
- **Encoder-Decoder**: Kosmos, PaliGemma (Cross-attention í™œìš©) âœ…
- **Decoder-only**: LLaVA, Qwen-VL (Self-attention í™œìš©) âœ…
- InterleavedëŠ” Decoder-onlyë§Œ ê°€ëŠ¥ âŒ

**3. Continuousì˜ ì •ë°€ë„ ìš°ìœ„**
- ë¡œë´‡íŒ” ë¯¸ì„¸ ì›€ì§ì„ ì œì–´
- MSE + BCE Lossë¡œ ì§ê´€ì  í•™ìŠµ (ë…¼ë¬¸ Eq. 7)
- DiscreteëŠ” 256 bin ì œì•½ (ì •ë°€ë„ 0.0078 ê°„ê²©)

**4. ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ (CALVIN Benchmark)**
```
Policy-Head Continuous (RoboVLMs Kosmos): 4.49 â­ (1ìœ„)
Interleaved Continuous (GR-1):           4.21    (2ìœ„)
One-Step Discrete (OpenVLA ì¶”ì •):       ~3.5    (ì¶”ì •)
```
**ì¶œì²˜**: `RoboVLMs/README.md:113-136`

**5. `discrete`ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ **
- ì¶”ê°€ì ì¸ tokenization/de-tokenization overhead
- Bin ê°œìˆ˜ì— ë”°ë¥¸ ì •ë°€ë„ ì œí•œ
- VLMì˜ next-token predictionì€ **ì–¸ì–´ íƒœìŠ¤í¬**ì— ìµœì í™”
- ë¡œë´‡ ì¡°ì‘ì—ëŠ” **ì—°ì† ê°’ íšŒê·€**ê°€ ë” íš¨ê³¼ì 
- RoboVLMs ì‹¤í—˜: ëª¨ë“  configì—ì„œ discrete ë¯¸ì‚¬ìš© (0/13)

---

#### **0.10.5 ìš”ì•½: RoboVLMsì˜ ì„ íƒ**

| **ëª¨ë¸** | **VLA êµ¬ì¡°** | **Action Space** | **Down Sample** | **Policy Head** | **VLM êµ¬ì¡°** | **ì„±ëŠ¥** |
|---------|------------|-----------------|----------------|----------------|------------|---------|
| **Kosmos** (ì „ì²´) | Policy-Head | `continuous` | `none` | LSTMDecoder | Encoder-Decoder | **4.49 â­** |
| **PaliGemma** | Policy-Head | `continuous` | `none` | LSTMDecoder | Encoder-Decoder | ê³ ì„±ëŠ¥ |
| **LLaVA** | Policy-Head | `continuous` | `none` | LSTMDecoder | Decoder-only | ê³ ì„±ëŠ¥ |
| **Qwen-VL** | Policy-Head | `continuous` | `none` | LSTMDecoder | Decoder-only | ê³ ì„±ëŠ¥ |
| **Moondream** | Policy-Head | `continuous` | `none` | LSTMDecoder | Decoder-only | ê³ ì„±ëŠ¥ |
| **Uform** | Policy-Head | `down_sample` | `pooling` | LSTMDecoder | Decoder-only | ê²½ëŸ‰ |
| **Flamingo** | Policy-Head | `down_sample` | `pooling` | LSTMDecoder | Decoder-only | ê¸°ë³¸ |

**í•µì‹¬**: RoboVLMsëŠ” **Policy-Head-Continuous-Action êµ¬ì¡° (VLM + LSTM + MSE/BCE Loss)**ë¥¼ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ CALVIN ìµœê³  ì„±ëŠ¥(4.49)ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

**ë…¼ë¬¸ ê·¼ê±°**:
- Section C.3: "Policy-head-continuous-action models include RoboFlamingo, RoboUniview, and DeeRVLA"
- Equation (13), (14): Policy Headê°€ íˆìŠ¤í† ë¦¬ ëª¨ë¸ë§ ë‹´ë‹¹
- Fig. 12: 4ê°€ì§€ VLA êµ¬ì¡° ë¹„êµ

**ì¶œì²˜**: RoboVLMs ë…¼ë¬¸ Section C, README.md ì„±ëŠ¥ í‘œ

---

## ğŸ” 1. Real-World ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •

### 1.1 CALVIN ë°ì´í„°ì…‹ì˜ Real-World íŠ¹ì„±

**ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½**
```python
# CALVIN ë°ì´í„°ì…‹ì˜ ì‹¤ì œ ë¡œë´‡ í™˜ê²½
obs_config = DictConfig({
    "rgb_obs": ["rgb_static", "rgb_gripper"],    # ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼
    "depth_obs": [],                             # ê¹Šì´ ì •ë³´ (ì‚¬ìš© ì•ˆí•¨)
    "state_obs": ["robot_obs"],                  # ë¡œë´‡ ìƒíƒœ ì •ë³´
    "actions": ["rel_actions"],                    # ìƒëŒ€ì  ì•¡ì…˜
    "language": ["language"],                     # ì–¸ì–´ ëª…ë ¹
})
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:63-71`

**Real-World ë°ì´í„° êµ¬ì„±**
- **Franka Emika Panda 7-DOF ë¡œë´‡íŒ”**: ì‹¤ì œ ë¡œë´‡ í•˜ë“œì›¨ì–´
- **ë‹¤ì¤‘ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ**: ì •ì  ì¹´ë©”ë¼ + ê·¸ë¦¬í¼ ì¹´ë©”ë¼
- **ì‹¤ì œ ë¬¼ë¦¬ í™˜ê²½**: í…Œì´ë¸”, ë¬¼ì²´, ì¡°ì‘ ê³µê°„
- **ë‹¤ì–‘í•œ íƒœìŠ¤í¬**: pick-and-place, navigation, manipulation
- **ì‹¤ì œ ë¡œë´‡ ì¡°ì‘**: ì „ë¬¸ê°€ê°€ ì§ì ‘ ì¡°ì‘í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘

### 1.2 ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •

**ì´ë¯¸ì§€ ì „ì²˜ë¦¬**
```python
# CALVIN ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ ì²˜ë¦¬
def process_rgb(self, episode, observation_space, transforms, seq_idx=0, window_size=0):
    # RGB ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
    rgb_static = episode["rgb_static"]      # ì •ì  ì¹´ë©”ë¼ ì´ë¯¸ì§€
    rgb_gripper = episode["rgb_gripper"]    # ê·¸ë¦¬í¼ ì¹´ë©”ë¼ ì´ë¯¸ì§€
    
    # ì´ë¯¸ì§€ ì •ê·œí™” ë° ë¦¬ì‚¬ì´ì§•
    transforms = [
        Resize((224, 224)),                 # 224x224ë¡œ ë¦¬ì‚¬ì´ì§•
        RandomHorizontalFlip(p=0.1),        # ì œí•œì  ì¦ê°•
        ColorJitter(brightness=0.1, contrast=0.1),
        Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                 std=[0.26862954, 0.26130258, 0.27577711])
    ]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:236-243`

**ì•¡ì…˜ ì •ê·œí™”**
```python
# ì•¡ì…˜ ì •ê·œí™” ê³¼ì •
def collater(self, sample):
    if self.norm_action:
        for s in sample:
            s["actions"] = normalize_action(
                s["actions"], 
                self.norm_min,    # -1
                self.norm_max,    # 1
                maintain_last=True
            )
    
    # ê·¸ë¦¬í¼ ì•¡ì…˜ ì´ì§„í™”
    action_tensors[..., -1] = ((action_tensors[..., -1] + 1) // 2).float()
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:823-868`

## ğŸ¯ 2. VLM Fine-tuning ê³¼ì •

### 2.1 VLM ì•„í‚¤í…ì²˜ ì„ íƒ

**ì§€ì›ë˜ëŠ” VLM ëª¨ë¸ë“¤**
```python
# ë‹¤ì–‘í•œ VLM ë°±ë³¸ ì§€ì›
vlm_configs = {
    "PaliGemmaForConditionalGeneration": "paligemma-3b-pt-224",
    "RoboFlamingo": "flamingo-3b",
    "RoboKosmos": "kosmos-2",
    "RoboUform": "uform-vl-14b",
    "RoboPaligemma": "paligemma-3b-pt-224"
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:280-284`

### 2.2 Fine-tuning ì„¤ì •

**Full Fine-tuning (F-FT) ì„¤ì •**
```python
# Full Fine-tuning ì„¤ì •
train_setup = {
    "lora_enable": False,           # LoRA ë¹„í™œì„±í™”
    "freeze_backbone": False,       # ë°±ë³¸ ëª¨ë¸ ë™ê²° í•´ì œ
    "freeze_mm_mlp_adapter": False, # ë©€í‹°ëª¨ë‹¬ ì–´ëŒ‘í„° ë™ê²° í•´ì œ
    "train_vision": True,           # ë¹„ì „ ëª¨ë¸ í•™ìŠµ
    "train_text_embedding": True,   # í…ìŠ¤íŠ¸ ì„ë² ë”© í•™ìŠµ
    "precision": "bf16",            # BFloat16 ì •ë°€ë„
    "gradient_checkpointing": True  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
}
```

**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json:41-62`

**LoRA Fine-tuning ì„¤ì •**
```python
# LoRA Fine-tuning ì„¤ì •
train_setup = {
    "lora_enable": True,            # LoRA í™œì„±í™”
    "lora_r": 64,                   # LoRA rank
    "lora_alpha": 16,               # LoRA alpha
    "lora_dropout": 0.05,           # LoRA dropout
    "lora_bias": "none",            # LoRA bias
    "freeze_backbone": True,        # ë°±ë³¸ ëª¨ë¸ ë™ê²°
    "train_vision": False,          # ë¹„ì „ ëª¨ë¸ ë™ê²°
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:244-248`

### 2.3 VLM Fine-tuning ì½”ë“œ

**BaseRoboVLM ì´ˆê¸°í™”**
```python
class BaseRoboVLM(nn.Module):
    def __init__(
        self,
        configs,
        train_setup_configs,
        act_head_configs=None,
        window_size=None,
        **kwargs,
    ):
        super().__init__()
        
        # 1ë‹¨ê³„: VLM ë°±ë³¸ ì´ˆê¸°í™”
        self.model = AutoModelForCausalLM.from_pretrained(
            configs["pretrained_model_name_or_path"]
        )
        
        # 2ë‹¨ê³„: ë¹„ì „ íƒ€ì›Œ ì´ˆê¸°í™”
        self.vision_tower = self.model.vision_tower
        
        # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ íƒ€ì›Œ ì´ˆê¸°í™”
        self.text_tower = self.model.language_model
        
        # 4ë‹¨ê³„: ì•¡ì…˜ í—¤ë“œ ì´ˆê¸°í™”
        self.act_head = self._init_heads()
        
        # 5ë‹¨ê³„: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
        self._trainable_params_setup()
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:34-174`

**VLM Forward Pass**
```python
def forward(
    self,
    vision_x: torch.Tensor,
    lang_x: torch.Tensor,
    attention_mask: torch.Tensor = None,
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,
    action_mask: torch.Tensor = None,
    **kwargs,
):
    # 1ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ìœµí•©
    multimodal_embeds = self.merge_multi_modal_input(
        input_embeds=self.word_embedding(lang_x),
        vision_x=vision_x,
        attention_mask=attention_mask
    )
    
    # 2ë‹¨ê³„: VLM Forward Pass
    output = self.model(
        inputs_embeds=multimodal_embeds,
        attention_mask=attention_mask,
        output_hidden_states=True
    )
    
    # 3ë‹¨ê³„: ì•¡ì…˜ í—¤ë“œ Forward Pass
    action_loss = self._forward_action_head(
        action_tokens=output.hidden_states[-1],
        action_labels=action_labels,
        action_mask=action_mask
    )
    
    return action_loss
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/backbone/base_backbone.py:1261-1318`

## ğŸ§  3. LSTM Layer í•™ìŠµ ê³¼ì •

### 3.1 LSTM Decoder ì•„í‚¤í…ì²˜

**LSTMDecoder ì´ˆê¸°í™”**
```python
class LSTMDecoder(BasePolicyHead):
    def __init__(
        self,
        in_features=1024,
        hidden_size=1024,
        action_dim=7,
        num_layers=2,
        down_sample="none",
        window_size=1,
        fwd_pred_next_n=1,
        **kwargs,
    ):
        super().__init__()
        
        # 1ë‹¨ê³„: LSTM ì´ˆê¸°í™”
        self.rnn = nn.LSTM(
            input_size=in_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        
        # 2ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡ í—¤ë“œ
        self.actions = nn.Linear(hidden_size, (action_dim - 1) * fwd_pred_next_n)
        self.gripper = nn.Linear(hidden_size, fwd_pred_next_n)
        
        # 3ë‹¨ê³„: ë‹¤ìš´ìƒ˜í”Œë§ ì„¤ì •
        self.down_sample = down_sample
        if down_sample == "pooling":
            self.global_1d_pool = nn.AdaptiveAvgPool1d(1)
        
        # 4ë‹¨ê³„: íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        self.history_memory = []
        self.hidden_state = None
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:142-192`

### 3.2 LSTM Forward Pass

**LSTMDecoder.forward()**
```python
def forward(self, tok_seq, h_0=None, **kwargs):
    # 1ë‹¨ê³„: ë‹¤ìš´ìƒ˜í”Œë§ ì²˜ë¦¬
    if self.down_sample == "none":
        tok_seq = rearrange(tok_seq, "b l n d-> b l (n d)")
    elif self.down_sample == "pooling":
        tok_seq = self.global_1d_pool(tok_seq.permute(0, 2, 1))
    
    # 2ë‹¨ê³„: íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    if tok_seq.shape[1] == 1:
        self.history_memory.append(tok_seq)
        if len(self.history_memory) <= self.history_len:
            # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ë‚´ì—ì„œ LSTM ì²˜ë¦¬
            x, h_n = self.rnn(tok_seq, self.hidden_state)
            self.hidden_state = h_n
        else:
            # ìœˆë„ìš° ìŠ¬ë¼ì´ë”©
            for _ in range(len(self.history_memory) - self.history_len):
                self.history_memory.pop(0)
            hist_feature = torch.cat(self.history_memory, dim=1)
            self.hidden_state = None
            x, h_n = self.rnn(hist_feature, self.hidden_state)
    else:
        # ë°°ì¹˜ ì²˜ë¦¬
        x, h_n = self.rnn(tok_seq, h_0)
        self.hidden_state = h_n
    
    # 3ë‹¨ê³„: ì•¡ì…˜ ì˜ˆì¸¡
    actions = self.actions(x)      # íŒ” ì•¡ì…˜ (6-DOF)
    gripper = self.gripper(x)      # ê·¸ë¦¬í¼ ì•¡ì…˜ (1-DOF)
    
    # 4ë‹¨ê³„: ì¶œë ¥ í˜•íƒœ ì¡°ì •
    actions = rearrange(actions, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    gripper = rearrange(gripper, "b l (n d) -> b l n d", n=self.fwd_pred_next_n)
    
    return actions, gripper
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:223-224`

### 3.3 LSTM Loss ê³„ì‚°

**LSTMDecoder.loss()**
```python
def loss(self, pred_action_logits, labels, attention_mask=None):
    # 1ë‹¨ê³„: ë¼ë²¨ ë¶„ë¦¬
    arm_action_labels, gripper_action_labels = labels
    
    # 2ë‹¨ê³„: íŒ” ì•¡ì…˜ Loss ê³„ì‚° (MSE Loss)
    arm_action_pred = pred_action_logits[..., :-1]
    loss_arm = F.mse_loss(arm_action_pred, arm_action_labels)
    
    # 3ë‹¨ê³„: ê·¸ë¦¬í¼ ì•¡ì…˜ Loss ê³„ì‚° (BCE Loss)
    gripper_action_pred = pred_action_logits[..., -1]
    loss_gripper = F.binary_cross_entropy_with_logits(
        gripper_action_pred, 
        gripper_action_labels
    )
    
    # 4ë‹¨ê³„: ê·¸ë¦¬í¼ ì •í™•ë„ ê³„ì‚°
    gripper_discrete_pred = (gripper_action_pred > 0).float()
    gripper_acc = (gripper_discrete_pred == gripper_action_labels).float().mean()
    
    return {
        "loss_arm": loss_arm,
        "loss_gripper": loss_gripper,
        "acc_gripper": gripper_acc
    }
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:226-281`

## ğŸ”„ 4. ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸

### 4.1 í•™ìŠµ ë°ì´í„° ë¡œë”©

**DiskCalvinDataset - ë°ì´í„° ë¡œë”©**
```python
class DiskCalvinDataset(BaseCalvinDataset):
    def __getitem__(self, idx: Union[int, Tuple[int, int]]) -> Dict:
        # 1ë‹¨ê³„: ì—í”¼ì†Œë“œ ë¡œë”©
        if isinstance(idx, int):
            episode = self._load_episode(idx, self.window_size)
        
        # 2ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬
        image_seq = self.process_rgb(
            episode, 
            self.observation_space, 
            self.transforms
        )
        
        # 3ë‹¨ê³„: ì•¡ì…˜ ì²˜ë¦¬
        action_seq = episode["rel_actions"]
        
        # 4ë‹¨ê³„: ì–¸ì–´ ì²˜ë¦¬
        task_description = episode["language"]["ann"][0]
        
        return {
            "image": image_seq,
            "action": action_seq,
            "task": task_description,
            "episode_mask": episode_mask
        }
```

**ì¶œì²˜**: `RoboVLMs/robovlms/data/calvin_dataset.py:428-532`

### 4.2 í•™ìŠµ ìŠ¤í…

**BaseTrainer.training_step()**
```python
def training_step(self, batch, batch_idx):
    # 1ë‹¨ê³„: ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
    (rgb, hand_rgb, attention_mask, language, text_mask,
     arm_action, gripper_action, instr_and_action_ids,
     instr_and_action_labels, instr_and_action_mask) = self._process_batch(batch)
    
    # 2ë‹¨ê³„: ëª¨ë¸ Forward Pass
    prediction = self.model.forward(
        rgb,                    # ë¹„ì „ ì…ë ¥
        language,               # ì–¸ì–´ ì…ë ¥
        attention_mask=text_mask,
        action_labels=(arm_action, gripper_action),
        action_mask=chunk_mask,
        instr_and_action_ids=instr_and_action_ids,
        instr_and_action_labels=instr_and_action_labels,
        instr_and_action_mask=instr_and_action_mask
    )
    
    # 3ë‹¨ê³„: Loss ê³„ì‚°
    output = self._get_loss(prediction)
    
    return output
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:565-621`

### 4.3 Loss ê³„ì‚°

**BaseTrainer._get_loss()**
```python
def _get_loss(self, prediction):
    loss = {}
    total_loss = 0
    
    # 1ë‹¨ê³„: íŒ” ì•¡ì…˜ Loss
    if "loss_arm" in prediction:
        loss_arm = prediction["loss_arm"]
        loss["loss_arm"] = loss_arm
        total_loss += loss_arm
    
    # 2ë‹¨ê³„: ê·¸ë¦¬í¼ ì•¡ì…˜ Loss
    if "loss_gripper" in prediction:
        loss_gripper = prediction["loss_gripper"]
        loss["loss_gripper"] = loss_gripper
        total_loss += self.arm_gripper_loss_ratio * loss_gripper
    
    # 3ë‹¨ê³„: VL Co-training Loss (ì„ íƒì )
    if "loss_vl" in prediction:
        loss_vl = prediction["loss_vl"]
        loss["loss_vl"] = loss_vl
        total_loss += self.vl_cotrain_ratio * loss_vl
    
    # 4ë‹¨ê³„: ì´ Loss
    loss["loss"] = total_loss
    
    return loss
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:386-456`

### 4.4 Optimizer ì„¤ì •

**BaseTrainer.configure_optimizers()**
```python
def configure_optimizers(self):
    # 1ë‹¨ê³„: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê·¸ë£¹í™”
    params = self.get_grouped_params(self.model)
    
    # 2ë‹¨ê³„: Optimizer ì´ˆê¸°í™”
    if self.configs["optimizer"] == "adam":
        optimizer = torch.optim.Adam(
            params,
            lr=self.configs["learning_rate"],
            weight_decay=self.configs["weight_decay"]
        )
    elif self.configs["optimizer"] == "adamw":
        optimizer = torch.optim.AdamW(
            params,
            lr=self.configs["learning_rate"],
            weight_decay=self.configs["weight_decay"]
        )
    
    # 3ë‹¨ê³„: Learning Rate Scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=self.configs["max_epochs"],
        eta_min=self.configs["learning_rate"] * self.configs["min_lr_scale"]
    )
    
    return [optimizer], [scheduler]
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/base_trainer.py:716-722`

## ğŸ”§ 5. ì‹¤ì œ FT ì½”ë“œì™€ LSTM Layer í•™ìŠµ ì½”ë“œ

### 5.1 VLM Fine-tuning ì½”ë“œ

**BaseRoboVLM._trainable_params_setup() - íŒŒë¼ë¯¸í„° ë™ê²° ì„¤ì •**
```python
def _trainable_params_setup(self):
    model = self.model  # ë°±ë³¸ VLM ëª¨ë¸ (PaliGemma, Kosmos, LLaVA ë“±)
    
    # 1ë‹¨ê³„: ë°±ë³¸ ëª¨ë¸ ë™ê²° ì„¤ì •
    if self.train_setup_configs["freeze_backbone"]:
        model.requires_grad_(False)  # ì „ì²´ ëª¨ë¸ ë™ê²°
    else:
        if self.train_setup_configs.get("train_decoder_layers", -1) == -1:
            model.requires_grad_(True)  # ì „ì²´ ëª¨ë¸ í•™ìŠµ
        else:
            # ë§ˆì§€ë§‰ Nê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
            model.requires_grad_(False)
            for layer in self.text_tower.layers[-self.train_setup_configs["train_decoder_layers"]:]:
                layer.requires_grad_(True)
    
    # 2ë‹¨ê³„: ë¹„ì „ ì¸ì½”ë” ë™ê²° ì„¤ì •
    # vision_tower: VLMì˜ ë¹„ì „ ì¸ì½”ë” (CLIP, SigLIP ë“±)
    if self.train_setup_configs.get("train_vision", False):
        self.vision_tower.requires_grad_(True)
    else:
        self.vision_tower.requires_grad_(False)
    
    # 3ë‹¨ê³„: LoRA ì„¤ì •
    if self.train_setup_configs["lora_enable"]:
        # LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        pass
```

**vision_towerì™€ text_tower ì„¤ëª…**:
- **`vision_tower`**: VLMì˜ ë¹„ì „ ì¸ì½”ë” ë¶€ë¶„ (ì´ë¯¸ì§€ â†’ íŠ¹ì§• ë²¡í„°)
  - PaliGemma: `model.vision_tower` (SigLIP ê¸°ë°˜)
  - Kosmos: `model.vision_model` (CLIP ê¸°ë°˜)
  - LLaVA: `model.get_vision_tower()` (CLIP ê¸°ë°˜)
  - Flamingo: `self.vision_encoder` (CLIP ê¸°ë°˜)

- **`text_tower`**: VLMì˜ í…ìŠ¤íŠ¸/ì–¸ì–´ ëª¨ë¸ ë¶€ë¶„ (í…ìŠ¤íŠ¸ â†’ íŠ¹ì§• ë²¡í„°)
  - PaliGemma: `model.language_model.model` (Gemma Decoder)
  - Kosmos: `model.text_model.model` (Decoder-only Transformer)
  - LLaVA: `model.transformer` (GPT-style Transformer)
  - Flamingo: `self.model` (ì–¸ì–´ ëª¨ë¸ ì „ì²´)

**ë°±ë³¸ë³„ êµ¬í˜„ ì˜ˆì‹œ**:
```python
# RoboPaligemma (robopaligemma.py:19-24)
@property
def text_tower(self):
    return self.model.language_model.model  # Gemma Decoder

@property
def vision_tower(self):
    return self.model.vision_tower  # SigLIP

# RoboKosMos (robokosmos.py:16-21)
@property
def text_tower(self):
    return self.model.text_model.model  # Transformer Decoder

@property
def vision_tower(self):
    return self.model.vision_model  # CLIP Vision

# RoboLLaVA (robollava.py:19-24)
@property
def text_tower(self):
    return self.model.transformer  # GPT Transformer

@property
def vision_tower(self):
    return self.model.get_vision_tower()  # CLIP Vision
```

**Kosmos2Processor ê³µì‹ ë¬¸ì„œ ê·¼ê±°**:

Hugging Face ê³µì‹ ë¬¸ì„œì— ë”°ë¥´ë©´, Kosmos-2ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë©ë‹ˆë‹¤:

```python
class transformers.Kosmos2Processor(
    image_processor,  # CLIPImageProcessor
    tokenizer,        # XLMRobertaTokenizerFast
    num_patch_index_tokens = 1024,
    **kwargs
)
```

**Parameters**:
- **image_processor** (`CLIPImageProcessor`) â€” An instance of `CLIPImageProcessor`. The image processor is a required input.
- **tokenizer** (`XLMRobertaTokenizerFast`) â€” An instance of `['XLMRobertaTokenizerFast']`. The tokenizer is a required input.

> "Constructs an KOSMOS-2 processor which wraps a KOSMOS-2 image processor and a KOSMOS-2 tokenizer into a single processor."

> "Kosmos2Processor offers all the functionalities of **CLIPImageProcessor** and some functionalities of **XLMRobertaTokenizerFast**."

ì´ê²ƒì´ Kosmos-2ì˜ `vision_tower`ê°€ CLIP ê¸°ë°˜ì´ê³ , `text_tower`ê°€ XLM-Roberta ê¸°ë°˜ Transformerì¸ ì´ìœ ì…ë‹ˆë‹¤.

**ì¶œì²˜**: 
- [Hugging Face KOSMOS-2 Documentation](https://huggingface.co/docs/transformers/en/model_doc/kosmos-2)
- `RoboVLMs/robovlms/model/backbone/base_backbone.py:470-512`
- `RoboVLMs/robovlms/model/backbone/robopaligemma.py:19-24`
- `RoboVLMs/robovlms/model/backbone/robokosmos.py:16-21`
- `RoboVLMs/robovlms/model/backbone/robollava.py:19-24`
- `RoboVLMs/robovlms/model/backbone/roboflamingo.py:35-40`

### 5.2 LSTM Layer í•™ìŠµ ì½”ë“œ

**LSTM í•™ìŠµ ë£¨í”„ ì˜ˆì‹œ**
```python
# LSTM í•™ìŠµ ë£¨í”„ (base_policy.py:625-642)
net = LSTMDecoder(
    in_features=1024,
    action_dim=7,
    down_sample="pooling",
    latent=1,
    fwd_pred_next_n=2,
    window_size=12,
)

optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)
bs = 5
window_size = 12
text_len = 8
tokens = torch.randn(bs, window_size, text_len, 1024)
labels = (torch.randn(bs, window_size, 2, 6), torch.ones(bs, window_size, 2))
att_mask = torch.ones(bs, window_size, 2)

for i in range(10000):
    # Forward Pass
    actions, gripper = net(tokens)
    pred_action_logitss = torch.cat([actions, gripper.unsqueeze(-1)], dim=-1)
    
    # Loss ê³„ì‚°
    optimizer.zero_grad()
    loss = net.loss(pred_action_logitss, labels, att_mask)
    
    # Backward Pass
    loss_arm = loss["loss_arm"]
    loss_gripper = loss["loss_gripper"]
    acc_gripper = loss["acc_gripper"]
    loss_act = loss_arm + 0.01 * loss_gripper
    loss_act.backward()
    optimizer.step()
    
    print("iter: {}, loss: {} gripper: {} acc: {}".format(
        i, loss_act.item(), loss_gripper.item(), acc_gripper
    ))
```

**ì¶œì²˜**: `RoboVLMs/robovlms/model/policy_head/base_policy.py:625-642`

### 5.3 Loss ê³„ì‚° í•¨ìˆ˜

**calculate_vl_cross_entropy() - Vision-Language Cross Entropy**
```python
def calculate_vl_cross_entropy(logits, labels, mask=None):
    # 1ë‹¨ê³„: ì‹œí€€ìŠ¤ ì‹œí”„íŠ¸
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # 2ë‹¨ê³„: Loss ê³„ì‚°
    if mask is None:
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(
            shift_logits.view(-1, logits.shape[-1]),
            shift_labels.view(-1),
        )
    else:
        # ë§ˆìŠ¤í‚¹ëœ Loss ê³„ì‚°
        loss_fct = nn.CrossEntropyLoss(reduction="none")
        loss = loss_fct(
            shift_logits.view(-1, logits.shape[-1]),
            shift_labels.view(-1),
        )
        # ë§ˆìŠ¤í¬ ì ìš©
        mask = mask[..., 1:].contiguous()
        loss = loss * mask.reshape(-1)
        loss = loss.mean()
    
    return loss
```

**ì¶œì²˜**: `RoboVLMs/robovlms/train/loss.py:5-28`

### 5.4 ì„¤ì • íŒŒì¼ ì˜ˆì‹œ

**CALVIN Fine-tuning ì„¤ì •**
```json
{
  "train_setup": {
    "precision": "bf16",
    "predict_action": true,
    "predict_forward": false,
    "predict_caption": false,
    "train_vision": true,
    "freeze_backbone": false,
    "freeze_mm_mlp_adapter": false,
    "lora_enable": false,
    "train_text_embedding": true
  },
  "act_head": {
    "type": "LSTMDecoder",
    "hidden_size": 1024,
    "action_dim": 7,
    "down_sample": "none",
    "latent": 1,
    "fwd_pred_next_n": 1,
    "window_size": 1,
    "action_space": "continuous"
  }
}
```

**ì¶œì²˜**: `RoboVLMs/README.md:228-267`

## ğŸ“Š 6. í•™ìŠµ íë¦„ ìš”ì•½

### 6.1 ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸

```
[ë°ì´í„° ë¡œë”©]
    â†“
[ì´ë¯¸ì§€ ì „ì²˜ë¦¬] â† RGB ì´ë¯¸ì§€ (224x224)
    â†“
[ì•¡ì…˜ ì •ê·œí™”] â† 7-DOF ì•¡ì…˜ (-1 to 1)
    â†“
[VLM Forward Pass] â† ë©€í‹°ëª¨ë‹¬ ìœµí•©
    â†“
[LSTM Forward Pass] â† ì‹œí€€ìŠ¤ ì²˜ë¦¬
    â†“
[Loss ê³„ì‚°] â† MSE (íŒ”) + BCE (ê·¸ë¦¬í¼)
    â†“
[Backward Pass] â† ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    â†“
[Optimizer Step] â† íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
```

### 6.2 ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| **Learning Rate** | 1e-4 ~ 2e-5 | í•™ìŠµë¥  |
| **Batch Size** | 2 ~ 8 | ë°°ì¹˜ í¬ê¸° |
| **Window Size** | 8 ~ 16 | íˆìŠ¤í† ë¦¬ ê¸¸ì´ |
| **Hidden Size** | 1024 | LSTM ì€ë‹‰ ì°¨ì› |
| **Action Dim** | 7 | ì•¡ì…˜ ì°¨ì› (6-DOF + ê·¸ë¦¬í¼) |
| **Precision** | bf16 | BFloat16 ì •ë°€ë„ |
| **Weight Decay** | 0 | ê°€ì¤‘ì¹˜ ê°ì‡  |
| **Arm/Gripper Loss Ratio** | 0.01 | íŒ”/ê·¸ë¦¬í¼ Loss ë¹„ìœ¨ |

**ì¶œì²˜**: `RoboVLMs/configs/calvin_finetune/`, `RoboVLMs/main.py:136-309`

