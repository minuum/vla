# RoboVLMs Feedback ë¶„ì„ ì¢…í•©

## GitHub Repository ì •ë³´
- **Repository**: [RoboVLMs](https://github.com/robovlms/robovlms)
- **Paper**: [Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models](https://arxiv.org/abs/2412.14058)
- **Website**: [robovlms.github.io](https://robovlms.github.io)

## ğŸ“ Feedback ë¶„ì„ íŒŒì¼ë“¤

### 1. **Action, Image, Textì˜ Syncing ë¬¸ì œ**
- **íŒŒì¼**: `action_image_text_syncing.md`
- **ë‚´ìš©**: VLM Fine-tuning, Action-rel_action ë™ê¸°í™”, 7 DOF ë¡œë´‡íŒ” ì œì–´, ë©€í‹°ëª¨ë‹¬ ìœµí•©, Embedded Token ì²˜ë¦¬, CALVIN ë°ì´í„°ì…‹ ë¶„ì„

### 2. **CALVIN Dataset ìƒì„¸ ë¶„ì„**
- **íŒŒì¼**: `calvin_dataset_analysis.md`
- **ë‚´ìš©**: CALVIN ë°ì´í„°ì…‹ êµ¬ì¡°, ë¶„í•  ì „ëµ, í‰ê°€ ë©”íŠ¸ë¦­, ë°ì´í„°ì…‹ í™œìš© ì „ëµ, ì„±ëŠ¥ ê²°ê³¼

### 3. **Multi-modal ë™ê¸°í™” ë¶„ì„**
- **íŒŒì¼**: `multimodal_sync_analysis.md`
- **ë‚´ìš©**: LSTM í•œê³„, VLM ì¥ì , Fine-tuning ê³¼ì •, Action Head ë™ì‹œ í•™ìŠµ, ì¢Œí‘œê³„ ë™ê¸°í™”, Embedded Token ì²˜ë¦¬

## ğŸ” í•µì‹¬ ë¶„ì„ ë‚´ìš©

### 1. **VLM Fine-tuning ë°©ë²•**
- **F-FT (Full Fine-Tuning)**: ì „ì²´ ëª¨ë¸ íŒŒì¸íŠœë‹
- **LoRA (Low-Rank Adaptation)**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  íŒŒì¸íŠœë‹
- **GitHub Code Reference**: `5.robovlms_github/learning_pipeline/README.md:95-107`

### 2. **Actionê³¼ rel_action ë™ê¸°í™”**
```python
# Action (ì ˆëŒ€ ì¢Œí‘œ)
['actions'] (dtype=np.float32, shape=(7,))
tcp position (3): x,y,z in absolute world coordinates
tcp orientation (3): euler angles x,y,z in absolute world coordinates
gripper_action (1): binary (close = -1, open = 1)

# rel_action (ìƒëŒ€ ì¢Œí‘œ)
['rel_actions'] (dtype=np.float32, shape=(7,))
tcp position (3): x,y,z in relative world coordinates normalized and clipped to (-1, 1) with scaling factor 50
tcp orientation (3): euler angles x,y,z in relative world coordinates normalized and clipped to (-1, 1) with scaling factor 20
gripper_action (1): binary (close = -1, open = 1)
```

### 3. **7 DOF ë¡œë´‡íŒ” ì œì–´**
- **TCP Position (3)**: x, y, z ìœ„ì¹˜
- **TCP Orientation (3)**: x, y, z íšŒì „ (Euler angles)
- **Gripper Action (1)**: ê·¸ë¦¬í¼ ì—´ë¦¼/ë‹«í˜
- **GitHub Code Reference**: `5.robovlms_github/learning_pipeline/README.md:150-160`

### 4. **ë©€í‹°ëª¨ë‹¬ ìœµí•©**
- **ì´ë¯¸ì§€ ì²˜ë¦¬**: VLMì˜ vision towerë¡œ ì´ë¯¸ì§€ í† í° ìƒì„±
- **í…ìŠ¤íŠ¸ ì²˜ë¦¬**: VLMì˜ text towerë¡œ í…ìŠ¤íŠ¸ í† í° ìƒì„±
- **ë©€í‹°ëª¨ë‹¬ ìœµí•©**: Visionê³¼ text í† í°ì„ ìœµí•©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ í‘œí˜„ ìƒì„±
- **ì•¡ì…˜ ì˜ˆì¸¡**: Policy headë¡œ ì•¡ì…˜ ì‹œí€€ìŠ¤ ì˜ˆì¸¡
- **GitHub Code Reference**: `5.robovlms_github/methodology/README.md:104-107`

### 5. **Embedded Token ì²˜ë¦¬**
```python
# Learnable Token ìƒì„±
[LRN] = VLM(o_t, l_prompt)
Ã¢_{t:t+L-1} = MLP([LRN])
```
- **GitHub Code Reference**: `5.robovlms_github/methodology/README.md:82-84`

### 6. **CALVIN Dataset ë¶„ì„**
- **ì´ ì‹œì—°**: 24K ì¸ê°„ í…”ë ˆì˜¤í¼ë ˆì´ì…˜ ì‹œì—°
- **ì–¸ì–´ ì§€ì‹œ**: ëª¨ë“  ì‹œì—°ì— ì–¸ì–´ ì§€ì‹œ í¬í•¨
- **ê¶¤ì  ê¸¸ì´**: 64 ì‹œê°„ ë‹¨ê³„ ì´í•˜
- **ê¸°ë³¸ ê¸°ìˆ **: 34ê°œ ì‚¬ì „ ì •ì˜ëœ ê¸°ë³¸ ê¸°ìˆ 
- **ë¶„í• **: A, B, C, D 4ê°œ ë¶„í• 
- **GitHub Code Reference**: `5.robovlms_github/experiments/README.md:18-42`

### 7. **Multi-modal í•´ì„ êµ¬ì¡°**
- **LSTM í•œê³„**: ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë¶€ì¡±
- **VLM ì¥ì **: ê°•ë ¥í•œ vision-language ì´í•´ ëŠ¥ë ¥
- **End-to-End í•™ìŠµ**: VLMê³¼ Action Head ë™ì‹œ í•™ìŠµ
- **GitHub Code Reference**: `5.robovlms_github/learning_pipeline/README.md:8-12`

### 8. **2ì°¨ì›ê³¼ 3ì°¨ì› ë™ê¸°í™”**
- **ì ˆëŒ€ ì¢Œí‘œ**: 3D world coordinates
- **ìƒëŒ€ ì¢Œí‘œ**: normalized relative coordinates
- **ì •ê·œí™”**: (-1, 1) ë²”ìœ„ë¡œ í´ë¦¬í•‘
- **ìŠ¤ì¼€ì¼ë§**: ìœ„ì¹˜(50), íšŒì „(20)ì— ë”°ë¥¸ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§
- **GitHub Code Reference**: `5.robovlms_github/feedback/action_image_text_syncing.md:45-65`

## ğŸ¯ í•µì‹¬ í•™ìŠµ ë°©ë²•ë¡ 

### 1. **VLM ê¸°ë°˜ VLA êµ¬ì¶• ì „ëµ**
```python
VLA = VLM + Action_Head + History_Modeling
```

### 2. **ì•¡ì…˜ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸**
```python
# ì—°ì† ì•¡ì…˜ ì˜ˆì¸¡
multimodal_representation = VLM(images, language_instruction)
action_sequence = ActionHead(multimodal_representation)
loss = MSE(action_sequence[..., :6], target_actions[..., :6]) + 
       BCE(action_sequence[..., -1:], target_actions[..., -1:])
```

### 3. **íˆìŠ¤í† ë¦¬ ì •ë³´ ëª¨ë¸ë§**
```python
# Policy Head ë°©ì‹
representations = []
for t in range(history_length):
    repr_t = VLM(observation_tokens[t], language_instruction)
    representations.append(repr_t)

action = PolicyHead(representations)
```

## ğŸ“Š ì„±ëŠ¥ ê²°ê³¼

### 1. **CALVIN ì„±ëŠ¥**
- **ABCD â†’ D**: 96.7% ë‹¨ì¼ ì‘ì—… ì„±ê³µë¥ , 4.49 Avg. Len.
- **ABC â†’ D**: 98.0% ë‹¨ì¼ ì‘ì—… ì„±ê³µë¥ , 4.25 Avg. Len.
- **ê¸°ì¡´ SOTA ëŒ€ë¹„**: GR-1 ëŒ€ë¹„ ëŒ€í­ í–¥ìƒ

### 2. **ì‹¤ì œ ë¡œë´‡ ì„±ëŠ¥**
- **Simple ì„¤ì •**: 75% ì„±ê³µë¥ 
- **Unseen Distractor**: 60% ì„±ê³µë¥ 
- **Unseen Background**: 50% ì„±ê³µë¥ 
- **Unseen Object**: 55% ì„±ê³µë¥ 
- **Novel Skill Description**: 33% ì„±ê³µë¥ 

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. **í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```python
hyperparameter_grid = {
    'learning_rate': [1e-4, 2e-5, 1e-5],
    'weight_decay': [0, 1e-1],
    'batch_size': [128, 256, 512],
    'warmup_ratio': [0.25, 0.5]
}
```

### 2. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**
```python
# ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
with torch.cuda.amp.autocast():
    outputs = model(batch)
    loss = compute_loss(outputs, batch['targets'])

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
loss = loss / accumulation_steps
loss.backward()
```

### 3. **ëª¨ë¸ ë³‘ë ¬í™”**
```python
# ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ë¶„ì‚°
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])
```

## ğŸ¯ ì‹¤ìš©ì  ê°€ì¹˜

### 1. **VLA ì„¤ê³„ ê°€ì´ë“œë¼ì¸**
- **ë°±ë³¸ ì„ íƒ**: ì¶©ë¶„í•œ VL ì‚¬ì „ í›ˆë ¨ëœ VLM
- **êµ¬ì¡° ì„ íƒ**: Policy Head + Continuous Action
- **ë°ì´í„° ì „ëµ**: Post-training ì „ëµ

### 2. **ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œ**
- **VL ì‚¬ì „ í›ˆë ¨**: 1.79ê°œ ì‘ì—… í–¥ìƒ
- **íˆìŠ¤í† ë¦¬ ëª¨ë¸ë§**: 0.25ê°œ ì‘ì—… í–¥ìƒ
- **Cross-embodiment**: 0.17ê°œ ì‘ì—… í–¥ìƒ

### 3. **ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±**
- **ê°•ë ¥í•œ ì¼ë°˜í™”**: ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥
- **ìê°€ ìˆ˜ì • ëŠ¥ë ¥**: ì˜ˆìƒì¹˜ ëª»í•œ ëŠ¥ë ¥ ë°œê²¬
- **ì‹¤ì‹œê°„ ì œì–´**: ëª¨ë¸ ìµœì í™”ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë°°í¬

## ğŸ“ ê²°ë¡ 

RoboVLMsì˜ í•µì‹¬ì€ **Action, Image, Textì˜ ì •í™•í•œ ë™ê¸°í™”**ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´:

1. **VLMì˜ ê°•ë ¥í•œ ë©€í‹°ëª¨ë‹¬ ì´í•´ ëŠ¥ë ¥**ì„ í™œìš©
2. **7 DOF ë¡œë´‡íŒ” ì œì–´**ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜í–‰
3. **CALVIN ë°ì´í„°ì…‹**ì„ í†µí•œ ì²´ê³„ì  í•™ìŠµ
4. **End-to-End í•™ìŠµ**ìœ¼ë¡œ ìµœì  ì„±ëŠ¥ ë‹¬ì„±
5. **ì‹¤ì œ ë¡œë´‡ í™˜ê²½**ì—ì„œì˜ ê°•ë ¥í•œ ì„±ëŠ¥

ì´ëŸ¬í•œ ë¶„ì„ì„ í†µí•´ RoboVLMsì˜ í•µì‹¬ í•™ìŠµ ë°©ë²•ë¡ ê³¼ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ì„ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
