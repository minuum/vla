# RoboVLMs ν•µμ‹¬ κ°λ… κ°€μ΄λ“

## π“ λ””λ ‰ν† λ¦¬ κµ¬μ΅°

```
core/
β”β”€β”€ concepts/           # ν•µμ‹¬ κ°λ…
β”‚   β””β”€β”€ action_synchronization.md
β”β”€β”€ architecture/       # μ•„ν‚¤ν…μ²
β”‚   β””β”€β”€ vlm_lstm_integration.md
β”β”€β”€ data_flow/          # λ°μ΄ν„° ν”λ΅μ°
β”‚   β””β”€β”€ calvin_dataset_flow.md
β”β”€β”€ training/           # ν•™μµ κ³Όμ •
β”‚   β””β”€β”€ end_to_end_learning.md
β””β”€β”€ README.md          # μ΄ νμΌ
```

## π― ν•µμ‹¬ μ§λ¬Έκ³Ό λ‹µλ³€

### Q1. VLM Finetuning (F-FT)κ³Ό LoRAλ” λ¬΄μ—‡μΈκ°€?

**A**: 
- **Full-FT**: VLM μ „μ²΄ νλΌλ―Έν„° μ¬ν•™μµ (RoboVLMs μ‚¬μ©)
- **LoRA**: μ €μ°¨μ› ν–‰λ ¬λ§ ν•™μµ (λ©”λ¨λ¦¬ ν¨μ¨μ , μ„±λ¥ μ•½κ°„ λ‚®μ)

**μμ„Έν• λ‚΄μ©**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md#2-vlm-fine-tuning-fftκ³Ό-lora)

### Q2. actionκ³Ό rel_actionμ€ μ–΄λ–»κ² λ‹¤λ¥΄κ³ , μ–΄λ–»κ² λ™κΈ°ν™”λλ”κ°€?

**A**: 
- **action**: World frame μ λ€ μΆν‘
- **rel_action**: TCP frame μƒλ€ λ³€ν™”λ‰ (RoboVLMs μ‚¬μ©)
- **λ³€ν™**: `world_to_tcp_frame()` ν•¨μλ΅ λ³€ν™

**μμ„Έν• λ‚΄μ©**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md#1-action-vs-rel_action-ν•µμ‹¬-μ°¨μ΄μ )

### Q3. 7-DOF λ΅λ΄‡ν” μ›€μ§μ„μ΄ μ–΄λ–»κ² ν‘ν„λκ³  ν•™μµλλ”κ°€?

**A**: Translation(3) + Rotation(3) + Gripper(1) = 7μ°¨μ› λ²΅ν„°

**μμ„Έν• λ‚΄μ©**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md#12-7-dof-μƒλ€-μ•΅μ…-κµ¬μ΅°)

### Q4. Image, Text, Actionμ΄ μ–΄λ–»κ² λ™μ‹μ— ν•™μµλλ”κ°€?

**A**: 
- λ¨λ‘ **Token**μΌλ΅ λ³€ν™ β†’ VLM AttentionμΌλ΅ μµν•©
- **[LRN] Token**μ΄ Multi-modal μ •λ³΄ ν†µν•©

**μμ„Έν• λ‚΄μ©**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md#3-embedded-token-multi-modal-fusionμ-ν•µμ‹¬)

### Q5. Embedded Tokenμ΄ λ¬΄μ—‡μ΄κ³  μ–΄λ–»κ² λ™κΈ°ν™”λλ”κ°€?

**A**: 
- **[LRN]**: ν•™μµ κ°€λ¥ν• Action Token
- VLMμ„ ν†µκ³Όν•λ©° Image + Text μ •λ³΄λ¥Ό μµν•©

**μμ„Έν• λ‚΄μ©**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md#32-action-token-lrn-embeddingμΌλ΅-multi-modal-μ •λ³΄-μµν•©)

### Q6. CALVIN λ°μ΄ν„°μ…‹μ€ μ–΄λ–»κ² κµ¬μ„±λμ–΄ μλ”κ°€?

**A**: 
- Image(2κ°) + Text + robot_obs(15μ°¨μ›) + rel_actions(7μ°¨μ›)
- 24K demonstrations, 34 basic skills

**μμ„Έν• λ‚΄μ©**: [`data_flow/calvin_dataset_flow.md`](data_flow/calvin_dataset_flow.md#1-calvin-λ°μ΄ν„°μ…‹-κ°μ”)

### Q7. μ‹¤μ  ν•™μµ κ³Όμ •μ—μ„ VLMκ³Ό Action Headλ” λ™μ‹μ— ν•™μµλλ”κ°€?

**A**: **μ!** End-to-Endλ΅ λ¨λ“  νλΌλ―Έν„° λ™μ‹ ν•™μµ

**μμ„Έν• λ‚΄μ©**: [`training/end_to_end_learning.md`](training/end_to_end_learning.md#1-ν•™μµ-νμ΄ν”„λΌμΈ-μ „μ²΄-νλ¦„)

## π—οΈ μ „μ²΄ μ•„ν‚¤ν…μ² κ°μ”

### μ‹μ¤ν… κµ¬μ„±λ„

```
Input Data
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                    Multi-modal Input                    β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚   Image     β”‚  β”‚    Text     β”‚  β”‚  [LRN]      β”‚    β”‚
β”‚  β”‚ (2 cameras) β”‚  β”‚ (language)  β”‚  β”‚ (learnable) β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                  VLM Backbone                          β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚   Vision    β”‚  β”‚    Text     β”‚  β”‚  Attention  β”‚    β”‚
β”‚  β”‚  Encoder    β”‚  β”‚  Encoder    β”‚  β”‚   Layers    β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                Multi-modal Fusion                       β”‚
β”‚              (Self-Attention Mechanism)                 β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚              Fused [LRN] Token Output                   β”‚
β”‚            (Image + Text + Action Info)                 β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                Policy Head (LSTM)                       β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β”‚  β”‚   LSTM      β”‚  β”‚   Linear    β”‚  β”‚   Output    β”‚    β”‚
β”‚  β”‚  (History)  β”‚  β”‚   Layers    β”‚  β”‚ (7-DOF)     β”‚    β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
    β†“
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                7-DOF Action Prediction                  β”‚
β”‚        [Ξ”x, Ξ”y, Ξ”z, Ξ”roll, Ξ”pitch, Ξ”yaw, gripper]      β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

**μμ„Έν• λ‚΄μ©**: [`architecture/vlm_lstm_integration.md`](architecture/vlm_lstm_integration.md#1-μ „μ²΄-μ•„ν‚¤ν…μ²-κ°μ”)

## π“ λ°μ΄ν„° ν”λ΅μ°

### CALVIN λ°μ΄ν„°μ…‹ μ²λ¦¬

```
CALVIN Dataset
    β†“
Episode Loading (24K demonstrations)
    β†“
Multi-modal Data Extraction
    β”β”€β”€ RGB Images (rgb_static, rgb_gripper)
    β”β”€β”€ Robot State (robot_obs)
    β”β”€β”€ Actions (rel_actions)
    β””β”€β”€ Language (language)
    β†“
Data Preprocessing
    β”β”€β”€ Image: Resize, Normalize, Augmentation
    β”β”€β”€ Action: Normalize [-1, 1]
    β””β”€β”€ Language: Tokenization
    β†“
Sequence Sampling (window_size=8)
    β†“
Batch Creation (batch_size=8)
    β†“
Model Input
```

**μμ„Έν• λ‚΄μ©**: [`data_flow/calvin_dataset_flow.md`](data_flow/calvin_dataset_flow.md#8-λ°μ΄ν„°-ν”λ΅μ°-μ”μ•½)

## π”„ ν•™μµ κ³Όμ •

### End-to-End ν•™μµ ν”λ΅μ°

```
1. λ°μ΄ν„° λ΅λ“
   β†“
2. Image β†’ Vision Tokens (VLM Vision Encoder)
   β†“
3. Text β†’ Text Tokens (VLM Tokenizer)
   β†“
4. [LRN] Token μ¶”κ°€
   β†“
5. Multi-modal Fusion (VLM Backbone)
   β†“
6. [LRN] Token μ¶”μ¶
   β†“
7. LSTMμ— [LRN] μ…λ ¥
   β†“
8. 7-DOF Action μμΈ΅
   β†“
9. Loss κ³„μ‚° (MSE + BCE)
   β†“
10. Backpropagation (VLM + LSTM λ™μ‹ μ—…λ°μ΄νΈ)
```

**μμ„Έν• λ‚΄μ©**: [`training/end_to_end_learning.md`](training/end_to_end_learning.md#1-ν•™μµ-νμ΄ν”„λΌμΈ-μ „μ²΄-νλ¦„)

## π― ν•µμ‹¬ κ°λ… μ”μ•½

### 1. Action Synchronization
- **μ λ€ μ•΅μ… vs μƒλ€ μ•΅μ…**: World frame vs TCP frame
- **7-DOF ν‘ν„**: Translation(3) + Rotation(3) + Gripper(1)
- **μ •κ·ν™”**: [-1, 1] λ²”μ„λ΅ ν΄λ¦¬ν•‘

### 2. VLM Integration
- **Multi-modal Fusion**: Text + Vision + Action ν† ν° μµν•©
- **[LRN] Token**: ν•™μµ κ°€λ¥ν• μ•΅μ… ν† ν°
- **End-to-End ν•™μµ**: VLMκ³Ό LSTM λ™μ‹ ν•™μµ

### 3. Data Processing
- **CALVIN λ°μ΄ν„°μ…‹**: 24K demonstrations, 34 skills
- **μ „μ²λ¦¬**: μ΄λ―Έμ§€ μ •κ·ν™”, μ•΅μ… μ •κ·ν™”, ν† ν°ν™”
- **μ‹ν€€μ¤ μ²λ¦¬**: 8ν”„λ μ„ μλ„μ° ν¬κΈ°

### 4. Training Process
- **Loss Function**: MSE (pose) + BCE (gripper)
- **Gradient Flow**: Loss β†’ LSTM β†’ VLM β†’ Vision/Text Encoder
- **νλΌλ―Έν„° μ—…λ°μ΄νΈ**: λ¨λ“  λ¨λ“ λ™μ‹ ν•™μµ

## π“ λ¬Έμ„ κ°€μ΄λ“

### κ°λ… μ΄ν•΄ μμ„
1. **`concepts/action_synchronization.md`**: κΈ°λ³Έ κ°λ… μ΄ν•΄
2. **`architecture/vlm_lstm_integration.md`**: μ•„ν‚¤ν…μ² κµ¬μ΅° νμ•…
3. **`data_flow/calvin_dataset_flow.md`**: λ°μ΄ν„° μ²λ¦¬ κ³Όμ •
4. **`training/end_to_end_learning.md`**: ν•™μµ κ³Όμ • μƒμ„Έ

### λΉ λ¥Έ μ°Έμ΅°
- **Action vs Rel_Action**: [`concepts/action_synchronization.md#1`](concepts/action_synchronization.md#1-action-vs-rel_action-ν•µμ‹¬-μ°¨μ΄μ )
- **VLM + LSTM κµ¬μ΅°**: [`architecture/vlm_lstm_integration.md#1`](architecture/vlm_lstm_integration.md#1-μ „μ²΄-μ•„ν‚¤ν…μ²-κ°μ”)
- **CALVIN λ°μ΄ν„°μ…‹**: [`data_flow/calvin_dataset_flow.md#1`](data_flow/calvin_dataset_flow.md#1-calvin-λ°μ΄ν„°μ…‹-κ°μ”)
- **ν•™μµ κ³Όμ •**: [`training/end_to_end_learning.md#1`](training/end_to_end_learning.md#1-ν•™μµ-νμ΄ν”„λΌμΈ-μ „μ²΄-νλ¦„)

## π”— κ΄€λ ¨ νμΌ

### μ½”λ“ νμΌ
- `RoboVLMs/robovlms/model/backbone/base_backbone.py`: κΈ°λ³Έ VLM + LSTM ν†µν•©
- `RoboVLMs/robovlms/data/calvin_dataset.py`: CALVIN λ°μ΄ν„° λ΅λ”
- `RoboVLMs/robovlms/data/data_utils.py`: λ°μ΄ν„° μ ν‹Έλ¦¬ν‹° ν•¨μ
- `RoboVLMs/configs/calvin_finetune/`: CALVIN ν•™μµ μ„¤μ •

### μ„¤μ • νμΌ
- `RoboVLMs/configs/calvin_finetune/finetune_kosmos_cont-lstm-post_full-ft_text_vision_wd-0_ws-8_act-10.json`: Kosmos-2 Full-FT μ„¤μ •
- `RoboVLMs/configs/calvin_finetune/finetune_paligemma_cont-lstm-post_full-ft_text_vision_wd=0_ws-8_act-10.json`: PaLI-Gemma Full-FT μ„¤μ •

## π“– λ…Όλ¬Έ μ°Έμ΅°

- **RoboVLMs λ…Όλ¬Έ**: Section B (VLA Models), Section C (VLA Structures)
- **CALVIN λ…Όλ¬Έ**: "CALVIN: A Benchmark for Multimodal Language-Conditioned Imitation Learning for Long-Horizon Robot Manipulation Tasks"

## π€ μ‹μ‘ν•κΈ°

1. **κ°λ… μ΄ν•΄**: [`concepts/action_synchronization.md`](concepts/action_synchronization.md)λ¶€ν„° μ‹μ‘
2. **μ•„ν‚¤ν…μ² νμ•…**: [`architecture/vlm_lstm_integration.md`](architecture/vlm_lstm_integration.md)λ΅ κµ¬μ΅° μ΄ν•΄
3. **λ°μ΄ν„° μ²λ¦¬**: [`data_flow/calvin_dataset_flow.md`](data_flow/calvin_dataset_flow.md)λ΅ λ°μ΄ν„° ν”λ΅μ° νμ•…
4. **ν•™μµ κ³Όμ •**: [`training/end_to_end_learning.md`](training/end_to_end_learning.md)λ΅ ν•™μµ μ›λ¦¬ μ΄ν•΄

---

**μ΄ λ¬Έμ„λ” RoboVLMs ν”„λ΅μ νΈμ ν•µμ‹¬ κ°λ…λ“¤μ„ μ²΄κ³„μ μΌλ΅ μ •λ¦¬ν• κ°€μ΄λ“μ…λ‹λ‹¤. κ° μ„Ήμ…μ€ λ…λ¦½μ μΌλ΅ μ½μ„ μ μλ„λ΅ κµ¬μ„±λμ–΄ μμΌλ©°, μƒνΈ μ°Έμ΅°λ¥Ό ν†µν•΄ μ „μ²΄μ μΈ μ΄ν•΄λ¥Ό λ„μΈ μ μμµλ‹λ‹¤.**
