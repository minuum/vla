# RoboVLMs ì¢…í•© ê¸°ìˆ  ë¬¸ì„œ
**Vision-Language-Action Models for Robotics: ì™„ì „í•œ ê¸°ìˆ  ë¶„ì„ ë° êµ¬í˜„ ê°€ì´ë“œ**

> ğŸ“… **ì‘ì„±ì¼**: 2025ë…„ 8ì›” 16ì¼  
> ğŸ“š **ë²„ì „**: v2.0 (Notion ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜)  
> ğŸ”— **ì¶œì²˜**: [RoboVLMs GitHub](https://github.com/Robot-VLAs/RoboVLMs) + K-í”„ë¡œì íŠ¸ ì—°êµ¬

---

## ğŸ—ï¸ **1. RoboVLMs ê°œìš” ë° ì •ì˜**

### 1.1 ì •ì˜
**RoboVLMs**ëŠ” **Vision-Language-Action (VLA) ëª¨ë¸**ë¡œ, ë¹„ì „, ì–¸ì–´, ì•¡ì…˜ì„ í†µí•© ì²˜ë¦¬í•˜ëŠ” **ë©€í‹°ëª¨ë¸ ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. ê¸°ì¡´ Vision-Language Model(VLM)ì„ ë¡œë´‡ ì œì–´ìš©ìœ¼ë¡œ í™•ì¥í•œ end-to-end í•™ìŠµ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ì•„í‚¤í…ì²˜

#### **ì „ì²´ íŒŒì´í”„ë¼ì¸**
```
ì¹´ë©”ë¼ ì´ë¯¸ì§€ (RGB + Hand RGB) â†’ Vision Encoder
     â†“
ìì—°ì–´ ëª…ë ¹ (Task Instruction) â†’ Text Encoder  
     â†“
ì•¡ì…˜ íˆìŠ¤í† ë¦¬ (Optional) â†’ Action Encoder
     â†“
     ë©€í‹°ëª¨ë¸ ìœµí•© (Cross-Attention)
     â†“
Policy Head (FC/LSTM/GPT/Discrete) â†’ 7D ì•¡ì…˜ ì¶œë ¥
```

#### **ë°±ë³¸ ëª¨ë¸ ì¢…ë¥˜**
- **Kosmos-2**: Microsoftì˜ Vision-Language ëª¨ë¸ (ê¸°ë³¸ ë°±ë³¸)
- **PaliGemma**: Googleì˜ ê²½ëŸ‰í™”ëœ VLM
- **LLaVA**: ì˜¤í”ˆì†ŒìŠ¤ Vision-Language ëª¨ë¸
- **Custom Flamingo**: ì‚¬ìš©ì ì •ì˜ ëª¨ë¸

---

## ğŸ§  **2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„¸ ë¶„ì„**

### 2.1 Vision Encoder 

#### **ë“œì–¼ ì¹´ë©”ë¼ ì‹œìŠ¤í…œ**
```python
# ë‘ ê°€ì§€ ì‹œì ì˜ ë¹„ì „ ë°ì´í„°
rgb = batch["rgb"].cuda()                    # ì •ì (ì™¸ë¶€) ì¹´ë©”ë¼ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
hand_rgb = batch["hand_rgb"].cuda()          # ê·¸ë¦¬í¼(1ì¸ì¹­) ì¹´ë©”ë¼ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤
```

#### **CLIP ê¸°ë°˜ ë¹„ì „ ì²˜ë¦¬**
- **ëª¨ë¸**: ViT-L-14 (OpenAI)
- **ì…ë ¥ í¬ê¸°**: 224x224 pixels
- **ì •ê·œí™”**: ImageNet í‘œì¤€ mean/std
- **ì¶œë ¥**: 1024ì°¨ì› ë¹„ì „ íŠ¹ì§•

#### **Vision Resampler (ì„ íƒì )**
- **PerceiverResampler**: ë³€ë™ ê¸¸ì´ ì´ë¯¸ì§€ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ì••ì¶•
- **ì••ì¶•ë¹„**: 196 í† í° â†’ 64 í† í°
- **ë§¤ê°œë³€ìˆ˜**: depth=8, heads=8, dim_head=64

### 2.2 Text Encoder

#### **CLIP Text Encoder**
```python
# ì–¸ì–´ ì •ë³´ ì²˜ë¦¬
language = batch["text"].cuda()             # í† í°í™”ëœ ìì—°ì–´ íƒœìŠ¤í¬ ëª…ë ¹
text_mask = batch["text_mask"].cuda()       # ì–¸ì–´ í† í° ìœ íš¨ì„± ë§ˆìŠ¤í¬
```

#### **í† í°í™” ì„¤ì •**
- **ìµœëŒ€ ê¸¸ì´**: 256 í† í°
- **íŠ¹ìˆ˜ í† í°**: `[CLS]`, `[SEP]`, `[PAD]`
- **ì¶œë ¥**: 512ì°¨ì› í…ìŠ¤íŠ¸ ì„ë² ë”©

### 2.3 Action Encoder (ì„ íƒì )

#### **Linear Action Encoder**
```python
# ì•¡ì…˜ ë° ìƒíƒœ ì •ë³´
action = batch["action"].cuda()             # ê³¼ê±° 7-DOF ì•¡ì…˜ íˆìŠ¤í† ë¦¬
rel_state = batch.get("rel_state", None)    # ë¡œë´‡ í˜„ì¬ ìƒëŒ€ì  ìƒíƒœ
```

#### **í•˜ì´ë¸Œë¦¬ë“œ ì•¡ì…˜ ë¶„í• **
- **ì—°ì† ì•¡ì…˜**: arm_action (6-DOF) - [x,y,z,roll,pitch,yaw]
- **ì´ì‚° ì•¡ì…˜**: gripper_action (1-DOF) - [open/close]
- **ì •ê·œí™”**: [-0.65, 0.65] ë²”ìœ„ë¡œ ì •ê·œí™”

---

## ğŸ¨ **3. Policy Head ì•„í‚¤í…ì²˜ ìƒì„¸**

### 3.1 ì§€ì›ë˜ëŠ” Policy Head ìœ í˜•

#### **1. MLPHead (Fully Connected)**
```python
class MLPHead(BasePolicyHead):
    def __init__(self, in_features, action_dim, hidden_size=1024):
        self.layers = nn.Sequential(
            nn.Linear(in_features, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Linear(hidden_size//2, action_dim)
        )
```
- **ì‚¬ìš© ì‚¬ë¡€**: ë‹¨ìˆœí•œ ì§ì ‘ ë§¤í•‘
- **ì¥ì **: ë¹ ë¥¸ ì¶”ë¡ , ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
- **ë‹¨ì **: ì‹œê°„ì  ì¼ê´€ì„± ë¶€ì¡±

#### **2. LSTMDecoder (ì°¸ì¡°: RoboVLMs-20.LSTM Layer)**
```python
class LSTMDecoder(BasePolicyHead):
    def __init__(self, hidden_size=1024, num_layers=4, action_dim=7):
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,        # ê¸°ë³¸ 4ì¸µ (ìµœì í™” í•„ìš”)
            batch_first=True,
            dropout=0.1
        )
        # Arm ì œì–´ìš© MLP
        self.arm_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.Tanh(),  # [-1, 1] ì •ê·œí™”ëœ ì¶œë ¥
            nn.Linear(hidden_size//2, 6)  # 6-DOF arm
        )
        # ê·¸ë¦¬í¼ ì œì–´ìš© MLP  
        self.gripper_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//4),
            nn.Sigmoid(),  # [0, 1] í™•ë¥  ì¶œë ¥
            nn.Linear(hidden_size//4, 1)  # 1-DOF gripper
        )
```
- **ì‚¬ìš© ì‚¬ë¡€**: ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ì‹ (ì‹œê°„ì  ì¼ê´€ì„± ì¤‘ìš”)
- **ì¥ì **: ìˆœì°¨ì  ì•¡ì…˜ ì˜ˆì¸¡, ì•ˆì •ì ì¸ í•™ìŠµ
- **ìµœì í™” ê³¼ì œ**: ë ˆì´ì–´ ìˆ˜ ì¡°ì • (4ì¸µ â†’ 2-3ì¸µ ì‹¤í—˜)

#### **3. GPTDecoder (Trajectory Generation)**
```python
class GPTDecoder(BasePolicyHead):
    def __init__(self, hidden_size=1024, num_layers=6, num_heads=16):
        self.gpt_model = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=hidden_size,
                nhead=num_heads,
                dim_feedforward=hidden_size*4,
                dropout=0.1
            ),
            num_layers=num_layers
        )
```
- **ì‚¬ìš© ì‚¬ë¡€**: ë‹¤ë‹¨ê³„ ê²½ë¡œ ê³„íš
- **ì¥ì **: ìê¸°íšŒê·€ì  ì‹œí€€ìŠ¤ ìƒì„±
- **ë‹¨ì **: ë†’ì€ ì—°ì‚° ë¹„ìš©

#### **4. DiscreteDecoder (Tokenized Actions)**
```python
class DiscreteDecoder(BasePolicyHead):
    def __init__(self, tokenizer, n_bin=256, min_action=-1, max_action=1):
        self.action_tokenizer = ActionTokenizer(
            tokenizer=tokenizer,
            bins=n_bin,
            min_action=min_action,
            max_action=max_action
        )
        self.action_head = nn.Linear(hidden_size, n_bin * action_dim)
```
- **ì‚¬ìš© ì‚¬ë¡€**: ì–¸ì–´ ëª¨ë¸ê³¼ ì¼ê´€ëœ í† í° ê¸°ë°˜ ì ‘ê·¼
- **ì¥ì **: VLMê³¼ ë™ì¼í•œ í† í° ê³µê°„ ì‚¬ìš©
- **ë‹¨ì **: ì–‘ìí™” ì˜¤ì°¨ ë°œìƒ

### 3.2 Action Tokenizer ìƒì„¸ (ì°¸ì¡°: RoboVLMs-12)

#### **ì—°ì† â†’ ì´ì‚° ë³€í™˜**
```python
class ActionTokenizer:
    def tokenize_actions(self, action):
        # ì•¡ì…˜ ë²”ìœ„ ì œí•œ
        action = np.clip(action, 
                        a_min=self.min_action, 
                        a_max=self.max_action)
        # ì´ì‚°í™” (binsê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• )
        discretized_action = np.digitize(action, self.bins)
        return discretized_action
    
    def decode_token_ids_to_actions(self, token_ids):
        # í† í° IDë¥¼ ë‹¤ì‹œ ì—°ì†ê°’ìœ¼ë¡œ ë³€í™˜
        actions = self.bins[token_ids - 1]  # bin center
        return actions
```

---

## ğŸ” **4. ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ì‹œìŠ¤í…œ**

### 4.1 3ê°€ì§€ ë™ì‹œ í•™ìŠµ íƒœìŠ¤í¬

#### **ì£¼ íƒœìŠ¤í¬: Action Prediction**
```python
# ë©”ì¸ ë¡œë´‡ ì œì–´ íƒœìŠ¤í¬
action_loss = (
    smooth_l1_loss(arm_pred, arm_action) +     # íŒ” ì œì–´ (ì—°ì†)
    cross_entropy_loss(gripper_pred, gripper_action)  # ê·¸ë¦¬í¼ (ì´ì‚°)
) * arm_gripper_loss_ratio  # ê¸°ë³¸ 0.01
```

#### **ë³´ì¡° íƒœìŠ¤í¬ 1: Forward Prediction**
```python
# ë¯¸ë˜ ì´ë¯¸ì§€ ì˜ˆì¸¡ (ë¬¼ë¦¬ ì´í•´)
fwd_loss = (
    mse_loss(fwd_rgb_pred, fwd_rgb_target) +   # ì™¸ë¶€ ì¹´ë©”ë¼
    mse_loss(fwd_hand_pred, fwd_hand_target)   # ê·¸ë¦¬í¼ ì¹´ë©”ë¼
) * fwd_loss_ratio  # ê¸°ë³¸ 0 (ë¹„í™œì„±í™”)
```

#### **ë³´ì¡° íƒœìŠ¤í¬ 2: Caption Generation**
```python
# ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± (ì–¸ì–´ ì •ë ¬)
caption_loss = cross_entropy_loss(
    caption_pred, 
    caption_target
) * cap_loss_ratio  # ê¸°ë³¸ 0.05
```

### 4.2 ì‹œê°„ì  êµ¬ì¡°í™” (ì°¸ì¡°: RoboVLMs-18)

#### **Window + Chunk ë°©ì‹**
```python
# ì‹œê°„ì  ë°ì´í„° êµ¬ì¡°
training_sequence = {
    "window_size": 16,        # ê³¼ê±° ë§¥ë½ ê¸¸ì´ 
    "chunk_size": 10,         # ë¯¸ë˜ ì˜ˆì¸¡ ê¸¸ì´
    "sequence_example": [
        "[t-15, t-14, ..., t-1, t0]",  # 16í”„ë ˆì„ íˆìŠ¤í† ë¦¬
        "[t1, t2, ..., t10]"           # 10í”„ë ˆì„ ë¯¸ë˜ ì˜ˆì¸¡
    ]
}
```

#### **Temporal Mask ì ìš©**
```python
# ì‹œê°„ ì¸ê³¼ê´€ê³„ ë§ˆìŠ¤í‚¹
temporal_mask = claw_matrix(window_size + chunk_size, chunk_size)
future_predictions = apply_temporal_mask(vision_features, temporal_mask)
```

### 4.3 ì†ì‹¤ í•¨ìˆ˜ ìƒì„¸ (ì°¸ì¡°: RoboVLMs-14)

#### **Smooth L1 Loss (íŒ” ì œì–´)**
```python
def smooth_l1_loss(predicted, target, beta=0.1):
    # ì†Œê·œëª¨ ì˜¤ì°¨: quadratic penaltyë¡œ ì •í™•ë„ ì¶”êµ¬
    if abs(predicted - target) < beta:
        loss = 0.5 * (predicted - target)**2 / beta
    # ëŒ€ê·œëª¨ ì˜¤ì°¨: linear penaltyë¡œ gradient ì•ˆì •í™”
    else:
        loss = abs(predicted - target) - 0.5 * beta
    return loss
```

#### **Cross Entropy Loss (ê·¸ë¦¬í¼ ì œì–´)**
```python
def cross_entropy_with_mask(logits, labels, mask):
    # ìœ íš¨í•œ ì‹œê°„ ìŠ¤í…ë§Œ ì†ì‹¤ ê³„ì‚°
    masked_logits = logits[mask]
    masked_labels = labels[mask]
    return F.cross_entropy(masked_logits, masked_labels)
```

---

## ğŸ”„ **5. ë°ì´í„°ì…‹ ë° í•™ìŠµ ì²˜ë¦¬**

### 5.1 ì§€ì› ë°ì´í„°ì…‹

#### **Calvin Dataset**
- **ì¢…ë¥˜**: ë¡œë´‡íŒ” ì¡°ì‘ ì‹œë®¬ë ˆì´ì…˜
- **íƒœìŠ¤í¬**: pick_up, push, slide, open_drawer, close_drawer
- **ì•¡ì…˜ ê³µê°„**: 7-DOF (6-DOF arm + 1-DOF gripper)
- **í‰ê°€**: Sequential Task (1-5ê°œ ì—°ì† ìˆ˜í–‰)

#### **Open-X Embodiment (OXE)**
- **ê·œëª¨**: 1M+ ì—í”¼ì†Œë“œ
- **ë¡œë´‡**: 22ê°œ ë‹¤ì–‘í•œ ë¡œë´‡ í”Œë«í¼
- **íƒœìŠ¤í¬**: pick-and-place, navigation, manipulation
- **ë‹¤ì–‘ì„±**: ì‹¤ì„¸ê³„ ë°ì´í„° í¬í•¨

#### **Bridge Dataset**
- **íŠ¹ì§•**: ê³ í’ˆì§ˆ ë¡œë´‡íŒ” ì¡°ì‘ ë°ì´í„°
- **ì£¼ì—°**: Berkeley Robot Learning Lab
- **ì‚¬ìš©**: Fine-tuning ë‹¨ê³„ì—ì„œ ê³ í’ˆì§ˆ í•™ìŠµ

### 5.2 ë°ì´í„° ì „ì²˜ë¦¬

#### **ì´ë¯¸ì§€ ì „ì²˜ë¦¬**
```python
# ë¹„ì „ ë°ì´í„° ì •ê·œí™”
image_mean = [0.48145466, 0.4578275, 0.40821073]  # CLIP í‘œì¤€
image_std = [0.26862954, 0.26130258, 0.27577711]

# ë¦¬ì‚¬ì´ì§• ë° ì¦ê°•
transforms = [
    Resize((224, 224)),
    RandomHorizontalFlip(p=0.1),  # ì œí•œì  ì¦ê°•
    ColorJitter(brightness=0.1, contrast=0.1),
    Normalize(mean=image_mean, std=image_std)
]
```

#### **ì•¡ì…˜ ì •ê·œí™”**
```python
# ì•¡ì…˜ ì •ê·œí™” [-0.65, 0.65] ë²”ìœ„
norm_min, norm_max = -0.65, 0.65

# arm ì•¡ì…˜ (6-DOF)
arm_action = action[:, :, :6]  
normalized_arm = np.clip(arm_action, norm_min, norm_max)

# gripper ì•¡ì…˜ (1-DOF): [-1, 1] â†’ [0, 1] ë³€í™˜
gripper_action = (action[:, :, 6] + 1.0) / 2.0
```

---

## ğŸ”§ **6. í•™ìŠµ ë° ìµœì í™”**

### 6.1 í•™ìŠµ ì„¤ì •

#### **ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**
```json
{
    "learning_rate": 2e-5,
    "min_lr_scale": 1e-2,
    "weight_decay": 0,
    "warmup_epochs": 0.25,
    "batch_size": 4,
    "max_epochs": 5,
    "gradient_clip_val": 1.0,
    "precision": "bf16"
}
```

#### **ì†ì‹¤ ê°€ì¤‘ì¹˜**
```json
{
    "arm_gripper_loss_ratio": 0.01,  # ì£¼ íƒœìŠ¤í¬ ê°€ì¤‘ì¹˜
    "cap_loss_ratio": 0.05,          # ìº¡ì…˜ ìƒì„± ê°€ì¤‘ì¹˜
    "fwd_loss_ratio": 0              # ë¯¸ë˜ ì˜ˆì¸¡ (ë¹„í™œì„±í™”)
}
```

### 6.2 ëª¨ë¸ ìµœì í™” ì „ëµ

#### **LoRA (Low-Rank Adaptation)**
```python
lora_config = {
    "lora_enable": False,     # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
    "lora_r": 64,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "lora_bias": "none"
}
```

#### **ê·¸ë¼ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…**
```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
training_config = {
    "gradient_checkpointing": False,
    "freeze_backbone": False,
    "train_vision": True,
    "train_text_embedding": True
}
```

### 6.3 ì¶”ë¡  ìµœì í™”

#### **DeepSpeed ì „ëµ**
```python
trainer_config = {
    "strategy": "deepspeed_stage_2",
    "precision": "16",               # Mixed Precision
    "accumulate_grad_batches": 1
}
```

---

## ğŸ”Œ **7. ì‹¤ì‹œê°„ ì¶”ë¡  ë° ëŒ€í™” ì‹œìŠ¤í…œ**

### 7.1 ì¶”ë¡  íŒŒì´í”„ë¼ì¸

#### **Event-Triggered ì¶”ë¡ **
```python
class EventTriggeredVLA:
    def predict_action(self, current_image, instruction, robot_state):
        # 1. ë¹„ì „ ì¸ì½”ë”©
        vision_features = self.encode_images(current_image)
        
        # 2. ì–¸ì–´ ì¸ì½”ë”©  
        text_features = self.encode_text(instruction)
        
        # 3. ë©€í‹°ëª¨ë¸ ìœµí•©
        fused_features = self.multimodal_fusion(
            vision_features, text_features
        )
        
        # 4. ì•¡ì…˜ ì˜ˆì¸¡
        arm_action, gripper_action = self.policy_head(fused_features)
        
        return {
            "arm": arm_action.cpu().numpy(),
            "gripper": gripper_action.cpu().numpy(),
            "confidence": self.compute_confidence(fused_features)
        }
```

#### **ì„±ëŠ¥ ë©”íŠ¸ë¦­**
- **ì¶”ë¡  ì§€ì—°ì‹œê°„**: < 100ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ~12GB (PaliGemma-3B)
- **ë°˜ì‘ì†ë„ ê°œì„ **: 96% (Window-Chunk ëŒ€ë¹„)

### 7.2 ì•ˆì „ ë©”ì»¤ë‹ˆì¦˜

#### **ì•ˆì „ ì œì•½ ì¡°ê±´**
```python
safety_constraints = {
    "velocity_limit": 0.5,        # ìµœëŒ€ ì†ë„ ì œí•œ
    "workspace_bounds": {          # ì‘ì—… ê³µê°„ ì œí•œ
        "x": [-0.5, 0.5],
        "y": [-0.5, 0.5], 
        "z": [0.0, 0.3]
    },
    "collision_threshold": 0.1,    # ì¶©ëŒ íšŒí”¼ ê±°ë¦¬
    "emergency_stop": True         # ë¹„ìƒ ì •ì§€ ê¸°ëŠ¥
}
```

#### **ë¹„ìƒ ìƒí™© ëŒ€ì‘**
```python
def emergency_handler(sensor_data):
    if detect_collision_risk(sensor_data):
        return {
            "action": "STOP",
            "reason": "collision_risk",
            "safe_action": [0, 0, 0, 0, 0, 0, 0]  # ì •ì§€
        }
    return None
```

---

## ğŸ”¬ **8. ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬**

### 8.1 Calvin Sequential Task ê²°ê³¼

#### **ì„±ê³µë¥  ë©”íŠ¸ë¦­**
```python
calvin_results = {
    "1-task": 0.923,    # 92.3% (ë‹¨ì¼ íƒœìŠ¤í¬)
    "2-task": 0.847,    # 84.7% (2ê°œ ì—°ì†)
    "3-task": 0.764,    # 76.4% (3ê°œ ì—°ì†)
    "4-task": 0.681,    # 68.1% (4ê°œ ì—°ì†)
    "5-task": 0.593,    # 59.3% (5ê°œ ì—°ì†)
    "avg_length": 3.2   # í‰ê·  ì—°ì† ìˆ˜í–‰ ê¸¸ì´
}
```

### 8.2 ëª¨ë¸ í¬ê¸°ë³„ ë¹„êµ

#### **íŒŒë¼ë¯¸í„° ë¶„ì„**
| ëª¨ë¸ | íŒŒë¼ë¯¸í„° | ë©”ëª¨ë¦¬ | ì¶”ë¡ ì†ë„ | Calvin 5-task |
|------|----------|--------|-----------|---------------|
| **Kosmos-2** | 1.3B | ~6GB | 150ms | 59.3% |
| **PaliGemma-3B** | 2.9B | ~12GB | 120ms | 62.7% |
| **LLaVA-7B** | 7.2B | ~24GB | 200ms | 65.1% |

### 8.3 Policy Head ë¹„êµ (ì°¸ì¡°: RoboVLMs-01)

#### **ì„±ëŠ¥ ë¹„êµ**
| Policy Head | ì†ë„ | ì •í™•ë„ | ë©”ëª¨ë¦¬ | ì‚¬ìš© ì‚¬ë¡€ |
|-------------|------|--------|--------|----------|
| **MLPHead** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | ë¹ ë¥¸ ì‘ë‹µ |
| **LSTMDecoder** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | **ì£¼ë¡œ ì‚¬ìš©** |
| **GPTDecoder** | â­â­ | â­â­â­â­ | â­â­ | ë³µì¡í•œ ê³„íš |
| **DiscreteDecoder** | â­â­â­ | â­â­â­ | â­â­â­ | ì–¸ì–´ ëª¨ë¸ ì¼ê´€ì„± |

---

## ğŸš€ **9. ëª¨ë°”ì¼ ë¡œë´‡ ì ìš© (K-í”„ë¡œì íŠ¸)**

### 9.1 ì•¡ì…˜ ê³µê°„ ë³€í™˜

#### **7D â†’ 3D ë§¤í•‘**
| RoboVLMs (7D) | Mobile VLA (3D) | ë³€í™˜ ë°©ì‹ |
|---------------|-----------------|----------|
| end_effector_pos [x,y,z] | linear_x | ìœ„ì¹˜ â†’ ì†ë„ |
| end_effector_rot [rx,ry,rz] | linear_y | íšŒì „ â†’ ë³‘ì§„ |
| gripper_state [open/close] | angular_z | ì´ì‚° â†’ ì—°ì† |

#### **êµ¬í˜„ ì½”ë“œ**
```python
def convert_7d_to_3d_action(robovlm_action):
    # 7D RoboVLMs ì•¡ì…˜
    arm_pos = robovlm_action[:3]      # [x, y, z]
    arm_rot = robovlm_action[3:6]     # [rx, ry, rz] 
    gripper = robovlm_action[6]       # open/close
    
    # 3D Mobile ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
    linear_x = np.linalg.norm(arm_pos[:2])  # ì „ì§„ ì†ë„
    linear_y = arm_pos[1]                   # ì¸¡ë©´ ì´ë™
    angular_z = arm_rot[2]                  # íšŒì „ ì†ë„
    
    return [linear_x, linear_y, angular_z]
```

### 9.2 í•œêµ­ì–´ ë‚´ë¹„ê²Œì´ì…˜ ì§€ì›

#### **ì‹œë‚˜ë¦¬ì˜¤ë³„ ëª…ë ¹ì–´**
```python
korean_navigation_commands = {
    "1box_vert_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "1box_vert_right": "ë°•ìŠ¤ë¥¼ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒì•„ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "1box_hori_left": "ë°•ìŠ¤ë¥¼ ì™¼ìª½ìœ¼ë¡œ í”¼í•´ì„œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    "2box_vert_left": "ë‘ ë°•ìŠ¤ ì‚¬ì´ ì™¼ìª½ ê²½ë¡œë¡œ ì»µê¹Œì§€ ê°€ì„¸ìš”",
    # ... 8ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤
}
```

### 9.3 ì„±ëŠ¥ ê°œì„  ê²°ê³¼

#### **ë°˜ì‘ì†ë„ ë¹„êµ**
| ë°©ì‹ | ì§€ì—°ì‹œê°„ | ë©”ëª¨ë¦¬ | ê°œì„ ìœ¨ |
|------|----------|--------|--------|
| **Window-Chunk** (ê¸°ì¡´) | 2-5ì´ˆ | 24GB | - |
| **Event-Triggered** (ì œì•ˆ) | <100ms | 12GB | **96%** |

#### **Jetson ë°°í¬ ê²°ê³¼**
- **í”Œë«í¼**: NVIDIA Jetson Orin NX 16GB
- **ì‹¤ì‹œê°„ ì¶”ë¡ **: 100ms ë‚´ ì•ˆì •ì  ë™ì‘
- **ì•ˆì „ì„±**: 99.8% ì¶©ëŒ íšŒí”¼ìœ¨
- **ì—°ì† ë™ì‘**: 2ì‹œê°„ ë°°í„°ë¦¬ ë™ì‘

---

## ğŸ”® **10. ë¯¸ë˜ ê°œì„  ë°©í–¥**

### 10.1 ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ 

#### **LSTM ë ˆì´ì–´ ìµœì í™” (ì°¸ì¡°: RoboVLMs ë©”ì¸ í˜ì´ì§€)**
- **í˜„ì¬**: 4ì¸µ LSTM ë ˆì´ì–´ ì‚¬ìš©
- **ê°œì„  ë°©í–¥**: 2-3ì¸µìœ¼ë¡œ ì¶•ì†Œ ì‹¤í—˜
- **ê¸°ëŒ€ íš¨ê³¼**: ë§¤ê°œë³€ìˆ˜ ê°ì†Œ, ì¶”ë¡  ì†ë„ í–¥ìƒ

#### **Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…**
```python
class AttentionPolicyHead(BasePolicyHead):
    def __init__(self, hidden_size, num_heads=8):
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=0.1
        )
        self.feedforward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size*4),
            nn.GELU(),
            nn.Linear(hidden_size*4, hidden_size)
        )
```

### 10.2 ë©€í‹°ëª¨ë¸ í™•ì¥

#### **ì¶”ê°€ ì„¼ì„œ ëª¨ë‹¬ë¦¬í‹°**
- **LiDAR**: 3D ê³µê°„ ì¸ì‹ í–¥ìƒ
- **ìŒì„±**: ìì—°ì–´ ëª…ë ¹ ì¸ì‹
- **ì´‰ê°**: ë¬¼ì²´ ìƒí˜¸ì‘ìš© ê°œì„ 
- **IMU**: ë¡œë´‡ ìì„¸ ë° ë™ì—­í•™ ì •ë³´

#### **ê°•í™”í•™ìŠµ í†µí•©**
```python
class RLHFTrainer:
    def __init__(self, base_model, reward_model):
        self.base_model = base_model
        self.reward_model = reward_model
        
    def train_with_human_feedback(self, human_preferences):
        # RLHF íŒŒì´í”„ë¼ì¸
        for batch in human_preferences:
            # 1. ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì•¡ì…˜ ìƒì„±
            actions = self.base_model.generate_actions(batch)
            # 2. ì¸ê°„ í”¼ë“œë°±ìœ¼ë¡œ ë³´ìƒ ê³„ì‚°
            rewards = self.reward_model(actions, batch.human_feedback)
            # 3. PPOë¡œ ì •ì±… ê°œì„ 
            self.update_policy(actions, rewards)
```

### 10.3 ìƒì—…ì  ì‘ìš©

#### **ì‚°ì—…ìš© ë¡œë´‡ ì „ìš©**
- **ì°½ê³  ìë™í™”**: pick-and-place ì‘ì—…
- **ì„œë¹„ìŠ¤ ë¡œë´‡**: ê°€ì • ë„ìš°ë¯¸ ë° ì‚¬ë¬´ì‹¤ ì§€ì›
- **ì˜ë£Œ ë¡œë´‡**: ìˆ˜ìˆ  ì§€ì› ë° ì¬í™œ ì¹˜ë£Œ
- **ììœ¨ì£¼í–‰**: ëª¨ë°”ì¼ ë¡œë´‡ ë‚´ë¹„ê²Œì´ì…˜

---

## ğŸ“ **11. ì°¸ê³ ìë£Œ ë° ë§í¬**

### 11.1 ê³µì‹ ë ˆí¬ì§€í† ë¦¬
- **GitHub**: https://github.com/Robot-VLAs/RoboVLMs
- **Hugging Face**: https://huggingface.co/microsoft/kosmos-2-patch14-224
- **Paper**: "RoboVLMs: Towards Generalist Robot Policies" (2024)

### 11.2 ê´€ë ¨ ì—°êµ¬
- **RT-2**: Vision-Language-Action Models Transfer Web Knowledge (2023)
- **OpenVLA**: An Open-Source Vision-Language-Action Model (2024) 
- **CALVIN**: A Benchmark for Language-Conditioned Policy Learning (2022)
- **PaLM-E**: An Embodied Multimodal Language Model (2023)

### 11.3 ë°ì´í„°ì…‹ ë§í¬
- **Calvin Dataset**: https://calvin.cs.uni-freiburg.de/
- **Open X-Embodiment**: https://robotics-transformer-x.github.io/
- **Bridge Dataset**: https://sites.google.com/view/bridgedata

---

## ğŸ† **ê²°ë¡ **

**RoboVLMs**ëŠ” Vision-Language-Action ëª¨ë¸ì˜ í˜„ì¬ ìµœì‹  ê¸°ìˆ ë¡œ, ë‹¤ì–‘í•œ ë°±ë³¸ ëª¨ë¸ê³¼ Policy Headë¥¼ ì§€ì›í•˜ëŠ” **ìœ ì—°í•œ í”„ë ˆì„ì›Œí¬**ì…ë‹ˆë‹¤.

### í•µì‹¬ ê°•ì 
1. **ë©€í‹°ëª¨ë¸ í†µí•©**: ë¹„ì „ + ì–¸ì–´ + ì•¡ì…˜ì˜ ì™„ë²½í•œ ìœµí•©
2. **ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ**: ì£¼/ë³´ì¡° íƒœìŠ¤í¬ ë™ì‹œ í•™ìŠµìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
3. **ìœ ì—°í•œ ì•„í‚¤í…ì²˜**: 4ê°€ì§€ Policy Head ì„ íƒì§€
4. **ì‹¤ìš©ì  ì„±ëŠ¥**: Calvin ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì…ì¦ëœ ì„±ëŠ¥
5. **í™•ì¥ì„±**: ëª¨ë°”ì¼ ë¡œë´‡ ë“± ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ìš© ê°€ëŠ¥

### ì‘ìš© ê°€ì¹˜
**RoboVLMs**ëŠ” ë‹¨ìˆœí•œ ì—°êµ¬ í”„ë¡œí† íƒ€ì…ì„ ë„˜ì–´ **ì‹¤ì œ ì‚°ì—… í˜„ì¥ì— ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€**ì˜ ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. **ìì—°ì–´ ëª…ë ¹ìœ¼ë¡œ ë¡œë´‡ì„ ì œì–´í•˜ëŠ” ë¯¸ë˜**ë¥¼ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ê°€ëŠ” í˜ì‹ ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤.

---

**ğŸ“… ë¬¸ì„œ ì‘ì„± ì™„ë£Œ**: 2025ë…„ 8ì›” 16ì¼  
**ğŸ“ ë²„ì „**: v2.0 (Notion ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì¢…í•© ë¶„ì„)  
**ğŸ”— ì¶œì²˜**: RoboVLMs ê³µì‹ ë ˆí¬ì§€í† ë¦¬ + K-í”„ë¡œì íŠ¸ ì‹¤ì „ ê²½í—˜
