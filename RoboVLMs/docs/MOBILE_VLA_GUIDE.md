# Mobile VLA with RoboVLMs - ì™„ì „ ê°€ì´ë“œ

## ê°œìš”

ì´ ê°€ì´ë“œëŠ” RoboVLMs í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Mobile VLA ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ë°°í¬í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ëª©ì°¨

1. [íƒœìŠ¤í¬ ì •ì˜](#íƒœìŠ¤í¬-ì •ì˜)
2. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
3. [ë°ì´í„°ì…‹ ì¤€ë¹„](#ë°ì´í„°ì…‹-ì¤€ë¹„)
4. [í•™ìŠµ ì‹¤í–‰](#í•™ìŠµ-ì‹¤í–‰)
5. [ì¶”ë¡  ì‹¤í–‰](#ì¶”ë¡ -ì‹¤í–‰)
6. [ROS2 í†µí•©](#ros2-í†µí•©)
7. [Docker ë°°í¬](#docker-ë°°í¬)
8. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## íƒœìŠ¤í¬ ì •ì˜

### ğŸ¯ **í•µì‹¬ íƒœìŠ¤í¬: ëª¨ë°”ì¼ ë¡œë´‡ ë„¤ë¹„ê²Œì´ì…˜**

Mobile VLAì˜ í•µì‹¬ íƒœìŠ¤í¬ëŠ” **ëª¨ë°”ì¼ ë¡œë´‡ì˜ ì¥ì• ë¬¼ íšŒí”¼ ë„¤ë¹„ê²Œì´ì…˜**ì…ë‹ˆë‹¤.

#### **8ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤**
- **1ë°•ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤**: 1ê°œ ë°•ìŠ¤ ì¥ì• ë¬¼ì„ í”¼í•´ ëª©í‘œ ì§€ì  ë„ë‹¬
- **2ë°•ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤**: 2ê°œ ë°•ìŠ¤ ì¥ì• ë¬¼ì„ í”¼í•´ ëª©í‘œ ì§€ì  ë„ë‹¬
- **ê²½ë¡œ ì„ íƒ**: ê° ì‹œë‚˜ë¦¬ì˜¤ë§ˆë‹¤ "left" ë˜ëŠ” "right" ê²½ë¡œ ì„ íƒ

#### **ì•¡ì…˜ ê³µê°„ (2D)**
```python
action_space = {
    'linear_x': [-1.15, 1.15],    # ì „ì§„/í›„ì§„ (m/s)
    'linear_y': [-1.15, 1.15],    # ì¢Œìš° ì´ë™ (m/s)
    'angular_z': [-1.15, 1.15],   # íšŒì „ (rad/s)
    'action_type': [0, 3]         # ì•¡ì…˜ íƒ€ì… (0:ì´ë™, 1:íšŒì „, 2:ì •ì§€, 3:íŠ¹ìˆ˜)
}
```

#### **ì…ë ¥ ë°ì´í„°**
- **ì‹œê°ì **: RGB ì¹´ë©”ë¼ ì´ë¯¸ì§€ (720Ã—1280Ã—3)
- **ì–¸ì–´ì **: ìì—°ì–´ ëª…ë ¹ ("Navigate around obstacles")
- **ë¡œë´‡ ìƒíƒœ**: ìœ„ì¹˜, ì†ë„, ë°©í–¥ ë“± 15ì°¨ì› ìƒíƒœ ë²¡í„°

ìì„¸í•œ íƒœìŠ¤í¬ ì •ì˜ëŠ” [MOBILE_VLA_TASK_DEFINITION.md](./MOBILE_VLA_TASK_DEFINITION.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

---

## í™˜ê²½ ì„¤ì •

### 1. í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- **í•˜ë“œì›¨ì–´**:
  - GPU: NVIDIA GPU with 16GB+ VRAM (RTX 4090, A6000, etc.)
  - RAM: 32GB+
  - Storage: 100GB+ (for models and datasets)

- **ì†Œí”„íŠ¸ì›¨ì–´**:
  - Ubuntu 22.04
  - CUDA 11.8+
  - Python 3.10+
  - ROS2 Humble (for inference)
  - Docker (optional, for containerized deployment)

### 2. RoboVLMs ì„¤ì¹˜

```bash
# Clone repository
cd /home/billy/25-1kp/vla
cd RoboVLMs

# Install dependencies
pip install -r requirements.txt

# Install RoboVLMs
pip install -e .
```

### 3. ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# PyTorch Lightning
pip install pytorch-lightning

# Transformers
pip install transformers>=4.40.0

# Image processing
pip install pillow opencv-python

# ROS2 (if not installed)
sudo apt install ros-humble-desktop
sudo apt install python3-colcon-common-extensions
```

---

## ë°ì´í„°ì…‹ ì¤€ë¹„

### 1. Mobile VLA ë°ì´í„°ì…‹ êµ¬ì¡°

```
ROS_action/mobile_vla_dataset/
â”œâ”€â”€ episode_20250101_120000_1box_vert_left_50cm.h5
â”œâ”€â”€ episode_20250101_120100_1box_vert_right_50cm.h5
â”œâ”€â”€ episode_20250101_120200_2box_hori_left_100cm.h5
â””â”€â”€ ...
```

ê° `.h5` íŒŒì¼ êµ¬ì¡°:
```
/observations/images  : (T, H, W, 3) RGB images
/action              : (T, 4) actions [linear_x, linear_y, angular_z, action_type]
/episode_metadata    : scenario, distance, etc.
```

### 2. ë°ì´í„°ì…‹ í†µê³„ í™•ì¸

```bash
python3 -c "
import h5py
import numpy as np
from pathlib import Path

data_dir = Path('../ROS_action/mobile_vla_dataset')
h5_files = list(data_dir.glob('*.h5'))

print(f'Total episodes: {len(h5_files)}')

total_frames = 0
for h5_path in h5_files:
    with h5py.File(h5_path, 'r') as f:
        total_frames += len(f['observations/images'])

print(f'Total frames: {total_frames}')
print(f'Average frames per episode: {total_frames / len(h5_files):.1f}')
"
```

---

## í•™ìŠµ ì‹¤í–‰

### 1. í•™ìŠµ ì„¤ì • í™•ì¸

ì„¤ì • íŒŒì¼: `configs/mobile_vla/train_mobile_vla_full_ft.json`

ì£¼ìš” íŒŒë¼ë¯¸í„°:
```json
{
  "task_name": "mobile_vla_full_finetune",
  "model": "kosmos",
  "window_size": 8,
  "fwd_pred_next_n": 1,
  "batch_size": 4,
  "learning_rate": 1e-5,
  "max_epochs": 50,
  "train_setup": {
    "freeze_backbone": false,
    "train_vision": true,
    "lora_enable": false,
    "gradient_checkpointing": true
  },
  "act_head": {
    "type": "LSTMDecoder",
    "num_layers": 4,
    "action_dim": 4,
    "action_space": "continuous"
  }
}
```

### 2. í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰ (ê¶Œì¥)

ë¨¼ì € í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸:

```bash
cd RoboVLMs
./scripts/run_mobile_vla_train.sh --test
```

ì˜ˆìƒ ì¶œë ¥:
```
Found 132 .h5 files in dataset
Train dataset: 2100 samples
Val dataset: 276 samples
Model created: RoboKosMos_MobileVLA
Total parameters: 1,234,567,890
Trainable parameters: 1,234,567,890
Starting training...
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00]
```

### 3. ì „ì²´ í•™ìŠµ ì‹¤í–‰

```bash
./scripts/run_mobile_vla_train.sh
```

í•™ìŠµ ëª¨ë‹ˆí„°ë§:
```bash
# TensorBoard
tensorboard --logdir runs/mobile_vla/logs

# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f runs/mobile_vla/logs/mobile_vla_full_finetune/version_0/events.out.tfevents.*
```

### 4. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

```bash
./scripts/run_mobile_vla_train.sh --resume runs/mobile_vla/checkpoints/mobile_vla-epoch=10-val_loss=0.1234.ckpt
```

### 5. í•™ìŠµ ê²°ê³¼ í™•ì¸

```bash
ls -lh runs/mobile_vla/checkpoints/
```

ì˜ˆìƒ ì¶œë ¥:
```
mobile_vla-epoch=10-val_loss=0.1234.ckpt  (6.9GB)
mobile_vla-epoch=20-val_loss=0.0987.ckpt  (6.9GB)
mobile_vla-last.ckpt                       (6.9GB)
```

---

## ì¶”ë¡  ì‹¤í–‰

### 1. í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ROS2 ì—†ì´)

```bash
./scripts/run_mobile_vla_inference.sh \
  --checkpoint runs/mobile_vla/checkpoints/mobile_vla-best.ckpt \
  --device cuda
```

ì˜ˆìƒ ì¶œë ¥:
```
Mobile VLA Inference initialized
Device: cuda
Window size: 8
Running in test mode (no ROS2)
Predicted action: [0.5, -0.2, 0.1, 1.0]
Action chunk shape: (1, 4)
```

### 2. Python API ì‚¬ìš©

```python
from eval.mobile_vla.inference_wrapper import MobileVLAInference
import numpy as np

# Initialize
inference = MobileVLAInference(
    checkpoint_path='runs/mobile_vla/checkpoints/mobile_vla-best.ckpt',
    config_path='configs/mobile_vla/train_mobile_vla_full_ft.json',
    device='cuda'
)

# Predict
image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
instruction = "Navigate around obstacles"

result = inference.predict(image, instruction)

print(f"Action: {result['action']}")
# Output: Action: [0.5, -0.2, 0.1, 1.0]
```

---

## ROS2 í†µí•©

### 1. ROS2 í™˜ê²½ ì„¤ì •

```bash
source /opt/ros/humble/setup.bash
export ROS_DOMAIN_ID=42
export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp
```

### 2. ROS2 ë…¸ë“œ ì‹¤í–‰

```bash
./scripts/run_mobile_vla_inference.sh \
  --checkpoint runs/mobile_vla/checkpoints/mobile_vla-best.ckpt \
  --ros2
```

### 3. ROS2 í† í”½ í™•ì¸

```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ
ros2 topic list
```

ì˜ˆìƒ ì¶œë ¥:
```
/camera/image_raw
/vla_text_command
/cmd_vel
```

### 4. í…ŒìŠ¤íŠ¸ ëª…ë ¹ ì „ì†¡

```bash
# ì¹´ë©”ë¼ ì´ë¯¸ì§€ í¼ë¸”ë¦¬ì‹œ (ì˜ˆì‹œ)
ros2 run image_publisher image_publisher_node /path/to/image.jpg

# í…ìŠ¤íŠ¸ ëª…ë ¹ ì „ì†¡
ros2 topic pub /vla_text_command std_msgs/msg/String \
  "data: 'Navigate around obstacles'"

# cmd_vel í™•ì¸
ros2 topic echo /cmd_vel
```

---

## Docker ë°°í¬

### 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
cd RoboVLMs
docker build -t robovlms-mobile-vla:latest .
```

### 2. Docker Composeë¡œ í•™ìŠµ ì‹¤í–‰

```bash
docker-compose -f docker-compose-mobile-vla.yml up train_mobile_vla
```

### 3. Docker Composeë¡œ ì¶”ë¡  ì‹¤í–‰

```bash
docker-compose -f docker-compose-mobile-vla.yml up inference_mobile_vla
```

### 4. í…ŒìŠ¤íŠ¸ ëª¨ë“œ

```bash
docker-compose -f docker-compose-mobile-vla.yml up test_mobile_vla
```

---

## ë¬¸ì œ í•´ê²°

### 1. CUDA Out of Memory

**ì¦ìƒ**: `RuntimeError: CUDA out of memory`

**í•´ê²°ì±…**:
```json
// configs/mobile_vla/train_mobile_vla_full_ft.json
{
  "batch_size": 2,  // 4 â†’ 2ë¡œ ê°ì†Œ
  "train_setup": {
    "gradient_checkpointing": true,
    "precision": "bf16-mixed"
  },
  "trainer": {
    "accumulate_grad_batches": 2  // Effective batch size = 2 * 2 = 4
  }
}
```

### 2. ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨

**ì¦ìƒ**: `No .h5 files found in dataset`

**í•´ê²°ì±…**:
```bash
# ë°ì´í„° ê²½ë¡œ í™•ì¸
ls -la ../ROS_action/mobile_vla_dataset/*.h5

# ì„¤ì • íŒŒì¼ ìˆ˜ì •
vim configs/mobile_vla/train_mobile_vla_full_ft.json
# "data_dir": "/absolute/path/to/mobile_vla_dataset"
```

### 3. ROS2 í† í”½ ì—°ê²° ì•ˆë¨

**ì¦ìƒ**: `No publishers on /camera/image_raw`

**í•´ê²°ì±…**:
```bash
# ROS_DOMAIN_ID í™•ì¸
echo $ROS_DOMAIN_ID

# ë„¤íŠ¸ì›Œí¬ ëª¨ë“œ í™•ì¸
docker-compose -f docker-compose-mobile-vla.yml config | grep network_mode
# network_mode: host í™•ì¸

# DDS ì„¤ì • í™•ì¸
export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp
```

### 4. ëª¨ë¸ ì„±ëŠ¥ ë‚®ìŒ

**ì¦ìƒ**: Validation lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**:
1. í•™ìŠµë¥  ì¡°ì •:
   ```json
   "learning_rate": 5e-6  // 1e-5 â†’ 5e-6
   ```

2. Warmup ì¶”ê°€:
   ```json
   "warmup_epochs": 5
   ```

3. ë°ì´í„° ì¦ê°• í™œì„±í™”:
   ```json
   "train_dataset": {
     "augmentation": {
       "color_jitter": 0.2,
       "gaussian_noise": 0.02
     }
   }
   ```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ í•™ìŠµ ì‹œê°„ (RTX 4090)

| Epochs | Batch Size | Time     | Final Val Loss |
|--------|-----------|----------|----------------|
| 10     | 4         | ~2 hours | 0.15           |
| 50     | 4         | ~10 hours| 0.08           |

### ì¶”ë¡  ì†ë„

| Device       | Latency (ms) | FPS  |
|--------------|--------------|------|
| RTX 4090     | 50           | 20   |
| Jetson Orin  | 150          | 6.7  |

---

## ë‹¤ìŒ ë‹¨ê³„

1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: Learning rate, batch size, window size ìµœì í™”
2. **ë°ì´í„° ì¦ê°•**: ë” ë§ì€ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜ì§‘
3. **ëª¨ë¸ ì••ì¶•**: Quantization, Pruningìœ¼ë¡œ Jetson ë°°í¬ ìµœì í™”
4. **ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸**: ì‹œë®¬ë ˆì´ì…˜ â†’ ì‹¤ì œ í™˜ê²½ ì „í™˜

---

## ì°¸ê³  ìë£Œ

- [RoboVLMs ë…¼ë¬¸](https://arxiv.org/abs/2412.04139)
- [RoboVLMs GitHub](https://github.com/RoboVLMs/RoboVLMs)
- [PyTorch Lightning ë¬¸ì„œ](https://lightning.ai/docs/pytorch/stable/)
- [ROS2 Humble ë¬¸ì„œ](https://docs.ros.org/en/humble/)

---

## ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” RoboVLMs ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

