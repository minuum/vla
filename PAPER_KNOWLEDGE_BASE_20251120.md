# ğŸ”¬ Mobile VLA ë…¼ë¬¸ ì‹¤í—˜ ë° ì•„í‚¤í…ì²˜ ì§€ì‹ ë² ì´ìŠ¤

## 1. ğŸ›ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ë°ì´í„° í”Œë¡œìš° (Architecture & Data Flow)

### ğŸ”„ 7DOF Manipulation vs 2DOF Navigation ë¹„êµ ë¶„ì„

**í•µì‹¬ ì§ˆë¬¸**: RoboVLMsëŠ” ì›ë˜ 7DOF (6D arm + 1D gripper) manipulationì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆëŠ”ë°, Mobile VLAì—ì„œëŠ” 2DOF navigationìœ¼ë¡œ ì–´ë–»ê²Œ ë³€í™˜ë˜ëŠ”ê°€?

#### ğŸ“Š 7DOF Manipulation íƒœìŠ¤í¬ (ì›ë³¸ RoboVLMs)

**íƒœìŠ¤í¬**: CALVIN ë°ì´í„°ì…‹ ê¸°ë°˜ ë¡œë´‡ íŒ” ì¡°ì‘ (Manipulation)

**ì•¡ì…˜ êµ¬ì¡°**:
- **ì°¨ì›**: 7D `[x, y, z, roll, pitch, yaw, gripper]`
- **íƒ€ì…**: ì—°ì† (6D arm) + ì´ì‚° (1D gripper)
- **ë²”ìœ„**: 
  - Arm: `[-0.5, 0.5]` (x), `[-0.3, 0.3]` (y), `[-0.4, 0.4]` (z), `[-Ï€, Ï€]` (roll, yaw), `[-Ï€/2, Ï€/2]` (pitch)
  - Gripper: `[0, 1]` (binary)

**ë°ì´í„° ì²˜ë¦¬ íë¦„**:

**1ë‹¨ê³„: ë°ì´í„° ë¡œë”©** (`calvin_dataset.py` **356ë²ˆì§¸ ì¤„**)
```python
# Calvin ë°ì´í„°ì…‹ì—ì„œ ì•¡ì…˜ ë¡œë“œ
seq_acts = process_actions(episode, self.observation_space, self.transforms)
# observation_space["actions"] = ["rel_actions"]  # ìƒëŒ€ ì¢Œí‘œê³„ ì•¡ì…˜
# Shape: (T, 7) - [x, y, z, roll, pitch, yaw, gripper]
```

**2ë‹¨ê³„: ì•¡ì…˜ ì •ê·œí™”** (`calvin_dataset.py` **826-828ë²ˆì§¸ ì¤„**)
```python
# collaterì—ì„œ ì •ê·œí™”
s["actions"] = normalize_action(
    s["actions"], self.norm_min, self.norm_max, maintain_last=True
)
# maintain_last=True: gripper ê°’ ìœ ì§€ (ì •ê·œí™” ì•ˆ í•¨)
# Arm 6D: [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
# Gripper: ì›ë³¸ ê°’ ìœ ì§€ (0 ë˜ëŠ” 1)
```

**3ë‹¨ê³„: Gripper ì´ì§„í™”** (`calvin_dataset.py` **868ë²ˆì§¸ ì¤„**)
```python
# Gripperë¥¼ ì´ì§„ ê°’ìœ¼ë¡œ ë³€í™˜
action_tensors[..., -1] = ((action_tensors[..., -1] + 1) // 2).float()
# [-1, 1] ë²”ìœ„ â†’ [0, 1] ì´ì§„ ê°’
```

**4ë‹¨ê³„: Trainerì—ì„œ ë¶„ë¦¬** (`base_trainer.py` **422-426ë²ˆì§¸ ì¤„**)
```python
# BaseTrainer._process_batch()
arm_action = action[:, :, :6]  # (B, seq_len, 6) - [x, y, z, roll, pitch, yaw]
gripper_action = action[:, :, 6]  # (B, seq_len) - gripper
gripper_action = (gripper_action + 1.0) / 2  # [-1, 1] â†’ [0, 1]
gripper_action = gripper_action.long()  # ì´ì§„í™”
```

**5ë‹¨ê³„: Policy Head ì¶œë ¥** (`base_policy.py` **120ë²ˆì§¸ ì¤„**)
```python
# BasePolicyHead.loss() ì£¼ì„
# pred_action_logits: [bs, seq_len, chunck_size, 7]
# 1-6 refers to ee pose (end-effector pose: x, y, z, roll, pitch, yaw)
# 7 refers to gripper open/close
```

**6ë‹¨ê³„: Loss ê³„ì‚°** (`base_policy.py` **137-141ë²ˆì§¸ ì¤„**)
```python
# BasePolicyHead.loss()
pose_loss = torch.nn.functional.huber_loss(
    pred_action[..., :6], labels[0]  # Arm 6D: Huber Loss
)
gripper_loss = torch.nn.functional.binary_cross_entropy_with_logits(
    pred_action[..., -1], labels[1]  # Gripper 1D: Binary Cross Entropy
)
# ìµœì¢… Loss: loss_arm + arm_gripper_loss_ratio * loss_gripper
```

#### ğŸ“Š 2DOF Navigation íƒœìŠ¤í¬ (Mobile VLA)

**íƒœìŠ¤í¬**: ëª¨ë°”ì¼ ë¡œë´‡ ì£¼í–‰ (Navigation)

**ì•¡ì…˜ êµ¬ì¡°**:
- **ì°¨ì›**: 2D `[linear_x, linear_y]`
- **íƒ€ì…**: ì—°ì† (ì†ë„ ê¸°ë°˜)
- **ë²”ìœ„**: `[-1.15, 1.15]` (ì‹¤ì œ ì‚¬ìš© ë²”ìœ„, ì •ê·œí™” í›„ `[-1, 1]`)

**ë°ì´í„° ì²˜ë¦¬ íë¦„**:

**1ë‹¨ê³„: ë°ì´í„° ë¡œë”©** (`mobile_vla_h5_dataset.py` **163ë²ˆì§¸ ì¤„**)
```python
# HDF5 íŒŒì¼ì—ì„œ ì•¡ì…˜ ë¡œë“œ
action_2d = f['actions'][t][:2]  # linear_x, linear_yë§Œ ì‚¬ìš©
# ì›ë³¸: (18, 3) - [linear_x, linear_y, angular_z]
# ì‚¬ìš©: (18, 2) - [linear_x, linear_y]ë§Œ ì¶”ì¶œ
```

**2ë‹¨ê³„: ì•¡ì…˜ ì •ê·œí™”** (`mobile_vla_h5_dataset.py` **177-178ë²ˆì§¸ ì¤„**)
```python
# ì•¡ì…˜ ì •ê·œí™” [-1, 1] (2D ì•¡ì…˜ ê¸°ì¤€)
actions_tensor = torch.clamp(actions_tensor, -1.0, 1.0)
# Config: norm_min=-1.0, norm_max=1.0
```

**3ë‹¨ê³„: Trainerì—ì„œ ì§ì ‘ ì‚¬ìš©** (`mobile_vla_trainer.py` **49-54ë²ˆì§¸ ì¤„**)
```python
# MobileVLATrainer._process_batch()
action = batch["action"].cuda()  # (B, seq_len, 2) - [linear_x, linear_y]
velocity = action  # âœ… 7DOF ë¶„ë¦¬ ì—†ì´ ì§ì ‘ ì‚¬ìš©
gripper_action = None  # Mobile VLAëŠ” gripper ì—†ìŒ
```

**4ë‹¨ê³„: Policy Head ì¶œë ¥** (`mobile_vla_policy.py` **53-55ë²ˆì§¸ ì¤„**)
```python
# MobileVLALSTMDecoder.__init__()
self.velocities = MLPTanhHead(
    self.hidden_size * latent, fwd_pred_next_n * action_dim  # action_dim=2
)
# ì¶œë ¥: (B, seq_len, fwd_pred_next_n * 2) = (B, seq_len, 20)
```

**5ë‹¨ê³„: Loss ê³„ì‚°** (`mobile_vla_policy.py` **163-173ë²ˆì§¸ ì¤„**)
```python
# MobileVLALSTMDecoder.loss()
velocities = pred_action[0]  # (B, seq_len, chunk_size, 2)
velocity_labels = labels[0]  # (B, seq_len, chunk_size, 2)
loss_velocity = torch.nn.functional.huber_loss(
    velocities, velocity_labels  # 2Dë§Œ: Huber Loss
)
# Gripper Loss ì—†ìŒ
```

#### ğŸ“Š ìƒì„¸ ë¹„êµí‘œ

| ë‹¨ê³„ | 7DOF Manipulation (ì›ë³¸) | 2DOF Navigation (Mobile VLA) | ì°¨ì´ì  |
|:---|:---|:---|:---|
| **ë°ì´í„° ì†ŒìŠ¤** | CALVIN Dataset<br/>`rel_actions` (ìƒëŒ€ ì¢Œí‘œê³„) | Mobile VLA HDF5<br/>`actions[:2]` (ì ˆëŒ€ ì†ë„) | ë°ì´í„° í˜•ì‹ ë‹¤ë¦„ |
| **ì•¡ì…˜ ì°¨ì›** | `(T, 7)`<br/>`[x, y, z, roll, pitch, yaw, gripper]` | `(T, 2)`<br/>`[linear_x, linear_y]` | ì°¨ì› ìˆ˜ ê°ì†Œ |
| **ì •ê·œí™”** | `normalize_action(..., maintain_last=True)`<br/>Arm 6D: `[-1, 1]`<br/>Gripper: ì›ë³¸ ìœ ì§€ | `torch.clamp(..., -1.0, 1.0)`<br/>2D ëª¨ë‘: `[-1, 1]` | Gripper ì²˜ë¦¬ ë‹¤ë¦„ |
| **Trainer ë¶„ë¦¬** | `arm_action = action[:, :, :6]`<br/>`gripper_action = action[:, :, 6]` | `velocity = action`<br/>`gripper_action = None` | ë¶„ë¦¬ ë¡œì§ ì œê±° |
| **Policy Head** | `action_dim=7`<br/>Arm 6D + Gripper 1D | `action_dim=2`<br/>Velocity 2Dë§Œ | ì¶œë ¥ ì°¨ì› ê°ì†Œ |
| **Loss ê³„ì‚°** | `loss_arm` (Huber) + `loss_gripper` (BCE)<br/>`loss = loss_arm + ratio * loss_gripper` | `loss_velocity` (Huber)<br/>Gripper Loss ì—†ìŒ | Loss ì¢…ë¥˜ ê°ì†Œ |
| **ì•¡ì…˜ íƒ€ì…** | ì—°ì† (6D) + ì´ì‚° (1D) | ì—°ì† (2D) | ì´ì‚° ì•¡ì…˜ ì œê±° |

#### ğŸ” í•µì‹¬ ì°¨ì´ì  ìš”ì•½

1. **ì•¡ì…˜ ê³µê°„**: 7D (6D arm + 1D gripper) â†’ 2D (linear_x, linear_y)
2. **ì•¡ì…˜ íƒ€ì…**: ì—°ì†+ì´ì‚° í˜¼í•© â†’ ì—°ì†ë§Œ
3. **Loss ê³„ì‚°**: 2ê°œ Loss (arm + gripper) â†’ 1ê°œ Loss (velocity)
4. **Trainer ë¶„ë¦¬**: Arm/Gripper ë¶„ë¦¬ í•„ìš” â†’ ë¶„ë¦¬ ë¶ˆí•„ìš”
5. **ì •ê·œí™”**: Gripper ë³„ë„ ì²˜ë¦¬ â†’ ë‹¨ì¼ ì •ê·œí™”

### ğŸ”„ 7DOF â†’ 2DOF ë³€í™˜ ë©”ì»¤ë‹ˆì¦˜

#### 1ë‹¨ê³„: ë°ì´í„° ë¡œë”© ë‹¨ê³„ (7DOF â†’ 2DOF ìŠ¬ë¼ì´ì‹±)

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/data/mobile_vla_h5_dataset.py` **163ë²ˆì§¸ ì¤„**

```python
# HDF5 íŒŒì¼ì—ì„œ ì•¡ì…˜ ë¡œë“œ
action_2d = f['actions'][t][:2]  # linear_x, linear_yë§Œ ì‚¬ìš©
# ì›ë³¸ ë°ì´í„°: (18, 3) - [linear_x, linear_y, angular_z]
# ì‚¬ìš©: (18, 2) - [linear_x, linear_y]ë§Œ ì¶”ì¶œ
```

**ë³€í™˜ ê³¼ì •**:
- **ì…ë ¥**: HDF5 íŒŒì¼ì˜ `actions` ë°°ì—´ `(18, 3)` - `[linear_x, linear_y, angular_z]`
- **ì²˜ë¦¬**: `[:2]` ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ `angular_z` ì œê±°
- **ì¶œë ¥**: `(18, 2)` - `[linear_x, linear_y]`ë§Œ ì‚¬ìš©

#### 2ë‹¨ê³„: Config ì„¤ì • (2DOF ëª…ì‹œ)

**ìœ„ì¹˜**: `Mobile_VLA/configs/mobile_vla_20251114_lora.json` **81ë²ˆì§¸ ì¤„**

```json
"act_head": {
    "type": "MobileVLALSTMDecoder",
    "action_dim": 2,  // âœ… 2DOF ëª…ì‹œì  ì„¤ì •
    "fwd_pred_next_n": 10,
    "window_size": 8
}
```

**ì˜ë¯¸**: Policy Headê°€ 2ì°¨ì› ì•¡ì…˜ë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •

#### 3ë‹¨ê³„: Trainer ë‹¨ê³„ (7DOF ë¶„ë¦¬ ë¡œì§ ìš°íšŒ)

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/train/mobile_vla_trainer.py` **49-54ë²ˆì§¸ ì¤„**

```python
# MobileVLATrainer._process_batch() - BaseTrainer ì˜¤ë²„ë¼ì´ë“œ
if batch.get("action", None) is not None:
    action = batch["action"].cuda()  # (B, seq_len, 2) - [linear_x, linear_y]
    # 2D ì†ë„ë¥¼ velocityë¡œ ì§ì ‘ ì‚¬ìš© (gripper ì—†ìŒ)
    velocity = action  # âœ… 7DOF ë¶„ë¦¬ ì—†ì´ ì§ì ‘ ì‚¬ìš©
    gripper_action = None  # Mobile VLAëŠ” gripper ì—†ìŒ
```

**BaseTrainerì™€ì˜ ì°¨ì´ì ** (ì›ë³¸ 7DOF ì²˜ë¦¬):

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/train/base_trainer.py` **422-426ë²ˆì§¸ ì¤„**

```python
# BaseTrainer._process_batch() - 7DOF ë¶„ë¦¬
if action is not None:
    arm_action = action[:, :, :6]  # b,len,6 - ì²˜ìŒ 6ì°¨ì› (x,y,z,roll,pitch,yaw)
    gripper_action = action[:, :, 6]  # b,len - 7ë²ˆì§¸ ì°¨ì› (gripper)
    gripper_action = (gripper_action + 1.0) / 2
    gripper_action = gripper_action.long()
```

**ë¹„êµ**:
- **BaseTrainer**: `action[:, :, :6]` (arm) + `action[:, :, 6]` (gripper) ë¶„ë¦¬
- **MobileVLATrainer**: `action` ì „ì²´ë¥¼ `velocity`ë¡œ ì§ì ‘ ì‚¬ìš© (2Dì´ë¯€ë¡œ ë¶„ë¦¬ ë¶ˆí•„ìš”)

#### 4ë‹¨ê³„: Policy Head ì¶œë ¥ (2DOF ìƒì„±)

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/model/policy_head/mobile_vla_policy.py` **53-55ë²ˆì§¸ ì¤„**

```python
# MobileVLALSTMDecoder.__init__()
self.velocities = MLPTanhHead(
    self.hidden_size * latent, fwd_pred_next_n * action_dim  # action_dim=2
)
# ì¶œë ¥: (B, seq_len, fwd_pred_next_n * 2) = (B, seq_len, 20)
```

**Forward ì¶œë ¥** (138ë²ˆì§¸ ì¤„):

```python
# MobileVLALSTMDecoder.forward()
velocities = rearrange(velocities, "b l (n d) -> b l n d", n=self.fwd_pred_next_n, d=self.action_dim)
# (B, seq_len, 20) -> (B, seq_len, 10, 2) - 10ê°œ ì²­í¬, ê°ê° 2D ì†ë„
return velocities, None  # âœ… gripperëŠ” None ë°˜í™˜
```

#### 5ë‹¨ê³„: Loss ê³„ì‚° (2DOFë§Œ ì²˜ë¦¬)

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/model/policy_head/mobile_vla_policy.py` **158-163ë²ˆì§¸ ì¤„**

```python
# MobileVLALSTMDecoder.loss()
velocities = pred_action[0]  # (B, seq_len, chunk_size, 2) - [linear_x, linear_y]
velocity_labels = labels[0]  # (B, seq_len, chunk_size, 2) - Ground Truth

# Huber Loss ê³„ì‚° (2Dë§Œ)
loss_velocity = torch.nn.functional.huber_loss(velocities, velocity_labels)
```

**BasePolicyHeadì™€ì˜ ì°¨ì´ì ** (ì›ë³¸ 7DOF Loss):

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/model/policy_head/base_policy.py` **137-141ë²ˆì§¸ ì¤„**

```python
# BasePolicyHead.loss() - 7DOF ë¶„ë¦¬ ì²˜ë¦¬
pose_loss = torch.nn.functional.huber_loss(pred_action[..., :6], labels[0])  # Arm 6D
gripper_loss = torch.nn.functional.binary_cross_entropy_with_logits(
    pred_action[..., -1], labels[1]  # Gripper 1D
)
```

**ë¹„êµ**:
- **BasePolicyHead**: `pred_action[..., :6]` (arm) + `pred_action[..., -1]` (gripper) ë¶„ë¦¬ ê³„ì‚°
- **MobileVLALSTMDecoder**: `pred_action[0]` ì „ì²´ë¥¼ 2D velocityë¡œ ì²˜ë¦¬ (ë¶„ë¦¬ ë¶ˆí•„ìš”)

#### ğŸ“Š ì „ì²´ ë³€í™˜ íŒŒì´í”„ë¼ì¸ ìš”ì•½

```mermaid
graph LR
    A["HDF5 Data<br/>(18, 3)<br/>[linear_x, linear_y, angular_z]"] --> B["Dataset<br/>[:2] ìŠ¬ë¼ì´ì‹±"]
    B --> C["(18, 2)<br/>[linear_x, linear_y]"]
    C --> D["Config<br/>action_dim: 2"]
    D --> E["MobileVLATrainer<br/>velocity = action<br/>(7DOF ë¶„ë¦¬ ìš°íšŒ)"]
    E --> F["MobileVLALSTMDecoder<br/>velocities MLP<br/>output: (B, seq_len, 10, 2)"]
    F --> G["Loss ê³„ì‚°<br/>Huber Loss<br/>(2Dë§Œ)"]
    
    style A fill:#ffe1e1
    style C fill:#e1f5ff
    style F fill:#fff4e1
    style G fill:#e1ffe1
```

**í•µì‹¬ í¬ì¸íŠ¸**:
1. **ë°ì´í„° ë‹¨ê³„**: HDF5ì—ì„œ `[:2]` ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ 3D â†’ 2D ë³€í™˜
2. **Config ë‹¨ê³„**: `action_dim: 2` ëª…ì‹œì  ì„¤ì •
3. **Trainer ë‹¨ê³„**: 7DOF ë¶„ë¦¬ ë¡œì§ ìš°íšŒ, 2D ì§ì ‘ ì‚¬ìš©
4. **Policy Head ë‹¨ê³„**: `action_dim=2`ë¡œ 2Dë§Œ ì¶œë ¥
5. **Loss ë‹¨ê³„**: 2D velocityë§Œ ê³„ì‚° (gripper Loss ì—†ìŒ)

### ğŸ¯ í•™ìŠµ ì‹œ 2DOF ì•¡ì…˜ì˜ ì •í™•í•œ ìœ„ì¹˜ (VLM ì „/í›„/ì¤‘)

**í•µì‹¬ ì§ˆë¬¸**: í•™ìŠµí•  ë•Œ 2DOF ì•¡ì…˜(Ground Truth)ì´ VLM ì „/í›„/ì¤‘ ì–´ë””ì— ë“¤ì–´ê°€ëŠ”ê°€?

#### âœ… ì •í™•í•œ ë‹µë³€: **VLM ì¶œë ¥ í›„, Action Headì—ì„œ Loss ê³„ì‚° ì‹œ ì‚¬ìš©**

#### ìƒì„¸ íë¦„ (ì½”ë“œ ê¸°ì¤€)

**1ë‹¨ê³„: Trainerì—ì„œ action_labels ì „ë‹¬**

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/train/mobile_vla_trainer.py` â†’ `base_trainer.py` **526ë²ˆì§¸ ì¤„** (validation) / **613ë²ˆì§¸ ì¤„** (training)

```python
# base_trainer.py training_step()
prediction = self.model.forward(
    rgb,
    language,
    action_labels=(arm_action_chunck, gripper_action_chunck),  # âœ… ì—¬ê¸°ì„œ ì „ë‹¬
    # MobileVLATrainerì—ì„œëŠ”: (velocity_chunck, None)
)
```

**2DOF ì•¡ì…˜ í˜•íƒœ**:
- `velocity_chunck`: `(B, 8, 10, 2)` - Ground Truth 2D ì†ë„
- `gripper_action_chunck`: `None` (Mobile VLAëŠ” gripper ì—†ìŒ)

**2ë‹¨ê³„: forward_continuousì—ì„œ action_labels ë°›ìŒ**

**ìœ„ì¹˜**: `RoboVLMs_upstream/robovlms/model/backbone/base_backbone.py` **1008ë²ˆì§¸ ì¤„**

```python
def forward_continuous(
    self,
    vision_x: torch.Tensor,
    lang_x: torch.Tensor,
    action_labels: Tuple[torch.Tensor, torch.Tensor] = None,  # âœ… ì—¬ê¸°ì„œ ë°›ìŒ
    # ...
):
```

**3ë‹¨ê³„: VLM ì…ë ¥ ì „ - Action Token ì£¼ì… (í•™ìŠµ ê°€ëŠ¥í•œ í† í°, ì‹¤ì œ ì•¡ì…˜ ê°’ ì•„ë‹˜)**

**ìœ„ì¹˜**: `base_backbone.py` **1114-1133ë²ˆì§¸ ì¤„**

```python
if action_space == "continuous":
    # Action Token (í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°) ì£¼ì…
    action_tokens = repeat(
        self.action_token,  # âœ… í•™ìŠµ ê°€ëŠ¥í•œ í† í° (ì‹¤ì œ ì•¡ì…˜ ê°’ ì•„ë‹˜!)
        "d -> b n d",
        b=multimodal_embeds.shape[0],
        n=self.latent_num,
    )
    # multimodal_embedsì— Action Token ì¶”ê°€
    multimodal_embeds = merge_multi_modal_input(
        multimodal_embeds,
        action_tokens,  # âœ… VLM ì…ë ¥ ì „ì— ì£¼ì…
        # ...
    )
```

**ì¤‘ìš”**: ì´ê²ƒì€ **í•™ìŠµ ê°€ëŠ¥í•œ í† í°**ì´ì§€, ì‹¤ì œ 2DOF ì•¡ì…˜ ê°’ì´ ì•„ë‹™ë‹ˆë‹¤!

**4ë‹¨ê³„: VLM í†µê³¼**

**ìœ„ì¹˜**: `base_backbone.py` **1145-1153ë²ˆì§¸ ì¤„**

```python
output = self.model(
    input_ids=None,
    attention_mask=multimodal_attention_mask,
    inputs_embeds=multimodal_embeds,  # âœ… Vision + Text + Action Token í¬í•¨
    output_hidden_states=True,
)
# output.hidden_states[-1]: (bs*seq_len, seq_length, hidden_size)
```

**5ë‹¨ê³„: VLM ì¶œë ¥ í›„ - action_hs ì¶”ì¶œ**

**ìœ„ì¹˜**: `base_backbone.py` **1409ë²ˆì§¸ ì¤„**

```python
# Action Token ìœ„ì¹˜ì˜ Hidden State ì¶”ì¶œ
action_hs = output_hs_reshaped[:, :, -self.latent_num:, :]
# Shape: (B, seq_len, latent_num, hidden_size) = (B, 8, 1, 2048)
```

**6ë‹¨ê³„: Action Headì—ì„œ action_labels ì‚¬ìš© (Loss ê³„ì‚°)**

**ìœ„ì¹˜**: `base_backbone.py` **1456-1458ë²ˆì§¸ ì¤„**

```python
# âœ… ì—¬ê¸°ì„œ action_labels (Ground Truth 2DOF) ì‚¬ìš©!
action_logits, action_loss = self._forward_action_head(
    action_hs,  # VLM ì¶œë ¥ì—ì„œ ì¶”ì¶œí•œ Hidden State
    action_labels,  # âœ… Ground Truth 2DOF ì•¡ì…˜ (VLM ì¶œë ¥ í›„ì— ì‚¬ìš©)
    action_mask
)
```

**7ë‹¨ê³„: _forward_action_head ë‚´ë¶€**

**ìœ„ì¹˜**: `base_backbone.py` **562-571ë²ˆì§¸ ì¤„**

```python
def _forward_action_head(self, action_tokens, action_labels, action_mask, **kwargs):
    # 1. Action Headë¡œ ì•¡ì…˜ ì˜ˆì¸¡
    action = self.act_head(
        action_tokens,  # action_hs
        actions=action_labels,  # âœ… Ground Truth ì „ë‹¬
        action_masks=action_mask,
        **kwargs
    )
    
    # 2. Loss ê³„ì‚° (action_labels ì‚¬ìš©)
    if action_labels is not None:
        action, action_labels, action_mask = self.act_head.get_labels(...)
        action_loss = self.act_head.loss(action, action_labels, action_mask)  # âœ… ì—¬ê¸°ì„œ ë¹„êµ
```

**8ë‹¨ê³„: MobileVLALSTMDecoder.lossì—ì„œ ì‹¤ì œ ë¹„êµ**

**ìœ„ì¹˜**: `mobile_vla_policy.py` **163-173ë²ˆì§¸ ì¤„**

```python
def loss(self, pred_action, labels, attention_mask=None):
    velocities = pred_action[0]  # ì˜ˆì¸¡: (B, 8, 10, 2)
    velocity_labels = labels[0]  # âœ… Ground Truth: (B, 8, 10, 2)
    
    # Huber Loss ê³„ì‚°
    loss_velocity = torch.nn.functional.huber_loss(
        velocities,      # ì˜ˆì¸¡ëœ 2DOF ì•¡ì…˜
        velocity_labels  # âœ… Ground Truth 2DOF ì•¡ì…˜ (ì—¬ê¸°ì„œ ë¹„êµ!)
    )
```

#### ğŸ“Š ì „ì²´ íë¦„ ë‹¤ì´ì–´ê·¸ë¨

```mermaid
sequenceDiagram
    participant T as Trainer
    participant B as BaseRoboVLM
    participant VLM as Kosmos-2 VLM
    participant AH as Action Head
    participant L as Loss Function
    
    Note over T: action_labels=(velocity_chunck, None)<br/>(B, 8, 10, 2)
    
    T->>B: forward(vision_x, lang_x, action_labels)
    
    Note over B: VLM ì…ë ¥ ì „
    B->>B: Action Token ì£¼ì…<br/>(í•™ìŠµ ê°€ëŠ¥í•œ í† í°,<br/>ì‹¤ì œ ì•¡ì…˜ ê°’ ì•„ë‹˜)
    B->>B: multimodal_embeds ìƒì„±<br/>(Vision + Text + Action Token)
    
    B->>VLM: inputs_embeds=multimodal_embeds
    Note over VLM: VLM í†µê³¼<br/>(Vision-Language Fusion)
    VLM->>B: output.hidden_states
    
    Note over B: VLM ì¶œë ¥ í›„
    B->>B: action_hs ì¶”ì¶œ<br/>(Action Token ìœ„ì¹˜ì˜ Hidden State)
    
    B->>AH: _forward_action_head(action_hs, action_labels)
    Note over AH: Action Head Forward
    AH->>AH: action = act_head(action_hs)<br/>â†’ ì˜ˆì¸¡: (B, 8, 10, 2)
    
    AH->>L: loss(action, action_labels)
    Note over L: âœ… ì—¬ê¸°ì„œ Ground Truth ì‚¬ìš©!<br/>velocities vs velocity_labels ë¹„êµ
    L->>AH: loss_velocity
    AH->>B: action_loss
    B->>T: loss
```

#### ğŸ¯ í•µì‹¬ ì •ë¦¬

| ë‹¨ê³„ | ìœ„ì¹˜ | 2DOF ì•¡ì…˜ ì—­í•  | VLM ê´€ê³„ |
|:---|:---|:---|:---|
| **1. Trainer ì „ë‹¬** | `base_trainer.py:526/613` | `action_labels` ì „ë‹¬ | - |
| **2. forward_continuous** | `base_backbone.py:1008` | `action_labels` ë°›ìŒ | - |
| **3. Action Token ì£¼ì…** | `base_backbone.py:1114-1133` | âŒ **ì‚¬ìš© ì•ˆ í•¨** (í•™ìŠµ ê°€ëŠ¥í•œ í† í°ë§Œ ì£¼ì…) | **VLM ì…ë ¥ ì „** |
| **4. VLM í†µê³¼** | `base_backbone.py:1145-1153` | âŒ **ì‚¬ìš© ì•ˆ í•¨** | **VLM ì¤‘** |
| **5. action_hs ì¶”ì¶œ** | `base_backbone.py:1409` | âŒ **ì‚¬ìš© ì•ˆ í•¨** | **VLM ì¶œë ¥ í›„** |
| **6. Action Head** | `base_backbone.py:1456-1458` | âœ… **Ground Truthë¡œ ì „ë‹¬** | **VLM ì¶œë ¥ í›„** |
| **7. Loss ê³„ì‚°** | `mobile_vla_policy.py:163-173` | âœ… **ì˜ˆì¸¡ê³¼ ë¹„êµ** | **VLM ì¶œë ¥ í›„** |

**ê²°ë¡ **:
- **2DOF ì•¡ì…˜ (Ground Truth)**: VLM **ì¶œë ¥ í›„**, Action Headì˜ Loss ê³„ì‚°ì—ì„œë§Œ ì‚¬ìš©ë¨
- **Action Token (í•™ìŠµ ê°€ëŠ¥í•œ í† í°)**: VLM **ì…ë ¥ ì „**ì— ì£¼ì…ë˜ì§€ë§Œ, ì‹¤ì œ ì•¡ì…˜ ê°’ì´ ì•„ë‹˜
- **VLMì€ ì•¡ì…˜ ê°’ì„ ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ**: Vision-Language ì •ë³´ë§Œ ì²˜ë¦¬í•˜ê³ , Action Headì—ì„œ ì•¡ì…˜ ì˜ˆì¸¡ ë° Loss ê³„ì‚°

**7DOF ë§¤ì¹­ ìœ„ì¹˜**:
- âŒ **ì‚¬ìš©í•˜ì§€ ì•ŠìŒ**: BaseTrainerì˜ 7DOF ë¶„ë¦¬ ë¡œì§ì€ MobileVLATrainerì—ì„œ ì˜¤ë²„ë¼ì´ë“œë˜ì–´ ìš°íšŒë¨
- âœ… **2DOF íŒŒì¸íŠœë‹**: Config â†’ Dataset â†’ Trainer â†’ Policy Head â†’ Loss ì „ì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ ì¼ê´€ë˜ê²Œ 2D ì²˜ë¦¬

### ì „ì²´ íŒŒì´í”„ë¼ì¸
ë…¼ë¬¸ì— ì„œìˆ ë  Mobile VLAì˜ ë°ì´í„° ì²˜ë¦¬ íë¦„ì…ë‹ˆë‹¤.

```mermaid
graph LR
    A[Image + Text Input] --> B[Kosmos-2 Backbone];
    B --> C{Action Token Injection};
    C --> D[Extract Action Hidden State];
    D --> E[LSTM Decoder];
    E --> F[Predicted Actions (Chunk)];
```

### ë°ì´í„° êµ¬ì¡° ë° í•™ìŠµ ê³¼ì • ì‹œê°í™”

#### ğŸ“Š 18í”„ë ˆì„ â†’ 10í”„ë ˆì„ ì•¡ì…˜ ìƒì„± ë©”ì»¤ë‹ˆì¦˜

**í•µì‹¬ êµ¬ì¡° í™•ì¸:**
- **ì…ë ¥**: 18ê°œ ì´ë¯¸ì§€ í”„ë ˆì„ (ì—í”¼ì†Œë“œ ì „ì²´)
- **Window Size**: 8 í”„ë ˆì„ (ê³¼ê±° íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸)
- **Chunk Size (fwd_pred_next_n)**: 10 í”„ë ˆì„ (ì˜ˆì¸¡í•  ë¯¸ë˜ ì•¡ì…˜)
- **Sliding Window**: `unfold` ì—°ì‚°ì„ í†µí•´ ì—¬ëŸ¬ ì‹œì ì—ì„œ ë™ì‹œì— í•™ìŠµ

```mermaid
graph TB
    subgraph "ì—í”¼ì†Œë“œ ë°ì´í„° (18 í”„ë ˆì„)"
        I0[Frame 0] --> I1[Frame 1] --> I2[Frame 2] --> I3[Frame 3] --> I4[Frame 4] --> I5[Frame 5] --> I6[Frame 6] --> I7[Frame 7]
        I7 --> I8[Frame 8] --> I9[Frame 9] --> I10[Frame 10] --> I11[Frame 11] --> I12[Frame 12] --> I13[Frame 13] --> I14[Frame 14] --> I15[Frame 15] --> I16[Frame 16] --> I17[Frame 17]
    end
    
    subgraph "Collater: Unfold Sliding Window"
        W0["Window t=0<br/>Frames 0-7<br/>(8 frames)"] --> C0["Chunk 0<br/>Actions 0-9<br/>(10 frames)"]
        W1["Window t=1<br/>Frames 1-8<br/>(8 frames)"] --> C1["Chunk 1<br/>Actions 1-10<br/>(10 frames)"]
        W2["Window t=2<br/>Frames 2-9<br/>(8 frames)"] --> C2["Chunk 2<br/>Actions 2-11<br/>(10 frames)"]
        W3["..."] --> C3["..."]
        W7["Window t=7<br/>Frames 7-14<br/>(8 frames)"] --> C7["Chunk 7<br/>Actions 7-16<br/>(10 frames)"]
    end
    
    I0 -.-> W0
    I7 -.-> W0
    I1 -.-> W1
    I8 -.-> W1
    
    style W0 fill:#e1f5ff
    style W1 fill:#e1f5ff
    style C0 fill:#fff4e1
    style C1 fill:#fff4e1
```

**Unfold ì—°ì‚° ê²°ê³¼:**
- **ì…ë ¥**: 
  - `image_tensors`: `(B, 18, C, H, W)` - 18ê°œ ì´ë¯¸ì§€ í”„ë ˆì„
  - `action_tensors`: `(B, 17, 2)` - 18ê°œ ì•¡ì…˜ì—ì„œ ë§ˆì§€ë§‰ 1ê°œ ì œê±° (`[:, :-1]`)
- **ì¶œë ¥**: 
  - `action_chunk`: `(B, 8, 10, 2)` - **8ê°œì˜ ì‹œì **ì—ì„œ ê°ê° 10í”„ë ˆì„ ì•¡ì…˜ ì˜ˆì¸¡
  - `image_chunk`: `(B, 8, 10, C, H, W)` - 8ê°œì˜ ì‹œì ì—ì„œ ê°ê° 10í”„ë ˆì„ ì´ë¯¸ì§€
- **ê³µì‹**: 
  - Action: `(17 - 10 + 1) = 8` ê°œì˜ í•™ìŠµ ìƒ˜í”Œ
  - Image: `(18 - 10 + 1 - 1) = 8` ê°œ (unfold í›„ `[:, 1:]` ì ìš©)

#### ğŸ”„ ê° ì‹œì  tì—ì„œì˜ í•™ìŠµ ê³¼ì • ìƒì„¸

```mermaid
sequenceDiagram
    participant D as Dataset
    participant C as Collater
    participant B as BaseRoboVLM
    participant L as LSTM Decoder
    participant LOSS as Loss Function
    
    Note over D: ì—í”¼ì†Œë“œ ë¡œë“œ (18 í”„ë ˆì„)
    D->>C: __getitem__(idx)<br/>18ê°œ ì´ë¯¸ì§€ + 18ê°œ ì•¡ì…˜
    
    Note over C: Unfold Sliding Window
    C->>C: action_tensors[:, :-1]<br/>â†’ (B, 17, 2) [ë§ˆì§€ë§‰ í”„ë ˆì„ ì œê±°]
    C->>C: action_tensors.unfold(1, 10, 1)<br/>â†’ (B, 8, 10, 2)
    C->>C: image_tensors.unfold(1, 10, 1)[:, 1:]<br/>â†’ (B, 8, 10, C, H, W)
    C->>C: image_tensors[:, :8]<br/>â†’ (B, 8, C, H, W) [Window]
    
    loop ê° ì‹œì  t (0~7)
        Note over B: ì‹œì  t ì²˜ë¦¬
        B->>B: Input: 8ê°œ ì´ë¯¸ì§€ (t~t+7)<br/>+ í…ìŠ¤íŠ¸ ëª…ë ¹ì–´
        B->>B: Kosmos-2 Vision Encoder<br/>â†’ Vision Features
        B->>B: Text Embedding + [LRN] Token<br/>â†’ Multimodal Embedding
        B->>B: Kosmos-2 Transformer<br/>â†’ Hidden States
        B->>B: Extract [LRN] Token Hidden State<br/>â†’ action_hs (B, 1, 1, 2048)
        
        B->>L: action_hs ì…ë ¥
        L->>L: LSTM Forward Pass<br/>(History Memory ê´€ë¦¬)
        L->>L: MLP Head<br/>â†’ Predicted Actions (B, 1, 10, 2)
        
        L->>LOSS: pred_actions (B, 1, 10, 2)
        LOSS->>LOSS: Ground Truth: action_chunk[t]<br/>(B, 1, 10, 2)
        LOSS->>LOSS: Huber Loss ê³„ì‚°
    end
    
    LOSS->>C: Total Loss (8ê°œ ì‹œì  í‰ê· )
```

#### ğŸ” Unfold ì—°ì‚° ìƒì„¸ ë¶„ì„

**Unfold ì—°ì‚°ì´ 8ê°œ ì‹œì ì„ ìƒì„±í•˜ëŠ” ì´ìœ :**

```python
# 1ë‹¨ê³„: __getitem__ì—ì„œ 18í”„ë ˆì„ ë¡œë“œ
images = (18, C, H, W)  # Frame 0~17
actions = (18, 2)       # Frame 0~17

# 2ë‹¨ê³„: Collaterì—ì„œ ë°°ì¹˜ ìŠ¤íƒ ë° ë§ˆì§€ë§‰ í”„ë ˆì„ ì œê±°
action_tensors = actions[:, :-1]  # (B, 17, 2) - Frame 0~16
image_tensors = images             # (B, 18, C, H, W) - Frame 0~17

# 3ë‹¨ê³„: Unfold Sliding Window
# action_tensors.unfold(1, 10, 1)
#   ì…ë ¥: (B, 17, 2)
#   ì¶œë ¥: (B, 17-10+1, 10, 2) = (B, 8, 10, 2)
#   â†’ 8ê°œì˜ ì‹œì  ìƒì„± (t=0~7)

# image_tensors.unfold(1, 10, 1)
#   ì…ë ¥: (B, 18, C, H, W)
#   ì¶œë ¥: (B, 18-10+1, 10, C, H, W) = (B, 9, 10, C, H, W)
#   â†’ [:, 1:] ì ìš©: (B, 8, 10, C, H, W)
#   â†’ 8ê°œì˜ ì‹œì  ìƒì„± (t=0~7)
```

**ê° í”„ë ˆì„ë³„ ì—­í•  (Frame 0~17):**

| Frame | ì—­í•  | ì‚¬ìš©ë˜ëŠ” ì‹œì  | ì„¤ëª… |
|:---:|:---:|:---:|:---|
| **0** | ì…ë ¥ ì´ë¯¸ì§€ | t=0 | ì‹œì  0ì˜ ì²« ë²ˆì§¸ ì…ë ¥ ì´ë¯¸ì§€ |
| **1** | ì…ë ¥ ì´ë¯¸ì§€ | t=0, t=1 | ì‹œì  0ì˜ ë‘ ë²ˆì§¸, ì‹œì  1ì˜ ì²« ë²ˆì§¸ |
| **2** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~2 | ì‹œì  0~2ì—ì„œ ì‚¬ìš© |
| **3** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~3 | ì‹œì  0~3ì—ì„œ ì‚¬ìš© |
| **4** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~4 | ì‹œì  0~4ì—ì„œ ì‚¬ìš© |
| **5** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~5 | ì‹œì  0~5ì—ì„œ ì‚¬ìš© |
| **6** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~6 | ì‹œì  0~6ì—ì„œ ì‚¬ìš© |
| **7** | ì…ë ¥ ì´ë¯¸ì§€ | t=0~7 | ì‹œì  0~7ì—ì„œ ì‚¬ìš© |
| **8** | ì…ë ¥ ì´ë¯¸ì§€ | t=1~7 | ì‹œì  1~7ì—ì„œ ì‚¬ìš© (ì‹œì  0ì—ì„œëŠ” ë¯¸ì‚¬ìš©) |
| **9** | ì…ë ¥ ì´ë¯¸ì§€ | t=2~7 | ì‹œì  2~7ì—ì„œ ì‚¬ìš© |
| **10** | ì…ë ¥ ì´ë¯¸ì§€ | t=3~7 | ì‹œì  3~7ì—ì„œ ì‚¬ìš© |
| **11** | ì…ë ¥ ì´ë¯¸ì§€ | t=4~7 | ì‹œì  4~7ì—ì„œ ì‚¬ìš© |
| **12** | ì…ë ¥ ì´ë¯¸ì§€ | t=5~7 | ì‹œì  5~7ì—ì„œ ì‚¬ìš© |
| **13** | ì…ë ¥ ì´ë¯¸ì§€ | t=6~7 | ì‹œì  6~7ì—ì„œ ì‚¬ìš© |
| **14** | ì…ë ¥ ì´ë¯¸ì§€ | t=7 | ì‹œì  7ì—ì„œë§Œ ì‚¬ìš© |
| **15** | ì˜ˆì¸¡ ëŒ€ìƒ | t=7 | ì‹œì  7ì˜ ì˜ˆì¸¡ ì•¡ì…˜ (Frame 7~16 ì¤‘ í•˜ë‚˜) |
| **16** | ì˜ˆì¸¡ ëŒ€ìƒ | t=7 | ì‹œì  7ì˜ ì˜ˆì¸¡ ì•¡ì…˜ (Frame 7~16 ì¤‘ í•˜ë‚˜) |
| **17** | ë¯¸ì‚¬ìš© | - | Collaterì—ì„œ ì œê±°ë¨ (`[:, :-1]`) |

#### ğŸ“Š ê° í”„ë ˆì„ë³„ ë³€ìˆ˜ ì‚¬ìš© ìƒì„¸ (Frame 0~17)

| Frame | ì—­í•  | ì…ë ¥ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹œì  | ì˜ˆì¸¡ ì•¡ì…˜ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹œì  | ë³€ìˆ˜ëª… |
|:---:|:---:|:---:|:---:|:---|
| **0** | ì…ë ¥ + ì˜ˆì¸¡ | t=0 | t=0 | `image_tensors[:, 0]`, `action_chunk[0][0]` |
| **1** | ì…ë ¥ + ì˜ˆì¸¡ | t=0, 1 | t=0, 1 | `image_tensors[:, 1]`, `action_chunk[0][1]`, `action_chunk[1][0]` |
| **2** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~2 | t=0~2 | `image_tensors[:, 2]`, `action_chunk[0~2]` |
| **3** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~3 | t=0~3 | `image_tensors[:, 3]`, `action_chunk[0~3]` |
| **4** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~4 | t=0~4 | `image_tensors[:, 4]`, `action_chunk[0~4]` |
| **5** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~5 | t=0~5 | `image_tensors[:, 5]`, `action_chunk[0~5]` |
| **6** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~6 | t=0~6 | `image_tensors[:, 6]`, `action_chunk[0~6]` |
| **7** | ì…ë ¥ + ì˜ˆì¸¡ | t=0~7 | t=0~7 | `image_tensors[:, 7]`, `action_chunk[0~7]` |
| **8** | ì…ë ¥ + ì˜ˆì¸¡ | t=1~7 | t=0~7 | `image_tensors[:, 8]`, `action_chunk[0~7]` |
| **9** | ì…ë ¥ + ì˜ˆì¸¡ | t=2~7 | t=0~7 | `image_tensors[:, 9]`, `action_chunk[0~7]` |
| **10** | ì…ë ¥ + ì˜ˆì¸¡ | t=3~7 | t=1~7 | `image_tensors[:, 10]`, `action_chunk[1~7]` |
| **11** | ì…ë ¥ + ì˜ˆì¸¡ | t=4~7 | t=2~7 | `image_tensors[:, 11]`, `action_chunk[2~7]` |
| **12** | ì…ë ¥ + ì˜ˆì¸¡ | t=5~7 | t=3~7 | `image_tensors[:, 12]`, `action_chunk[3~7]` |
| **13** | ì…ë ¥ + ì˜ˆì¸¡ | t=6~7 | t=4~7 | `image_tensors[:, 13]`, `action_chunk[4~7]` |
| **14** | ì…ë ¥ + ì˜ˆì¸¡ | t=7 | t=5~7 | `image_tensors[:, 14]`, `action_chunk[5~7]` |
| **15** | ì˜ˆì¸¡ë§Œ | - | t=6~7 | `action_chunk[6~7]` |
| **16** | ì˜ˆì¸¡ë§Œ | - | t=7 | `action_chunk[7]` |
| **17** | ë¯¸ì‚¬ìš© | - | - | Collaterì—ì„œ ì œê±° (`[:, :-1]`) |

**í•µì‹¬ í¬ì¸íŠ¸:**
- **Frame 0~7**: ëª¨ë“  ì‹œì ì—ì„œ ì…ë ¥ ì´ë¯¸ì§€ë¡œ ì‚¬ìš© (ì¤‘ë³µ ì‚¬ìš© ìµœëŒ€)
- **Frame 8~14**: ì¼ë¶€ ì‹œì ì—ì„œë§Œ ì…ë ¥ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©
- **Frame 15~16**: ì˜ˆì¸¡ ì•¡ì…˜ìœ¼ë¡œë§Œ ì‚¬ìš© (ì…ë ¥ ì´ë¯¸ì§€ë¡œëŠ” ë¯¸ì‚¬ìš©)
- **Frame 17**: ì™„ì „íˆ ì œê±°ë¨ (ì•¡ì…˜ í…ì„œì—ì„œë§Œ ì œê±°, ì´ë¯¸ì§€ëŠ” ìœ ì§€)

#### ğŸ“ ê° ì‹œì ë³„ ì…ë ¥/ì¶œë ¥ í…ì„œ Shape ë° ë³€ìˆ˜ëª… (t=0~7)

| ì‹œì  t | ì…ë ¥ ì´ë¯¸ì§€ ë³€ìˆ˜<br/>(Window) | ì˜ˆì¸¡ ì•¡ì…˜ ë³€ìˆ˜<br/>(Chunk) | Ground Truth ë³€ìˆ˜<br/>(Chunk) | ì„¤ëª… |
|:---:|:---:|:---:|:---:|:---|
| **t=0** | `image_tensors[:, 0:8]`<br/>`(B, 8, C, H, W)`<br/>**Frames 0~7** | `action_chunk[0]`<br/>`(B, 1, 10, 2)`<br/>**Actions 0~9** | `action_chunk[0]`<br/>`(B, 1, 10, 2)`<br/>**Actions 0~9** | Frame 0~7 ì´ë¯¸ì§€ë¡œ<br/>Frame 0~9 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=1** | `image_tensors[:, 1:9]`<br/>`(B, 8, C, H, W)`<br/>**Frames 1~8** | `action_chunk[1]`<br/>`(B, 1, 10, 2)`<br/>**Actions 1~10** | `action_chunk[1]`<br/>`(B, 1, 10, 2)`<br/>**Actions 1~10** | Frame 1~8 ì´ë¯¸ì§€ë¡œ<br/>Frame 1~10 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=2** | `image_tensors[:, 2:10]`<br/>`(B, 8, C, H, W)`<br/>**Frames 2~9** | `action_chunk[2]`<br/>`(B, 1, 10, 2)`<br/>**Actions 2~11** | `action_chunk[2]`<br/>`(B, 1, 10, 2)`<br/>**Actions 2~11** | Frame 2~9 ì´ë¯¸ì§€ë¡œ<br/>Frame 2~11 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=3** | `image_tensors[:, 3:11]`<br/>`(B, 8, C, H, W)`<br/>**Frames 3~10** | `action_chunk[3]`<br/>`(B, 1, 10, 2)`<br/>**Actions 3~12** | `action_chunk[3]`<br/>`(B, 1, 10, 2)`<br/>**Actions 3~12** | Frame 3~10 ì´ë¯¸ì§€ë¡œ<br/>Frame 3~12 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=4** | `image_tensors[:, 4:12]`<br/>`(B, 8, C, H, W)`<br/>**Frames 4~11** | `action_chunk[4]`<br/>`(B, 1, 10, 2)`<br/>**Actions 4~13** | `action_chunk[4]`<br/>`(B, 1, 10, 2)`<br/>**Actions 4~13** | Frame 4~11 ì´ë¯¸ì§€ë¡œ<br/>Frame 4~13 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=5** | `image_tensors[:, 5:13]`<br/>`(B, 8, C, H, W)`<br/>**Frames 5~12** | `action_chunk[5]`<br/>`(B, 1, 10, 2)`<br/>**Actions 5~14** | `action_chunk[5]`<br/>`(B, 1, 10, 2)`<br/>**Actions 5~14** | Frame 5~12 ì´ë¯¸ì§€ë¡œ<br/>Frame 5~14 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=6** | `image_tensors[:, 6:14]`<br/>`(B, 8, C, H, W)`<br/>**Frames 6~13** | `action_chunk[6]`<br/>`(B, 1, 10, 2)`<br/>**Actions 6~15** | `action_chunk[6]`<br/>`(B, 1, 10, 2)`<br/>**Actions 6~15** | Frame 6~13 ì´ë¯¸ì§€ë¡œ<br/>Frame 6~15 ì•¡ì…˜ ì˜ˆì¸¡ |
| **t=7** | `image_tensors[:, 7:15]`<br/>`(B, 8, C, H, W)`<br/>**Frames 7~14** | `action_chunk[7]`<br/>`(B, 1, 10, 2)`<br/>**Actions 7~16** | `action_chunk[7]`<br/>`(B, 1, 10, 2)`<br/>**Actions 7~16** | Frame 7~14 ì´ë¯¸ì§€ë¡œ<br/>Frame 7~16 ì•¡ì…˜ ì˜ˆì¸¡ |

**í•µì‹¬ í¬ì¸íŠ¸:**
1. **18ê°œ ì´ë¯¸ì§€ë¡œ 8ê°œ ì‹œì  ìƒì„±**: âœ… ë§ìŒ. Unfold ì—°ì‚°ì„ í†µí•´ **8ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‹œì **ì—ì„œ ê°ê° 10í”„ë ˆì„ì”© ì˜ˆì¸¡í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ ê·¹ëŒ€í™”
2. **Sliding Window ë°©ì‹**: ê° ì‹œì  tì—ì„œ ê³¼ê±° 8í”„ë ˆì„ì„ ë³´ê³  ë¯¸ë˜ 10í”„ë ˆì„ì„ ì˜ˆì¸¡
3. **ë°°ì¹˜ ì²˜ë¦¬**: í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œì—ì„œ 8ê°œì˜ í•™ìŠµ ìƒ˜í”Œì„ ë™ì‹œì— ìƒì„±í•˜ì—¬ ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒ
4. **Frame 17 ì œê±°**: Collaterì—ì„œ `[:, :-1]` ì—°ì‚°ìœ¼ë¡œ ë§ˆì§€ë§‰ í”„ë ˆì„ ì œê±° (ì•¡ì…˜ë§Œ, ì´ë¯¸ì§€ëŠ” ìœ ì§€)

### ì„¸ë¶€ ë‹¨ê³„ë³„ êµ¬í˜„ (Implementation Details)

| ë‹¨ê³„ (Stage) | êµ¬ì„± ìš”ì†Œ (Component) | ìƒì„¸ ì„¤ëª… (Description) | êµ¬í˜„ ìƒíƒœ |
| :--- | :--- | :--- | :--- |
| **1. Input Processing** | `BaseRoboVLM` | â€¢ RGB ì´ë¯¸ì§€ (224x224) + í…ìŠ¤íŠ¸ ëª…ë ¹ì–´<br>â€¢ Vision Encoder + LLM (Frozen) | âœ… ì™„ë£Œ |
| **2. Token Injection** | `[LRN]` Token | â€¢ í…ìŠ¤íŠ¸ ì„ë² ë”© ëì— **Learnable Action Token** ì¶”ê°€<br>â€¢ In-place ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ìˆ˜ë™ ì£¼ì… ë°©ì‹ ì ìš© | âœ… ì™„ë£Œ |
| **3. Feature Extraction** | Hidden States | â€¢ VLMì˜ ì¶œë ¥ ì¤‘ `[LRN]` í† í° ìœ„ì¹˜ì˜ Hidden State ì¶”ì¶œ (`action_hs`)<br>â€¢ ì‹œë§¨í‹± ì •ë³´ê°€ ì§‘ì•½ëœ ë²¡í„° | âœ… ì™„ë£Œ |
| **4. Temporal Decoding** | `MobileVLALSTMDecoder` | â€¢ LSTM ê¸°ë°˜ ë””ì½”ë”ë¡œ ì‹œê³„ì—´ ì •ë³´ ì²˜ë¦¬<br>â€¢ `action_hs`ë¥¼ ì´ˆê¸° ìƒíƒœ ë˜ëŠ” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© | âœ… ì™„ë£Œ |
| **5. Action Output** | Action Head | â€¢ **ì¶œë ¥ ì°¨ì›**: `(Batch, Chunk_Size=10, Action_Dim=2)`<br>â€¢ **2D Velocity**: `linear_x` (ì „ì§„/í›„ì§„), `linear_y` (ì¢Œ/ìš°) | âœ… ì™„ë£Œ |

#### ğŸ”§ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ (Technical Details)

**1. Kosmos-2 Vision-Language Fusion**
- **Vision Encoder**: ViT (Vision Transformer) ê¸°ë°˜ ì´ë¯¸ì§€ ì¸ì½”ë”©
- **Image Tokens**: `latent_query_num = 64` (Kosmos-2 ê¸°ë³¸ê°’)
- **Text Embedding**: Kosmos-2 Text Modelì˜ Word Embedding
- **Multimodal Fusion**: Transformer Cross-Attentionì„ í†µí•œ Vision-Language ê²°í•©

**2. Action Token (`[LRN]`) ë©”ì»¤ë‹ˆì¦˜**
- **Learnable Parameter**: `nn.Parameter(torch.zeros(hidden_size))` - í•™ìŠµ ê°€ëŠ¥í•œ ì•¡ì…˜ í† í°
- **Injection Position**: í…ìŠ¤íŠ¸ ì„ë² ë”©ì˜ ë§ˆì§€ë§‰ ìœ„ì¹˜ (EOS í† í° ì§ì „)
- **Token Count**: `latent_num = 1` (ê¸°ë³¸ê°’, configì—ì„œ ì„¤ì • ê°€ëŠ¥)
- **Hidden State Extraction**: Transformer ì¶œë ¥ì—ì„œ `[LRN]` í† í° ìœ„ì¹˜ì˜ Hidden State ì¶”ì¶œ
  - Shape: `(B, seq_len, latent_num, hidden_size)` â†’ `(B, seq_len, 1, 2048)`
  - Kosmos-2ì˜ ê²½ìš°: `hidden_size = 2048`

**3. LSTM Decoder ì•„í‚¤í…ì²˜**
- **Input**: `action_hs` (B, seq_len, latent_num, hidden_size) â†’ Flatten â†’ (B, seq_len, latent_num * hidden_size)
- **LSTM Layers**: 4 layers, `hidden_size = 1024`
- **History Memory**: `window_size = 8` í”„ë ˆì„ì˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- **Output Head**: MLP (Tanh Activation) â†’ `(B, seq_len, fwd_pred_next_n * action_dim)`
- **Down Sampling**: `pooling` ë°©ì‹ (AdaptiveMaxPool1d) ë˜ëŠ” `none` (ì§ì ‘ ì‚¬ìš©)

**4. Sliding Window Unfold ì—°ì‚°**
```python
# Collaterì—ì„œ ìˆ˜í–‰ë˜ëŠ” unfold ì—°ì‚°
# 1ë‹¨ê³„: ë§ˆì§€ë§‰ í”„ë ˆì„ ì œê±°
action_tensors = action_tensors[:, :-1]  # (B, 18, 2) â†’ (B, 17, 2)

# 2ë‹¨ê³„: Unfold Sliding Window
action_chunck = action_tensors.unfold(1, self.fwd_pred_next_n, 1).permute(0, 1, 3, 2)
# Input: (B, 17, 2) - 18í”„ë ˆì„ì—ì„œ ë§ˆì§€ë§‰ 1ê°œ ì œê±°
# Unfold: (B, 17, 2) â†’ (B, 17-10+1, 10, 2) = (B, 8, 10, 2)
# Output: (B, 8, 10, 2) - 8ê°œ ì‹œì ì—ì„œ ê°ê° 10í”„ë ˆì„ ì˜ˆì¸¡

# ì´ë¯¸ì§€ì˜ ê²½ìš°
image_chunk = image_tensors.unfold(1, self.fwd_pred_next_n, 1).permute(0, 1, 5, 2, 3, 4)[:, 1:]
# Input: (B, 18, C, H, W)
# Unfold: (B, 18, C, H, W) â†’ (B, 18-10+1, 10, C, H, W) = (B, 9, 10, C, H, W)
# [:, 1:]: (B, 9, 10, C, H, W) â†’ (B, 8, 10, C, H, W)
# Output: (B, 8, 10, C, H, W) - 8ê°œ ì‹œì ì—ì„œ ê°ê° 10í”„ë ˆì„ ì´ë¯¸ì§€
```

**5. Loss ê³„ì‚° (Huber Loss)**
- **Loss Function**: `torch.nn.functional.huber_loss`
- **Input Shape**: 
  - Predicted: `(B, seq_len, chunk_size, action_dim)` = `(B, 8, 10, 2)`
  - Ground Truth: `(B, seq_len, chunk_size, action_dim)` = `(B, 8, 10, 2)`
- **Masking**: `attention_mask` ë˜ëŠ” `chunck_mask`ë¥¼ í†µí•œ ìœ íš¨ í”„ë ˆì„ë§Œ ê³„ì‚°
- **Reduction**: `mean` (ë°°ì¹˜ ë° ì‹œí€€ìŠ¤ í‰ê· )

---

## 2. ğŸ§ª ì‹¤í—˜ ì„¤ì • ë° í˜„í™© (Experiment Setup)

### ë°ì´í„°ì…‹ (Small Data Efficiency)
*   **ì†ŒìŠ¤**: Mobile VLA Dataset (Custom Collected)
*   **ì´ ìˆ˜ëŸ‰**: **237 ì—í”¼ì†Œë“œ** (1-Box Scenarios Only)
*   **ì‹œí€€ìŠ¤ êµ¬ì¡°**: Total 18 Frames
    *   **Window (ê³¼ê±°)**: 8 í”„ë ˆì„ (Context)
    *   **Prediction (ë¯¸ë˜)**: 10 í”„ë ˆì„ (Action Chunk)

### í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„° (Hyperparameters)
*   **Method**: LoRA (Low-Rank Adaptation) Fine-tuning
*   **Rank (r)**: 32
*   **Epochs**: 10
*   **Loss Function**: Huber Loss (Robust Regression)
*   **Optimizer**: AdamW (lr=1e-4)

### í•™ìŠµ ê²°ê³¼ (Baseline Performance)
*   **ìµœì¢… ì„±ëŠ¥**: Val Loss **0.335** (MSE ê¸°ì¤€)
*   **Best Model**: Epoch 5 (Val Loss **0.280**)
*   **í•™ìŠµ ì†Œìš” ì‹œê°„**: ì•½ 2ì‹œê°„ 40ë¶„ (237ê°œ ë°ì´í„° ê¸°ì¤€)

---

## 3. ğŸ“ ë…¼ë¬¸ ì‘ì„± í¬ì¸íŠ¸ (Paper Key Points)

### Contribution 1: Data Efficiency
> "Large-scale VLMì˜ ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬, ë¶ˆê³¼ 200ì—¬ ê°œì˜ ì ì€ ì£¼í–‰ ë°ì´í„°ë§Œìœ¼ë¡œë„ ë³µì¡í•œ Vision-Language Navigation íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì„."

### Contribution 2: Architecture Adaptation
> "ê¸°ì¡´ Manipulation(7D) ì¤‘ì‹¬ì˜ VLA êµ¬ì¡°ë¥¼ Navigation(2D)ì— ìµœì í™”ëœ í˜•íƒœë¡œ ê²½ëŸ‰í™”. íŠ¹íˆ `[LRN]` í† í°ì„ í†µí•œ VLM-to-Action ì •ë³´ ì¦ë¥˜(Distillation) ë©”ì»¤ë‹ˆì¦˜ì´ ìœ íš¨í•¨ì„ ì…ì¦."

### Contribution 3: Robust Pipeline
> "ROS2 ë°ì´í„° ìˆ˜ì§‘ë¶€í„° LoRA í•™ìŠµ, Inferenceì— ì´ë¥´ëŠ” End-to-End íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê²€ì¦."

---

## 4. ğŸ” RoboVLMs í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### ğŸ“ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ êµ¬ë¶„

**ì¤‘ìš”**: ì´ í”„ë¡œì íŠ¸ëŠ” ë‘ ê°œì˜ RoboVLMs ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

1. **`RoboVLMs_upstream/`** - ì›ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Upstream)
   - ê²½ë¡œ: `/home/billy/25-1kp/vla/RoboVLMs_upstream/`
   - ì—­í• : ì›ë³¸ RoboVLMs í”„ë ˆì„ì›Œí¬ (GitHubì—ì„œ ê°€ì ¸ì˜¨ ë²„ì „)
   - ìš©ë„: Mobile VLA ì „ìš© ìˆ˜ì • íŒŒì¼ë“¤ì´ í¬í•¨ëœ ë²„ì „

2. **`RoboVLMs/`** - ì‚¬ìš©ì í¸ì§‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Modified)
   - ê²½ë¡œ: `/home/billy/25-1kp/vla/RoboVLMs/`
   - ì—­í• : ì‚¬ìš©ìê°€ ì§ì ‘ í¸ì§‘í•œ ë²„ì „
   - ìš©ë„: í”„ë¡œì íŠ¸ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§•ëœ ë²„ì „

### GitHub ê¸°ë°˜ ê¸°ìˆ  ìŠ¤íƒ (RoboVLMs Upstream)

**ì°¸ê³  ë ˆí¬ì§€í† ë¦¬**: [Robot-VLAs/RoboVLMs](https://github.com/Robot-VLAs/RoboVLMs)

#### í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë° Citation ìœ„ì¹˜

1. **BaseRoboVLM (`robovlms/model/backbone/base_backbone.py`)**
   - **ê²½ë¡œ**: 
     - ì›ë³¸: `RoboVLMs_upstream/robovlms/model/backbone/base_backbone.py`
     - ìˆ˜ì •: `RoboVLMs/robovlms/model/backbone/base_backbone.py`
   - **Citation ìœ„ì¹˜**: ì§ì ‘ì ì¸ citation ì—†ìŒ (RoboVLMs í”„ë ˆì„ì›Œí¬ì˜ í•µì‹¬ ë°±ë³¸)
   - **ê¸°ë°˜**: RoboVLMs í”„ë ˆì„ì›Œí¬ì˜ ê¸°ë³¸ ì•„í‚¤í…ì²˜
   - **ì—­í• **: Vision-Language-Action í†µí•© ë°±ë³¸
   - **ê¸°ëŠ¥**:
     - Kosmos-2 Vision Encoder í†µí•©
     - Multimodal Embedding ìƒì„± (Vision + Text)
     - Action Token (`[LRN]`) ì£¼ì… ë° Hidden State ì¶”ì¶œ
     - Forward/Inference ëª¨ë“œ ì§€ì›
   - **ì£¼ìš” ë©”ì„œë“œ**:
     - `forward_continuous()`: ì—°ì† ì•¡ì…˜ ê³µê°„ ì˜ˆì¸¡
     - `merge_multi_modal_input()`: Vision-Language ê²°í•©
     - `_forward_action_head()`: Action Head í˜¸ì¶œ

2. **MobileVLALSTMDecoder (`robovlms/model/policy_head/mobile_vla_policy.py`)**
   - **ê²½ë¡œ**: 
     - **ì›ë³¸ë§Œ ì¡´ì¬**: `RoboVLMs_upstream/robovlms/model/policy_head/mobile_vla_policy.py`
     - ìˆ˜ì • ë²„ì „: ì—†ìŒ (ì›ë³¸ë§Œ ì‚¬ìš©)
   - **Citation ìœ„ì¹˜**: ì§ì ‘ì ì¸ citation ì—†ìŒ (Mobile VLA ì „ìš© êµ¬í˜„)
   - **ê¸°ë°˜**: `robovlms/model/policy_head/base_policy.py`ì˜ `LSTMDecoder` í´ë˜ìŠ¤
   - **ì—­í• **: 2D Navigation ì „ìš© LSTM ë””ì½”ë”
   - **íŠ¹ì§•**:
     - Base LSTMDecoderë¥¼ ìƒì†ë°›ì•„ 2D ì†ë„ë§Œ ì¶œë ¥
     - Gripper ì•¡ì…˜ ì œê±° (Manipulation â†’ Navigation ë³€í™˜)
     - History Memory ê´€ë¦¬ (`window_size = 8`)
   - **ì•„í‚¤í…ì²˜**:
     ```python
     # LSTM êµ¬ì¡°
     self.rnn = lstm_decoder(
         in_features * latent,  # 1 * 2048 = 2048
         hidden_size * latent,  # 1024 * 1 = 1024
         num_layers=4,
         policy_rnn_dropout_p=0.0
     )
     # Action Head
     self.velocities = MLPTanhHead(
         hidden_size * latent,  # 1024
         fwd_pred_next_n * action_dim  # 10 * 2 = 20
     )
     ```

3. **MobileVLAH5Dataset (`robovlms/data/mobile_vla_h5_dataset.py`)**
   - **ê²½ë¡œ**: 
     - **ì›ë³¸ë§Œ ì¡´ì¬**: `RoboVLMs_upstream/robovlms/data/mobile_vla_h5_dataset.py`
     - ìˆ˜ì • ë²„ì „: ì—†ìŒ (ì›ë³¸ë§Œ ì‚¬ìš©)
   - **Citation ìœ„ì¹˜**: **1ë²ˆì§¸ ì¤„**
   - **Citation ë‚´ìš©**: 
     ```python
     # GitHub Citation: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/data/calvin_dataset.py
     # Mobile VLAìš© HDF5 ë°ì´í„°ì…‹ ë¡œë” (RoboVLMs CALVIN ë°ì´í„°ì…‹ êµ¬ì¡° ì°¸ê³ )
     ```
   - **ì—­í• **: HDF5 í˜•ì‹ Mobile VLA ë°ì´í„° ë¡œë”
   - **ê¸°ë°˜**: `robovlms/data/calvin_dataset.py`ì˜ `DiskCalvinDataset` êµ¬ì¡° ì°¸ê³ 
   - **êµ¬ì¡°**:
     - `__getitem__()`: 18í”„ë ˆì„ ì´ë¯¸ì§€ + ì•¡ì…˜ ë¡œë“œ
     - `collater()`: Unfold Sliding Window ìƒì„±
   - **ë°ì´í„° í˜•ì‹**:
     - Images: `(18, 720, 1280, 3)` â†’ Resize to `(18, 224, 224, 3)`
     - Actions: `(18, 3)` â†’ Slice to `(18, 2)` (linear_x, linear_y)

4. **MobileVLATrainer (`robovlms/train/mobile_vla_trainer.py`)**
   - **ê²½ë¡œ**: 
     - **ì›ë³¸ë§Œ ì¡´ì¬**: `RoboVLMs_upstream/robovlms/train/mobile_vla_trainer.py`
     - ìˆ˜ì • ë²„ì „: ì—†ìŒ (ì›ë³¸ë§Œ ì‚¬ìš©)
   - **Citation ìœ„ì¹˜**: ì§ì ‘ì ì¸ citation ì—†ìŒ (Mobile VLA ì „ìš© êµ¬í˜„)
   - **ê¸°ë°˜**: `robovlms/train/base_trainer.py`ì˜ `BaseTrainer` í´ë˜ìŠ¤
   - **ì—­í• **: 2D Navigation ì „ìš© Trainer
   - **ê¸°ëŠ¥**:
     - BaseTrainer ìƒì†, `_process_batch()` ì˜¤ë²„ë¼ì´ë“œ
     - 7D ì•¡ì…˜ â†’ 2D ì†ë„ ë³€í™˜ ë¡œì§
     - Gripper ê´€ë ¨ ì²˜ë¦¬ ì œê±°

5. **LoRA Utils (`robovlms/utils/lora_utils.py`)**
   - **ê²½ë¡œ**: 
     - ì›ë³¸: `RoboVLMs_upstream/robovlms/utils/lora_utils.py`
     - ìˆ˜ì •: `RoboVLMs/robovlms/utils/lora_utils.py` (ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í•„ìš”)
   - **Citation ìœ„ì¹˜**: **1ë²ˆì§¸ ì¤„**
   - **Citation ë‚´ìš©**:
     ```python
     # GitHub Citation: https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py
     # ì´ í•¨ìˆ˜ëŠ” LLaVA í”„ë¡œì íŠ¸ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒìœ¼ë¡œ, LoRAë¥¼ ì ìš©í•  Linear ë ˆì´ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
     ```
   - **ì—­í• **: LoRA ì ìš©ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
   - **ê¸°ë°˜**: LLaVA í”„ë¡œì íŠ¸ì˜ LoRA êµ¬í˜„ ì°¸ê³ 

6. **MobileVLAActionDataset (`robovlms/data/mobile_vla_action_dataset.py`)**
   - **ê²½ë¡œ**: 
     - **ìˆ˜ì • ë²„ì „ë§Œ ì¡´ì¬**: `RoboVLMs/robovlms/data/mobile_vla_action_dataset.py`
     - ì›ë³¸: ì—†ìŒ (ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ë²„ì „)
   - **ì—­í• **: Mobile VLA ì•¡ì…˜ ì˜ˆì¸¡ ë°ì´í„°ì…‹ (ì‚¬ìš©ì í¸ì§‘ ë²„ì „)
   - **íŠ¹ì§•**: `mobile_vla_data_collector.py` í˜•íƒœ ë°ì´í„° ì²˜ë¦¬

#### í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìƒì„¸ íë¦„

```mermaid
graph TB
    subgraph "Data Loading"
        D1[HDF5 Files<br/>237 Episodes] --> D2[MobileVLAH5Dataset]
        D2 --> D3["__getitem__(idx)<br/>18 Frames Load"]
    end
    
    subgraph "Batch Collation"
        D3 --> C1[Collater]
        C1 --> C2["Unfold Sliding Window<br/>18 â†’ 16 samples"]
        C2 --> C3["Image: (B, 8, C, H, W)<br/>Action Chunk: (B, 8, 10, 2)"]
    end
    
    subgraph "Forward Pass"
        C3 --> F1[BaseRoboVLM.forward_continuous]
        F1 --> F2["Kosmos-2 Vision Encoder<br/>8 Images â†’ Vision Features"]
        F2 --> F3["Text Embedding + [LRN] Token<br/>Multimodal Embedding"]
        F3 --> F4["Kosmos-2 Transformer<br/>Hidden States"]
        F4 --> F5["Extract [LRN] Hidden State<br/>action_hs: (B, 8, 1, 2048)"]
        F5 --> F6[MobileVLALSTMDecoder]
        F6 --> F7["LSTM + MLP Head<br/>Predicted Actions: (B, 8, 10, 2)"]
    end
    
    subgraph "Loss Calculation"
        F7 --> L1[Huber Loss]
        C3 --> L1
        L1 --> L2["Total Loss<br/>8 timesteps averaged"]
    end
    
    style F1 fill:#e1f5ff
    style F6 fill:#fff4e1
    style L1 fill:#ffe1e1
```

#### LoRA Fine-tuning ì„¤ì •

- **Target Modules**: Kosmos-2 Transformerì˜ Attention Layers
- **Rank (r)**: 32
- **Alpha**: 32 (LoRA scaling factor)
- **Trainable Parameters**: Action Token + LSTM Decoder + LoRA weights
- **Frozen Parameters**: Kosmos-2 Vision Encoder, Text Embedding (ëŒ€ë¶€ë¶„)

#### ğŸ“š ì „ì²´ Citation ìš”ì•½ ë° íŒŒì¼ ê²½ë¡œ

| íŒŒì¼ëª… | ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ | Citation ìœ„ì¹˜ | Citation ë‚´ìš© | ê¸°ë°˜ ë ˆí¬ì§€í† ë¦¬ |
|:---|:---|:---:|:---|:---|
| **MobileVLAH5Dataset**<br/>`robovlms/data/mobile_vla_h5_dataset.py` | **ì›ë³¸ë§Œ**:<br/>`RoboVLMs_upstream/robovlms/data/mobile_vla_h5_dataset.py` | **1ë²ˆì§¸ ì¤„** | `# GitHub Citation: https://github.com/Robot-VLAs/RoboVLMs/blob/main/robovlms/data/calvin_dataset.py` | [Robot-VLAs/RoboVLMs](https://github.com/Robot-VLAs/RoboVLMs) |
| **MobileVLALSTMDecoder**<br/>`robovlms/model/policy_head/mobile_vla_policy.py` | **ì›ë³¸ë§Œ**:<br/>`RoboVLMs_upstream/robovlms/model/policy_head/mobile_vla_policy.py` | - | ì§ì ‘ì ì¸ citation ì—†ìŒ | `robovlms/model/policy_head/base_policy.py` ê¸°ë°˜ |
| **MobileVLATrainer**<br/>`robovlms/train/mobile_vla_trainer.py` | **ì›ë³¸ë§Œ**:<br/>`RoboVLMs_upstream/robovlms/train/mobile_vla_trainer.py` | - | ì§ì ‘ì ì¸ citation ì—†ìŒ | `robovlms/train/base_trainer.py` ê¸°ë°˜ |
| **MobileVLAActionDataset**<br/>`robovlms/data/mobile_vla_action_dataset.py` | **ìˆ˜ì •ë§Œ**:<br/>`RoboVLMs/robovlms/data/mobile_vla_action_dataset.py` | - | ì§ì ‘ì ì¸ citation ì—†ìŒ | ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§• ë²„ì „ |
| **LoRA Utils**<br/>`robovlms/utils/lora_utils.py` | **ì›ë³¸**:<br/>`RoboVLMs_upstream/robovlms/utils/lora_utils.py`<br/>**ìˆ˜ì •**:<br/>`RoboVLMs/robovlms/utils/lora_utils.py` | **1ë²ˆì§¸ ì¤„** | `# GitHub Citation: https://github.com/haotian-liu/LLaVA/blob/main/llava/train/train.py` | [haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA) |
| **Video-LLaVA Dataset**<br/>`robovlms/data/vid_llava_dataset.py` | **ì›ë³¸**:<br/>`RoboVLMs_upstream/robovlms/data/vid_llava_dataset.py`<br/>**ìˆ˜ì •**:<br/>`RoboVLMs/robovlms/data/vid_llava_dataset.py` | **1ë²ˆì§¸ ì¤„** | `# Adopted from https://github.com/lm-sys/FastChat` | [lm-sys/FastChat](https://github.com/lm-sys/FastChat) |
| **Vision Resampler**<br/>`robovlms/model/vision_encoder/vision_resampler.py` | **ì›ë³¸**:<br/>`RoboVLMs_upstream/robovlms/model/vision_encoder/vision_resampler.py`<br/>**ìˆ˜ì •**:<br/>`RoboVLMs/robovlms/model/vision_encoder/vision_resampler.py` | **2ë²ˆì§¸ ì¤„** | `Based on: https://github.com/lucidrains/flamingo-pytorch` | [lucidrains/flamingo-pytorch](https://github.com/lucidrains/flamingo-pytorch) |
| **BaseRoboVLM**<br/>`robovlms/model/backbone/base_backbone.py` | **ì›ë³¸**:<br/>`RoboVLMs_upstream/robovlms/model/backbone/base_backbone.py`<br/>**ìˆ˜ì •**:<br/>`RoboVLMs/robovlms/model/backbone/base_backbone.py` | - | ì§ì ‘ì ì¸ citation ì—†ìŒ | RoboVLMs í”„ë ˆì„ì›Œí¬ í•µì‹¬ ë°±ë³¸ |

**ê²½ë¡œ êµ¬ë¶„ ì„¤ëª…:**
- **ì›ë³¸ë§Œ**: `RoboVLMs_upstream/`ì—ë§Œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ (ì›ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- **ìˆ˜ì •ë§Œ**: `RoboVLMs/`ì—ë§Œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ (ì‚¬ìš©ì ì»¤ìŠ¤í„°ë§ˆì´ì§•)
- **ì›ë³¸/ìˆ˜ì •**: ë‘ ê²½ë¡œ ëª¨ë‘ì— ì¡´ì¬í•˜ëŠ” íŒŒì¼ (ìˆ˜ì • ë²„ì „ì´ ìˆì„ ìˆ˜ ìˆìŒ)

---

## 5. ğŸš€ í–¥í›„ ê³„íš (Future Works for Paper)
1.  **ë°ì´í„° í™•ì¥**: 2-Box ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€ë¥¼ í†µí•œ ë³µì¡í•œ ì¥ì• ë¬¼ íšŒí”¼ ì„±ëŠ¥ ê²€ì¦.
2.  **ë¹„êµ ì‹¤í—˜**: CNN ê¸°ë°˜ Baseline ëª¨ë¸(e.g., ResNet+LSTM)ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ.
3.  **Ablation Study**: LoRA Rank ë³€í™”, Window Size ë³€í™”ì— ë”°ë¥¸ ì„±ëŠ¥ ì¶”ì´ ë¶„ì„.
4.  **Inference íŒŒì´í”„ë¼ì¸**: í•™ìŠµëœ ëª¨ë¸ì„ ì‹¤ì œ ë¡œë´‡ì— ë°°í¬í•˜ì—¬ ì‹¤ì‹œê°„ ì£¼í–‰ ì„±ëŠ¥ ê²€ì¦.

